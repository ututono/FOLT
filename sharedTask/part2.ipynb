{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis\n",
    "## 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 2,

   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZhuoY\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_21348/226763551.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mWord2Vec\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtranslate\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTranslator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'wordnet'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'translate'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import gensim.models\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from translate import Translator\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stoplist = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "testmsgs=[\n",
    "    'Where are these salafi innovators shouting shirk? https://t.co/3gGv14ILc0,2',\n",
    "    'Paris warns radicals are trying to exploit Yellow Vests &amp; overthrow the... https://t.co/3Y4Whnsctw,2',\n",
    "    '\"Hello \\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n #selfie #blackgirlmagic #loveyourself #london #shoreditch #eastlondon #melanin #melaninpoppin #muslimah #influencer #photography #artist #art #beautiful #designerâ€¦ https://t.co/swIqxdWvyF\",2',\n",
    "    '\"Plakataktion zur Thematik â€žMuslime gegen Rasissmusâ€œ von 22.10.2021 - 01.11.2021 ',\n",
    "    '@KbfMajor @Nigel_Farage ðŸ‘‡watch my galleryðŸ‘‡ Thatâ€™s the problem with being thick as two short planks. Whether itâ€™s an Islamic Fundamentalist :) in the streets or Nigel FÃ¼hrage on Twitter or GBeebies; itâ€™s the same thing. Both are equally as abhorrent.,2'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_url(raw_text):\n",
    "    restr=r\"(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b\"\n",
    "    text_noURL=re.sub(restr,\" \", raw_text,flags=re.MULTILINE)\n",
    "    return text_noURL\n",
    "\n",
    "def deEmojize(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def translate(text,from_lang='en'):\n",
    "    def get_lang(tag):\n",
    "        if tag.startswith('de'):\n",
    "            return 'german'\n",
    "        else:\n",
    "            return 'english'\n",
    "    translator= Translator(from_lang=get_lang(from_lang),to_lang='english')\n",
    "    return translator.translate(text)\n",
    "\n",
    " # Visual inspection of the above function working on testmsg\n",
    "# for item in testmsgs:\n",
    "#     print(translate(item))\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 18,

   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    '''\n",
    "    Remove punctuation ; lowercase words and remove words which consists of less than 2 alphabets\n",
    "    :param text: a sentence\n",
    "    :return: word list of input sentence\n",
    "    '''\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    raw_words=tknzr.tokenize(text)\n",
    "    tokenized_words=[word.lower() for word in raw_words if len(word)>2 and re.search(r'[a-zA-Z]+',word) ]\n",
    "    return tokenized_words\n",
    "\n",
    " # Visual inspection of the above function working on testmsg\n",
    "# for item in testmsgs:\n",
    "#     result= extract_words(item)\n",
    "#     display(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 17,

   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_pos_word(words):\n",
    "    '''\n",
    "\n",
    "    :param words:\n",
    "    :return:\n",
    "    '''\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # label POS for words\n",
    "    words = pos_tag(words)\n",
    "\n",
    "    # lemmatize words\n",
    "    pos_word = [wnl.lemmatize(tag[0], pos=get_wordnet_pos(tag[1]) or wordnet.NOUN) for tag in words]\n",
    "\n",
    "    # remove stopwords\n",
    "    cleanwords = [word for word in pos_word if word not in stoplist]\n",
    "\n",
    "    return cleanwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_nonsense(cleanwords:list):\n",
    "    sentslen=0\n",
    "    for word in cleanwords:\n",
    "        if re.search(r'\\B(\\#[a-zA-Z]+\\b)(?!;)',word):\n",
    "            continue\n",
    "        sentslen+=1\n",
    "    if sentslen<4:\n",
    "        cleanwords=list('#######' for i in range(len(cleanwords)))\n",
    "    return cleanwords\n",
    "            \n",
    "\n",
    "def tokenize_words(sentence:str):\n",
    "\n",
    "    # remove url\n",
    "    noURLsent=remove_url(sentence)\n",
    "\n",
    "    # translate emoji to word\n",
    "    deEmojisent=deEmojize(noURLsent)\n",
    "\n",
    "    # lowercase words and remove words which consists of less than 2 alphabets\n",
    "    words=extract_words(deEmojisent)\n",
    "\n",
    "    # label POS for words for stemming them and removing stopwords\n",
    "    cleanwords=get_pos_word(words)\n",
    "    \n",
    "    vaildwords=remove_nonsense(cleanwords)\n",
    "\n",
    "\n",
    "    return ' '.join(vaildwords)\n",
    "\n",
    "def translate_tweets(raw_data:pd.DataFrame):\n",
    "    translated_tweets=[]\n",
    "    langs=raw_data['lang'].to_numpy()\n",
    "    for index,lang in enumerate(langs):\n",
    "\n",
    "        tweet=raw_data.loc[index]['content']\n",
    "        \n",
    "        if lang !='en':\n",
    "            tweet=remove_url(tweet)\n",
    "            words=extract_words(tweet)\n",
    "            tweet=' '.join(words)\n",
    "            translated_tweets.append(translate(tweet,lang))\n",
    "#             translated_tweets.append(' ')\n",
    "\n",
    "        else:\n",
    "            translated_tweets.append(tweet)\n",
    "\n",
    "    return translated_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TA_TRAIN=500\n",
    "TA_TEST=0\n",
    "def preprocessing(train_path:str,trial_path:str,storepath=None,translate_tag=False):\n",
    "\n",
    "    # Loading raw data from the file\n",
    "    raw_data_train=pd.read_csv(train_path)\n",
    "    raw_data_trial=pd.read_csv(trial_path)\n",
    "\n",
    "\n",
    "    global TA_TRAIN\n",
    "    TA_TRAIN=raw_data_train.shape[0]\n",
    "    global TA_TEST\n",
    "    TA_TEST=raw_data_trial.shape[0]\n",
    "\n",
    "    \n",
    "    raw_data=pd.concat([raw_data_train,raw_data_trial])\n",
    "    raw_data=raw_data.reset_index(drop=True)\n",
    "    # Extract engish words from sentence\n",
    "\n",
    "    if(translate_tag==True):\n",
    "        translated_tweets=translate_tweets(raw_data)\n",
    "        raw_data['translated_tweets']=translated_tweets        \n",
    "\n",
    "    raw_data['tokenized_content']=raw_data['translated_tweets'].apply(tokenize_words)\n",
    "    \n",
    "    if(storepath!=None):\n",
    "        raw_data.to_csv(storepath)\n",
    "\n",
    "\n",
    "#     raw_data['tokenized_content']=raw_data['content'].apply(tokenize_words)\n",
    "\n",
    "\n",
    "    # Remove empyte row data\n",
    "#     raw_data = raw_data[~(raw_data['tokenized_content'].str.len() == 0)]\n",
    "    \n",

    "    return raw_data\n"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_tweet(data:list,model_w2v:Word2Vec,vector_size=200):\n",
    "\n",
    "    def word_vector(token:list,model_w2v:Word2Vec,verctor_size:int):\n",
    "        vec = np.zeros(verctor_size).reshape((1, vector_size))\n",
    "        count = 0\n",
    "        for word in token:\n",
    "            try:\n",
    "                vec += model_w2v.wv.get_vector(word).reshape((1, vector_size))\n",
    "                count += 1.\n",
    "            except KeyError:  # handling the case where the token is not in vocabulary\n",
    "                continue\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "\n",
    "    wordvec_arrays = np.zeros((len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        wordvec_arrays[i,:] = word_vector(data[i], verctor_size=vector_size, model_w2v=model_w2v)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "    return wordvec_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def word2ved_model_train(data:list,vector_size=200):\n",
    "    '''\n",
    "    :param data: [[tokens_sent_1],[tokens_sent_2]...]\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    model_w2v=gensim.models.Word2Vec(\n",
    "        data,\n",
    "        vector_size=vector_size,\n",
    "        window=5,\n",
    "        min_count=2\n",
    "    )\n",
    "\n",
    "    model_w2v.train(data,total_examples=len(data),epochs=20)\n",
    "    model_path='./src/model/model_'+str(len(model_w2v.wv.index_to_key))+'_words_'+str(len(data))+'tws.model'\n",
    "    model_w2v.save(model_path)\n",
    "\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 62,

   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        34\n",
      "         1.0       1.00      1.00      1.00        20\n",
      "         2.0       0.91      1.00      0.95       346\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.64      0.67      0.65       400\n",
      "weighted avg       0.84      0.92      0.87       400\n",
      "\n",
      "zero 0\n",
      "one 37\n",
      "two 963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],

   "source": [
    "TRAINSIZE=0.8\n",
    "DEVSIZE=0.2\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "def splitDataset(data:pd.DataFrame):\n",
    "    '''\n",
    "\n",
    "    :param data:\n",
    "    :return: 5 lists\n",
    "    '''\n",
    "    data=raw_data\n",
    "\n",
    "    train_df=data[:TA_TRAIN]\n",
    "    trial_df=data[TA_TRAIN:]\n",
    "\n",
    "    x_train=train_df['tokenized_content'].loc[:int(len(train_df)*TRAINSIZE)]\n",
    "    x_vaild=train_df['tokenized_content'].loc[int(len(train_df)*TRAINSIZE):]\n",
    "\n",
    "    x_test=trial_df['tokenized_content']\n",
    "\n",
    "    y_train=train_df['label'].loc[:int(len(train_df)*TRAINSIZE)].to_numpy().tolist()\n",
    "    y_vaild=train_df['label'].loc[int(len(train_df)*DEVSIZE):].to_numpy().tolist()\n",
    "\n",
    "\n",
    "    return x_train, x_vaild, x_test, y_train, y_vaild\n",
    "\n",
    "def finetune(xtrain_w2v_np,ytrain_np,xvalid_w2v_np,yvaild_np):\n",
    "    '''\n",
    "    Using GridSearchCV to find best hyperparamters for svm\n",
    "    :param xtrain_w2v_np:\n",
    "    :param ytrain_np:\n",
    "    :param xvalid_w2v_np:\n",
    "    :param yvaild_np:\n",
    "    :return:\n",
    "    '''\n",
    "    param_grid = {'C': [0.0125,0.01,0.05,0.08,0.1],\n",
    "              'gamma': [0.1,0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf','linear','poly']}\n",
    "    grid = GridSearchCV(SVC(degree=3,decision_function_shape='ovr'), param_grid, refit = True, verbose = 3)\n",
    "\n",
    "    # fitting the model for grid search\n",
    "    grid.fit(xtrain_w2v_np, ytrain_np)\n",
    "\n",
    "    # print best parameter after tuning\n",
    "    print('Best paragrams: ',grid.best_params_)\n",
    "\n",
    "    # print how our model looks after hyper-parameter tuning\n",
    "    print(grid.best_estimator_)\n",
    "\n",
    "    grid_predictions = grid.predict(xvalid_w2v_np)\n",
    "\n",
    "    # print classification report\n",
    "    print(classification_report(yvaild_np, grid_predictions))\n",
    "\n",
    "def word2vec_svm(data:pd.DataFrame,vector_size=200,finetuned=False):\n",
    "    '''\n",
    "    Train a word2vec model, then train svm classifier based on the word embedding model\n",
    "    :param data:\n",
    "    :param vector_size:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # split dataset into trainset, validset and testset\n",
    "    x_train, x_vaild, x_test, y_train, y_vaild=splitDataset(data)\n",
    "\n",
    "    # concentrate tweets from all sets to train word2vec model\n",
    "    combined=pd.concat([x_train,x_vaild,x_test])\n",
    "    combined=list(combined.to_numpy())\n",
    "\n",
    "    # train a word2vec model and return its stored path\n",
    "    model_path=word2ved_model_train(combined,vector_size=vector_size)\n",
    "\n",
    "    model_w2v=Word2Vec.load(model_path)\n",
    "\n",
    "    # vectorize sentences\n",
    "    wordvec_df=vectorize_tweet(combined,model_w2v,vector_size=vector_size)\n",
    "\n",
    "    # split sentences vector matrx into trainset, validset and testset\n",
    "    train_w2v = wordvec_df.iloc[:x_train.shape[0]+x_vaild.shape[0],:]\n",
    "    test_w2v = wordvec_df.iloc[x_train.shape[0]+x_vaild.shape[0]:,:]\n",
    "    xtrain_w2v = train_w2v.iloc[y_train,:]\n",
    "    xvalid_w2v = train_w2v.iloc[y_vaild,:]\n",
    "\n",
    "    # convert dataformat from dataframe to numpy\n",
    "    xtrain_w2v_np=xtrain_w2v.to_numpy()\n",
    "    xvalid_w2v_np=xvalid_w2v.to_numpy()\n",
    "    ytrain_np=np.array(y_train)\n",
    "    yvaild_np=np.array(y_vaild)\n",
    "    test_w2v_np=test_w2v.to_numpy()\n",
    "\n",
    "    # Using GridSearchCV to finetune model\n",
    "    if finetuned:\n",
    "        finetune(xtrain_w2v_np,ytrain_np,xvalid_w2v_np,yvaild_np)\n",
    "\n",
    "    # Using one versus rest decision_function provided by sklearn\n",
    "    svc=OneVsRestClassifier(SVC(kernel='linear',degree=3, C=0.07,gamma=0.1,decision_function_shape='ovr',random_state=20)).fit(xtrain_w2v_np, ytrain_np)\n",
    "\n",
    "    # Predict valid dataset by trained classifier svc\n",
    "    y_pred_valid=svc.predict(xvalid_w2v_np)\n",
    "    y_pred_valid=y_pred_valid.astype(int)\n",
    "\n",
    "    print(classification_report(yvaild_np, y_pred_valid))\n",
    "\n",
    "    # Predict test dataset by trained classifier svc\n",
    "    pred_test=svc.predict(test_w2v_np)\n",
    "    pred_test=pred_test.astype(int)\n",
    "\n",
    "    countclasses(pred_test)\n",
    "\n",
    "    return pred_test\n",
    "\n",
    "\n",
    "sd=word2vec_svm(raw_data,vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def savepredictions(filename:str,prediction:list):\n",
    "    # write to file:\n",
    "    file=open(filename,'w',encoding='utf-8')\n",
    "    try:\n",
    "        for label in prediction:\n",
    "            file.write(str(label)+'\\n')\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "def printWrongResults(expected:list, actuall:list,expected_df:np.array):\n",
    "    train_path='./src/dataset/train.csv'\n",
    "    trial_path='./src/dataset/trial.csv'\n",
    "    train_dataset=pd.read_csv(train_path,index_col=None)\n",
    "    trial_dataset=pd.read_csv(trial_path,index_col=None)\n",
    "    w_tweets=[]\n",
    "    index_tweets=[]\n",
    "    for index in range(len(expected)):\n",
    "        if expected[index]!=actuall[index]:\n",
    "            print(train_dataset['content'].loc[index]+'Was '+str(actuall[index])+' Should be '+str(expected[index]))\n",
    "            print('----------------------------\\n')\n",
    "\n",
    "def countclasses(data):\n",
    "    nrofzeros=0;\n",
    "    nrofones=0;\n",
    "    nroftwos=0;\n",
    "    for j in range(len(data)):\n",
    "        if data[j]==0:\n",
    "          nrofzeros+=1;\n",
    "        elif data[j]==1:\n",
    "          nrofones+=1;\n",
    "        else:\n",
    "          nroftwos+=1;\n",
    "\n",
    "    print(\"zero\",nrofzeros)\n",
    "    print(\"one\",nrofones)\n",
    "    print(\"two\",nroftwos)\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        34\n",
      "         1.0       1.00      1.00      1.00        20\n",
      "         2.0       0.91      1.00      0.95       346\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.64      0.67      0.65       400\n",
      "weighted avg       0.84      0.92      0.87       400\n",
      "\n",
      "zero 0\n",
      "one 37\n",
      "two 963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Marco Yu\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_path='./src/dataset/train.csv'\n",
    "    trial_path='./src/dataset/trial.csv'\n",
    "    test_path='./src/dataset/test.csv'\n",
    "\n",
    "    \n",
    "    # preprocessing row data from train.csv. However, because of limited access of translator API,\n",
    "    # Highly recommend invoking preprocessing function on train_translated.csv where store already translated tweets\n",
    "    # raw_data=preprocessing(train_path,test_path,storepath='./src/dataset/train_translated.csv',translate_tag=False)\n",
    "\n",
    "    # Or you can directly read preprocessed data from train_translated.csv in 'tokenized_content' column\n",
    "    raw_data=pd.read_csv('./src/dataset/train_translated.csv')\n",
    "    raw_data['tokenized_content']=raw_data['translated_tweets'].apply(tokenize_words)\n",
    "    \n",
    "\n",
    "    predic_w2v_svm=word2vec_svm(raw_data,vector_size=300)\n",
    "\n",
    "    countclasses(predic_w2v_svm)\n",
    "\n",
    "    # save prediction in file\n",
    "#     savepredictions(\n",
    "#         './src/predictions/y_pred_svm_w2c(C=0.088,gamma=1)_OVR_translated_test.txt', predic_w2v_svm)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}