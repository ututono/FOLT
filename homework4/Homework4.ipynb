{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7778da30-67dc-4266-8ac4-48ca4605bea0",
   "metadata": {},
   "source": [
    "Homework4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68465f42-d7b1-4299-98ae-24dba1e3edaa",
   "metadata": {},
   "source": [
    "#### Homework 4.1\n",
    "text=[('restored', 'pos'), ('dissention', 'neg'), ('dismaying', 'neg'), ('condescend', 'neg'), ('exhort', 'neg'), ('solicitous','pos'), ('overtaken', 'pos'), ('impasse', 'neg'), ('dexterous', 'pos'), ('headway', 'pos'), ('entranced', 'pos'),('fanfare', 'pos'), ('distracting', 'neg'), ('exceptional', 'pos'), ('respect', 'pos'), ('problems', 'neg'), ('disparage','neg'), ('contaminated', 'neg'), ('dissonantly', 'neg'), ('tender', 'pos')]\n",
    "##### a) using the 20 words from above, fill in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a768ff-09c1-49e2-8aa4-8e9fa1c90719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('brown')\n",
    "nltk.download('udhr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7898a2-f83a-4d59-8474-8fe79874009d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>starts with 'dis'</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does not start with 'dis'</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            positive  negative  sum\n",
       "starts with 'dis'                  0         5    5\n",
       " does not start with 'dis'        10         5   15\n",
       "sum                               10        10   20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[('restored', 'pos'), ('dissention', 'neg'), ('dismaying', 'neg'), ('condescend', 'neg'), ('exhort', 'neg'), ('solicitous','pos'), ('overtaken', 'pos'), ('impasse', 'neg'), ('dexterous', 'pos'), ('headway', 'pos'), ('entranced', 'pos'),('fanfare', 'pos'), ('distracting', 'neg'), ('exceptional', 'pos'), ('respect', 'pos'), ('problems', 'neg'), ('disparage','neg'), ('contaminated', 'neg'), ('dissonantly', 'neg'), ('tender', 'pos')]\n",
    "dispos=0\n",
    "disneg=0\n",
    "testChangeOrder=[]\n",
    "for item in text:\n",
    "    element=(item[1],item[0])\n",
    "    testChangeOrder.append(element)\n",
    "\n",
    "            \n",
    "cfd=nltk.ConditionalFreqDist(testChangeOrder)\n",
    "\n",
    "for item in text:\n",
    "    if(item[0].startswith('dis')):\n",
    "        if(item[1]=='pos'):\n",
    "            dispos+=1\n",
    "        else:\n",
    "            disneg+=1\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data={\n",
    "    'positive':[dispos,len(cfd['pos'])-dispos,len(cfd['pos'])],\n",
    "    'negative':[disneg,len(cfd['neg'])-disneg,len(cfd['neg'])],\n",
    "    'sum':[dispos+disneg,len(cfd['neg'])+len(cfd['pos'])-dispos-disneg,len(cfd['neg'])+len(cfd['pos'])]\n",
    "},index=['starts with \\'dis\\'',' does not start with \\'dis\\'','sum'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75164b19-b0d5-46e0-878c-158897f6ef56",
   "metadata": {},
   "source": [
    "##### b)Based on the table from a), calculate the probability that a word starting with ‘dis’ has negative sentiment. Give a suitable formula to calculate this probability. You can either implement a code that does the calculation automatically or you can calculate it by hand in the markdown field and provide a short explanation of your calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfc719e-c0cd-4278-9774-ae1d0dab73df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prossiblity of the negative word with dis- within all words: 0.25\n"
     ]
    }
   ],
   "source": [
    "def cal_possibility_withPrefix(prefix:str, words:list, label:int):\n",
    "    '''\n",
    "    Calculate the possiblity of word with specific prefix under words\n",
    "    @label: '1':negative, '0':positive,'3':all words\n",
    "    @words:List[(sentiment,word)]\n",
    "    '''\n",
    "    cfd=nltk.ConditionalFreqDist(words)\n",
    "    dispos=0\n",
    "    disneg=0\n",
    "    for item in words:\n",
    "        if(item[1].startswith(prefix)):\n",
    "            if(item[0]=='pos'):\n",
    "                dispos+=1\n",
    "            else:\n",
    "                disneg+=1\n",
    "    \n",
    "    if(label==3):\n",
    "        return (dispos+disneg)/len(words)\n",
    "    elif(label==0):\n",
    "        return disneg/len(words)\n",
    "    elif(label==1):\n",
    "        return dispos/len(words)\n",
    "    \n",
    "\n",
    "print('The prossiblity of the negative word with dis- within all words: '+str(cal_possibility_withPrefix('dis',testChangeOrder,0)))\n",
    "#print('The prossiblity of word with dis- within negative words: '+str(cal_possibility_withPrefix('dis',testChangeOrder,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d80abe-973d-4c50-9b6c-152a89aeead1",
   "metadata": {},
   "source": [
    "##### c)Use the whole opinion_lexicon corpus to verify the hypothesis that words starting with ‘dis’ most likely express negative sentiment. (Multiple solutions are possible here. Please provide a brief explanation of your solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12433dce-9311-47ff-93ae-9a770ddc2f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypothesis is ture. Becase 98.77551020408163% words with \"dis\" as prefix are negative\n"
     ]
    }
   ],
   "source": [
    "text=nltk.corpus.opinion_lexicon\n",
    "posfilename='positive-words.txt'\n",
    "negfilename='negative-words.txt'\n",
    "\n",
    "    \n",
    "cfd=nltk.ConditionalFreqDist((sentiment,word) \n",
    "                             for sentiment in [posfilename,negfilename]\n",
    "                                for word in text.words(sentiment)\n",
    "                                    if word.startswith('dis'))\n",
    "\n",
    "sumDis=len(cfd[posfilename])+len(cfd[negfilename])\n",
    "prob=len(cfd[negfilename])/sumDis\n",
    "if(prob>0.5):\n",
    "    print('The hypothesis is ture. Becase '+str(prob*100)+ '% words with \"dis\" as prefix are negative')\n",
    "else:\n",
    "    print(prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b926f-a844-4ae2-9aa2-c04695c477c1",
   "metadata": {},
   "source": [
    "### Homework 4.2\n",
    "Find the 10 most frequent bigrams in every category of the brown corpus (brown.categories()). Stopwords and punctuation should be removed, and the difference between lower and upper case should be ignored (i.e. caseinsensitive).\n",
    "\n",
    "Print the categories and the respective results in an alphabetical order (according to the category name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16c60cad-0f93-447a-9f42-47d19465b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure\n",
      "[(('of', 'the'), 368), (('in', 'the'), 281), (('to', 'the'), 180), (('on', 'the'), 177), (('he', 'was'), 136), (('it', 'was'), 133), (('he', 'had'), 129), (('at', 'the'), 110), (('into', 'the'), 104), (('from', 'the'), 99)]\n",
      "belles_lettres\n",
      "[(('of', 'the'), 2014), (('in', 'the'), 1207), (('to', 'the'), 731), (('on', 'the'), 488), (('and', 'the'), 470), (('to', 'be'), 344), (('with', 'the'), 339), (('at', 'the'), 333), (('it', 'is'), 328), (('he', 'was'), 323)]\n",
      "editorial\n",
      "[(('of', 'the'), 2577), (('in', 'the'), 1506), (('to', 'the'), 959), (('on', 'the'), 622), (('and', 'the'), 590), (('to', 'be'), 457), (('it', 'is'), 446), (('for', 'the'), 435), (('with', 'the'), 413), (('at', 'the'), 397)]\n",
      "fiction\n",
      "[(('of', 'the'), 2943), (('in', 'the'), 1841), (('to', 'the'), 1142), (('on', 'the'), 799), (('and', 'the'), 712), (('at', 'the'), 552), (('to', 'be'), 547), (('he', 'was'), 546), (('for', 'the'), 520), (('it', 'was'), 515)]\n",
      "government\n",
      "[(('of', 'the'), 3798), (('in', 'the'), 2220), (('to', 'the'), 1457), (('on', 'the'), 916), (('and', 'the'), 855), (('for', 'the'), 716), (('to', 'be'), 655), (('at', 'the'), 619), (('with', 'the'), 593), (('he', 'was'), 576)]\n",
      "hobbies\n",
      "[(('of', 'the'), 4415), (('in', 'the'), 2614), (('to', 'the'), 1713), (('on', 'the'), 1105), (('and', 'the'), 1018), (('for', 'the'), 888), (('to', 'be'), 743), (('at', 'the'), 737), (('with', 'the'), 703), (('it', 'is'), 686)]\n",
      "humor\n",
      "[(('of', 'the'), 4531), (('in', 'the'), 2705), (('to', 'the'), 1755), (('on', 'the'), 1142), (('and', 'the'), 1045), (('for', 'the'), 912), (('to', 'be'), 774), (('at', 'the'), 766), (('with', 'the'), 720), (('it', 'is'), 709)]\n",
      "learned\n",
      "[(('of', 'the'), 6591), (('in', 'the'), 3878), (('to', 'the'), 2317), (('on', 'the'), 1521), (('and', 'the'), 1476), (('for', 'the'), 1246), (('to', 'be'), 1087), (('it', 'is'), 1038), (('with', 'the'), 983), (('of', 'a'), 974)]\n",
      "lore\n",
      "[(('of', 'the'), 7611), (('in', 'the'), 4530), (('to', 'the'), 2649), (('on', 'the'), 1770), (('and', 'the'), 1704), (('for', 'the'), 1405), (('to', 'be'), 1286), (('it', 'is'), 1203), (('with', 'the'), 1149), (('of', 'a'), 1111)]\n",
      "mystery\n",
      "[(('of', 'the'), 7834), (('in', 'the'), 4747), (('to', 'the'), 2835), (('on', 'the'), 1938), (('and', 'the'), 1777), (('for', 'the'), 1453), (('to', 'be'), 1359), (('at', 'the'), 1223), (('it', 'is'), 1215), (('with', 'the'), 1207)]\n",
      "news\n",
      "[(('of', 'the'), 8683), (('in', 'the'), 5337), (('to', 'the'), 3112), (('on', 'the'), 2192), (('and', 'the'), 1916), (('for', 'the'), 1673), (('to', 'be'), 1467), (('at', 'the'), 1419), (('with', 'the'), 1349), (('of', 'a'), 1280)]\n",
      "religion\n",
      "[(('of', 'the'), 9073), (('in', 'the'), 5537), (('to', 'the'), 3234), (('on', 'the'), 2251), (('and', 'the'), 2010), (('for', 'the'), 1722), (('to', 'be'), 1521), (('at', 'the'), 1452), (('with', 'the'), 1402), (('it', 'is'), 1372)]\n",
      "reviews\n",
      "[(('of', 'the'), 9419), (('in', 'the'), 5739), (('to', 'the'), 3331), (('on', 'the'), 2318), (('and', 'the'), 2108), (('for', 'the'), 1789), (('to', 'be'), 1587), (('at', 'the'), 1522), (('with', 'the'), 1461), (('it', 'is'), 1440)]\n",
      "romance\n",
      "[(('of', 'the'), 9654), (('in', 'the'), 6013), (('to', 'the'), 3470), (('on', 'the'), 2459), (('and', 'the'), 2229), (('for', 'the'), 1841), (('to', 'be'), 1698), (('at', 'the'), 1643), (('with', 'the'), 1530), (('of', 'a'), 1464)]\n",
      "science_fiction\n",
      "[(('of', 'the'), 9737), (('in', 'the'), 6054), (('to', 'the'), 3499), (('on', 'the'), 2482), (('and', 'the'), 2257), (('for', 'the'), 1858), (('to', 'be'), 1718), (('at', 'the'), 1660), (('with', 'the'), 1541), (('of', 'a'), 1479)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "df=pd.DataFrame(columns=['10 most popular bigrams'])\n",
    "pattern=re.compile(r'[,.;:\"/\\!\\*\\?\\\\\\'\\'|``]')\n",
    "i=0\n",
    "df=pd.DataFrame()\n",
    "wordsclear=[]\n",
    "for cate in brown.categories():\n",
    "    words=list(brown.words(categories=cate))\n",
    "    #copy words except punctuation to wrodsclear.\n",
    "    for word in words:\n",
    "        if(pattern.match(word)==None):\n",
    "            wordsclear.append(word.lower())\n",
    "            \n",
    "    bigrams=list(nltk.bigrams(wordsclear))\n",
    "    freq=nltk.FreqDist(bigrams)\n",
    "    print(cate)\n",
    "    print(freq.most_common(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8dcc3-a5a0-420c-9b9d-4f4a37b42e27",
   "metadata": {},
   "source": [
    "### Homework 4.3 (7 points)\n",
    "\n",
    "Implement a language guesser, i.e. a function that takes a given text and outputs the language it thinks the text is written\n",
    "in. The function should base its decision on the frequency of character bigrams in each language. We will build our\n",
    "language model based on the udhr corpus. Udhr contains the Universal Declaration of Human Rights in over 300\n",
    "languages. We will use only 4 of them. To get the list of all languages, you can use the function udhr.fileids()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ce7df-a379-4c05-83e3-58480a780724",
   "metadata": {},
   "source": [
    "##### (a) (2 points) Implement a function build_language_models(languages,words) which takes a list of languages and a dictionary of words as arguments and returns a conditional frequency distribution where:\n",
    "- the languages are the conditions\n",
    "- the values are the lower cased character bigrams found in words[language]\n",
    "\n",
    "Call the function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2132460-941a-480c-8051-f0a7bfaedf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ('u', 'n') -> 0.0063174114021571645\n",
      "English ('n', 'i') -> 0.004930662557781202\n",
      "English ('i', 'v') -> 0.0032357473035439137\n",
      "English ('v', 'e') -> 0.01140215716486903\n",
      "English ('e', 'r') -> 0.020338983050847456\n",
      "English ('r', 's') -> 0.003389830508474576\n",
      "English ('s', 'a') -> 0.001694915254237288\n",
      "English ('a', 'l') -> 0.019106317411402157\n",
      "English ('d', 'e') -> 0.00600924499229584\n",
      "English ('e', 'c') -> 0.007087827426810477\n",
      "German_Deutsch ('d', 'i') -> 0.0099361249112846\n",
      "German_Deutsch ('i', 'e') -> 0.016465578424414477\n",
      "German_Deutsch ('a', 'l') -> 0.008232789212207239\n",
      "German_Deutsch ('l', 'l') -> 0.005961674946770759\n",
      "German_Deutsch ('l', 'g') -> 0.00127750177430802\n",
      "German_Deutsch ('g', 'e') -> 0.02185947480482612\n",
      "German_Deutsch ('e', 'm') -> 0.0055358410220014195\n",
      "German_Deutsch ('m', 'e') -> 0.006813342796309439\n",
      "German_Deutsch ('e', 'i') -> 0.02995031937544358\n",
      "German_Deutsch ('i', 'n') -> 0.01973030518097942\n",
      "Finnish_Suomi ('i', 'h') -> 0.0037654653039268424\n",
      "Finnish_Suomi ('h', 'm') -> 0.002555137170521786\n",
      "Finnish_Suomi ('m', 'i') -> 0.011565357719203874\n",
      "Finnish_Suomi ('i', 's') -> 0.03388918773534158\n",
      "Finnish_Suomi ('s', 'o') -> 0.0034965034965034965\n",
      "Finnish_Suomi ('o', 'i') -> 0.013717052178590641\n",
      "Finnish_Suomi ('i', 'k') -> 0.01681011296395912\n",
      "Finnish_Suomi ('k', 'e') -> 0.01277568585260893\n",
      "Finnish_Suomi ('e', 'u') -> 0.008606777837547068\n",
      "Finnish_Suomi ('u', 'k') -> 0.007530930607853685\n",
      "Italian ('d', 'i') -> 0.0317743132887899\n",
      "Italian ('i', 'c') -> 0.009651076466221232\n",
      "Italian ('c', 'h') -> 0.005642167780252413\n",
      "Italian ('h', 'i') -> 0.0014847809948032665\n",
      "Italian ('i', 'a') -> 0.009651076466221232\n",
      "Italian ('a', 'r') -> 0.012472160356347439\n",
      "Italian ('r', 'a') -> 0.009651076466221232\n",
      "Italian ('a', 'z') -> 0.006829992576095026\n",
      "Italian ('z', 'i') -> 0.010987379361544172\n",
      "Italian ('i', 'o') -> 0.015738678544914626\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "\n",
    "languages = ['English', 'German_Deutsch', 'Finnish_Suomi', 'Italian']\n",
    "\n",
    "def build_language_models(languages, words,tokenized=True):\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    pattern=re.compile(r'[,.;:\"/\\!\\*\\?\\\\\\'\\'|``]')\n",
    "\n",
    "    container={}\n",
    "    # intergrate bigrams of each word into a list\n",
    "    for lang in languages:\n",
    "        entirewords=[]\n",
    "        for word in udhr.words(lang + '-Latin1'):\n",
    "            for bigram in tuple(nltk.bigrams(word.lower())):\n",
    "                entirewords.append(bigram)\n",
    "        container[lang]=entirewords  \n",
    "    cfds=nltk.ConditionalFreqDist(\n",
    "        (lang,bigram)\n",
    "        for lang in languages\n",
    "            for bigram in container[lang]\n",
    "    )\n",
    "    return cfds\n",
    "\n",
    "\n",
    "language_base = dict((language, udhr.words(language + '-Latin1')) for language\n",
    "in languages)\n",
    "\n",
    "language_model_cfd = build_language_models(languages, language_base)\n",
    "\n",
    "\n",
    "# print the models for visual inspection\n",
    "for language in languages:\n",
    "    for key in list(language_model_cfd[language].keys())[:10]:\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3637d66-0b27-475f-932e-685129feeb2e",
   "metadata": {},
   "source": [
    "##### (b) (2 points) Develop an algorithm which calculates the overall score of a given text based on the frequency of character bigrams accessible by language_model_cfd[language].freq(char_bigram). Explain how the algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8606a84-e5da-4066-bbe0-15cd50d4f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(language_model_cfd,text):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #lower case\n",
    "    text=text.lower()\n",
    "    #remove punctuation\n",
    "    pattern=re.compile(r'[,\\.;:\"/\\!\\*\\?\\\\\\'\\'|``|\\s]+')\n",
    "    text_clear=pattern.sub(' ', text)\n",
    "    words=text_clear.split()\n",
    "    bigrams=[]\n",
    "    for word in words:\n",
    "        for bigram in nltk.bigrams(word):\n",
    "            bigrams.append(bigram)\n",
    "    freq=nltk.FreqDist(bigrams)\n",
    "    probs_list=[]\n",
    "    for lang in languages:\n",
    "        probs=0.0\n",
    "        for item in freq:\n",
    "            probs=probs+language_model_cfd[lang].freq(item)*freq.freq(item)\n",
    "        probs_list.append((lang,probs))\n",
    "        \n",
    "    probs_list.sort(reverse=True,key=lambda entry: entry[1])\n",
    "\n",
    "    return probs_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e19363-c1ad-4301-8589-36fa8faa06ce",
   "metadata": {},
   "source": [
    "##### (c) (2 points) On the basis of the algorithm from the last part, implement a function guess_language(language_model_cfd,text) that returns the most likely language for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b42dd08f-dc22-45bb-8968-4c5d3f09eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_language(language_model_cfd,text):\n",
    "    '''\n",
    "    To guess a given text belongs to which language\n",
    "    @ return name of guessed language:str\n",
    "    \n",
    "    '''\n",
    "    scores=cal_score(language_model_cfd,text)\n",
    "    return scores[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2e93b-df86-4192-83c0-39d6911672e1",
   "metadata": {},
   "source": [
    "##### (d) (1 point) Test your implementation with the following data:\n",
    "\n",
    "If your function does not detect the correct language for at least two of these sentences, improve your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d229766-0181-46f4-b935-7df44c609975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess for finnish text is Finnish_Suomi\n",
      "guess for german text is German_Deutsch\n",
      "guess for italian text is Italian\n",
      "guess for english text is English\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Syksy on kaunis vuodenaika, varsinkin kun ei sada.\"\n",
    "text2 = \"Erkenntnisfortschritte ergeben sich durch das Wechselspiel von Beobachtung oder Experiment mit der Theorie.\"\n",
    "text3 = \"Come in altri paesi europei del mediterraneo, sono presenti tratti distintivi ed elementi che caratterizzano la dieta mediterranea.\"\n",
    "text4 = \"A healthy diet is important if you want to live a healthy life.\"\n",
    "\n",
    "# guess the language by comparing the frequency distributions\n",
    "print('guess for finnish text is', guess_language(language_model_cfd, text1))\n",
    "print('guess for german text is', guess_language(language_model_cfd, text2))\n",
    "print('guess for italian text is', guess_language(language_model_cfd, text3))\n",
    "print('guess for english text is', guess_language(language_model_cfd, text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a9560-ca0c-452b-a4af-9f031531bb47",
   "metadata": {},
   "source": [
    "(e) (Ungraded) The previous language guesser was based on the frequency of character bigrams. Implement alternative\n",
    "language guesser based on tokens (token unigrams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89575b-bdd8-4fe3-bf94-4513c90e8b31",
   "metadata": {},
   "source": [
    "### Homework 4.4 \n",
    "This part is ungraded. However, we recommend to work it out. It will save you some time in the future.\n",
    "Copy all functions implemented in the tasks and homeworks to one file and name it My_Lib.py. You may easily access\n",
    "for examle the function word_freq of the previous tasks with the following statement:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10a3d-9ee2-4d26-b1b3-e373d61c728d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
