{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7778da30-67dc-4266-8ac4-48ca4605bea0",
   "metadata": {},
   "source": [
    "### Homework4\n",
    "Author:Zhuo Yu\n",
    "\n",
    "TU-ID: 2752408\n",
    "\n",
    "Date:17.11.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68465f42-d7b1-4299-98ae-24dba1e3edaa",
   "metadata": {},
   "source": [
    "#### Homework 4.1\n",
    "text=[('restored', 'pos'), ('dissention', 'neg'), ('dismaying', 'neg'), ('condescend', 'neg'), ('exhort', 'neg'), ('solicitous','pos'), ('overtaken', 'pos'), ('impasse', 'neg'), ('dexterous', 'pos'), ('headway', 'pos'), ('entranced', 'pos'),('fanfare', 'pos'), ('distracting', 'neg'), ('exceptional', 'pos'), ('respect', 'pos'), ('problems', 'neg'), ('disparage','neg'), ('contaminated', 'neg'), ('dissonantly', 'neg'), ('tender', 'pos')]\n",
    "##### a) using the 20 words from above, fill in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a768ff-09c1-49e2-8aa4-8e9fa1c90719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('brown')\n",
    "nltk.download('udhr')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7898a2-f83a-4d59-8474-8fe79874009d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>starts with 'dis'</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does not start with 'dis'</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            positive  negative  sum\n",
       "starts with 'dis'                  0         5    5\n",
       " does not start with 'dis'        10         5   15\n",
       "sum                               10        10   20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[('restored', 'pos'), ('dissention', 'neg'), ('dismaying', 'neg'), ('condescend', 'neg'), ('exhort', 'neg'), ('solicitous','pos'), ('overtaken', 'pos'), ('impasse', 'neg'), ('dexterous', 'pos'), ('headway', 'pos'), ('entranced', 'pos'),('fanfare', 'pos'), ('distracting', 'neg'), ('exceptional', 'pos'), ('respect', 'pos'), ('problems', 'neg'), ('disparage','neg'), ('contaminated', 'neg'), ('dissonantly', 'neg'), ('tender', 'pos')]\n",
    "dispos=0\n",
    "disneg=0\n",
    "testChangeOrder=[]\n",
    "\n",
    "#allocate sentiment label in the first place of the tuple\n",
    "for item in text:\n",
    "    element=(item[1],item[0])\n",
    "    testChangeOrder.append(element)\n",
    "\n",
    "            \n",
    "cfd=nltk.ConditionalFreqDist(testChangeOrder)\n",
    "\n",
    "for item in text:\n",
    "    if(item[0].startswith('dis')):\n",
    "        if(item[1]=='pos'):\n",
    "            dispos+=1\n",
    "        else:\n",
    "            disneg+=1\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data={\n",
    "    'positive':[dispos,len(cfd['pos'])-dispos,len(cfd['pos'])],\n",
    "    'negative':[disneg,len(cfd['neg'])-disneg,len(cfd['neg'])],\n",
    "    'sum':[dispos+disneg,len(cfd['neg'])+len(cfd['pos'])-dispos-disneg,len(cfd['neg'])+len(cfd['pos'])]\n",
    "},index=['starts with \\'dis\\'',' does not start with \\'dis\\'','sum'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75164b19-b0d5-46e0-878c-158897f6ef56",
   "metadata": {},
   "source": [
    "str(prob*100)##### b)Based on the table from a), calculate the probability that a word starting with ‘dis’ has negative sentiment. Give a suitable formula to calculate this probability. You can either implement a code that does the calculation automatically or you can calculate it by hand in the markdown field and provide a short explanation of your calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e23351-d79b-4e34-a90f-f6a19c94d6b7",
   "metadata": {},
   "source": [
    "**Answer**: Assuming the probability that a word is negative presents $P(neg)$, the probability that a word starting with 'dis' presents $P(s\\_dis)$. Hence, $P(neg|s\\_dis)=\\frac{P(neg \\cap s\\_dis)}{P(s\\_dis)}=\\frac{(5/20)}{(5/20)}=1$\n",
    "\n",
    "I also wrote a simple method to calculate the probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cfc719e-c0cd-4278-9774-ae1d0dab73df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prossiblity of the negative word with dis- within all words: 1.0\n"
     ]
    }
   ],
   "source": [
    "def cal_possibility_withPrefix(prefix:str, words:list):\n",
    "    '''\n",
    "    Calculate the possiblity of word with specific prefix under words\n",
    "    @label: '1':negative, '0':positive,'3':all words\n",
    "    @words:List[(sentiment,word)]\n",
    "    '''\n",
    "    dispos=0\n",
    "    disneg=0\n",
    "    # count word starting with 'dis' in postive and negative sentiment separately\n",
    "    for item in words:\n",
    "        if(item[1].startswith(prefix)):\n",
    "            if(item[0]=='pos'):\n",
    "                dispos+=1\n",
    "            else:\n",
    "                disneg+=1\n",
    "    # the probability that a word is negative and starting with 'dis'\n",
    "    p_neg_cap_dis=disneg/len(words)\n",
    "    \n",
    "     # the probability that a word starting with 'dis'\n",
    "    p_s_dis=(disneg+dispos)/len(words)\n",
    "    return p_neg_cap_dis/p_s_dis\n",
    "    \n",
    "print('The prossiblity of the negative word with dis- within all words: '+str(cal_possibility_withPrefix('dis',testChangeOrder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d80abe-973d-4c50-9b6c-152a89aeead1",
   "metadata": {},
   "source": [
    "##### c)Use the whole opinion_lexicon corpus to verify the hypothesis that words starting with ‘dis’ most likely express negative sentiment. (Multiple solutions are possible here. Please provide a brief explanation of your solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f09a1-1610-4040-9b33-622953c3732a",
   "metadata": {},
   "source": [
    "**Answer**: Similar assumtion and formula as above:Assuming the probability that a word is negative presents $P(neg)$, the probability that a word starting with 'dis' presents $P(s\\_dis)$. Hence, $P(neg|s\\_dis)=\\frac{P(neg \\cap s\\_dis)}{P(s\\_dis)}$. The result shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12433dce-9311-47ff-93ae-9a770ddc2f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypothesis is ture. Becase P(neg|s\\_dis) is 0.9877551020408165, which means it's round 99% possible that a word starting with ‘dis’ has negative sentiment\n"
     ]
    }
   ],
   "source": [
    "text=nltk.corpus.opinion_lexicon\n",
    "posfilename='positive-words.txt'\n",
    "negfilename='negative-words.txt'\n",
    "\n",
    "    \n",
    "cfd=nltk.ConditionalFreqDist((sentiment,word) \n",
    "                             for sentiment in [posfilename,negfilename]\n",
    "                                for word in text.words(sentiment)\n",
    "                                    if word.startswith('dis'))\n",
    "\n",
    "sumwords=len(text.words(posfilename))+len(text.words(negfilename))\n",
    "\n",
    "# the probability that a word is negative and starting with 'dis'\n",
    "p_neg_cap_dis=len(cfd[negfilename])/sumwords\n",
    "\n",
    "# the probability that a word starting with 'dis'\n",
    "p_s_dis=(len(cfd[posfilename])+len(cfd[negfilename]))/sumwords\n",
    "\n",
    "prob=p_neg_cap_dis/p_s_dis\n",
    "#Another method to verify this hypothesis\n",
    "#sumDis=len(cfd[posfilename])+len(cfd[negfilename])\n",
    "#prob=len(cfd[negfilename])/sumDis\n",
    "if(prob>0.5):\n",
    "    print('The hypothesis is ture. Becase P(neg|s\\_dis) is '+str(prob)+ ', which means it\\'s round '+str(round(prob*100))+\n",
    "          '% possible that a word starting with ‘dis’ has negative sentiment')\n",
    "else:\n",
    "    print('The hypothesis is not accuate. Becase '+str(prob*100)+ '% words with \"dis\" as prefix are negative')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b926f-a844-4ae2-9aa2-c04695c477c1",
   "metadata": {},
   "source": [
    "### Homework 4.2\n",
    "Find the 10 most frequent bigrams in every category of the brown corpus (brown.categories()). Stopwords and punctuation should be removed, and the difference between lower and upper case should be ignored (i.e. caseinsensitive).\n",
    "\n",
    "Print the categories and the respective results in an alphabetical order (according to the category name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16c60cad-0f93-447a-9f42-47d19465b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure\n",
      "[(('could', 'see'), 24), (('miss', 'langford'), 18), (('blue', 'throat'), 16), (('two', 'men'), 14), (('let', 'go'), 13), (('mary', 'jane'), 12), (('shook', 'head'), 11), (('big', 'man'), 10), (('years', 'ago'), 9), (('long', 'time'), 9)]\n",
      "---------------------------------------------\n",
      "belles_lettres\n",
      "[(('new', 'york'), 59), (('united', 'states'), 51), (('bang', 'jensen'), 31), (('nineteenth', 'century'), 25), (('mrs', 'coolidge'), 23), (('even', 'though'), 22), (('civil', 'war'), 19), (('years', 'ago'), 19), (('u', 'n'), 19), (('one', 'day'), 18)]\n",
      "---------------------------------------------\n",
      "editorial\n",
      "[(('united', 'states'), 56), (('new', 'york'), 24), (('mr', 'podger'), 21), (('west', 'berlin'), 20), (('per', 'cent'), 19), (('president', 'kennedy'), 18), (('united', 'nations'), 18), (('east', 'greenwich'), 18), (('last', 'year'), 17), (('u', 'n'), 17)]\n",
      "---------------------------------------------\n",
      "fiction\n",
      "[(('miss', 'ada'), 19), (('linda', 'kay'), 17), (('bobby', 'joe'), 16), (('could', 'see'), 14), (('mr', 'jack'), 14), (('old', 'man'), 13), (('simms', 'purdew'), 12), (('big', 'hans'), 12), (('new', 'york'), 10), (('uncle', 'randolph'), 9)]\n",
      "---------------------------------------------\n",
      "government\n",
      "[(('united', 'states'), 137), (('rhode', 'island'), 75), (('fiscal', 'year'), 54), (('du', 'pont'), 44), (('peace', 'corps'), 41), (('general', 'motors'), 40), (('small', 'business'), 39), (('new', 'york'), 33), (('states', 'america'), 25), (('sam', 'rayburn'), 24)]\n",
      "---------------------------------------------\n",
      "hobbies\n",
      "[(('1', '2'), 37), (('new', 'york'), 27), (('per', 'head'), 20), (('chemical', 'name'), 18), (('hanover', '2'), 17), (('drug', 'chemical'), 17), (('years', 'ago'), 16), (('1', '4'), 16), (('per', 'day'), 15), (('interior', 'design'), 15)]\n",
      "---------------------------------------------\n",
      "humor\n",
      "[(('mr', 'crombie'), 10), (('mr', 'blatz'), 9), (('general', 'burnside'), 9), (('years', 'ago'), 7), (('burnside', 'horse'), 7), (('mr', 'gorboduc'), 7), (('humor', 'comedy'), 6), (('los', 'angeles'), 5), (('one', 'said'), 5), (('police', 'captain'), 4)]\n",
      "---------------------------------------------\n",
      "learned\n",
      "[(('af', 'af'), 122), (('1', '2'), 39), (('per', 'cent'), 37), (('united', 'states'), 36), (('dominant', 'stress'), 31), (('let', 'us'), 26), (('wage', 'rate'), 25), (('index', 'words'), 24), (('degrees', 'c'), 22), (('middle', 'class'), 22)]\n",
      "---------------------------------------------\n",
      "lore\n",
      "[(('united', 'states'), 44), (('new', 'york'), 31), (('high', 'school'), 27), (('per', 'cent'), 25), (('anti', 'semitism'), 22), (('part', 'time'), 17), (('years', 'ago'), 16), (('white', 'house'), 15), (('middle', 'class'), 15), (('economic', 'integration'), 15)]\n",
      "---------------------------------------------\n",
      "mystery\n",
      "[(('mr', 'skyros'), 22), (('mrs', 'meeker'), 15), (('prime', 'minister'), 13), (('went', 'back'), 10), (('old', 'man'), 10), (('could', 'see'), 10), (('door', 'open'), 9), (('new', 'york'), 8), (('last', 'night'), 8), (('station', 'wagon'), 8)]\n",
      "---------------------------------------------\n",
      "news\n",
      "[(('new', 'york'), 54), (('per', 'cent'), 50), (('mr', 'mrs'), 42), (('united', 'states'), 40), (('last', 'year'), 36), (('last', 'week'), 36), (('white', 'house'), 29), (('year', 'old'), 28), (('high', 'school'), 24), (('president', 'kennedy'), 24)]\n",
      "---------------------------------------------\n",
      "religion\n",
      "[(('jesus', 'christ'), 22), (('new', 'members'), 22), (('lo', 'shu'), 21), (('st', 'john'), 20), (('new', 'birth'), 16), (('years', 'ago'), 14), (('born', 'god'), 14), (('new', 'england'), 13), (('real', 'estate'), 11), (('anti', 'slavery'), 11)]\n",
      "---------------------------------------------\n",
      "reviews\n",
      "[(('new', 'york'), 31), (('last', 'night'), 20), (('mr', 'white'), 10), (('mr', 'sansom'), 10), (('field', 'marshal'), 9), (('saturday', 'night'), 8), (('mr', 'kennedy'), 8), (('per', 'cent'), 8), (('dr', 'keys'), 8), (('world', 'war'), 7)]\n",
      "---------------------------------------------\n",
      "romance\n",
      "[(('old', 'man'), 30), (('could', 'see'), 16), (('mike', 'deegan'), 14), (('new', 'york'), 13), (('poor', 'john'), 11), (('cousin', 'elec'), 11), (('go', 'back'), 10), (('young', 'men'), 10), (('next', 'morning'), 10), (('mrs', 'kirby'), 10)]\n",
      "---------------------------------------------\n",
      "science_fiction\n",
      "[(('b', 'dikkat'), 22), (('half', 'man'), 9), (('said', 'hal'), 7), (('lady', 'da'), 7), (('shell', 'people'), 6), (('water', 'brother'), 4), (('first', 'time'), 4), (('hal', 'yarrow'), 4), (('would', 'longer'), 4), (('fusion', 'power'), 4)]\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#TODO PUNCTUATION\n",
    "punctuation=['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~','``',\"''\",'--']\n",
    "pattern=re.compile(\"[\" + re.escape(\"\".join(punctuation)) + \"]\")\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for cate in brown.categories():\n",
    "    sent=''\n",
    "    wordsclear=[]\n",
    "    #assemble words in words(categories) into a str\n",
    "    for word in list(brown.words(categories=cate)):\n",
    "        sent=sent+\" \"+word.lower()\n",
    "        \n",
    "    #replace all punctuation with a space i.e. ' '\n",
    "    sentsclear=pattern.sub(\" \",sent)\n",
    "    words=sentsclear.split()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            wordsclear.append(word)\n",
    "            \n",
    "    bigrams=list(nltk.bigrams(wordsclear))\n",
    "    freq=nltk.FreqDist(bigrams)\n",
    "    print(cate)\n",
    "    print(freq.most_common(10))\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8dcc3-a5a0-420c-9b9d-4f4a37b42e27",
   "metadata": {},
   "source": [
    "### Homework 4.3 (7 points)\n",
    "\n",
    "Implement a language guesser, i.e. a function that takes a given text and outputs the language it thinks the text is written\n",
    "in. The function should base its decision on the frequency of character bigrams in each language. We will build our\n",
    "language model based on the udhr corpus. Udhr contains the Universal Declaration of Human Rights in over 300\n",
    "languages. We will use only 4 of them. To get the list of all languages, you can use the function udhr.fileids()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ce7df-a379-4c05-83e3-58480a780724",
   "metadata": {},
   "source": [
    "##### (a) (2 points) Implement a function build_language_models(languages,words) which takes a list of languages and a dictionary of words as arguments and returns a conditional frequency distribution where:\n",
    "- the languages are the conditions\n",
    "- the values are the lower cased character bigrams found in words[language]\n",
    "\n",
    "Call the function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2132460-941a-480c-8051-f0a7bfaedf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ('u', 'n') -> 0.0063174114021571645\n",
      "English ('n', 'i') -> 0.004930662557781202\n",
      "English ('i', 'v') -> 0.0032357473035439137\n",
      "English ('v', 'e') -> 0.01140215716486903\n",
      "English ('e', 'r') -> 0.020338983050847456\n",
      "English ('r', 's') -> 0.003389830508474576\n",
      "English ('s', 'a') -> 0.001694915254237288\n",
      "English ('a', 'l') -> 0.019106317411402157\n",
      "English ('d', 'e') -> 0.00600924499229584\n",
      "English ('e', 'c') -> 0.007087827426810477\n",
      "German_Deutsch ('d', 'i') -> 0.0099361249112846\n",
      "German_Deutsch ('i', 'e') -> 0.016465578424414477\n",
      "German_Deutsch ('a', 'l') -> 0.008232789212207239\n",
      "German_Deutsch ('l', 'l') -> 0.005961674946770759\n",
      "German_Deutsch ('l', 'g') -> 0.00127750177430802\n",
      "German_Deutsch ('g', 'e') -> 0.02185947480482612\n",
      "German_Deutsch ('e', 'm') -> 0.0055358410220014195\n",
      "German_Deutsch ('m', 'e') -> 0.006813342796309439\n",
      "German_Deutsch ('e', 'i') -> 0.02995031937544358\n",
      "German_Deutsch ('i', 'n') -> 0.01973030518097942\n",
      "Finnish_Suomi ('i', 'h') -> 0.0037654653039268424\n",
      "Finnish_Suomi ('h', 'm') -> 0.002555137170521786\n",
      "Finnish_Suomi ('m', 'i') -> 0.011565357719203874\n",
      "Finnish_Suomi ('i', 's') -> 0.03388918773534158\n",
      "Finnish_Suomi ('s', 'o') -> 0.0034965034965034965\n",
      "Finnish_Suomi ('o', 'i') -> 0.013717052178590641\n",
      "Finnish_Suomi ('i', 'k') -> 0.01681011296395912\n",
      "Finnish_Suomi ('k', 'e') -> 0.01277568585260893\n",
      "Finnish_Suomi ('e', 'u') -> 0.008606777837547068\n",
      "Finnish_Suomi ('u', 'k') -> 0.007530930607853685\n",
      "Italian ('d', 'i') -> 0.0317743132887899\n",
      "Italian ('i', 'c') -> 0.009651076466221232\n",
      "Italian ('c', 'h') -> 0.005642167780252413\n",
      "Italian ('h', 'i') -> 0.0014847809948032665\n",
      "Italian ('i', 'a') -> 0.009651076466221232\n",
      "Italian ('a', 'r') -> 0.012472160356347439\n",
      "Italian ('r', 'a') -> 0.009651076466221232\n",
      "Italian ('a', 'z') -> 0.006829992576095026\n",
      "Italian ('z', 'i') -> 0.010987379361544172\n",
      "Italian ('i', 'o') -> 0.015738678544914626\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "\n",
    "languages = ['English', 'German_Deutsch', 'Finnish_Suomi', 'Italian']\n",
    "\n",
    "def build_language_models(languages, words,tokenized=True):\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    container={}\n",
    "    # intergrate bigrams of each word into a dict\n",
    "    for lang in languages:\n",
    "        entirewords=[]\n",
    "        for word in words[lang]:\n",
    "            for bigram in tuple(nltk.bigrams(word.lower())):\n",
    "                entirewords.append(bigram)\n",
    "        container[lang]=entirewords\n",
    "    \n",
    "    #Count bigrams according to languages\n",
    "    cfds=nltk.ConditionalFreqDist(\n",
    "        (lang,bigram)\n",
    "        for lang in languages\n",
    "            for bigram in container[lang]\n",
    "    )\n",
    "    return cfds\n",
    "\n",
    "\n",
    "language_base = dict((language, udhr.words(language + '-Latin1')) for language\n",
    "in languages)\n",
    "\n",
    "language_model_cfd = build_language_models(languages, language_base)\n",
    "\n",
    "\n",
    "# print the models for visual inspection\n",
    "for language in languages:\n",
    "    for key in list(language_model_cfd[language].keys())[:10]:\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3637d66-0b27-475f-932e-685129feeb2e",
   "metadata": {},
   "source": [
    "##### (b) (2 points) Develop an algorithm which calculates the overall score of a given text based on the frequency of character bigrams accessible by language_model_cfd[language].freq(char_bigram). Explain how the algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7f0cf-5129-4bf7-8515-ddeb56884696",
   "metadata": {},
   "source": [
    "**Answer:** The main idea of the following algorithm is making multiplication of the frequence of a bigram in language_model_cfd[lang] and the frequence of the same bigram  from bigrams generated for characters in the given text. Each language has own result which, in fact, represents the probability that the given text is this language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8606a84-e5da-4066-bbe0-15cd50d4f781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('German_Deutsch', 0.011981179191966982),\n",
       " ('English', 0.007004127908082716),\n",
       " ('Italian', 0.006835491764964665),\n",
       " ('Finnish_Suomi', 0.0048711971788894855)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def cal_score(language_model_cfd,text):\n",
    "    '''\n",
    "    Calculate a frequence score for text\n",
    "    '''\n",
    "    words=word_tokenize(text.lower())\n",
    "    bigrams=[]\n",
    "    for word in words:\n",
    "        for bigram in nltk.bigrams(word):\n",
    "            bigrams.append(bigram)\n",
    "    freq=nltk.FreqDist(bigrams)\n",
    "    probs_list=[]\n",
    "    for lang in languages:\n",
    "        probs=0.0\n",
    "        for item in freq:\n",
    "            probs=probs+language_model_cfd[lang].freq(item)*freq.freq(item)\n",
    "        probs_list.append((lang,probs))\n",
    "        \n",
    "    probs_list.sort(reverse=True,key=lambda entry: entry[1])\n",
    "\n",
    "    return probs_list  \n",
    "\n",
    "text='Erkenntnisfortschritte ergeben sich durch das Wechselspiel von Beobachtung oder Experiment mit der Theorie.'\n",
    "cal_score(language_model_cfd,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e19363-c1ad-4301-8589-36fa8faa06ce",
   "metadata": {},
   "source": [
    "##### (c) (2 points) On the basis of the algorithm from the last part, implement a function guess_language(language_model_cfd,text) that returns the most likely language for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b42dd08f-dc22-45bb-8968-4c5d3f09eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_language(language_model_cfd,text):\n",
    "    '''\n",
    "    To guess a given text belongs to which language\n",
    "    @ return name of guessed language:str\n",
    "    \n",
    "    '''\n",
    "    scores=cal_score(language_model_cfd,text)\n",
    "    return scores[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2e93b-df86-4192-83c0-39d6911672e1",
   "metadata": {},
   "source": [
    "##### (d) (1 point) Test your implementation with the following data:\n",
    "\n",
    "If your function does not detect the correct language for at least two of these sentences, improve your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d229766-0181-46f4-b935-7df44c609975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess for finnish text is Finnish_Suomi\n",
      "guess for german text is German_Deutsch\n",
      "guess for italian text is Italian\n",
      "guess for english text is English\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Syksy on kaunis vuodenaika, varsinkin kun ei sada.\"\n",
    "text2 = \"Erkenntnisfortschritte ergeben sich durch das Wechselspiel von Beobachtung oder Experiment mit der Theorie.\"\n",
    "text3 = \"Come in altri paesi europei del mediterraneo, sono presenti tratti distintivi ed elementi che caratterizzano la dieta mediterranea.\"\n",
    "text4 = \"A healthy diet is important if you want to live a healthy life.\"\n",
    "\n",
    "# guess the language by comparing the frequency distributions\n",
    "print('guess for finnish text is', guess_language(language_model_cfd, text1))\n",
    "print('guess for german text is', guess_language(language_model_cfd, text2))\n",
    "print('guess for italian text is', guess_language(language_model_cfd, text3))\n",
    "print('guess for english text is', guess_language(language_model_cfd, text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a9560-ca0c-452b-a4af-9f031531bb47",
   "metadata": {},
   "source": [
    "(e) (Ungraded) The previous language guesser was based on the frequency of character bigrams. Implement alternative\n",
    "language guesser based on tokens (token unigrams)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89575b-bdd8-4fe3-bf94-4513c90e8b31",
   "metadata": {},
   "source": [
    "### Homework 4.4 \n",
    "This part is ungraded. However, we recommend to work it out. It will save you some time in the future.\n",
    "Copy all functions implemented in the tasks and homeworks to one file and name it My_Lib.py. You may easily access\n",
    "for examle the function word_freq of the previous tasks with the following statement:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10a3d-9ee2-4d26-b1b3-e373d61c728d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
