{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8380ba7-4c2b-4aa2-a0cc-434b6c5abc64",
   "metadata": {},
   "source": [
    "\n",
    "# Homework 6 Processing Raw Text\n",
    "Please send your solution as Jupyter notebooks (.ipynb) or Python files (.py). In case your submission consists of several files, compress these to a zip-file. Indicate clearly which submission corresponds to which question. Include comments in your program code to make it easier readable. Naming schema: YourName_Homework6.ipynb, YourName_Homework6.py or YourName_Homework6.zip. The deadline for the homework is Thursday, 02.12.2021 09:40 CET. Group submissions are not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff69270d-aff2-41a0-bfbb-96cd5b757e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ZhuoY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus.europarl_raw import english\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "nltk.download('europarl_raw')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebab077-e9ea-46c4-a9f5-9ad1944cad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=80\n",
    "for i in webtext.words():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b61ab-86d2-4a80-ac8a-c435185bb9d5",
   "metadata": {},
   "source": [
    "## Task 6.1 (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018871fe-350b-424b-99e9-6b181b7654dd",
   "metadata": {},
   "source": [
    "### a. Create a function trigrams_regex(tokens), which finds from a given text all sequences of 3-grams that have an article, a potential adjective ending with -ing and a following word by using regular expressions only. \n",
    "\n",
    "Make sure that the third element of the 3-gram is a word (not punctuation). The function should take a list of tokens as an argument and return a list of all found n-grams. Try your function on austen-persuasion.txt from the Gutenberg corpus and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479610e-90fc-4f3f-baac-79296957878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threeGrams(tokens:list): \n",
    "    '''\n",
    "    To find all 3-grams which consists of an article, an adjective ending \n",
    "    with -ing and a following word.\n",
    "    \n",
    "    Only allow to match these grams by regular expresseion\n",
    "    \n",
    "    @param: a list of tokens\n",
    "    @return: a list of all found n-grams.\n",
    "    \n",
    "    '''\n",
    "    threegrams=[]\n",
    "    tokens=iter(tokens)\n",
    "    for token in tokens:\n",
    "        if re.search(r'\\bthe\\b|\\ban\\b|\\ba\\b',token):\n",
    "            adj=next(tokens)\n",
    "            if re.search(r'^.*ing$',adj):\n",
    "                word=next(tokens)\n",
    "                if re.search(r\"\\.|-|!|\\(|#|\\$|&|\\|%|\\\\|'|\\)|\\*|;|:|\\?|,\",word):\n",
    "                    i=0\n",
    "                else:\n",
    "                    threegrams.append((token,adj,word))\n",
    "    return threegrams\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# show visual inspection\n",
    "# TODO Duplicates?\n",
    "text=nltk.corpus.gutenberg.words('austen-persuasion.txt')\n",
    "df=pd.DataFrame()\n",
    "df['3-grams']=threeGrams(text)\n",
    "print(threeGrams(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909bae3-7620-4611-88fb-27592e84f8c5",
   "metadata": {},
   "source": [
    "### b. Considering the output of the previous task, describe the failure cases of the regular expression – cases where the -ing version is not an adjective. Think about how you can filter those whole matched word sequences containing an -ing word that is not an adjective. Adjust your function and run it one more time. Are there other cases that do not meet the condition “3-grams that have an article, an adjective ending with -ing and a following word”? How could you adjust the function by using other methods? Describe your solution in up to 4 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83cd514-768a-4995-b5c3-91771fef111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'blessing', 'of'), ('the', 'blessing', 'of'), ('the', 'neighbouring', 'market'), ('an', 'obliging', 'landlord'), ('a', 'throwing', 'away'), ('the', 'arranging', 'of'), ('the', 'preceding', 'month'), ('the', 'intervening', 'time'), ('a', 'projecting', 'tooth'), ('a', 'degrading', 'match'), ('the', 'making', 'of'), ('the', 'existing', 'connection'), ('the', 'declining', 'year'), ('the', 'meaning', 'of'), ('a', 'gleaning', 'of'), ('an', 'adjoining', 'hedge'), ('a', 'thing', 'that'), ('a', 'withdrawing', 'on'), ('a', 'relenting', 'on'), ('the', 'resembling', 'scenes'), ('the', 'preceding', 'summer'), ('a', 'pleasing', 'face'), ('a', 'thing', 'of'), ('a', 'bewitching', 'charm'), ('an', 'increasing', 'acquaintance'), ('the', 'sitting', 'down'), ('the', 'engaging', 'mildness'), ('the', 'flowing', 'of'), ('an', 'adjoining', 'apartment'), ('the', 'preceding', 'evening'), ('an', 'increasing', 'degree'), ('a', 'mortifying', 'reception'), ('the', 'governing', 'principle'), ('the', 'distressing', 'communication'), ('the', 'lingering', 'and'), ('the', 'prevailing', 'topic'), ('a', 'roaring', 'Christmas'), ('the', 'bawling', 'of'), ('a', 'sinking', 'heart'), ('the', 'footing', 'of'), ('a', 'liking', 'formerly'), ('the', 'spring', 'months'), ('a', 'liking', 'to'), ('a', 'charming', 'woman'), ('a', 'charming', 'woman'), ('the', 'meeting', 'had'), ('the', 'interesting', 'charm'), ('the', 'blessing', 'of'), ('a', 'passing', 'emotion'), ('the', 'remaining', 'restraints'), ('the', 'following', 'autumn'), ('a', 'thing', 'into'), ('an', 'interesting', 'state'), ('the', 'meeting', 'took'), ('a', 'thing', 'here'), ('the', 'thing', 'is'), ('a', 'foreboding', 'that'), ('a', 'whispering', 'between'), ('a', 'reading', 'man'), ('the', 'rejoicing', 'sound'), ('the', 'meaning', 'of'), ('the', 'meaning', 'as'), ('an', 'interesting', 'sound'), ('a', 'penetrating', 'glance'), ('a', 'thing', 'of'), ('the', 'overflowing', 'spirits'), ('a', 'footing', 'in'), ('an', 'amusing', 'idea'), ('the', 'concluding', 'arrangements'), ('a', 'blessing', 'to'), ('the', 'reigning', 'power'), ('the', 'rising', 'sun'), ('the', 'remaining', 'dues'), ('the', 'following', 'morning'), ('a', 'gnawing', 'solicitude'), ('a', 'quivering', 'lip'), ('a', 'faltering', 'voice'), ('the', 'writing', 'table'), ('the', 'following', 'words'), ('a', 'blessing', 'indeed'), ('the', 'retarding', 'weight'), ('the', 'astonishing', 'and')]\n"
     ]
    }
   ],
   "source": [
    "def threeGrams_2(tokens:list): \n",
    "    '''\n",
    "    To find all 3-grams which consists of an article, an adjective ending \n",
    "    with -ing and a following word.\n",
    "    \n",
    "    Only allow to match these grams by regular expresseion\n",
    "    \n",
    "    **In contrast to the function above, it add a word list containing common \n",
    "    word which are ending with -ing but not adjective.**\n",
    "    \n",
    "    \n",
    "    @param: a list of tokens\n",
    "    @return: a list of all found n-grams.\n",
    "    \n",
    "    '''\n",
    "    # a array consists of words with -ing suffix but are not adjective\n",
    "    specialwords=['morning','beginning','evening','feeling','living','dancing','something','anything']\n",
    "    threegrams=[]\n",
    "    tokens=iter(tokens)\n",
    "    for token in tokens:\n",
    "        if re.search(r'\\bthe\\b|\\ban\\b|\\ba\\b',token):\n",
    "            adj=next(tokens)\n",
    "            if re.search(r'^.*ing$',adj) and adj not in specialwords:\n",
    "                    word=next(tokens)\n",
    "                    if re.search(r\"\\.|-|!|\\(|#|\\$|&|\\|%|\\\\|'|\\)|\\*|;|:|\\?|,\",word):\n",
    "                        i=0\n",
    "                    else:\n",
    "                        threegrams.append((token,adj,word))\n",
    "    return threegrams\n",
    "\n",
    "# show visual inspection\n",
    "# TODO Duplicates?\n",
    "text=nltk.corpus.gutenberg.words('austen-persuasion.txt')\n",
    "df=pd.DataFrame()\n",
    "df['3-grams']=threeGrams_2(text)\n",
    "print(threeGrams_2(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e5721-f74c-40e5-9b41-459ad6f8aad3",
   "metadata": {},
   "source": [
    "Using `spaCy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e6c5f-a9b9-459c-a0ba-b02af32c2a94",
   "metadata": {},
   "source": [
    "## Task 6.2 (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91841118-302d-4dff-9c5f-2965151aaa95",
   "metadata": {},
   "source": [
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. The Automated Readability Index (ARI) – a measure invented in\n",
    "the 1960s and not in use anymore in state-of-the-art NLP – of a text is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32580efc-8955-4cfd-96db-d64c06b79eaf",
   "metadata": {},
   "source": [
    "$$\n",
    "ARI=4.71\\mu_{\\omega}+0.5\\mu_s-24.43.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1be81-63a8-4677-8043-bf5f7b3cb812",
   "metadata": {},
   "source": [
    "Where $\\mu_\\omega$ is the average number of letters per word, and $\\mu_s$ is the average number of words per sentence in a given\n",
    "text. As a rough guide, a score of 1 (is claimed to) correspond to the reading level at an age of 6 to 8, a score of 8\n",
    "to the typical reading level of a 14 year-old US child. A score of 12 allegedly corresponds to the reading level of 17\n",
    "years-old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccadff3-9b18-4364-8a7c-cc510ee3cf91",
   "metadata": {},
   "source": [
    "### a. Compute the ARI score for every category of the Brown Corpus. Which category is the easiest to understand? Which one is the most difficult one?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2bcf5300-83d4-4ee6-b80e-1e8ac9a16466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category mysteryis the easiest to understand with ARI 0.8335518942055167\n",
      "category government is the most difficult one with ARI 9.08430349501021\n"
     ]
    }
   ],
   "source": [
    "def ari_score(mo,ms):\n",
    "    return 4.71*mo+0.5*ms-24.43\n",
    "\n",
    "mu_omega=[]\n",
    "mu_s=[]\n",
    "for cate in brown.categories():\n",
    "    totaletter=0\n",
    "    for word in brown.words(categories=cate):\n",
    "        totaletter+=len(word)\n",
    "    mu_omega.append(totaletter/len(brown.words(categories=cate)))\n",
    "    mu_s.append(len(brown.words(categories=cate))/len(brown.sents(categories=cate)))\n",
    "\n",
    "df=pd.DataFrame(index=brown.categories())\n",
    "df['mu_omega']=mu_omega\n",
    "df['mu_s']=mu_s\n",
    "df['ARI score']=df.apply(lambda x: ari_score(x['mu_omega'],x['mu_s']),axis=1)\n",
    "df=df.sort_values('ARI score')\n",
    "print('category '+df[:1].index.values[0]+'is the easiest to understand with ARI '+str(df.at[df[:1].index.values[0],'ARI score']))\n",
    "print('category '+df[-1:].index.values[0]+' is the most difficult one with ARI '+str(df.at[df[-1:].index.values[0],'ARI score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ecc45269-1f2b-43a9-8a47-ff227b752540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mu_omega</th>\n",
       "      <th>mu_s</th>\n",
       "      <th>ARI score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mystery</th>\n",
       "      <td>3.802078</td>\n",
       "      <td>14.711529</td>\n",
       "      <td>0.833552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adventure</th>\n",
       "      <td>3.829541</td>\n",
       "      <td>14.954065</td>\n",
       "      <td>1.084168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romance</th>\n",
       "      <td>3.795721</td>\n",
       "      <td>15.802753</td>\n",
       "      <td>1.349224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fiction</th>\n",
       "      <td>3.881351</td>\n",
       "      <td>16.118616</td>\n",
       "      <td>1.910474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science_fiction</th>\n",
       "      <td>3.986455</td>\n",
       "      <td>15.263713</td>\n",
       "      <td>1.978058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mu_omega       mu_s  ARI score\n",
       "mystery          3.802078  14.711529   0.833552\n",
       "adventure        3.829541  14.954065   1.084168\n",
       "romance          3.795721  15.802753   1.349224\n",
       "fiction          3.881351  16.118616   1.910474\n",
       "science_fiction  3.986455  15.263713   1.978058"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd78dba-c0e0-408f-9ebe-8e8f7edaf63e",
   "metadata": {},
   "source": [
    "### b. (1 point) Use the europarl_raw.english corpus (Sample European Parliament Proceedings Parallel Corpus, we are only interested in the English texts) to prove the hypothesis: “Speeches are easier to understand than news.” \n",
    "\n",
    "Hint: you can use news from the Brown corpus ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5eb5dc7a-2f45-488b-a74d-eaa01541b7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech score: 10.744030503190167\n",
      "news score: 7.176684595052684\n"
     ]
    }
   ],
   "source": [
    "def cal_ari(words:list,sents:list):\n",
    "    '''\n",
    "    @param corpus: \n",
    "    @return ari score\n",
    "    '''\n",
    "    totaletter=0\n",
    "    for word in words:\n",
    "        totaletter+=len(word)\n",
    "    return ari_score(mo=totaletter/len(words),\n",
    "                     ms=len(words)\n",
    "                             /len(sents))\n",
    "    \n",
    "ari_news=cal_ari(brown.words(categories='news'),brown.sents(categories='news'))\n",
    "ari_speech=cal_ari(english.words(),english.sents())\n",
    "print(\"speech score: \"+str(ari_speech))\n",
    "print(\"news score: \"+str(ari_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b164ee-b582-4e18-a3e0-0b2b6f307829",
   "metadata": {},
   "source": [
    "## Task 6.3 (6 points) \n",
    "\n",
    "The T9 system1 is used for entering text on mobile phones (see the picture below). Two or more words that are entered with the same sequence of keystrokes are known as textonyms. For example, both “hole” and “golf” are entered by pressing the sequence “4653”. Implement an SMS decoder – a function that is similar to the T9 system on mobile phones and that translates from given digit sequences to words (e.g. “96753” -> “world”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb42985-ba0a-4427-89c0-8362f5f0bd08",
   "metadata": {},
   "source": [
    "### a. (1 point) Choose at least one appropriate corpus and explain why you chose this corpus in one or two sentences. You will use the corpus to estimate which word is more frequent and should be a preferred output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d0dd9-8b92-4a24-97ed-da335b472ab8",
   "metadata": {},
   "source": [
    "Using 'nps_chat' corpus. \n",
    ">The NPS Chat Corpus, Release 1.0 consists of 10,567 posts out of approximately 500,000 posts we have gathered from various online chat services in accordance with their terms of service. \n",
    ">- Part-of-speech tagged\n",
    ">- Dialogue-act tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403b64b-9c60-4743-9e20-c15143ab7392",
   "metadata": {},
   "source": [
    "### b. (0.5 points) Define a reasonable mapping scheme for your decoder that maps characters to digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53d37af8-0262-401f-afa7-72188f5c8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping=[\n",
    "    (' '), # first element ([0]) stands for a whtiespace\n",
    "    ('@','-','_'), # [1] represent number 1 on the T9 keyboard\n",
    "    ('a','b','c'),\n",
    "    ('d','e','f'),\n",
    "    ('g','h','i'),\n",
    "    ('j','k','l'),\n",
    "    ('m','n','o'),\n",
    "    ('p','q','r','s'),\n",
    "    ('t','u','v'),\n",
    "    ('w','x','y','z')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0962e806-e837-4b4a-a5d1-77de33aba133",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={\n",
    "    '2' : '[abc]',\n",
    "    '3' : '[def]',\n",
    "    '4' : '[ghi]',\n",
    "    '5' : '[jkl]',\n",
    "    '6' : '[mno]',\n",
    "    '7' : '[pqrs]',\n",
    "    '8' : '[tuv]',\n",
    "    '9' : '[wxyz]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ed11a-27bb-46e6-b670-7071f74d388a",
   "metadata": {},
   "source": [
    "### c. (3 points) Implement a function get_t9_word(digits,dictionary) which for a given sequence of digits,\n",
    "e.g. “96753”, returns the most likely word, e.g. “world”. Multiple solutions are possible here. Your function might take another argument instead of a dictionary, but it should take the digits. You are also allowed to write and use additional functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8c67e28c-5b25-4e96-b025-739842e9d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def t9_word(digits:str,freq_dict):\n",
    "    '''\n",
    "    '''\n",
    "    # generate regular expression. Each digital represents 3 possible alphabet\n",
    "    rex='^'+''.join(mapping[dig] for dig in digits)+\"\\\\b\"\n",
    "    match_word=''\n",
    "    match_prob=0\n",
    "    for k,v in freq_dict[len(digits)].items():\n",
    "        if(re.search(rex,k,flags=re.IGNORECASE)):\n",
    "            if v > match_prob: # select the matched word with highest occurence\n",
    "                match_word=k\n",
    "                match_prob=v\n",
    "                \n",
    "     # if no match return origin digits\n",
    "    match_word=match_word if match_word else digits\n",
    "    return match_word\n",
    "\n",
    "def t9_wordlist(digitslist:list,freq_dict):\n",
    "    words=list(t9_word(digits,freq_dict) for digits in digitslist)\n",
    "    return words\n",
    "\n",
    "corpus_words=nps_chat.words()+webtext.words()\n",
    "freq=nltk.ConditionalFreqDist((len(word),word) for word in corpus_words)\n",
    "\n",
    "test='43556'\n",
    "print(t9_word(test,freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a8ab0-61de-450f-86ff-5e7d9541d8f1",
   "metadata": {},
   "source": [
    "### d. (1.5 points) Apply your decoder to the following digit sequences (the last two are sentences) and print the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a1b5deb-1038-4e83-bf00-2b3d1f0c95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '273253278' =>breakfast\n",
    "\n",
    "# '78674463' =>sunshine\n",
    "\n",
    "# '53276' =>learn\n",
    "\n",
    "# '33333'\n",
    "\n",
    "# ['4663','6676464','469','927','9687','25277']\n",
    "\n",
    "# ['273','968','3463']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbef9d-e7a6-4e09-b742-9d6dce9b33cb",
   "metadata": {},
   "source": [
    "Is the output been decoded in the right way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "930e527d-b700-42c0-94ec-115dc24b5b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"273253278\" is breakfast\n",
      "\"78674463\" is sunshine\n",
      "\"53276\" is learn\n",
      "\"33333\" is Eeeee\n",
      "['4663','6676464','469','927','9687','25277'] is  ['good', 'morning', 'how', 'was', 'your', 'class']\n",
      "['273','968','3463'] is  ['are', 'you', 'find']\n",
      "Wall time: 68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('\"273253278\" is '+ t9_word('273253278',freq))\n",
    "print('\"78674463\" is '+ t9_word('78674463',freq))\n",
    "print('\"53276\" is '+ t9_word('53276',freq))\n",
    "print('\"33333\" is '+ t9_word('33333',freq))\n",
    "print(\"['4663','6676464','469','927','9687','25277'] is \", \n",
    "      list(t9_wordlist(['4663','6676464','469','927','9687','25277'],freq)))\n",
    "print(\"['273','968','3463'] is \", \n",
    "      list(t9_wordlist(['273','968','3463'],freq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fe0ed-252a-48f2-af9c-92bb82b40395",
   "metadata": {},
   "source": [
    "'3463' is decoded as \"find\" which is probabliy a wrong result. 'Are you fine' has more sense. The reason is that the occuence of \"find\" is more frequent than \"fine\". The *t9_word* function is not contet-aware or grammar-aware, it really depends on the frequence of the occuance of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b21f8e6b-aaef-40d7-a2d6-008bc69346a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of \"fine\":0.0019405172072393172\n",
      "The probability of \"fine\":0.0027325650469288347\n"
     ]
    }
   ],
   "source": [
    "print('The probability of \"fine\":'+str(freq[len('3463')].freq('fine')))\n",
    "print('The probability of \"fine\":'+str(freq[len('3463')].freq('find')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede77f3-a02d-4861-9e5a-03abe06463a2",
   "metadata": {},
   "source": [
    "## Task 6.4 (not graded) \n",
    "This is an optional task that you might explore if you are interested. It will not count as a part of the official homework assignment. Improve the SMS decoder of homework 6.3 as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d83fd9-e711-4980-a78d-6fc3fce4d1eb",
   "metadata": {},
   "source": [
    "(a) Take the context into account, guess the word using the bigram probability of the previous entered word with the\n",
    "function get_t9_word(previous_word, digits). Test the improvement with the (context_word, digit) tuples\n",
    "in the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf7854-5450-4916-aed5-644bdfbbb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_t9_word('i', '26'))\n",
    "print(get_t9_word('its', '26'))\n",
    "print(get_t9_word('a', '3463'))\n",
    "print(get_t9_word('will', '3463'))\n",
    "print(get_t9_word('the', '22222'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce71a12-6595-4cd0-837a-05fa41491bb6",
   "metadata": {},
   "source": [
    "(b) Apply the decoder to each digit sequence in this “sentence”:\n",
    "['43556','73837','4','26','3463']\n",
    "Is the output readable? What errors have been made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3d0bc-8ef5-404a-9683-9676729abeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
