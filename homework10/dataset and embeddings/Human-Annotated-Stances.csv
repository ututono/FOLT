title,abstract,year,venue,stance
Prospects for Practical Natural Language Systems,"As the author of a ""practical"" NL data base query system, one of the suggested topics for this panel is of particular interest to me. The issue of what hurdles remain before NL systems become practical strikes particulary close to home. %s someone with a more pragmatic view of NL processing, my feeling is, not surprisingly, that we already have the capability to construct practical ~:L systems. Significant enhancement of existing manmachine communication is possible within the current NL technology if we set our sights appropriately and are willing to take the additional effort to craft systems actually worthy of being used. The missing link isn't a utopian parsing algorithm yet to be discovered. The hurdles to practical NL systems are of a much more conventional variety that require, as Edison said, more perspiration than inspiration.",1980,ACL,-0.5
Chronometric Studies of Lexical Ambiguity Resolution,"Languages such as English contain a large number of words with multiple meanings. These words are commonly termed ""lexlcal ambiguities"", although it is probably more accurate to speak of them as potentially ambiguous. Determining how the contextually appropriate reading of a word is identified presents an important and unavoidable problem for persons developing theories of natural language processing. A large body of psycholingulstlc research on ambiguity resolution has failed to yield a consistent set of findings or a general, non-controverslal theory. In this paper, we review the results of six experiments which form the basis of a model of ambiguity resolution in context, and at the same account for some of the contradictions in the existing literature.",1980,ACL,0.0
Natural Language and Computer Interface Design,"Considering the problems we have in communicating with other h~rmans using natural language, it is not clear that we want to recreate these problems in dealing with the computer. While there is some evidence that natural language is useful in communications among humans, there is also considerable evidence that it is neither perfect nor ideal. Natural language is wordy (redundant) and imprecise. Most b,*m,m groups who have a need to communicate quickly and accurately tend to develop a rather well specified subset of natural language that is highly coded and precise in nature. Pilots and police are good examples of this. Even working groups within a field or discipline tend over time to develop a jargon that minimizes the effort of communication and clarifies shared precise meanings.",1980,ACL,0.0
Representation of Texts for Information Retrieval,Representation Evaluation % YES % NO % % NO INTERM. RESP.,1980,ACL,0.0
If The Parser Fails,"The unforgiving nature of natural language components when someone uses an unexpected input has recently been a concern of several projects. For instance, Carbonell (1979) discusses inferring the meaning of new words. Hendrix, e t .a l . (1978) describe a system that provides a means for naive users to define personalized paraphrases and that l i s ts the items expected next at a point where the parser blocks. Weischedel, e t .a l . (1978) show how to relax both syntactic and semantic constraints such that some classes of ungrammatical or semantically inappropriate input are understood. Kwasny aod Sondheimer (1979) present techniques for understanding several classes of syntactically il l-formed input. Codd, e t .a l . (1978) and Lebowitz (1979) present alternatives to top-down, le f t to r igh t parsers as a means of dealing with some of these problems.",1980,ACL,0.0
Natural vs. Precise Concise Languages for Human Operation of Computers: Research Issues and Experimental Approaches,"This paper raises concerns that natural language front ends for computer systems can limit a researcher's scope of thinking, yield inappropriately complex systems, and exaggerate public fear of computers. Alternative modes of computer use are suggested and the role of psychologically oriented controlled experimentation is emphasized. Research methods and recent experimental results are briefly reviewed.",1980,ACL,0.0
Book Reviews,"To determine whether someone understands a text, such as a story, essay, or poem, he is asked questions that require him to draw inferences f rom what he has read. Since the text, questions, and answers are all in natural language, a theory of natural language understanding is not sat isfactory if it cannot support a model of how questions are answered. When linguists propose explanations for natural language, therefore, they must consider the inference procedures that will be needed to extract information from the representations in their theories.",1980,CL,0.0
Guidelines for Submissions,"The submission of a paper to the AJCL for refereeing means that the author certifies the manuscript is not copyrighted; nor has it been published or accepted for publication by any refereed journal; nor is it being refereed elsewhere. If any version of the paper has appeared, or wil l appear, in a non-refereed publication, the details of such publication must be made known to the Editor at the time of submission.",1980,CL,0.0
Announcements,"On the Difference Between Natural Language and High Level Query Languages S. Jerrold Kaplan Depar tment of Compute r and In format ion Science Moore School of Electrical Engineering University of Pennsylvania Philadelphia, Pennsylvania 19104 ACM78, Proc. 1978 Annual Conf., Dec. 1978, 27-38. Natural Language questions differ from most existing formal query languages in that they tend to admit a wider range of responses than their formal counterparts, and provide cues for selecting among the variety of appropriate responses. These differences reflect the fact that in Natural Language conversation, a respondent is expected to take an active role in the process of selecting and organizing responses, in contrast to formal query systems, where control of the interaction typically resides with the user or applications programmer. This paper explores some specific ways in which Natural Language questions are particularly well suited for their environment, and discusses the potential role of similar capabilities in formal query systems, particularly with respect to Natural Language Data Base query systems. * Edi tor ' s note: The abs t rac t s in this f irst issue are no t as "" cu r r en t "" as I would like, but , because we did not publ i sh the Journal in 1979, there is a lot of mater ia l to get caugh t up on. 54 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980",1980,CL,0.0
Program of the 1980 ACL Meeting,"The 18th Annual Meeting of the Association for Computational Linguistics and Parasession on Topics in Interactive Discourse will be held at the University of Pennsylvania, in Philadelphia, Thursday, June 19, through Sunday, June 22, 1980. Twenty-five contributed papers will be presented in four morning sessions, and a parasession panel will be held on each of the first three afternoons. The program committee was chaired by Gary Hendrix, SRI International, and consisted of Charles Fillmore, University of California, Jane Robinson, SR1 International, Ivan Sag, Stanford University, Henry Thompson, Xerox PARC, and Robert Wilensky, University of California. The parasession was organized by Bonnie Lynn Webber, University of Pennsylvania. Local arrangements are being handled by Kathleen McKeown, University of Pennsylvania. Copies of the proceedings will be available at the time of the conference.",1980,CL,0.0
On Computational Sentence Generation From Logical Form,"This paper describes some computational and linguistical mechanisms in our program written in SIMULA to generate natural language sentences from their underlying logical structures in an extended predicate logic. After the presentation of the augumented logical formalism to deal with illocutionary acts,we explain then the basic devices used in the generation process:semiotic interpretation,orders of quantifications or derivational constraints,the referential property of variables and the Leibniz-Frege idea. Examples from system output will be given.",1980,COLING,0.0
An Automatic Translation System of Non-Segmented Kana Sentences Into Kanji-Kana Sentences,"This paper p r e s e n t s t h e a l g o r i t h m s t o s o l v e t h e two main problems compr ised i n t he au tomat ic Kana-KanJi t r a n s l a t i o n sys tem, in which the i n p u t s e n t e n c e s in Kana a re t r a n s l a t e d i n t o o r d i n a r y Japanese s e n t e n c e s i n Kanj i and Kana : t he s e g m e n t a t i o n o f non-segmented s e n t e n c e s i n t o Bunsetsu and t h e word i d e n t i f i c a t i o n from homonyms. Employing this algorithm, non-segmented Kana input sentences could be automatically translated into KanJi and Kana output sentences with 96.2 per cent success.",1980,COLING,1.0
Russian-French at GETA: Outline of the Method and Detailed Example,"This paper is an attempt to present the computer models and linguistic strategies used in the current version of the Russian-French translation system developed at GETA, within the framework of several other applications which are developed in a parallel way, using the same computer system. This computer system, called ARIANE-78, offers to linguists not trained in programming an interactive environment, together with specialized metalanguages in which they write linguistic data and procedures (essentially, dictionaries and grammars) used to build translation systems. In ARIANE-78, translation of a text occurs in six steps : morphological analysis, multilevel analysis, lexical transfer, structural transfer, syntactic generation, morphological generation. To each such step corresponds a computer model (nondeterministic finite-state string to tree transducer, tree to tree transducer,...), a metalanguage, a compiler and execution programs. The units of translation are not sentences, but rather one or several paragraphs, so that the context usable, for instance to resolve anaphores, is larger than in other secondgeneration systems.",1980,COLING,0.0
Using a Natural-Artificial Hybrid Language for Database Access,"In this paper we propose a naturalartificial hybrid language for database access. The global construction of a sentence in this language is highly schematic, but allows expressions in the chosen language such as Japanese or English. Its artificial language part, SML, is closely related to our newly introduced data model, called scaled lattice. Adopting Japanese as its natural language part, we implemented a Japanese-SML hybrid language processing system for our compact database system SCLAMS, whose database consists of scaled lattices. The main features of this implementation are (i) a small lexicon and limited grammar, and (2) an almost free form in writing Kana Japanese.",1980,COLING,0.5
Formal Properties of Rule Orderings in Linguistics,"The discovery in the late 1960's that standard linguistic theory (of Chomsky's Aspects) was equivalent in generative power to unrestricted rewrite rules caused linguists to search for a ""stronger linguistic metatheory"". It seemed to some of these researchers that this meant describing linguistic theory by means of rules which were more restricted than type 0 languages. Such a view we call the L-view of constraints on linguistic theory: it advocates constraining the allowable rules in such a way that legitimate grammars can no longer generate arbitrary r.e. sets, but only some subset of them. To other researchers this discovery meant rather that one should place restrictions on linguistic theory so that the kinds of grammars allowed would be limited, regardless of whether such limitations affected the generative power of the theory. We call this the G-view of constraints. The Land G-views are not equivalent limitations. For example, a G-view limitation on the class of regular grammars that any legitimate grammar be right-embedding is not thereby a L-view limitation, since this does not effect an alteration in generative power of the grammars allowed. The G-view is avowedly psychological; according to it, the point of placing constraints on grammars is to lessen directly the language learner's burden of choosing the correct grammar from all the possible ones. For the L-view, this is a side effect of disallowing whole classes of grammars in the first place.",1980,COLING,0.0
Parsing,"One reason for the wide variety of views on many subjects in computational linguistics (such as parsing) is the diversity of objectives which lead people to do research in this area. Some researchers are motivated primarily by potential applications the development of natural language interfaces for computer systems. Others are primarily concerned with the psychological processes which underlie human language, and view the computer as a tool for modeling and thus improving our understanding of these processes. Since, as is often observed, man is our best example of a natural language processor, these two groups do have a strong commonality of research interest. Nonetheless, their divergence of objective must lead to differences in the way they regard the component processes of natural language understanding. (If when human processing is better understood it is recognized that the simulation of human processes is not the most effective way of constructing a natural language interface, there may even be a deliberate divergence in the processes themselves.) My work, and this position paper, reflect an applications orientation; those with different research objectives will come to quite different conclusions.",1981,ACL,0.0
What Makes Evaluation Hard?,"Ideally, an evaluation technique should describe an algorithm that an evaluator could use that would result in a score or a vector of scores that depict the level of performance of the natural language system under test. The scores should mirror the subjective evaluation of the system that a qualified judge would make. The evaluation technique should yield consistent scores for multiple tests of one system, and the scores for several systems should serve as a means for comparison among systems. Unfortunately, there is no such evaluation technique for natural language understanding systems. In the following sections, I will attempt to highlight some of the difficulties",1981,ACL,1.0
A View of Parsing,"The questions before this panel presuppose a distinction between parsing and interpretation. There are two other simple and obvious distinctions that I think are necessary for a reasonable discussion of the issues. First, we must clearly distinguish between the static specification of a process and its dynamic execution. Second, we must clearly distinguish two purposes that a natural language processing system might serve: one legitimate goal of a system is to perform some practical ~sk efficiently and well. while a second goal is to assist in developing a scientific understanding of the cognitive operations that underlie human language processing. 1 will refer to p a ~ r s primarily oriented towards the former goal as Practical Parsers (PP) and refer to the others as Performance Model Parsers (PMP). With these distinctions in mind. let me now turn to the questions at hand.",1981,ACL,0.0
Perspectives on Parsing Issues,"Nowhere is t h e tension between the two areas of our field--computatlon and llnguistlcs--more apparent than in the issues that arise in connection with parsing natural language input. This panel addresses those issues from both computational and linguisric perspectives. Each panelist has submitted a position paper on some of the questions that appear below. The questions are loosely grouped in three sections. The first concentrates on the computational aspect, the second on the linguistic aspect, and the third on their interactions.",1981,ACL,0.0
Some Issues in Parsing and Natural Language Understanding,"Our response to the questions posed to this panel is influenced by a number of beliefs (or biasesl) which we h a v e d e v e l o p e d i n t h e c o u r s e o f b u i l d i n g and a n a l y z i n ~ the operation of several natural language understanding (NLU) systems. [I, 2, 3, 12] While the emphasis of the panel i~ on parslnK, we feel that the recovery of the syntactic structure of a natural lan~unKe utterance must be viewed as part of a larger process of reeoverlnK the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluatln~ natural language parsers or Erem,~ra without taking into account the architecture of the whole ~LU system of which they're a part. I This is the premise from which Our beliefs arise, beliefs which concern two thinks:",1981,ACL,0.0
Technical Correspondence: A Note on the Utility of Computing Inferences in a Real Data Base Query Environment,"It is quite clear that as computer systems are made available to a wider class of more naive users than at present, the interfaces to these systems will need to have properties which have been termed ""cooperative"", ""user friendly"" and the like. One of the many desirable features of such an interface is the ability to take into account not only a user's direct input, but also things which are implied in that input, to draw inferences about knowledge assumed but not expressed. A good deal of experimental and theoretical work has gone into determining how to make inferences , what inferences to make, how to stop making inferences, etc. For recent examples, see Charniak (1978), Minsky (1975), Schank and Abelson (1977).",1981,CL,0.0
Technical Correspondence: On the Utility of Computing Inferences in Data Base Query Systems,"As Joshi and Kaplan suspected (point 3), the users of this system did indeed thoroughly understand the data base. This makes quite a difference in thinking about the relative importance of facilities in a natural language query system. In particular, such users tend to check strange answers, so that a reply of ""no"" , as in their point 4, would p robab ly result in an additional question of "" H o w many parcels ... """,1981,CL,0.0
Natural-Language Access to Databases-Theoretical/Technical Issues,"Although there have been many experimental systems for natural-language access to databases, with some now going into actual use, many problems in this area remain to be solved. The purpose of this panel is to put some of those problems before the conference. The panel's motivation stems partly from the fact that, too often in the past, discussion of natural-language access to databases has focused, at the expense of the underlying issues, on what particular systems can or cannot do. To avoid this, the discussions of the present panel will be organized around issues rather than systems.",1982,ACL,0.0
Building Non-Normative Systems - The Search for Robustness: An Overview,"The teacher consciously stonewalls when confronted with non-standard usage to prescribe quite rigidly what is acceptable linguistic usage and what is not. What is so artificial about this behavlour, of course, is that our implicit linguistic models are descriptive and not prescriptive; they model what we expect, not what we demand. People are quite good at understanding language which they, when asked, would consider to he non-standard in some way or other.",1982,ACL,-0.30000000000000004
On the Linguistic Character of Non-Standard Input,"If natural language understanding systems are ever to cope with the full range of English language forms, their designers will have to incorporate a number of features of the spoken vernacular language. This communication discusses such features as non-standard grammatical rules, hesitations and false starts due to self-correction, systematic errors due to mismatches between the grammar and sentence generator, and uncorrected true errors. There are many ways in which the input to a natural language system can be non-standard without being uninterpretable ~ Most obviously, such input can be the well-formed output of a grammar other than the standard language grammar with which the interpreter is likely to be equipped. This difference of grammar is presumably what we notice in language that we call ""non-standard"" in everyday life. Obviously, at least from the perspective of a linguist, it is wrong to think of this difference as being due to errors made by the non-standard language user; it is simply a dialect difference. Secondly, the non-standard input can contain hesitations and self-correctlons which make the string uninterpretable unless some parts of it are edited out. This is the normal state of affairs in spoken language so that any system designed to understand spoken communication, even at a rudimentary level must be able to edit its input as well as interpret it. Thirdly, the input may be ungrammatical even by the rules of the grammar of the speaker but be the expected output of the speaker's sentence generating device. This case has not been much discussed, but it is important because in certain environments speakers (and to some extent unskilled writers) regularly produce ungrammmatical output in preference to grammatically unimpeachable alternatives. Finally, the input t~at the system receives may simply contain uncorrected errors. How important this last source of non-standard input would be in a functioning system is hard to judge and would * The discussion in this paper is based an on-going study of the syntactic differences between written and of spoken language funded by the National Institute of Education under grants G78-0169 and G80-0163. depend on the environment of use. Uncorrected errors are, in our experience, reasonably rare in fluent speech but they are more common in unskilled writing. These errors may be typographical, a case we shall ignore in this discussion, or they may be grammatical. Of most interest to us are the cases where the error is due to a language user attempting to use a standard language construction that he/she does not natively command. In the course of this brief communication we shall discuss each of the above cases with examples, drawing on work we have done describing the differences between the syntax of vernacular speech and of standard writing (Kroch and Nindle, 1981). Our work indicates that these differences are sizable enough to cause problems for the acquisition of writing as a skill, and they may arise'as well when natural language understanding systems come to be used by a wider public. Whether problems will indeed arise is, of course, hard to say as it depends on so many factors. The most important of these is whether natural language systems are ever used with oral, as well as typed-in, language. We do not know whether the features of speech that we will be outlining will also show up in ""keyboard"" language; for its special characteristics have been little studied from a linguistic point of view (for a recent attempt see Thompson 1980). They will certainly occur more sporadically and at a lower incidence than they do in speech; and there may be new features of ""keyboard"" language that are not predictable from other language modes. We shall have little to say about how the problem of non-standard input can be best handled in a working system; for solving that problem will require more research. If we can give researchers working on natural language systems a clearer idea of what their devices are likely to have to cope with in an environment of widespread public use, our remarks will have achieved their purpose. Informal. generally spoken, English exists in a number of regional, class and ethnic varieties, each with its own grammatical peculiarities. Fortunately, the syntax of these dialects is somewhat less varied than the phonology so that we may reasonably approximate the situation by speaking of a general ""non-standard vernacular (NV)"", which contrasts in numerous ways with standard written English (SWE). Some of the differences between the two dialects can lead to problems for parsing and interpretation. Thus,",1982,ACL,0.0
An Improved Heuristic for Ellipsis Processing,"Several natural language systems (e.g., Bobrow et al., 1977; Hendrix et al., 1978; Kwasny and Sondheimer, 1979) include heuristics for replacement and repetition ellipsis, but not expansion ellipsis. One general strategy has been to substitute fragments into the analysis of the previous input, e.g., substituting parse trees of the elliptical input into the parse trees of the previous input in LIFER (Hendrix, et al., 1978). This only applies to inputs of the same type, e.g., repeated questions.",1982,ACL,0.0
Scruffy Text Understanding: Design and Implementation of 'Tolerant' Understanders,"Most large text-understanding systems have been designed under the assumption that the input text will be in reasonably ""neat"" form, e.g., newspaper stories and other edited texts. However, a great deal of natural language texts e.g.~ memos, rough drafts, conversation transcripts~ etc., have features that differ significantly from ""neat"" texts, posing special problems for readers, such as misspelled words, missing words, poor syntactic constructlon, missing periods, etc. Our solution to these problems is to make use of exoectations, based both on knowledge of surface English and on world knowledge of the situation being described. These syntactic and semantic expectations can be used to figure out unknown words from context, constrain the possible word-senses of words with multiple meanings (ambiguity), fill in missing words (elllpsis), and resolve referents (anaphora). This method of using expectations to aid the understanding of ""scruffy"" texts has been incorporated into a working computer program called NOMAD, which understands scruffy texts in the domain of Navy messages.",1982,ACL,0.6000000000000001
Problems With Domain-Independent Natural Language Database Access Systems,"Zn the past decade, a number of natural language database access systems have been constructed (e.g. Hendrix 1976; Waltz et e l . 1976; Sacerdot i 1978; Harr is 1979; Lehner~ and Shwartz 1982; Shvartz 1982). The level of performance achieved by natural language database access systems var ies considerably, with the sore robust systems operat ing v i t h tn a narrow domain ( i . e . , content area) and re ly ing heavi ly on domain-specif i c knowledge to guide the language understanding process. Transport ing a system constructed for one domain into a new domain is extremely resource-intensive because a new set of domain-specif ic knowledge must be encoded.",1982,ACL,0.0
On the Present,"There are certainly many reasons. One is the attractiveness of our most abstract theories. They are widely presented and receive the most scholarly attention. The popular and technical press contributes by pub1~cizing our w~]der claims and broadest hopes. ~imilar]y, the press oversells our current systems, leading more careful observers to wonder even about these. Finally, mechanizing the understanding of natural language ~s very difficult. We can not hope to achieve many of our goals in the near future. Making do with the technology now available is very frustrating. All this contributes to we the members of the field gravitating to theorizing and small laboratory studies. We are choosing to focus on the £uture rather than the present. There is a real danger in this state of affairs. The build up of public and institutional expectations without a corresponding emergence of useful systems will produce a counter reaction. We have seen it before. To this day, machine translation research in the United States has not completely recovered. There is more need than ever~ there is more technology than before, word processing and computer typesetting have changed the price equation, but it is stilS not considered wise to be associated with MT. We can not let this sort of reversal happen to us again. Fortunately, we need not.",1982,ACL,0.0
Man-Assisted Machine Construction of a Semantic Dictionary for Natural Language Processing,ion of the semantic information from the semantic graph and embedding of the meaning of the headword in the frame (Par t ia l de f in i t i on of the headword) (A set of par t ia l def in i t ions),1982,COLING,0.0
Syntactic Privilege,"This pape r i s a d d r e s s e d t o the v iew o f Schank and B i r n baum (1981) t h a t s y n t a x has no "" p r i v i l e g e d ' p o s i t i o n i n p a r s i n g . E v i d e n t l y what i s meant i s ( a ) t h a t s y n t a c t i c p a r s i n g has no logical or temporal priority over semantic processing, and (b)that syntax has been as@igned attention far out of proportion to its interest or distinctiveness. (The latter is not asserted outright, but seems implicit in the overall tone of the discussion.) In the view of the authors (henceforth ""SB'), syntactic considerations come into play in sentence u~derstandi~E only where it is needed to resolve indeterminacies. It is this view that I wish to subject to scrutiny.",1982,COLING,0.0
Proposals for a Hierarchy of Formal Translation Models,"From the methodological point of view it appears diffloult to delineate a borderline between translation theory and modern theoretical lingu~stlos (availing itself of model theoretical semantics) or full natural language understanding systems as developed in Artificial Intelligence research. It seems plausible to postulate that any prospective translation theory should draw on ideas from both fields. Unfortunately, problems discussed in painstaking detail in linguistics like differences in quantifier scope appear to be of lesser concern to a translator (since these ambiguities may well remain present in the target language) , neither seems a full or deep understanding necessary in many cases, standard syntactic phrasing may suffice. More specifically, we regard the problems of disambi~ation, mandatory insertion of lexical items not conventionally implied in the source language and coreference/anaphora resolution as the crucial problem areas of machine translation.",1982,COLING,0.0
Components of Semantic Representation,"The problem of the meaning of the text the end and product of speech activity, Is a central one both for theoretIcal linguistics, and for the applied fields connected with itp such as artificial intelligence, question answ6ring, man-machlne communication, etc. The meaning of the text or, speaking more technically, the semantic representation is not, as it has been considered until recently , a homogeneous entity, that is why a specification of its main dissimilar components may be helpful for theoretical and applied investigations. Without striving for completeness and systematlcness I will enumerate some of thesecomponents. To avoid a possible misunderstanding it should be emphasized that I proceed from an assumption, which is far from being shared by everyone, that the semantic representation of an utterance has to reflect the complete Info~nation pertaining to the proper interpretation of thls utterance, In connection with which I would regard the opposition of semantics and prsgmatlcs as invalid. a. Sltustlonal component is that part of the semantic representation which is intended (in the norm) of express the main information content mapping some external (in relation to the message as such and to the speech act) situation. This component, undoubtedly, occupies the most prestigious position in the hierarchy of the components. It is no mere chance",1982,COLING,0.0
Procedural Meaning Representation by Connotative Dependency Structures. An Empirical Approach to Word Semantics for Analogical Referencing,"The c o n c e p t o f ' r e p r e s e n t a t i o n o f k n o w l e d g e ' seems l u c i d e n o u g h when talking about memories of sentences, numbers,or even faces, for one can imagine how to formulate these in terms of propositions, frames, or semantic networks. But it is much harder to do this for feelings, insights and understandings, with all the attitudes, dispositions, and 'ways of seeing things' that go with them. [The term 'disposition' is used here in its ordinary language sense to mean 'a momentary range of possible behaviours']) Traditionally, such issues are put aside, with the excuse that we should understand simpler things first. But what if feelings and viewpoints a r e the simpler things the elements of which the others are composed? Then, I assert, we should deal with dispositions directly, using a 'structural' approach ... (2) In the present case this has been developed in two stages: the semantic space as a distance-like data structure, and an algorithm to transform its distance relations to form source-oriented hierarchies of connotative dependency structures. SEMANTIC SPACE STRUCTURE Theoretical approaches in formal semantics tend to deny a dynamic linguistic meaning structure, but assume the existence of an external system structure of a world, o~ possible worlds, whose pre-formatted entities may referentially be related to language terms constituting their denotation. Structural approaches in linguistic semantics tend to deny the possibility of denotational, but presuppose the knowledge (and comprehension) of language systems whose semantic relations among their items are being described intra-lingually by means of syntagmatic and paradigmatic oppositions along certain dimensions in semantic fields. Other than these two, our present way of approach strives to presuppose as little and to reconstruct empirically as much as possible of the relational (not necessarily logically reconstructable) structure that in the course of discourse is constituted by the regular use of language terms as a system of linguistically labeled empirical objects, called meanings. We consider the natural language users' ability to intend and comprehend meanings in verbal interaction a phenomenologically undoubtable, empirically well established, and theoretically defensible basis for any semantic study of natural language performance. It is assumed that the usage regularities followed and/or established by employing different lexical items differently for communicative purposes in discourse may be analysed not only to describe the lexical structure of vocabulary items used, but also to model a fragment of the conEMPIRICAL APPROACH TO WORD SEMANTICS FOR ANALOGICAL INFERENCING 321 comitantly conveyed common knowledge or semantic memory structure constituted. This is achieved by an algorithm that takes lemmatized strings of natural language disc6urse of a certain domain as input and produces as output a distance-like data structure of linguistically labeled points whose positions represent their meanings. As the statistical means for the empirical analysis of prevailing interdependencies between lexical items in text strings have elsewhere (3) been developed and discussed to some extent (4) 9 and as the formal representation of vague word meanings derived from these analyses has previously (5) been outlined and illustrated~ too (6), an informal description will suffice here. The algorithm applied so far consists of a consecutive mapping of lexical items onto fuzzy subsets of the vocabulary according to the numerically specified statistical regularities and the differences~ these items have been used with in the discourse analysed. The resulting system of sets of fuzzy subsets may be interpreted topologically as a n-dimensional hyperspace with a natural metric. Its n linguistically labeled elements (representing meaning points) and their mutual distances (representing meaning differences) form discernable clouds and clusters (7). These determine the overall structuredness of a domain by measurable semantic (paradigmatic and/or syntagmatic) properties of the lexical items concerned. CONNOTATIVE DEPENDENCY STRUCTURE Stimulated by the theory of spreading activation in memory models (8) in conjunction with the psyhhological account of language understanding in procedural semantics (9) a dynamic meaning representation can be developed of the basis of the prototypical, but static representations provided by the semantic hyperspace strucure. This is mchieved by a recursively defined algorithm which has formally been introduced elsewhere (Io) so that it may verbally be described here as a procedure to generate a potential of latent relations among meaning points in the semantic space. In a way9 this procedure reconstructs for this model what recent theories of cognition and language comprehension have introduced in network models of semantic memory: paths of excitation that may be activated from any primed node and which spread along node relating links over the whole network with decreasing intensities. Compared to the execution of spreading activation processes in network models~ however, the present procedure speaking in model genetical terms must be considered of prior status. The semantic hyperspace is not a transitively related network of nodes, but a symmetrically related data structure of linguistically labeled n-tuples of numerical values. Therefore, priming of any item would immediately activate every other item rendering the process of spreading activation undiscriminating for semantic representation. So, the new procedure, first, has to establish links between items and evaluate them by processing the data base provided in order to let these links eventually serve as directed paths along which possible activation might spread. Operating on the distance-like data of the semantic space, the algorithm's generic procedure will start with any meaning point being primed to determine those two other points, the sum of distances between which form a triangle of minimum edges' lengths. Repeated successively for each of these meaning points listed and in turn primed in accordance with this procedure, particular fragments of the relational structure inherent in the semantic space will be selected depending on the aspect, i.e. the primed point the algorithm is initially started with. Working its way through and consuming all labeled points in the space system, the procedure transforms prevailing similarities of",1982,COLING,0.0
An Improper Treatment of Quantification in Ordinary English,"In the currently standard ways of representing quantification in logical form, this sentence has 120 different readings, or quantifier scopings. Moreover, they are truly distinct, in the sense that for any two readings, there is a model that satisfies one and not the other. With the standard logical forms produced by the syntactic and semantic translation components of current theoretical frameworks and implemented systems, it would seem that an inferencing component must process each of these 120 readings in turn in order to produce a best reading. Yet it is obvious that people do not entertain all 120 possibilities, and people really do understand the sentence. The problem is not Just that inferencing is required for disamblguation. It is that people never do dlsambiguate completely. A single quantifier scoping is never chosen. (Van Lehn [1978] and Bobrow and Webber [1980] have also made this point.) In the currently standard logical notations, it is not clear how this vagueness can be represented. 1",1983,ACL,-1.0
Letters to the Editor: Re Ballard on the Need for Careful Description,"I think, like the reviewer of Bruce Ballard 's previous paper, that he wants the moon. However , as one who has tried, for the purposes of lecturing, to extract concrete system descriptions with workedthrough examples f rom published material , I think he is right to call for bet ter standards. Alan Bundy (1981) has made a similar appeal for AI in general. I t ' s impor tant , in part icular, to apprec ia te that raising the s tandard of report ing raises the s tandard not only of the reader ' s work but that of the writer 's: anyone who is obliged to provide a coheren t account of a set of exper iments soon discovers which ones he hasn ' t done and needs to do, sometimes forthwith before continuing writing. The problem Ballard does not face, and should say something about, is the scale impact of his proposal: providing everything called for in useful, if not exhaustive, detail, is liable to generate very long papers. One can indeed plough through whole theses intended in principle, and not conspicuously failing in practice, to provide what Ballard calls for, and still not find sufficient evidence of what has been done and, more importantly, how it has been done. How much grammar, and more significantly, how much dict ionary, should you put in to support your description and claims for per formance? Ballard would give much more meat to his case if he provided some concrete examples of papers he feels comes closest to what he is asking for, with some comments on their successes and failures. How well, to take some random examples , do Wal tz ' s (1978) PLANES paper, Erman et al. (1980) on HearsayI I , or Warren and Pereira 's (1982) Chat -80 measure up, or, on a larger scale, Woods ' s (1972) LUNAR report? But perhaps the correct response to Ballard 's suggestion is to ask him to take some system and provide the kind of account of it he is looking for. Show us the way, friend.",1983,CL,0.0
Letters to the Editor: On the Need for Studying Ill-Formed Input,"I think, like the reviewer of Bruce Ballard 's previous paper, that he wants the moon. However , as one who has tried, for the purposes of lecturing, to extract concrete system descriptions with workedthrough examples f rom published material , I think he is right to call for bet ter standards. Alan Bundy (1981) has made a similar appeal for AI in general. I t ' s impor tant , in part icular, to apprec ia te that raising the s tandard of report ing raises the s tandard not only of the reader ' s work but that of the writer 's: anyone who is obliged to provide a coheren t account of a set of exper iments soon discovers which ones he hasn ' t done and needs to do, sometimes forthwith before continuing writing. The problem Ballard does not face, and should say something about, is the scale impact of his proposal: providing everything called for in useful, if not exhaustive, detail, is liable to generate very long papers. One can indeed plough through whole theses intended in principle, and not conspicuously failing in practice, to provide what Ballard calls for, and still not find sufficient evidence of what has been done and, more importantly, how it has been done. How much grammar, and more significantly, how much dict ionary, should you put in to support your description and claims for per formance? Ballard would give much more meat to his case if he provided some concrete examples of papers he feels comes closest to what he is asking for, with some comments on their successes and failures. How well, to take some random examples , do Wal tz ' s (1978) PLANES paper, Erman et al. (1980) on HearsayI I , or Warren and Pereira 's (1982) Chat -80 measure up, or, on a larger scale, Woods ' s (1972) LUNAR report? But perhaps the correct response to Ballard 's suggestion is to ask him to take some system and provide the kind of account of it he is looking for. Show us the way, friend.",1983,CL,0.0
English and the Class of Context-Free Languages,"Let L range over all natural languages (NLs). For any L, one can consider two collections of strings of symbols, one consisting of all strings over the terminal vocabulary of L, call it W*(L), the other consisting of that always very proper subcollection of W*(L) consisting of all and only those members of W*(L) that are well-formed, that is, that correspond to sentences of L. Call the latter collection WF(L). During the early development of generative grammar, a number of attempts were made to show, for various choices of L, that WF(L) was not a context-free (CF) string collection. These attempts all had, naturally, a common logical structure. First, it was claimed that there was some mathematical property P which, if possessed by some collection of strings, C, rendered C non-CF. Second, it was claimed that WF(C) had P, so the conclusion followed. Two sorts of criticisms can be, and have ‚Ä¢ been, directed at such attempted demonstrations. One attacks the mathematical foundations and argues, for particular choices of P, that a collection manifesting P is not necessarily not CF. The other type of criticism admits that if a collection manifests a particular property P, it is thereby not CF, but contends that the WF(L)s claimed to manifest P in fact don't. A survey of the various attempts, from roughly 1960 to 1982, to prove for various L that WF(L) is not CF is provided in Pullum and Gazdar (1982). These authors conclude, justifiably we believe, that for one or the other of the reasons mentioned above, none of these attempts, including those by the present authors, stand up. Despite widespread belief to the contrary, as of 1982 there had been no demonstration that there is some NL L for which W F ( L ) is not CF. 2 However, Langendoen and Postal (1984) have obtained a result infinitely stronger than the claim that for some L, WF(L) is not CF. This work shows that for any L, WF(L) is a proper class, hence not a set, much less a recursively enumerable set. There is thus no question of WF(L) being CF. Moreover, WF(L) can then have no constructive characterization (generative grammar), although there is no reason to doubt that it can be given a nonconstructive characterization. But the demonstration of Langendoen and Postal (1984) is based on principles that determine WF(L) includes nonfinite strings corresponding to nonfinite (transfinite) sentences. It is the existence of such sentences that places complete NLs beyond generative (constructive) characterization. Nevertheless, as noted in Langendoen and Postal (1984: 103), this novel result still leaves entirely open the question of whether that subpart of WF(L) consisting of all and only the well-formed finite strings in W*(L) is CF. Let F(inite)WF(L) be that subcollection of WF(L) consisting of all and only the finite strings corresponding to the finite sentences of L. What follows shows that there are dialects of English, E1 and E2, such that:",1984,CL,0.0
Comments on Pullum's Criticisms,"We are reasonably convinced by the remarks in Pullum (p. 182) that the sluicing construction, on which the argument in Postal and Langendoen (p. 177) for the non-CFhood of English is based, is not, contrary to what is assumed there, subject to an intrasentential matching condition. Consequently, the argument does not go through. However, we believe that an analogous argument can be given, which is not subject to Pullum's line of criticism. Consider the somewhat stilted but unquestionably grammatical doubling relative construction illustrated by (1).",1984,CL,-0.6000000000000001
On the Mathematical Properties of Linguistic Theories,"The development of new formalisms for expressing linguistic theories has been accompanied, at least since Chomsky and Miller's early work on context-free languages, by the study of their metatheory. In particular, numerous results on the decidability, generative capacity, and, more recently, the recognition complexity of these formalisms have been published (and rumored!). This paper surveys some of these results and discusses their significance for linguistic theory. However, we will avoid entirely the issue of whether one theory is more descriptively adequate than another. We will consider context• free, transformational, lexical-functional, generalized phrase structure, tree adjunct, and stratificational grammars) Although this paper focuses on metatheoretic results as arbiters among theories as models of human linguistic capacities, they may have other uses as well. Complexity results could be utilized for making decisions about the implementation of parsers as components of computerbased language-understanding systems. However, as Stanley Peters has pointed out, no one should underestimate""the pleasure to be derived from ferreting out these results! 3 2. Preliminary Definitions",1984,CL,0.0
Abstracts of Current Literature,"s of Current Literature The FINITE STRING Newsletter Abstracts of Current Literatures of Current Literature The following abstracts are from the papers selected for presentation at Coling84 the 10th International Conference on Computational Linguistics and the 22nd Annual Meeting of the Association for Computational Linguistics to be held 2-6 July 1984 at Stanford University, California. Proceedings of Coling84, $30 a copy, is available from Donald E. Walker, ACL Bell Communications Research 445 South Street Morristown, NJ 07960 Multil ingual Text Processing in a TwoByte Code Lloyd B. Anderson Ecological Linguistics 316 A Street, S.E. Washington, DC 20003",1984,CL,0.0
"Strong Generative Capacity, Weak Generative Capacity, and Modern Linguistic Theories","What makes a language a natural language? A longstanding tradition in generative grammar holds that a language is natural just in case it is learnable under a constellation of auxiliary assumptions about input evidence available to children. Yet another approach seeks some key mathematical property that distinguishes the natural languages from all possible symbol-systems. With some exceptions for example, Chomsky's demonstration that a complete characterization of our grammatical knowledge lies beyond the power of finite state languages the mathematical approach has not provided clear-cut results. For example, for a variety of reasons we cannot say that the predicate is context-free characterizes all and only the natural languages. Still another use of mathematical analysis in linguistics has been to diagnose a proposed grammatical formalism as too powerful (allowing too many grammars or languages) rather than as too weak. Such a diagnosis was supposed by some to follow from Peters and Ritchie's demonstration that the theory of transformational grammar as described in Chomsky's Aspects of the Theory of Syntax could specify grammars to generate any recursively enumerable set. For some this demonstration marked a watershed in the formal analysis transformational grammar. One general reaction (not prompted by the Peters and Ritchie result alone) was to turn to other theories of grammar designed to explicitly avoid the problems of a theory that could specify an arbitrary Turing machine computation. The proposals for generalized phrase structure grammar (GPSG) and lexical-functional grammar (LFG) have explicitly emphasized this point. GPSG aims for grammars that generate context-free languages (though there is some recent wavering on this point; see Pullum 1984); LFG, for languages that are at worst context-sensitive. Whatever the merits of the arguments for this restriction in terms of weak generative capacity and they are far from obvious, as discussed at length in Berwick and Weinberg (1983) one point remains: the switch was prompted by criticism of the nearly two-decades old Aspects theory. Much has changed in transformational grammar in twenty years. Modern transformational grammars no longer contain swarms of individual rules such as Passive, Raising, or Dative. The modern government-binding (GB) theory does not reconstruct a ""deep structure"", does not contain powerful deletion rules, and has introduced a whole host of new constraints. Given these sweeping changes, it would seem appropriate, then, to re-examine the Peters and Ritchie result, and compare the power of the newer GB-style theories to these other current linguistic theories. That is the aim of this paper. The basic points to be made are these:",1984,CL,0.0
On Parsing Preferences,"It is argued that syntactic preference principles such as Right Association and Minimal Attachment are unsatisfactory as usually formulated. Among the difficulties are: (I) dependence on ill-specified or implausible principles of parser operation; (2) dependence on questionable assumptions about syntax; (3) lack Of provision, even in principle, for integration with semantic and pragmatic preference principles; and (4) apparent counterexamples, even when discounting (I)-(3). A possible approach to a solution is sketched. I. Some preference principles The following are some standard kinds of sentences illustrating the role of syntactic preferences. (I) John bought the book which I had selected for Mary (2) John promised to visit frequently (3) The girl in the chair with the spindly legs looks bored (4) John carried the groceries for Mary (5) She wanted the dress on that rack (6) The horse raced past the Darn fell (7) The boy got fat melted (I) (3) illustrate Right Association of PP's and adverbs, i.e., the preferred association of these modifiers with the rightmost verb (phrase) or noun (phrase) they can modify (Kimball 1973). Some variants of Right Association (also characterized as Late Closure or Low Attachment) which have Dean proposed are Final Arguments (Ford et al. 1982) and Shifting Preference (Shieber 1983); the former is roughly Late Closure restricted to the last obligatory constituent and any following optional constituents of verb phrases, while the latter is Late Closure within the context of an LR(1) shiftreduce parser. Regarding (4), it would seem that according to Right Association the PP for Mar~ should be preferred as postmodifier of groceries rather than carried; yet the opposite is the case. Frazier & Fodor's (1979) explanation is based on the assumed phrase structure rules VP -> V NP PP, and NP -> NP PP: attachment of the PP into the VP minimizes the resultant number of nodes. This principle of Minimal Attachment is assumed to take precedence over Right Association. Ford et al's (1982) variant is Invoked Attachment, and Shieber's (1983) variant is Maximal Reduction; roughly speaking, the former amounts to early closure of no___nn-final constituents, while the latter chooses the longest reduction among those possible reductions whose initial constituent is ""strongest"" (e.g., reducing V NP PP to VP is preferred to reducing NP PP to PP). In (5), Minimal Attachment would predict association of the PP on that rack with wanted, while the actual preference is for association with dress. Both Ford et al. and Shieber account for this fact by appeal to lexical preferences: for Ford et al., the strongest form of want takes an NP complement only, so that Final Arguments prevails; for Shieber, the NP the dress is stronger than wanted, viewed as a V requiring NP and PP complements, so that the shorter reduction prevails. sentence (6) leads most people ""down the garden path"", a fact explainable in terms of Minimal Attachment or its variants. The explanation also works for (7) (in the case of Ford et al. with appeal to the additional principle that re-analysis of complete phrases requiring re-categorization of lexical constituents is not possible). Purportedly, this is an advantage over Marcus' (1980) parsing model, whose three-phrase buffer should allow trouble-free parsing of (7). 2. Problems with the preference principles 2.1 Dependence on ill-specified or implausible principles of parser operation. Frazier & Fodor's (1979) model does not completely specify what structures are built as each new word is accommodated. Consequently it is hard to tell exactly what the effects Of their preference principles are. Shieber's (1983) shift-reduce parser is welldefined. However, it postulates complete phrases only, whereas human parsing appears to involve integration of completely analyzed phrases into larger, incomplete phrases. Consider for example the following sentence Deginnings: (8) So I says to the ... (9) The man reconciled herself to the ... (10) The news announced on the ... (11) The reporter announced on the ... (12) John beat a rather hasty and undignified ... People presented with complete, spoken sentences beginning like (8) and (9) are able to signal detection of the errors about two or three syllables after their occurrence. Thus agreement",1984,COLING,0.0
Is There Natural Language after Data Bases?,"1. Why Not Data Base Query? The undisputed favorite application for natural language interfaces has been data base query. Why? The reasons range from the relative simplicity of the task, including shallow semantic processing, to the potential real-world utility of the resultant system. Because of such reasons, the data base query task was an excellent paradigmatic problem for computational linguistics, and for the very same reasons it is now time for the field to abandon its protective cocoon and progress beyond this rather limiting task. But, one may ask, what task shall then become the new paradigmatic problem? Alas, such question presupposes that a single, universally acceptable, syntactically and semantically challenging task exists. I will argue that better progress can be made by diversification and focusing on different theoretically meaningful problems, with some research groups opting to investigate issues arisinq from the development of integrated multi-purpose systems.",1984,COLING,0.0
What Not to Say,"A problem with most text production and language generation systems is that they tend to become rather verbose. This may be due to negleetion of the pragmatic factors involved in communication. In this paper, a text production system, COMMENTATOR, is described and taken as a starting point for a more general discussion of some problems in Computational Pragmatics. A new line of research is suggested, based on the concept of unification.",1984,COLING,0.30000000000000004
There Still Is Gold in the Database Mine,"Let me state clearly at the outset that I disagree with the premise that the problem of interfacing to database systems has outlived its usefulness as a productive environment for NL research. But I can take this stand strongly only by being very liberal in defining both ""natural language interface"" and ""database systems"". same as ""Are there any vice presidents who are either male or female"". This same system, when asked for all the Michigan doctors and Pennsylvania dentists, produced a list of all the people who were either doctors or dentists and who lived in either Michigan or Pennsylvania. This is the state of our art?",1984,COLING,-0.5
"A Survey of Machine Translation: Its History, Current Status and Future Prospects","Elements of the history, state of the art, and probable future of Machine Translation (MT) are discussed. The treatment is largely tutorial, based on the assumption that this audience is, for the most part, ignorant of matters pertaining to translation in general, and MT in particular. The paper covers some of the major MT R&D groups, the general techniques they employ(ed), and the roles they play(ed) in the development of the field. The conclusions concern the seeming permanence of the translation problem, and potential re-integration of MT with mainstream Computational Linguistics.",1985,CL,0.0
Automated Translation at Grenoble University,"The authors thank Dr. Slocum for the opportunity to present the work on machine translation at Grenoble. The plan he has proposed for the contributions to this special issue was certainly a very good starting point, as a common frame to present various systems around the world. It is, however, inevitable that we could not completely fit into it, so that we have sometimes taken some liberty for which we hope to be excused.",1985,CL,0.0
Machine Translation already does Work,"The first difficulty in answering a question like ""Does machine translation work is that the question itself is illposed. It takes for granted that there is one single thing called machine translation and that everyone is agreed about what it is. But in fact, even a cursory glance at the systems already around, either in regular operational use or under development, will reveal a wide range of different types of systems.",1986,ACL,0.0
Encoding and Acquiring Meanings for Figurative Phrases,"1.1 The Task D o m a i n Here we address the problem of mapping phrase meanings into their conceptual representations. Figurative phrases are pervasive in human communication, yet they are difficult to explain theoretically. In fact, the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation. Due to the huge number of such phrases in the English language, phrase representation must be amenable to parsing, generation, and also to learning. In this paper we demonstrate a semantic representation which facilitates, for a wide variety of phrases, both learning and parsing.",1986,ACL,0.5
What Should Machine Translation Be?,"After a considerable hiatus of interest and funding, machine translation has come in recent years to occupy a significant place in the discipline of natural language processing. It has also become one of the most visible representations of natural language processing to the outside world. Machine translation systems are relatively unique with respect to the extent of the coverage they at tempt , and, correspondingly, the size of the grammatical and lexicaI corpora involved. Adding to this the complexity introduced by multiple language directions into the same system design (and the enormous procedural problems imposed by simultaneous development in several sites) gives some clue as to the opt imism which presently exists for machine translation.",1986,ACL,0.0
Machine Translation will not Work,"Large expenditures on fundamental scientific research are usually limited to the hard sciences. It is therefore entirely reasonable to suppose that, if large sums of money are spent on machine translations, it will be with the clear expectation that what is being purchased is principally development and engineering, and that the result will contribute substantially to the solution of some pressing problem.",1986,ACL,0.0
Linguistic Coherence: A Plan-Based Alternative,"To fully unders tand a sequence of u t terances , one must be able to infer implici t re la t ionships be tween the ut terances . Al though the ident i f ica t ion of sets of u t terance relat ionships forms the basis for many theories of discourse, the formal iza t ion and recognition of such relat ionships has proven to be an ext remely difficult computa t iona l task. This paper presents a plan-based approach to the representa t ion and recognit ion of implici t re la t ionships be tween ut terances. Relat ionships are formulated as discourse plans, which allows their representation in terms of planning opera tors and their computation via a plan recogni t ion process. By incorpora t ing complex inferent ia l processes relat ing u t te rances into a plan-based f ramework , a formal iza t ion and computability not available in the ear l ie r works is provided.",1986,ACL,0.5
Book Reviews: Conceptual Structures,"inferences are generated, how many inferences are generated, and what knowledge sources contribute to the generation of inferences. In their book Structures and Procedures of Implicit Knowledge, Graesser and Clark, two psychologists, attempt to answer these questions by presenting a model of comprehension that primarily focuses on knowledgebased inferences (viz. products of what the comprehender knows about the world). The likelihood of a particular inference depends on the content of the inference, together with (a) the text 's context, (b) world knowledge structures and inference engines that are available, (c) the goals of the comprehender, and (d) the pragmatic context of the communication act. Admittedly, this is too much to handle at once, and the authors do not wait until the last section of the book to clearly state the goals and limitations of their work. There is no discussion of syntactic parsing, no formal theory of meaning, and no pragmatic model. This is not a book about linguistics but rather about conceptual modeling and, specifically, about the generation and usage of knowledge-based inferences during text comprehension. The authors are quick to point out that formal work in linguistics, logic, and philosophy, as well as AI research, ignore "" important characteristics .of human cognition"". Implicitly, researchers in those 'fields are invited to momentarily leave their idealistic vacuums or their Lisp code in order to refresh their knowledge about text comprehension and psychological plausibility. Though the book does not present an exhaustive survey of the available research on inferences and text comprehension, its concise discussion in the first chapter of inference taxonomies and engines and its rich bibliography make it an excellent reference. Text comprehension is extremely complex, and the authors can only offer a very partial yet quite interesting solution: they propose procedures to model comprehension, recall, summarization, and question answering. These procedures work on generic knowledge structures (represented by conceptual graphs) that they traverse and match in order to generate the inferences that make the text coherent, as well as other inferences that capture the comprehender 's expectations. From a computational point of view, since there is no implementation of the model, the discussion may sometimes appear superficial. Also, Graesser and Clark too often claim without any further explanation that their model includes previous work. As is usually the case for this domain of research, since the stories analyzed must minimize the role of the components left out of the model, the reader is confronted with truly artificial and insipid texts. Finally, be forewarned! The authors present a generous amount of statistics obtained from numerous experiments; a great deal of time is spent analyzing the data and defending the methodology. The reader may often want to skip to the end of chapters, where good summaries of the results and conclusions are provided. Jean-Pierre Corriveau Department of Computer Science University of Toronto Toronto, Ontario Canada M5S 1A4",1986,CL,0.0
Is MT Linguistics?,"Reading Jonathan Slocum's survey article of machine translation in Computational Linguisties, Volume 11, No. 1, some points struck me. Slocum's paper aims to be an overview of the state of the art in MT. As such it is good work. But some methodological points of view could have been considered more accurately; for example, the relation of MT and the computational linguistic paradigm to linguistics (seen as a study of the nature of human language), and the notion ""fully automatic high quality translation"". It was a stunning surprise to learn that translation as a profession and discipline is so underweighted in the U.S.A. This, coupled with the state of linguistic theory and the computational devices at hand, surely explains almost in itself the failure of early attempts made in machine translation in the 1950s and 1960s in the U.S.A. If one's premises are bad, one's methods will not be any better and results still worse. And this really seems to be the situation in the history of MT (cf. Zarachnak 1979). The second point I want to address is Slocum's argumentation in favour of level of automaticity and quality of current MT systems. As Slocum puts it, it goes something like this: human translation is a many-step process, where it is not unusual for products to revised many times. Thus far we agree. Everyone with some experience in translation knows this. And this is no wonder, considering that linguistic competence is open, without definite limits. One never ""learns"" language totally, competence is never ""perfect"" whether we speak about mother tongue or some foreign language (cf, Sampson 1980, 1983). But Slocum takes this step-by-step revision style of human translation to mean at least it seems so to me that we actually already have fully automatic high quality MT systems in some sense. Following is a crucial passage in Slocum's argumentation (p. 2)",1986,CL,-0.7000000000000001
Book Reviews: Computations from the English,"all sequential or serial order. They all presuppose a rulebased system that can be enumerated in a ""decidable"" sense and that includes hierarchical enumeration during rule-application. Even the discussion of the computational approach presented by Schank and Birnbaum includes nuances of this. There are relevant discussions of how one can acquire language and how it might map the real world into a ""usable"" representation that permits variability. However, these are presented at a theoretical level. There is not sufficient material included to ""make the theory computational"". It is very interesting to note that the processor, meaning the human brain, can constrain how the real world projects into a subjective perceptual world, and to provide discussion of same, but the issues of how such a processor might control such a happening are avoided. In contrast, for artificial intelligence, determining the process and the control presents the greatest problem. The psychological perspective provides a brief historical introduction to the issues considered important by psychologists with respect to the language problem. Many of the concerns regarding the function of representations, and the relationship between theoretical and applied work and psycholinguistics are similar to those expressed by the computational discussions. However, the stress on evaluation of theories which is of utmost importance to psychological approaches does not appear of such concern in AI. One area which the psychological discussion merely touches has to do with CONTROL of the processes. While control is mentioned, no suggestions as to how it should become a viable part of language process or theory from a psychological perspective are presented. Instead, all of the theories presuppose a sequential process, at least as they are presented in the included papers. No CONTROL alternatives are discussed, as if the problem is already solved. One other psychological paper discusses whether there are language-specific tasks, i.e., tasks that cannot be ""learned"" or performed unless one has language. While the issues raised are interesting, the presentation and discussion of the evidence is far from convincing. Perhaps it is available elsewhere, but certainly is just touched upon here. Finally, the discussion within the computational approaches to language is not anything that has not been presented many times before. Basically, two viewpoints are presented, one the semantically driven approach to language processing and the other which demonstrates that there are critical aspects of language understanding that can only be syntactically determined. The arguments presented in Marcus's paper advocating a separate role for syntactic processing were the most clearly described in the book. If anyone is seeking example inputs which require syntactic decision processes that cannot be handled entirely within semantic processing, or canriot even be recognized as a problem if one is doing Computations from the English",1986,CL,0.0
Abstracts of Current Literature,S OF CURRENT LITERATURE Copies of the technical reports abstracted below are available from,1986,CL,0.0
A Kana-Kanji Translation System for Non-Segmented Input Sentences Based on Syntactic and Semantic Analysis,"This paper presents a disambiguation approach for t ransla t ing non-segmented-Kana into Kanji. The method consists of two steps. In the first step, an input sentence is analyzed morphologically and ambiguous morphemes are stored in a network form. In the second step, the best path, which is a string of morphemes, is selected by syntactic and semantic analysis based on case grammar. In order to avoid the combinatorial explosion of possible paths, the following heuristic search method is adopted. First, a path that contains the smallest number of weighted-morphemes is chosen as the quasi-best path by a best-first-search technique. Next, the restricted range of morphemes near the quasi-best path is extracted from the morpheme network to construct preferential paths. An experimental system incorporating large dictionaries has been developed and evaluated. A translat ion accracy of 90.5% was obtained. This can be improved to about 95% by optimizing the dictionaries.",1986,COLING,1.0
A PROLOG Implementation of Government-Binding Theory,"A number of recent research efforts have explicitly grounded parser design on linguistic theory (e.g., Bayer et al. (1985), Berwick and Weinberg (1984), Marcus (1980), Reyle and Frey (1983), and Wehrli (1983)). Although many of these parsers are based on generative grammar, and transformational grammar in particular, with few exceptions (Wehrli (1983)) the modular approach as suggested by this theory has been lagging (Barton (1984)). Moreover, Chomsky (1986) has recently suggested that rule-based parsers are implausible and that parsers could be based on lexical properties and structure determining principles.",1986,COLING,0.0
Parsing Without (Much) Phrase Structure,"Approaches to NL syntax conform in varying degrees to the older relational/dependency model, (essentially that assumed in traditional grammar), which treats a sentence as a group of words united by various relations, and the newer constituent model. Some modern approaches have nonetheless involved shifts away from essentially constituent-based models of the sort associated with Bloomfield and Chomsky to more relation-based ones (e.g. case grammar, relational grammar, daughter-dependency and word grammar, corepresentational grammar) while some others, notably lexical-functional grammar, have nonetheless continued to rely crucially on certain techniques inherited from constituency-based grammar, particularly context-free grammar. In computational linguistics there is a strong (if not universal) reliance on phrase structure as the medium via which to represent syntactic structure; call this the CONSENSUS VIEW. A significant amount of effort has accordingly been invested in techniques by which to build such a representation efficiently, which has in turn led to considerable work on the formal and computational properties of context-free gramamrs (or natural extensions of them) and of the associated languages. In its strongest form, the consensus view says that the recovery of a fully specified parse tree is an essential step in computational language processing, and would, if correct, provide important support for the constituent model. In this paper, we shall critically examine the rationale for this view, and will sketch (informally) an alternative view which we find more defensible. The actual position we shall take for this discussion, however, is conservative in that we will not argue that there is no place whatever for constituent analysis in parsing or in syntactic analysis generally. What we WILL argue is that phrase structure is at least partly redundant in that a direct leap to the composition of some semantic units is possible from a relatively underspecified syntactic representation (as opposed to a complete parse tree). However, see Rindflesch forthcoming for an approach to.parsing which entails a much stronger denial of the consensus view.",1986,COLING,0.0
Q&A: Already a Success?,"When Prof. Wolfgang Wahlster (the organizer of this COLING-86 panel on ""Natural Language Interfaces: Ready for Commercial Success?"") sent out invitations to panelists, he stated that his goals were ""to evaluate three natural language interfaces which were introduced to the commercial market in 1985 and to relate them to current research in computational linguistics."" For comparison, he has asked each of us panelists to answer a standard set of questions. These I will answer, but first let me set the stage by answering two logically prior queries.",1986,COLING,0.0
Linguistic Bases For Machine Translation,"Researchers in MT do not work with linguistic theories which are 'on vogue' today. The two special issues on MT of the journal Computational Linguistics (CL 1985) contain eight contributions of the leading teams. In the bibliography of these articles you don't find names like Chomsky, Montague, Bresnan, Gazdar, Kamp, Barwise, Perry etc.[2] Syntactic theories like GB, GPSG, LFG are not mentioned (with one exception: R. Johnson et al. (1985 0.165) praise I.FG for its 'perspicuous notation', but do not (or not yet) incorporate ideas from LFG into their theory of MT). There arc no references whatsoever to recent semantic theories.",1986,COLING,0.0
A Logical Version of Functional Grammar,"Kay's functional-unification grammar notation [5] is a way of expressing grammars which relies on very few primitive notions. The primary syntactic structure is the feature structure, which can be visualised as a directed graph with arcs labeled by at t r ibutes of a constituent, and the primary structure-building operation is unification. In this paper we propose a mathematical formulation of FUG, using logic to give a precise account of the strings and the structures defined by any grammar written in this notation.",1987,ACL,0.5
Book Reviews,"This book is the first volume in a series, Studies in Natural Language Processing, launched in 1984 under the sponsorship of the Association for Computational Linguistics. Aimed at a very wide audience, with background in formal linguistics, psycholinguistics, cognitive psychology or artificial intelligence, the series addresses a number of issues in the growing interdisciplinary field of computational linguistics. As a representative of these concerns, the inaugural volume succeeds quite well in setting the tone, by demonstrating the range of treatments that the notion of parsing has received from the perspectives of formal linguistics, computational analysis of language, and psycholinguistics. The book is not yet another workshop or conference spin-off, even though earlier versions of several papers were originally presented at conferences on parsing in 1981 and on ""Syntactic theory and how people parse sentences"" in 1982. The individual contributions are of consistently higher quality than those in a number of other edited collections on a single topic within computational linguistics. The book manages to convey a feeling for the complexity of the phenomenon at its focus, as it has evolved in more recent studies of language and mental processes; it also indicates the diversity of research directions pursued within the general area of syntactic processing of natural language. Unfortunately, individual contributions stand pretty much on their own, as there are no immediately obvious connections between most of the papers in the volume. With one exception (Kay's ""Parsing in functional unification grammar"" and Karttunen and Kay's ""Parsing in a free word order language"", where a formalism introduced from a computational perspective in the first paper is used as a descriptive device for an analysis of Finnish word order in the second), the reader has to work hard to find common themes running through the rest of the papers. The eleven chapters in the book can, on a first approximation, be grouped into several (overlapping) categories that reflect the structure suggested by the title. In particular, there are contributions concerned with a number of psychological and computational models of parsing, presentations of formal linguistic frameworks and evaluations of properties of syntactic theories, and linguistic studies, including a comparative analysis of the syntax and semantics of constituent questions in English, Swedish, and other Scandinavian languages. The volume contains a range of detailed reports on, and conclusions drawn from, psycholinguistic experimental work on human language comprehension. On the whole, a number of papers seem to be concerned with seeking correlations between features of grammars and some aspects of human parsing performance. For instance, Crain and Fodor (""How can grammars help parsers?"") seek to validate the claim that the human sentence processing mechanism is capable of applying all relevant grammatical information on an 'as needed' basis, as opposed to being viewed as a sequentially decomposable processor. Frazier (""Syntactic complexity""), while analysing sources of processing complexity in order to derive a general metric for it, raises the question of whether (and how) language, and grammars, might have evolved to facilitate the parsing task. Tanenhaus, Carlson, and Seidenberg (""Do listeners compute linguistic representations?"") set out to study the manner in which syntactic theory and the human sentence parsing process are connected. Independently of the strength of their argument in favor of the modularity hypothesis, they focus on the aim of understanding ""the relationship between the grammar and the general cognitive system"". The bulk of Engdahl's paper ""Interpreting questions"" analyses a wide range of data from Swedish, Norwegian, German, and English that presents strong evidence in support of a correlation between the processes of extraction and wide scope interpretation. Ultimately, however, she proposes an account for this correlation by making a statement about the human sentence processing mechanism: the explanation rests on the same device (Cooper storage) underlying both processes. It is, however, only at such level of generality that connections between separate papers can be perceived. If the book attempts to promote, explicitly, cooperation amongst researchers representative of the different areas within the general field of syntactic processing of language, then it should have given a clearer picture of the relationships between these areas. If the book sets out to convey the impression that a coordinated, multidisciplinary research programme on parsing is under way, such an impression is largely lost in the process of reading the individual contributions. This should not be regarded as a strong criticism of the book, as it is probably attributable to the fact that, particularly at the",1987,CL,0.30000000000000004
Parsing Japanese Honorifics in Unification-Based Grammar,"This paper presents a unification-based approach to Japanese honorifics based on a version of HPSG (Head-driven Phrase Structure Grammar)ll]121. Utterance parsing is based on lexical specifications of each lexical item, including honorifics, and a few general PSG rules using a parser capable of unifying cyclic feature structures. It is shown that the possible word orders of Japanese honori f ic predicate constituents can be automatically deduced in the proposed f ramework w i thou t independent ly specifying them. Discourse Information Change Rules (DICRs) that a l low resolving a class of anaphors in honorific contexts are also formulated.",1988,ACL,0.30000000000000004
"Discourse Models, Dialog Memories, and User Models","In this paper, we discuss some terminological issues related to the notions of discourse models, dialog memories, and user models. It is not our goal to show how discourse modeling and user modeling should actually interact in a cooperat ive system, but to show how the notions of discourse model, dialog memory, and user model can be defined and related in order to prevent misunderstandings and confusion. We argue that dialog memory may be subsumed under user model, as well as under discourse model, but that the three concepts should not be identified. Several separating criteria are discussed. We conclude that discourse modeling and user modeling are two lines of research that are orthogonal to each other.",1988,CL,0.0
Book Reviews: From Text to Speech: The MITalk System,"are incredibly artificial! As with several other AI systems, one gets the distinct impression that the data is fitted to the software! Alshawi is looking for the algorithms that produce the optimal interpretation of a text. How can he justify these algorithms when he discards psychology and neurophysiology? What constitutes the optimal interpretation of a text? These questions are left unanswered. What remains is the implementation of a solution to some specific problems considered in a vacuum. The dissertation presents the advantages and limitations of a particular solution; several other solutions to these problems have been suggested. Each author compares his model to the others but the relative vacuity of it all persists. Suspiciously, all models share the same basic flaws (e.g., simplistic model of memory, and absence of mechanisms for the subsequent correction of an erroneous interpretation). By dissociating NLP, with its cartesian quest for optimal interpretation algorithms, from man, his language, his formidable ability to understand and to misunderstand and to not understand, computational linguists seem to have created the ultimate field of study, one where partial solutions are taken to intrinsically hold the promise of an eventual complete and correct solution to the problem of linguistic understanding. I reject the notion of the ""optimal interpretation"" of a text and, after reading Alshawi's book, I am left with the bitter taste of an interesting yet very artificial Lisp program.",1988,CL,0.0
User Models and Discourse Models: United They Stand . . .,"Opinions on the relationship between discourse models (DMs) and user models (UMs) are obviously influenced by preassumptions about their respective contents. As far as DMs are concerned, two divergent views have been expressed in the discussion published here: I. The DM contains only representations of the objects mentioned so far in the discourse (i.e., a mentioned-object memory--see Schuster, this issue). The term ""object"" will be used here in the broad sense of Schuster, thus also denoting events, properties, etc. 2. The DM contains in addition a. a representation of the purpose underlying the segments of the dialog (i.e. a dialog purpose--see Grosz Sidner 1986, Chin, this issue). b. an attentional structure, which is a subset of the representations mentioned in (1) containing the currently focused objects which are ordered in a focus stack (Cohen, this issue; Chin, this issue, who requires only that the user must be familiar with these objects). Less disagreement seems to exist about the components of a UM. Generally, it is regarded as containing explicit representations of the system's assumptions about all relevant aspects of the user, i.e., assumptions about his/ her ""objective situation"" (e.g., marital status, number of children), as well as about his/her prior knowledge, goals, plans and false beliefs with respect to the domain of discourse. In order to meet Wahlster's personneldatabase counterexample, it must be further required that the user model be separable by the system from the rest of the system's knowledge. To discuss the relationship between DMs and UMs, a general belief, goal, and plan maintenance system (BGP-MS) will be presented here, the purpose of which is to store and update the beliefs, goals, and plans of both the system and an arbitrary number of other agents, including the system's current user. Specific subcomponents and subfunctions of this system hopefully capture the general consensus on what constitutes a discourse model and a user model, respectively. However, we will see that these subcomponents are strongly interwoven and that--apart from a few rarely occurring exceptions--the DM is part of the UM at least at the level of content. The question arises then, of course, whether it makes sense to separate these notions conceptually. The belief, goal, and plan maintenance system outlined here is being implemented (in a somewhat simplified form) in XTRA, a natural language access system to expert systems (Allgayer et al. 1988). A previous implementation was VIE-DPM (Kobsa 1985a,b). In the knowledge base of BGP-MS, the representation of the various types of (nested) beliefs and goals (Kobsa 1988) is separated into a number of hierarchically ordered partitions (see Figure 1). If it is shared knowledge between S and U that U possesses certain beliefs (knowledge), then this knowledge or these beliefs are represented in MB(UB). 1 MB(UW) contains those goals and plans of the user, MB(SB) those beliefs of the system, and MB(SW) those goals of the system for which the same holds true. ""Private"" beliefs of the system about the domain of discourse / about the user's beliefs / about the user's beliefs about the system's goals are represented in SB, SBUB, and SBUBSW, respectively. MB contains the mutual beliefs (knowledge) with respect to the domain, and MW the mutual goals and plans of S and U. The arrows between the partitions denote inheritance relationships. In the partitions of BGP-MS, the content of the individual beliefs, goals, and plans can be expressed through arbitrary representational structures (e.g., a KL-ONElike representation as used in XTRA). Various markers for non-belief and uncertainty can be added: For instance, in SBUB it can be expressed, among other",1988,CL,0.0
Distinguishing User Models From Discourse Models,"Of course, the interpretation of these positions depends on the definition of the terms involved and the underlying notion of the ""part-of"", ""intersect"", and ""distinct"" relations. The relationships cannot simply be interpreted in a set-theoretic sense, since all definitions for UMs and DMs proposed so far depend not only on representation structures, but also on processes used for the construction, maintenance, and exploitation of these structures. Since this is a terminological, and not an empirical, discussion, as I pointed out in Wahlster (1986), P1-P3 are primarily normative statements. So, P3, for instance, must be interpreted as ""The terms UM and DM should be defined in such a way, that they do not overlap"". This view seems not to be shared by all participants in the discussion. Schuster, for example, tries to prove her position (PI) in a set-theoretic sense. First, she argues that ""the user model contains information that does not appear in the discourse model"" and then she ""proves"" that ""any information in the discourse model is also in the user model"". I disagree not only with the form, but also with the content of Schuster's argumentation. She writes ""only if the discourse model is part of the user model can the system take it into account in its responses and its reasoning about the users"". By considering an isomorphic argumentation like ""only if a tomato is part of cheese, can one use it to prepare pizza"" it becomes clear that this proof is flawed. Also, Morik points out correctly that if one follows Schuster's argumentation one should ""view the grammar as part of the user model, because the grammar is necessary for understanding and producing utterances"". Today, it is a standard hypothesis in AI and computational linguistics that models for the language understanding and generation process must exploit various knowledge sources, including in many cases a DM and a UM. For example, in Jameson and Wahlster (1982) we described the NP generator of the HAM-ANS system, in which the generation of a definite or indefinite description was influenced both by the UM and the DM. But this in no way means that one must be included in the other. As long as there is no definitive evidence (e.g., from psychology or the neurosciences) for a particular structure, content, and use (or even existence) of UMs and DMs in the human information processing system, in AI the notions of UM and DM are concepts that help on the one hand to construct a theory of natural language dialog behavior, and on the other hand to structure the software systems that realize natural language systems. From the second point of view, which is the engineering perspective, the question of whether P1, P2, or P3 holds, is easy to decide so far. In most of the implemented systems the data structures and procedures labeled UM and DM are completely distinct. Even the recent GUMS package (Finin 1988), a general user modeling component, contains no specific representation structures or processes for discourse modeling. Since the discussion above suggests that we view the relation between the UM and the DM mainly as a terminological problem, in the next section we focus on possible definitions for UMs and DMs. Although often terminological discussions become quite tedious, at this point it seems to be important to define these concepts as precisely as possible, since many researchers are discovering interesting relationships between discourse and user models.",1988,CL,0.0
Book Reviews: The Fifth Generation Fallacy: Why Japan is Betting Its Future on Artificial Intelligence,"But the way Hirst has souped up his thesis with general expository matter has had not altogether satisfactory results. The textbook-like sections against which the accounts of his own work are set treat some topics, like structural ambiguity, at length, but do not form a very well-balanced or comprehensive whole. They nevertheless dilute Hirst's own work enough to prevent the reader from experiencing that feeling of excitement that good theses provoke, and to leave her disappointed in not getting the fuller and more concentrated account of Hirst's system as such, and its performance, she would have liked. The idea that language interpretation involves quite disparate processes is one deserving investigation, but it is not clear whether Hirst's particular way of combining such different lines as Montague and markers, via frames, is on the right track, and his book does not compel his reader, in a Pied Piper imperative, to follow him.",1988,CL,-1.0
Book Reviews: The Formal Complexity of Natural Language,"To provide some details, Cleopatra's parsing is ""lexically driven"" in that ""each word in the input sentence invokes procedures that direct the parsing process"". Samad observes that ""we can invoke any arbitrary function when evaluating a constraint"". Similarly, the integration procedures that build meaning structures may also involve ""arbitrary Lisp functions"". Cleopatra attacks ambiguity by use of ""confidence levels"" related to such phenomena as ""the relative frequencies of occurrence of different senses of a word, the likelihood of particular structure% and the correspondence of conjuncts"". Parsing is viewed as a parallel process, but the author clearly and honestly states that the existing program is breadth-first. While the author claims to have achieved a ""sharp contrast to the limited domains of previous natural language interfaces"", I cannot concur. Although he does handle certain issues in greater detail than have most previous systems (for example,, time references and some types of conjunction), there are also, as he so often and honestly admits, many typical distinctions that have been largely ignored (apparently, without suffering a great loss). For example, the system ""is not very sophisticated about auxiliaries or adverbs"" and does not yet ""know about"" person or number. Samad suggests that the ""final determinant"" of a system with the practical aspirations of his is the evaluation it receives from its intended users. But his claim that ""Cleopatra is more than a vehicle to demonstrate the feasibility of our ultimate goal. It is a useful CAD tool"" is simply not substantiated. Although ""we are confident that experimental studies will confirm the utility of Cleopatra"", there's no indication that such investigations have been conducted. In conclusion, the author is to be commended for his interest in building a complete system that can be used for some meaningful purpose. His frank and honest discussion of the details and limitations of his work are also to be praised, and the presence of some inherently interesting example sentences in the domain under study should be mentioned. However, I find little in the book that helps clarify any problems of language processing, nor do I suspect the techniques presented can provide any ""value added"" over what's available from existing literature. I am also disappointed by the fact that most of the book concerns implementational issues discussed at the level of data structures. Although I cannot recommend the book as a text, it could certainly be found useful as a case study.",1988,CL,0.0
Book Reviews: The Computational Analysis of English: A Corpus based Approach,"name--see Schiitzenberger (1956). For a summary of the issues, see Ryckman (1986), chap. 5. 2. Carnap and Bar-Hillel (1952), Bar-Hillel (1952). The present book seems in part responsive to this program, having the same title as Bar-Hillel (1964). 3. See papers collected in Hintikka and Suppes (1970). 4. Dretske (1981), Israel and Perry (forthcoming). Peer commentary in Dretske (1983), especially that of Haber, did not accept Dretske's attempted analogies to the metrics of Shannon and Weaver. The notion of ""information pickup"" implies a preestablished harmony of the world and the mind, disregarding the well-known arbitrariness of language. 5. While Fodor (1986) does gives a cogent criticism of attempts to locate information ""in the world"", the alternative ""intentional"" conception that he advocates relies on questionable assumptions of an ""internal code"" wherein such information is ""encoded"". The problem, of course, lies in unpacking this metaphor. Falling into the custom of taking the computational metaphor of mind literally, he resuscitates our old familiar homunculus (in computational disguise as the ""executive"") to provide a way out of the problem of node labels being of higher logical type than the nodes that they label. A simpler resolution follows from Harris's recognition that natural language has no :separate metalanguage. See also Fodor (forthcoming). 6. See especially Harris (1982), and Harris, Gottfried, Ryckman, et al. (in press). 7. This thus cuts deeper than the naive rule-counting metrics for adjudication of grammars advocated not so long ago by generativists (see Ryckman 1986). 8. This work is reported in depth in Harris et al. (in press). These science languages occupy a place between natural language and mathematics, the chief difference from the former being that operator-argument likelihoods are much more strongly defined, amounting in most cases to simple binary selection rather than a graded scale. One of the many interesting aspects of this research is determining empirically the form of argumentation in science. The logical apparatus of deduction and other forms of inference are required only for various uses to which language may be put, rather than being the semantic basis for natural language, as has sometimes been claimed. 9. This is a refinement of the notion of distributional meaning developed in, e.g., Harris (1954). 10. The case of zero likelihood is covered by the word classes of the first constraint. 11. An example is the elision of one of a small set of operators including appear, arrive, show up, which have high likelihood under expect, in I expect John momentarily. The adverb momentarily can only modify the elided to arrive, etc., since neither expect nor John is asserted to be momentary. The infinitive to, the suffix -ly, and the status of momentarily as a modifier are the results of other reductions that are described in detail in Harris (1982). 12. For a computer implementation, see Johnson (1987). I am grateful to Tom Ryckman for helpful comments on an early draft of this review. THE COMPUTATIONAL ANALYSIS OF ENGLISH: A CORPUS-BASED APPROACH",1988,CL,0.0
Book Reviews: Cognitive Science: An Introduction,"For instance, for a direct causal relationship, once the simple sentences that express the ACT and RESULT have been chosen, the choice of syntactic structure, the ordering of information, and the number of sentences still remain. Moreover, not all combinations of these choices yield acceptable texts. The discourse grammar encodes the acceptable choice combinations for the semantic relationship. Thus, in order to build a generation system which is able to handle some particular semantic relation, one must first do a detailed linguistic analysis to find the simple sentences that could be used to convey the information (and encode this in the lexicon grammar), and next do another linguistic analysis to see how these simple sentences can be combined, ordered, and syntactically presented so as to convey the semantic relationship (and encode this in the discourse grammar). Once the analysis has been done, the generation system can use these two grammars to do generation. It is the case, however, that the two grammars encode choices that are mutually dependent. Thus a choice in one will limit the available choices in the other. The priority of these decisions can only be determined within a particular domain. In applying this generation model to several domains, Danlos is extremely thorough and insightful. While one would hope that the domain dependence that Danlos advocates is not necessary, her analysis is quite convincing. Throughout the book she points out areas where ""general principles"" used by others must actually be operationalized in a very domain-dependent fashion. Thus the usefulness of such principles is called into question. In all I found the book to be most interesting. As a computer scientist I found the book's linguistic analysis very helpful. It forced me to look at generation fi'om a new point of view. I would expect that linguists will have a similar reaction because of the book's strong commitment to processing. I believe that Danlos has been able to successfully straddle the fence that lies between these two fields. In doing so, she has made a real contribution to both.",1988,CL,0.0
Chart Parsing According to the Slot and Filler Principle,"A parser is an algorithm that assigns a structural description to a string according to a grammar. It follows from this definition that there are three general issues in parser design: the structure to be assigned, the type of grammar, the recognition algo~ rithm. Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition. The following choices result in a new parser: The structure to be assigned to the input is a dependency tree with lexical, morpho-syntactic and functional-syntactic information associated with each node and coded by complex categories which are subject to unification. The grammar is lexicalized, i.e. the syntactical relationships are stated as part of the lexical descriptions of the elements of the language. The algorithm relies on the slot and filler principle in order to draw up complex structures. It utilizes a well-formed substring table (chart) which allows for discontinuous segments. 1. D e p e n d e n c y S t r u c t u r e The structuring principle of constituency trees is concatenation and the part-whole -relationship. The structuring principle of dependency trees is the relationship between lexemes and their complements. Note: It is not correct (or at least misleading) to define dependency as a relationship between words, as it is often done. The possibility and necessity of complements depend on the lexical meaning of words, i.e. a word which denotes a relationship asks for entities which it relates, a word which denotes a modification asks for an entity which it modifies etc. While it is awkward to associate functions (deep cases, roles, grammatical relationships) with phrase structures, it is not difficult to paraphrase the functions of complements on a lexical basis. For example, the argument of the predicate ""sleep"" denotes the sleeper; the meaning of ""persuade"" includes the persuader, the persuaded person and the contents of the persuasion. In a next step, one can abstract from the concrete function of dependents and arrive at abstract functions like subject, object, adjunct etc. Of course, the complements covering these roles can be single words as well as large phrases; for example ""John"", ""my father"", ""the president of the United States"" can all fill the role of the sleeper with respect to the predicate ""sleep"". However, phrases need not be represented by separate nodes in dependency trees (as they do in phrase markers) because their internal structure is again a question of dependency between lexemes and their complements. In a dependency tree, phrases are represented directly by their internal structure, which results in an arc between the superodinated head and the head within the complementary phrase. Nevertheless, the real principle of depen242 dency is a relationship between words and structures, or, formally, between single nodes and trees. Taking this into account, dependency trees are much more appealing than has been recognized so far. In order to restrict linguistic structures according to syntactic and semantic requirements, the use of complex categories is state of the art. Complex categories are sets of parameters (attributes) and values (features). Agreement between entities can be formulated in a general way in terms of parameters; the assignment of actual feature values is achieved by the mechanism of unification. If dependency J.s the relationship along which the catagories are unified, functional=syntactic and mo~ho-syntactic features can be handeled completely in parallel, as opposed to the two-phase mechanism which, for example, characterizes Lexical Functional Grammar. Each element in the dependency tree carries three labels: a role (which applies to the (sub)tree of which the element is the head), a lexeme, and a set of grammatical features. Constituency and dependency both have to be represented somehow or other in the syntactic description. As a consequence, recent developments have led to a convergence of formalisms of both origins with respect to their contents. (A good example is the similarity between Head-Driven Phrase Structure Grammar /Pollard, Sag 1987/ and Dependency Unification Grammar /Hellwig 1986/.) If phrase structure trees are used, the difference between governor and dependent must be denoted by the categories that label the nodes, e.g. by a x-bar notation. If dependency trees are used, the concatenation relationship must be denoted by positional features which are part of the complex morpho-svntactic category. 2. C h a r t p a r s i n g b a s e d on a l ex ica l i zed g r a m m a r The structure to be associated with a wellformed string can be defined in two ways: either by a set of abstract rules which describe the possible constructions of the ~language or by a description of the combi-. nation capabilities of the basic elements. The latter fits with the dependency approach. Given a lexical item and its morphosyntactic properties, it is relatively easy to give a precise description of its possible complements. The main advantage of this lexicalistic approach is the fact that augmenting or changing the description of an item normally does not interfere with the rest while any change in a rule-based grammar might produce unforeseen side effects with regard to the whole. The prerequisite for a lexicalized dependency grammar are trees that comprise slots. A slot is a description of the head of a tree that fits into another tree. Formally, a slot is a direct dependent of a head with a role associated to it, with a variable in the lexeme position, and with a categorization that covers all of the morpho-syntactic properties of the appertaining complement. If cross categorization does not allow all of the p~ssible properties of a complement within one category to be stated, a disjunction of slots is used to express the alternatives. The only mechanism needed for draw-ing up complex structures is the unification of slots and potential fillers. The control of the parsing process is achieved by means of a well-formed substring table ((]hart). It is widely accepted that chart parsing is superior to backtracking or to parallel processing of every path. A common version of a chart can be vizualized as a network of vertices representing points in the input, linked by edges representing segments. The edges are labelled with the categories that the parser has assigned to the constituents concerned. Alternatively, each edge is associated with a complete structural descrLption, including the information which is carried by the covered edges. In this case, a chart is simply a collect]on of trees (implemented as lists) projected on the various segments in the input. The innovation with regard to chart parsing th~vt is proposed in this paper is a label.ling of edges by trees that comprise slots. At the beginning, an edge for each word is entered into the chart. Each edge is label] o~ ed with a tree. The head of this tree contains the lexeme that is associated with the word according to the ].exicon; it carries a morpho-syntactic category according to the morphological properties of the word in question: it normally contains a variab].e as a role l~arker, since the syntagmatic function of the corresponding segment is still unknown. A slot is subordinated to the head for each element that is to be dependent in the resulting structure, if any. Slots are added to a lexical item according to c~>mpletion patterns refered to in the lexicon. (We can not qo into details here.) Subsequently, each tree in the chart looks for a slot in a ""tree that is associated with annother edge. If the head of the searching tree fitn the description in the slot then a new edge is drawn and labelled with the compound tree that results from inserting the first tree into the second. The categories of the ~ew tree are the result of unifying the categories of the slot tree and the filler tree. Special features state the positional re~/irements, e.g. whether the segment corresponding to the filler has to preceed or to follow of the segment corresponding to the element dominating the slot. This process continues until no new tree is produced. Parsing was successful if at ].east one edge covers the whole input. The dependency tr~e associated with this edge is the desired structural description. The fo].lowing example illustrates the m e -",1988,COLING,0.0
Schema Method: A Framework for Correcting Grammatically Ill-formed Input,"The schema method is a framework for correcting grammatically ill-formed input. In a natural language processing system ill-formed input cannot be overlooked. A computer assisted instruction (CAD system, in particular, needs to show the user's errors. This framework diagnoses ill-formed input, corrects it and explains the error, if an input is ill-~'ormed. The framework recognizes a sentence at two steps: first parses weak grammar, and then strongly filters the parsed sentence. When it is known what sentences are passed by the filter, it can be used even if it is imperfect. As the strong filter, a new method is used: an interpretation schema and an interpretation rule. An interpretation schema collects input information schemata and then an interpretat ion rule judges whether the collected schemata are correct or incorrect. This approach overcomes the problem of relaxation control, the major drawback of the previous syntactically-oriented methods, and is also more efficient. 1¬∞ In t roduct ion Ill-formed input cannot be ignored when a natural language processing system such as a computer assisted instruction (CAD system or a machine translation system is built. Particularly in a CAI System, students often make mistakes, such as mispunctuation, lack of agreement, misplaced/improperly-used words, etc. In these cases, a CAI system needs to point out input errors, and show why the input it~ wrong. In order to do so, the system needs to diagnose and correct ill-formed input to explain the errors. The schema method as a f ramework for correct ing grammatical ly ill-formed input is suggested and the diagnosis and correction of errors is discussed. There have been many studies for processing ill-formed input for English. The point of those studi.es is the diagnosis: how does the system find an error? The approaches are c lass i f ied in to two g roups : the syntactically-oriented group and the frame-based group. The syntact ical ly-oriented group includes robust parsers based on Augmented Transition Networks (ATN) which Use the relaxation technique/Kwansny 1981./or the meta-rule/Weisehedel 1980, 82, 87/, and the EPISTLE system which addresses the problems of the checking grammar and style of texts, such as letters, reports and manuals, written in ordinary Engl ish/Heidorn 1982/, /Jensen 1983/. The f r ame-based group a t t emp t s to deal wi th ungramnmtical input through extensions to pa t te rn matching parsing/Hayes 1981/, through conceptual case frame instantiation/Schank 1980/and through approaches involving mult iple coopera t ing pa r s ing s t r a t eg ies /Carbonell 1983/. The target of that study is dialogue phenomena in communicat ion with l imi ted-domain systems, such as data-base systems, electronic mail systems, etc. The aim of this study is error-correction of non-native speakers wri t ten Engl i sh text. This approach is syntactically oriented. The syntactically-oriented approaches/Kwansny 1981/ /Weischedel 1980,82,87/,/Heidorn 1982/,/Jensen 1983/are very similar. Their basic idea is relaxation. They first attempt to parse the input, using fully grammatical rules. If the sentence is not parsed, some of the conditions are relaxed. However these approaches have two major drawback. (1)Relaxation control strategies: when inputs are illformed, some means of ranking alternatives is appropriate. The number of relaxed configurations may be large. One of the most critical problems is control. The need to relax the very rules that constrain the search for an interpretation is like opening Pandora's box./Weischedel 1987(PP.117)/ (2)Computational inefficiency: the relaxation approach cannot recognize ill-formed input before the analysis with well-formed grammar is finished. Furthermore, fully wellformed grammar is needed. To make fully well-formed grammar, subcategorization of parts of speech is needed and other conditions are added. As a result, there are too many rules. In comparison to previous approaches, this approach does not use the relaxation technique. The difference between previous approaches and this one is the method of recognizing an ill-formed sentence. Previous approaches first use a strong filter, then relax the conditions. This approach, however, first uses weak grammars, and then strongly filters the passed sentence. This approach recognizes a sentence at two steps. An at tempt is made to expand lexical-functional grammar (LFG) /Kaplan 1982/ to deal with ill-formed input. LFG has two drawbacks: (1) LFG can't deal with errors of omission and (2) LFG has no framework for error correction. If an input sentence is well-formed, this framework obtains an LFG f-structure. If not, the sentence is corrected. Examples of error correction are given in the next section. In the section following the basic idea is described",1988,COLING,1.0
Reasons why I do not care grammar formalism,"Computational linguistics (CL) has borrowed a lot of ideas from Theoretical Linguistics (TZ). We could not have developed even a simple parser without the research results in TL. It is obviously nonsense to claim that we, computational linguists, do not care research results in TL. llowever, the researchers in TL, it seems to me, are very fond of fighlinq~ especially, those who are called Synlaclicians. They always fight with e~h other by asserting that their grammar formalisms are superior to the others'. They are oversensitive and tend to distinguish people into two groups, the ally and the enemy.",1988,COLING,-0.5
Using a Logic Grammar to Learn a Lexicon,"It is suggested that the concept of ""logic grammar"" as re la t ion be tween a str ing and a parset ree can be ex tended by admi t t i ng the lexicon as par t of the relation. This makes it possible to give a s imple and elegant formulat ion of the process of infering a lexicon f rom e x a m p l e sen tences in con junc t ion wi th a g r a m m a r . V a r i o u s p r o b l e m s a r i s i n g f r o m implementa t ion and complexity factors are considered, and examples are shown to support the claim that the method shows potential as a practical tool for automatic lexicon acquisition.",1988,COLING,0.7000000000000001
Robust parsing of severely corrupted spoken utterances,This paper describes a technique for enabling a speech understanding system to deal with sentences for which some monosyllabic words are not recognized. Such words are supposed to act as mere syntactic markers within the system linguistic domain. This result is achieved by combining a modified caseframe approach to linguistic knowledge representation with a parsing strategy able to integra te expectat ions from the language model and predictions from words. Experimental results show that the proposed technique permits to greatly increase the quota of corrupted sentences correctly understandable without sensibly decreasing parsing efficiency.,1988,COLING,1.0
Linguistic Processing Using a Dependency Structure Grammar for Speech Recognition and Understanding,"This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar. The strategy includes parsing and phrase prediction algorithms. After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships. A fast parsing algorithm using breadth-first search is also proposed. The predictor pre-selects the p}~.ase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing. The proposed linguistic processor has been tested through speech recognition experiments. The experimental results show that it greatly increases the accuracy of speech recognitions, and the breadth-first parsing algorithm and predictor increase processing speed.",1988,COLING,1.0
A System for Creating and Manipulating Generalized Wordclass Transition Matrices From Large Labelled Text-Corpora,"This paper deals with the training phase of a Markov-type linguistic model that is based on transition probabilities between pvirs and triplets of syntactic categories. To determine the o?timal level of detail for a set of syntactic classes we developed a systetn that uses a set-theoretical formalism to defiue such sets mid has some measm~s to comp~uce and c,ptimize them fildividually. In section two we describe the optimizafiou problem (hi terms of piediction, infoimation and economy requilements) and our approach to its solution. Section three introduces the system dlat will assist a lhlguist in h,'mdling the prediction and economy criteria and in the last section we plesent some slunple lemtlts that can be achieved with it.",1988,COLING,0.30000000000000004
Why Computational Grammarians Can Be Skeptical About Existing Linguistic Theories,"The bottle neck in building a practical natural language processing system is not those problems which have been often discussed in research papers, but in ilandling much more dirty, exceptional (for theoreticians, but we frequently encounter) expressions. This panel will focus on the problem which has been rarely written but has been argued informally among researchers who have tried to build a practical natural language processing system at least once. Theory is important and valuable for the explanation and understanding, but is essentially the first order approximation of a target object. As for language~ current theories are Just for the basic part of the language structure. Real language usage is quite different from the basic language structure and a supposed mechanism of interpretation. Natural language processing system must cover real language usage as much as possible. The system model must be designed in such a way that it is clearly understandable by the support of a powerful linguistic theory, and still can accept varieties of exceptional linguistic phenomena which the theory is difficult to treat. How we can design such a system is a major problem in natural language processing, especially for machine translation between the languages of different linguistic families. We have to be concerned with both linguistic and non-llngulstlc world. While we have to study these difficult problems, we must not forget about the realizability of a useful system from the standpoint of engineering. I received valuable comments from Dr. Karen Jensen who cannot participate in our panel, and kindly offered me to use her comments freely in our panel. I want to cite her comments in the followings.",1988,COLING,0.0
Treatment of Long Distance Dependencies in LFG and TAG: Functional Uncertainty in LFG Is a Corollary in TAG,"In this paper the functional uncertainty machinery in LFG is compared with the treatment of long distance dependencies in TAG. It is shown that the functional uncertainty machinery is redundant in TAG, i.e., what functional uncertainty accomplishes for LFG follows f~om the TAG formalism itself and some aspects of the linguistic theory instantiated in TAG. It is also shown that the analyses provided by the functional uncertainty machinery can be obtained without requiring power beyond mildly context-sensitive grammars. Some linguistic and computational aspects of these results have been briefly discussed also. 1 I N T R O D U C T I O N The so-called long distance dependencies are characterized in Lexical Functional Grammars (LFG) by the use of the formal device of functional uncertainty, as defined by Kaplan and Zaenan [3] and Kaplan and Maxwell [2]. In this paper, we relate this characterization to that provided by Tree ~,djoining Grammars (TAG), showing a direct correspondence between the functional uncertainty equations in LFG analyses and the elementary trees in TAGs that give analyses for ""long distance"" dependencies. We show that the functional uncertainty machinery is redundant in TAG, i.e., what functional uncertainty accomplishes for LFG follows from the TAG formalism itself and some fundamental aspects of the linguistic theory instantiated in TAG. We thus show that these analyses can be obtained without requiring power beyond mildly context-sensitive grammars. We also *Thi s work was par t ia l ly suppo r t ed (for the first author) by the D R R P A gran t N00014-85-K0018, A l tO gran t DAA29-84-9-0027, a n d NSF gran t IRI84-10413-A02. T h e first a u t h o r also benef i ted f rom some discuss ion wi th Mark Johnson a n d R on K a p l a n a t the Tit isee Workshop on Unif ication G r a m m a r s , March, 1988. briefly discuss the linguistic and computational significance of these results. Long distance phenomena are associated with the so-called movement. The following examples, 1. Mary Henry telephoned. 2. Mary Bill said that Henry telephoned. 3. Mary John claimed that Bill said that Henry telephoned. illustrate the long distance dependencies due to topicalization, where the verb telephoned and its object Mary can be arbitrarily apart. It is difficult to state generalizations about these phenomena if one relies entirely on the surface structure (as defined in CFG based frameworks) since these phenomena cannot be localized at this level. Kaplan and Zaenan [3] note that, in LFG, rather than stating the generalizations on the c-structure, they must be stated on f-structures, since long distance dependencies are predicate argument dependencies, and such functional dependencies are represented in the f-structures. Thus, as stated in [2, 3], in the sentences (1), (2), and (3) above, the dependencies are captured by the equations (in the LFG notation 1) by 1"" T O P I C =T OBJ, T T O P I C =T C O M P OBJ, and 1"" T O P I C =T C O M P C O M P OBJ, respectively, which state that. the topic Mary is also the object of tele. phoned. In general, since any number of additional complement predicates may be introduced, these equations will have the general form ""f T O P I C =T C O M P C O M P ... OBJ Kaplan and Zaenen [3] introduced the formal device of functional unc'ertainty, in which this general case is stated by the equation 1 Because of lack of space, we will no t define the LFG nota t ion . We a s s u m e tha t the reader is famil iar wi th it.",1989,ACL,-0.2
Evaluating Discourse Processing Algorithms,"In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.",1989,ACL,-1.0
Two Constraints on Speech Act Ambiguity,"Existing plan-based theories of speech act interpretation do not account for the conventional aspect of speech acts. We use patterns of linguistic features (e.g. mood, verb form, sentence adverbials, thematic roles) to suggest a range of speech act interpretations for the utterance. These are filtered using plan-bused conversational implicatures to eliminate inappropriate ones. Extended plan reasoning is available but not necessary for familiar forms. Taking speech act ambiguity seriously, with these two constraints, explains how ""Can you pass the salt?"" is a typical indirect request while ""Are you able to pass the salt?"" is not.",1989,ACL,0.6000000000000001
A Hybrid Approach to Representation in the Janus Natural Language Processor,"In BBN's natural language understanding and generation system (Janus), we have used a hybrid approach to representation, employing an intensional logic for the representation of the semantics of utterances and a taxonomic language with formal semantics for specification of descriptive constants and axioms relating them. Remarkably, 99.9% of 7,000 vocabulary items in our natural language applications could be adequately axiomatlzed in the taxonomic language.",1989,ACL,0.6000000000000001
Book Reviews: Generalized Quantifiers: Linguistic and Logical Approaches,"lack of an index. An even greater deficit is the absence of a comprehensive bibliography. One could receive the false impression that the book is a first work by the group (it builds directly on Harris 1982, 1988), or that they are the only group working in sublanguage (cf. the collections by Kittredge and Lehrberger (1982) and Grishman and Kittredge (1986). The absence of references to related work in theoretical or computational linguistics makes the book much less accessible to readers unfamiliar with the sublanguage approach. This is truly unfortunate since there are many fruitful correspondences. In summary, the book offers a clear description of a much-needed methodology for knowledge acquisition, and a concise, formulaic representat ion for science information. It is highly recommended to anyone developing text-processing applications in restricted semantic domains.",1989,CL,-0.8
"Book Reviews: Philosophy, Language, and Artificial Intelligence: Resources for Processing Natural Language","Sometimes philosophers and linguists say things that are of interest to computer scientists or would be of interest if properly explained. Unfortunately, philosophical work and theoretical linguistics are seldom explained in ways that make them seem relevant to computer scientists. That is, they are seldom explained in ways that suggest directions for future research. Given this communication problem, there is a definite need for a volume that can bring together some of the best work in the philosophy of language, linguistics, and natural language processing, and show how work in the philosophy of language and linguistics has been and can be applied to problems in natural language processing. Unfortunately, despite the title ~ and the promotional literature, this volume does not fulfil the need. Part of the problem is that none of the papers in the volume describe work in which the theoretical insights of philosophers and linguists are implemented. There are simply no papers on natural language processing. There is an introduction by Kulas that is supposed to show why these essays are important to AI researchers. But while the introduction occasionally provides reasonable summaries of the essays, it doesn't come close to suggesting applications of the theoretical work to AI. It may well be that computer scientists ought to be familiar with the essays in the book. Most of the essays are, after all, classics. But it is pedagogically naive to think that one can drop, for example, Davidson's ""Truth and Meaning"" in the lap of a computer scientist and suppose that it suggests anything in the way of a research program. Another part of the problem is that the volume has been limited to papers that originally appeared in books or journals published by D. Reidel. This is unfortunate, for a number of sections of the book could have been greatly strengthened with the addition of material pub-",1989,CL,-1.0
Book Reviews: Knowledge Systems and Prolog: A Logical Approach to Expert Systems and Natural Language Processing,"with SWESIL?), it might have been better to wait until more care could be taken. Although the title leads one to assume the authors will concern themselves with ""word experts,"" which usually implies lexical procedures in Small's sense (they cite him several times), it turns out that nothing in DLT can be construed as such. The dictionary entries are intended to be numerous, but the authors themselves proscribe large entries (p. 145), or even ones wherein procedures appear (""There is no level of abstraction within the LKB in which meaning can be viewed as separate from words"" (p. 118), and other pro-modularity arguments). DLT's semantic rules seem much more akin to Wilks's preference semantics than to Small's word expert procedures; and syntactic parsing in DLT, it turns out, is based on dependency grammar (p. 105) and is quite independent of any semantic processing. More serious are certain scientific and technical claims that are un(der)supported. Here, of course, one tends to leave the realm of reviewing the book and move on to critiquing the work behind it. As this may not be in the purview of a book review proper, I will constrain myself to two points. First, the ""lexical taxonomy"" issue, for all the attention paid to it, is glossed over. In particular, while the authors promote the ""good"" results, identification of noun and verb hierarchies, achieved by Calzolari and Amsler, they fail to mention the problems that Amsler, at least, brings up: the noun hierarchies tend to attach themselves to the verb hierarchies eventually (which seems to violate a structural assumption built into DLT) and, worse, hierarchies seem quite inappropriate for any other part of speech (e.g., adjectives and adverbs). Second, it is perhaps most unfortunate that the writers, who discount results reported on the basis of small prototypes (Part III, Chapter 1), are reduced to just such claims themselvesmtesting but seven phrases (p. 172) and one single sentence (p. 176) using a system with a ""Semantic Dictionary [that] contains some 800 headwords"" (p. 205). True, such a vocabulary is an order of magnitude larger than that found in most academic AUNLP systems; but it is still an order of magnitude below the I0,000 that the authors themselves think reasonable (p. 145)---which is in turn one or two orders of magnitude below that probably required for such uses as they anticipate for DLT, if claims like those of Walker and Amsler (based on several months of New York Times text) are to be believed. Regardless, the trivial number of test cases put to SWESIL prove absolutely nothing, notwithstanding statements like ""some interesting successes have already been achieved"" (p. 205). (I grade SWESIL at 79% on the 7-phrase test (p. 172). At least they are honest: many AUNLP reports neglect to mention errors at all, and fewer yet quantify anything.) In short, the authors are inconsistent about the evaluation of other NLP systems against their own, and too much of th i sbook talks about DLT in the future tense (again, not problems unique to this book). It was written too early, and apparently in too great a hurry. For the most part, it is interesting reading for the uninformed; there is no news for the professional. But for this I could have liked it, whether or not I agree with their approach.",1989,CL,-0.5
Book Reviews: The Vastness of Natural Languages,"and English. His extension is to my mind ad hoc and although it works for his examples, might not adapt itself to more complex problems like negation in French. Karen Jensen (""Binary rules and nonbinary trees: Breaking down the concept of phrase structure"") is concerned with the passage from binary rules (and trees) that capture significant generalizations about natural languages to list structures that are more satisfactory for further processing. Her solution can handle discontinuous constituents and is suitable for treating languages with free word order. In ""The notion of 'rule of grammar' reconsidered"" Michael Kac defends the notion that ""grammatical analysis requires that we have a way of formally representing the variety of distinct etiological properties that can be manifested by ungrammatical strings, this diversity corresponding to the variety of distinct rules of grammar"" (p. 137). He argues that getting the standard linguistic theories (various versions of TG, GPSG, etc.) to serve the purpose of etiological analysis is ""problematical"". His ""fundamental principles"" (pp. 120, 122) appear to require that a grammar supply a structure not only for elements of the language L that it generates but also for the elements in the complement of L. In his formal development, however, he defines an object (Definition 9) in terms of itself and this circularity would appear to render the result ill-defined. Since his main argument depends on this definition, I stopped reading. It is a good practice to buttress complicated definitions with examples both for the good of the writer as well as that of the reader. There are three papers on tree-adjoining grammars: an introduction by Joshi, ""Unbounded dependencies and subjacency in a tree adjoining grammar"" by A.S. Kroch, and ""On the progression from context-free to tree adjoining languages"" by Joshi et al. This presents an easy access to a useful collection of results concerning a rather pregnant linguistic model. Finally, three of the papers are concerned with semantics proper. G.N. Carlson's ""Exceptions to generic generalizations"" deals with the construction of a formal semantics using a sort of default mechanism in order to interpret statements like ""Dogs bark"" when clearly barkless dogs exist. Davis and Papcun in ""The structure underlying a semantic domain"" provide a rather metaphorical model vr (volumetric representation) ""to formalize lexical knowledge in a practical way"". They investigate various models--semantic networks, multi-dimensional scaling, and clustering--before settling on their own spatial (and somewhat intensional) model of a semantic domain. The third paper, R.H. Thomason's ""Remarks on linguistic semantics"", is an expository article concerned with the interface between linguistics and philosophy, dealing with the literature of such topics as tense and aspect, propositional attitudes, and vagueness.",1989,CL,0.0
Book Reviews: An Introduction to Formal Language Theory,"This book announces itself in its preface (p. vii) as ""the first textbook to combine the topics of formal language theory traditionally taught in the context of programming languages with an introduction to issues in computational linguistics"". This is an interesting idea; interesting enough to have made me turn to the book with some anticipation; but I was disappointed. The natural language material (virtually all syntax) is segregated in the last two chapters (52 pp.), drafted by James Pustejovsky (henceforth P). Moll, Arbib, and Kfoury (henceforth MAK) offer in the first seven chapters (143 pp.) a standard (and quite brief) introduction to formal language theory with no natural language perspective at all (except for a brief and slightly confusing illustration of the phrase structures for two English sentences on pages 20-21). The main problem with the book is that P's two chapters refect little of the formal rigor for which the first seven chapters have laid the groundwork. It is hard to believe MAK worked over P's chapters with a critical eye. By the standards of the material in the first seven chapters, the last two are vague at best, and frequently confusing or even confused. The two sections of the book have entirely different philosophies even in bibliographical policy. Neither section gets it quite right. MAK adopt school textbook style, avoiding literature citations. They mention about a dozen standard names (in order of appearance: Chomsky, Backus, Naur, DeMorgan, Kleene, Cocke, Kasami, Younger, Earley, Turing, Church, Cantor, GOdel, and Post), but they cite no specific literature except for five introductory books (Aho and Ullmann's The theory of parsing, translation, and compiling, Minsky's Computation: Finite and infinite machines, and three books of their own). I think this is inappropriate in a university-level textbook; students should be given directions to at least some of the more important primary literature in computer science journals. P's two chapters, on the other hand, have their own list of references, more like a linguistics article; but this list inc, ludes several works with scant formal content that will mean little to the student of computer science. MAK's treatment of the standard topics in formal language theory is clear and effective. Their presentations of proofs and introductions of new concepts frequently have an appealing freshness and directness. They cover a good selection of the most central topics in formal language theory: context-free (CF) and contextsensitive grammars and languages, the Chomsky hierarchy, closure properties of languages, regular expressions and finite-state languages, pushdown automata and their equivalence to CF grammars, normal forms for CF grammars, the CKY and Earley algorithms, Turing machines, linear bounded automata, halting and undecidability, and parsing, including top-down parsing (with LL grammars) and bottom-up parsing (with LR grammars). They also have a chapter on the algebraic approach more favored by European theoretical computer scientists: fixed-point principles, representing CF grammars by equations, and so on. P's two chapters attempt to cover the aims of linguistic', theory (generative grammar), generative capacity as applied to natural language grammars (a section that P calls ""The generative power of natural languages"", a mistaken turn of phrase, since languages do not generate anything), ATNs, lexicalism and X-bar theory, generalized phrase structure grammar (a section riddled with misstatements), and government and binding theory (GB). The lack of rigor is unmistakable throughout. On page 9, in MAK's section, the distinction between a node and its label is clear (""Every node that is not a leaf is labeled with a variable""), but on pages 177-178, P confuses nodes with their labels (""the node N"" ; ""the number of 'bars' associated with a node""). On page 178, P refers to ""having n bars"" as an equivalence relation (he may mean ""has the same number of bars as""). After presenting on page 80 the X-bar rule ~I ~ Spec ~,/ on page 180, P tells the reader, gratuitously and bafflingly: Although the notation may be new, the structures here are familiar. The rule for ~I, for example, is a slightly different version of the following NP rule. NP--~ Det A N PP But in fact the structures induced by these rules are strikingly different, as the reader who had understood even the first ten pages of this book could not fail to see. What is more, vital issues in syntax and semantics have",1989,CL,0.0
Memory Capacity and Sentence Processing,"The limited capacity of working memory is intrinsic to human sentence processing, and therefore must be addressed by any theory of human sentence processing. This paper gives a theory of garden-path effects and processing overload that is based on simple assumptions about human short term memory capacity.",1990,ACL,0.2
Book Reviews: Generating Natural Language under Pragmatic Constraints,"the book more of a recommended reading is the renewed empiricism in the field, largely promoted by the very practical need to scale up natural language systems, and largely due to the realization that linguistic information about words could be derived from massive on-line text resources. Whether these resources come in the shape of on-line dictionaries or text corpora is immaterial here. By reading Looking Up, one becomes acutely aware of the richness of lexical information available in (and distributed over) millions of words of text. One also understands that careful inspection of a dictionary entry (or a set of related entries) is likely to reveal considerably more in terms of lexical properties of the word (or class of words) than is apparently visible. The nature of lexical information in, and its extractability from, such text resources has been much discussed recently in the computational linguistics and computational lexicography literature; in particular, the statement that there is a Wealth of implicit information available in on-line dictionaries and corpora has been made over and over again recently (see, for instance, Atkins et al. 1988; Boguraev and Briscoe 1989; Hindle 1989; Church and Hanks 1989). However, there is a world of difference between ""retro-engineering,"" by whatever means, methods and rules for inferring lexical information from dictionary entries, and being told in advance the kinds of lexical regularities, generalizations, and properties encoded in these entries. For that reason alone, and particularly given that the COBUILD dictionary is available in machine-readable form, Looking Up is a book that should not be ignored by researchers interested in computational lexicography, lexical semantics, or simply the nature of word meaning.",1990,CL,0.0
Book Reviews: Phonological Parsing in Speech Recognition,"engineering standpoint) of an architecture that provides a clear separation of linguistic and nonlinguistic knowledge, since it is hard to see how shallow processing could be implemented without this separation. Although SPAR is implemented as a complete natural language system, it unfortunately still has something of a ""toy"" flavor for two reasons. First of all, the data texts were written specifically for this project, although not by people who knew about SPAR. Although many of the phenomena in these texts undoubtedly occur in more realistic texts, the work would perhaps have been more convincing had Carter used texts written for other purposes. Naturally occurring texts often contain problematic constructions such as nominalizations, which present many interesting challenges for semantics and anaphor resolution in natural language systems (Dahl et al. 1987), but which don't occur in Carter 's texts. The system also has a toy flavor because the end application, paraphrase, is less obviously useful than many other applications that might have been selected. There is no reason to think that these problems affect the fundamental soundness of the work, but they do tend to make it less interesting. This work presents a very comprehensive implementation of the state of the art of reference resolution in natural language processing. However, one is left at the end with a frustrating sense that the whole process consists of exploiting a set of more or less unrelated heuristics, which in fact lead to very accurate reference resolution, but which don't seem to fit together into a general picture of a unified phenomenon. For example, Carter points out (using Sidner's terminology) that the discourse focus is preferred to intra-sentential candidates, but that intra-sentential candidates are preferred to potential discourse foci. It is only natural to wonder why these preferences (and others) should be the way they are, and whether they can be expected to fall out from more general principles. This is not specifically a criticism of Carter, but points out an unsatisfying aspect of much computational work in anaphora resolution. It is in fact at least partially the result of the clarity of his presentation that this issue emerges. I found this book very stimulating, interesting, and clear. I would recommend it to anyone interested in reference resolution or computational pragmatics in general.",1990,CL,0.0
Briefly Noted,"M. Martin Taylor obtained his B.A.Sc. in engineering physics at the University of Toronto, his M.S.E. in industrial engineering at the Johns Hopkins University, and his Ph.D. in psychology at the Johns Hopkins University. He holds the position of Senior Experimental Psychologist at the Defence and Civil Institute of Environmental Medicine in Toronto. Insup Taylor obtained her B.A. at Seoul National University, and her M.A. and Ph.D. at the Johns Hopkins University, all in psychology. She is a Research Fellow at the McLuhan Centre for Culture and Technology, University of Toronto. They are the co-authors of The psychology of reading (Academic Press, 1983) and Psycholinguistics: Learning and using language (Prentice-Hall, 1990). Martin Taylor's address is: DCIEM, P.O. Box 2000, North York, Ontario, Canada M3M 3B9: Insup Taylor's address is: McLuhan Program, University of Toronto, 39A Queen's Park Crescent East, Toronto, Ontario, Canada M5S1A1. E-mail for both authors: mmt@ben.dciem.dnd.ca",1990,CL,0.0
"Book Reviews: An Introduction to Chinese, Japanese and Korean Computing","Turing machine (the author asserts that it has never been built?), nonstandard logics, or nonmonotonic reasoning. The chapter on expert systems does not really explain what an expert system is, nor how it works, nor how it could be used by a CALL system. So these topics will be rather confusing for the nonspecialists. To conclude, it seems to me that this book will not contribute to familiarizing language teachers with notions of computer science and artificial intelligence.",1990,CL,-0.5
Resolving Quasi Logical Forms,"The paper describes intermediate and resolved logical form representations of sentences involving referring expressions and a reference resolution process for mapping between these representations. The intermediate representation, Quasi Logical Form (or QLF), may contain unresolved terms corresponding to anaphoric noun phrases covering bound variable anaphora, reflexives, and definite descriptions. Implict relations arising in constructs such as compound nominals appear in QLF as unresolved formulae. The QLF representation is also neutral with respect to ambiguities corresponding to quantifier scope and the collective/distributive distinction, the latter being treated as quantifier resolution. Reference candidates are proposed according to an ordered set of ""reference resolution rules"" producing possible resolved logical forms to which linguistic and pragmatic constraints are then applied.",1990,CL,-0.2
Book Reviews: Machine Translation: How Far Can It Go?,"This is a very short book (150 pages, including index and references), which introduces the basic concepts of machine translation and offers a concise and clear exposition of the problems and issues faced by MT. It was meant for a Japanese audience (this translation appears three years after the original book did), and in some respects it can be seen as self-advertising for the Japanese industry; at the same time, it is also a good description of the specific problems encountered in translating between Japanese and English, and it gives a refreshing view of the history of the field, which has usually been recounted from the Western side. As each example contains the original Japanese text, its transliteration, and a word-for-word gloss as well as the English translation, a reader with no knowledge of Japanese can follow quite easily the explanations that were meant for a Japanese reader. However, I have a few reservations. The translation is sometimes awkward, especially in the second, more technical, part of the book, and it suffers from not having been checked by a linguist: such expressions as referent and referring are not used consistently, nor in a standard way (e.g., when a subject NP is said to refer to its predicate), and there is one case of actual mistranslation, where Chomsky's theory is referred to as transformational generation grammar. Another unfortunate choice of terminology is that of analytical, transformational, and generative grammars instead of the more widely used and theory-neutral grammars for analysis, transfer, and generation (p. 125). More important is the lack of references, which raises the question of who the intended audience is. The bibliography is very short and not very representative of the field, either in Japan or the rest of the world. Some works mentioned in the main text (e.g., Nida's on the definition of translation; p. 49) are not referenced. Others are casually mentioned without even any name (e.g., tense and time reference problems are supposed to be greatly clarified by ""recent linguistic work""; p. 104). Although it is clear that Nagao's original book was not meant for academic research and that the Japanese version was aimed at the open-rainded lay reader, the difficulties resulting from the translation render the English version less easily accessible to the corresponding English public. Nevertheless, in a field where the few books available aim at a specialized audience, Nagao's is a valuable contribution that could prove a good addition to a supplementary reading list in an introductory course in NLP, and could be useful for translators who are not familiar with MT. The two main issues Nagao addresses throughout the book are those of pivot versus transfer and syntax versus semantics. About the first, Nagao claims that the nuances of Japanese linguistic expressions reflecting Japanese culture preclude the use of the same pivot language to translate both within the European languages and between European languages and Japanese. This is too cursorily expressed, but the whole question of the choice between a pivot language and a transfer technique approach is well explained. About the second, unsurprisingly and uncontroversially, Nagao advocates a balanced use of syntax and semantics. More open to controversy is his claim that the word order of the source text should be preserved as a means to maintain focus, and thus pragmatic information. To the question ""Which syntax?"", Nagao unequivocally advocates case grammar, which is presented as a means of ""resolving questions of syntax on the basis of the meaning relations between nouns and verbs."" To the question ""Which semantics?"", Nagao answers that the semantic methods, i.e., ""meaning tables"" or ""semantic networks,"" which he had proposed for sentence generation in 1963, are now recognized as being unavoidable. His assessment that ""at that time, linguistics was not dealing with the problem of meanJlng"" would need some qualification, and so would his categorical assertion that Montague semantics, as any truth-value-based semantic theory, has proven inadequate for translation (the ROSETTA project, for instance, cannot be described as a research failure). N agao's conclusion that ""language is a massive conglomeration of exceptions"" is no surprise, nor is his requirement that tlhe design of any MT system should be flexible, robust, and transparent. However, while it is clear that a good MT system must be open (""allowing not only for the handling of a wide variety of phenomena, but also allowing further additions and modifications of the system""), it is less clear what is meant by a system ""capable of self-correcting evolution"" (p. 12). Chapter 1, a short history of the field of MT in the world, cow;r.s developments in Japan, as might be expected, but also in the Soviet Union, and it ends with an optimistic asses,lment of the effects of the ALPAC Report, one of",1990,CL,0.0
Book Reviews: Studies in Computer-Aided Lexicology,"The last two chapters of the book form Part IV, subtitled ""Explorations."" The first of these comes as something of an agreeable surprise, and is certainly the best chapter in the book. The authors, Hauenschild and Busemann, investigate the possibility of adapting GPSG to MT. In particular, they address the problem of developing a ""constructive"" (or constructional) version of the ""purely axiomatic"" version found in Gazdar et al. (1985). Here at last is something of an answer to the very apposite question posed by Kimmo Kettunen (1986) in this journal. The second chapter might have been a similar exploration of LFG by Schmidt. Instead, it ""tries to give an idea of how to overcome some weaknesses of [EUROTRA's] CATformal ism. . , by relating it to LFG"" (p. 239). The last two sentences of the book seem to confirm the impression that perhaps they might have done better just to start with LFG in the first place:",1990,CL,-0.2
Briefly Noted,"indexed under ""Pr~iferenzsemantik,"" the other under ""preferences."" A work as big as this is sure to contain something to offend everyone. Some of the articles, rather than being well-fitting pieces in structure, are perhaps a little too idiosyncratic; for example, Allen's lengthy complaints about the paucity of computer-assisted stylistic studies of Spanish text. And some authors are more adept than others at making their topic comprehensible to the newcomer; the same article by Allen is particularly reader-friendly. But despite the handbook's size, what I noticed most were the omissions. The short article on computer-assisted language teaching does little justice to current research into the application of sophisticated CL methods to the problem. I could find nothing on dealing with ill-formed input (or did the index let me down?). Transformational g rammar is mentioned a number of times, but government-binding theory is not. Research in discourse structure is hardly mentioned, except, unexpectedly, in the article on language generation. No form of the term ""anaphora"" appears in the index (though there is at least a passing mention of the problem (p. 270) in Lenders's introduction to the sections on processing). But such complaints should not be allowed to obscure the wealth that can be found in this handbook. I just wish that it were a little easier to find what one is looking for.",1990,CL,0.0
A Multi-Level Account of Cleft Constructions in Discourse,"This pal~;r presents an analysis and synthesis of the factors relevant to the decision to use a cleft construction in discourse. The model described, based on a corpus of 587 naturally-occurring cleft constructions in written and spoken discourse, consists of two stages. The first stage concerus the decision to use a cleft construction rather than a non-cleft; the second describes the factors relevant in deciding between three types of cleft: it-clefts, such as that in example (1), wh-clefts, such as (2), m~d reverse wh-clefts, such as (3):",1990,COLING,0.0
"GPSG Parsing, Bidirectional Charts, and Connection Graphs","Ttfs paper describes a tractable method for parsing GPSG grmnmars without ,altering the modularity and expressiveness of this tbnnalism. The proposed method is based on a constraint propagation mech,'ufism which reduces the number of unnecessary structures built at parse thne through the early detection of inadmissible local trees. The propagation of constraints is rendered efficient by indexing constraints and categories in a connection graph m~d by using a bidirectional chat1 pm~er together with a botlore-up strategy centered around head constituents.",1990,COLING,1.0
Corpus Work With Pc Beta,"0. Abstract. PC Beta is a PC oriented tool for corpus work in this term's broadest possible sense. With PC Beta one can prepare texts for corpus work, e.g. standardize texts in different ways (very important when texts from different sources together will constitute a corpus), one can process texts, and one can analyze texts. Making ordinary concordances and similar things with PC Beta is, of course, very simple, and, in fact, PC Beta give, s ""concordance making"" a new dimension. One can perform morphological analyses, one can use PC Beta as a ""tagger"", i.e. provide the words with different kinds of tags. In all, PC Beta is a versatile program, and it is in many cases the only program needed (together with functions belonging to the MS/PC-DOS operative system) for pursuing a complete corpus project. The program's main distinctive feature is simplicity: it is rule controlled, and the rules adhere to a format that any linguist can learn to understand very quickly. But beware, in spite of its innocent appearence the program i,; a little tiger.",1990,COLING,0.0
Structured Meanings in Computational Linguistics,"Many natural language processing systems employ t ruth conditional knowledge representations (%ret)resentations' , etc.) to represent meanings of nata rm language expressions. T-representations have their strong and their weak sides. A strong side is logic: a relation of logical consequence can be de-. fined between such representations. A weak side is e x p r e s s i v e p o w e r : the capacity of t-representations to convey the subtleties of natural language is limited. For instance, let SL be a sentence that is true on purely logical grounds; then it is predicted ttmt any sentence S is synonymous with ""S and SL"". This deficiency comes out clearest in propositional attitude constructions, i.e. constructions of the form 'x V that S'; where V is an epistemic verb ('knows', ~believes') and S a sentence. Truth conditional accounts of nleaning (including intensional ones such an [Montague 1974]) predict wrongly ~hat anybody who knows that S is bound to also know that :'S and SL"", since t;he two sentences are t-indistinguishable ([Peters and Saarinen 1982]). The same lack of expressive power dooms, for example, automatic translation on the basis of t-representations to failure: trepresentations contain only information that is relevant for the truth or falsity of a sentence, dismissing all other information, such an mood, topic-con:merit ,,,tructure, etc. ([van Deemter-89]).",1990,COLING,0.0
Linear Encodings of Linguistic Analyses,"1. (1) is of course highly unnatural in a sense. However, it effectively isolates for study a phenomenon that is intrinsic to natural language. Similar observations apply to the examples below. 2. It is of course also the case that an exponentially long answer caunot be produced in polynomial time. If the problem cannot be reformulated so that answers are not exponentially long, the question of tractability does not arise. See [Garey and Johnson 79] and [Barton, Berwick, and Ristad 87] for related discussions. basis than exponential-space encodings for explanations of how humans process language.",1990,COLING,0.0
A Type-theoretical Analysis of Complex Verb Generation,"Tense and aspect, together with mood and modality, usually form the entangled structure of a complex verb. They are often hard to translate by machines, because of both syntactic and semantic differences between languages. This problem seriously affects upon the generation process because those verb components in interlingua are hardly rearranged correctly in the target language. We propose here a method in which each verb element is defined as a mathematical function according to its type of type theory. This formalism gives each element its legal position in the complex verb in the target language and certifies so-called partial translation. In addition, the generation algorithm is totally free from the stepwise calculation, and is available on parallel architecture.",1990,COLING,0.30000000000000004
Expressive Power of Grammatical Formalisms,"We propose formalisms and concepts which allow to make precise the m'gmnents in controversies over the adequacy of competing models of language, and over their formal equivalence.",1990,COLING,0.5
Discourse Processing in MT: Problems in Pronominal Translation,"1. Intrcnluction Translation of anaphoric expressions has been problematic in most of the MT systems (Key 1986). One ot' the main reasons for the difficulties lies in the lack of discourse information representation in the MT systems. In this paper, we report an implementation of the Discourse Representation Theory in an LFG-based Engl ishto-Japanese MT program, and discuss problems in translating anaphoric expressions in this system. 2. Problems in Translating Anaplmra Problems in translation of anaphoric expressions can be seen on three different but interactive levels of l inguistic information: lexical, syntact ic and pragmatic. The main problem on the lexical level is due to a difference in the language specific parameters in the pronominal system such as F features (person, gender, number, etc.). Surface forms of pronominals depend on the F features of their antecedents, so that the translation of a particular pronominal form cannot be determined sorely by the pronominal itself. For example, 'ship' in English is feminine, but its translation, 'hune', is neutral in Japanese. Thus, a preform 'she' for 'ship' should not be translated as 'kanojo' (3rd, sing, fem), but as 'sore' (3rd, sing, neut). Problems on the syntactic level are mainly due to a difference in the distribution of anaphoric expressions. Mapping relations between English pronominals and their Japanese counterparts are shown below:",1990,COLING,0.0
Organizing linguistic knowledge for multilingual generation,"Abs t rac t We propose an architecture for the organisation of linguistic knowledge which allows to (1) separately formulate generalizations for different types of linguistic information, and (2) state interrelations between partial information belonging to different levels of description. We use typed feature structures for encoding linguistic knowledge. We show the application of this representational device for the architecture of linguistic knowledge sources for nmltilingum generation. As an example, we describe the use of interacting collocational and syntactic constraints in the generation of French and German sentences.",1990,COLING,0.5
Designing Linear Threshold Based Neural Network Pattern Classifiers,"The three problems that concern us are identifying a natural domain of pattern classification applications of feed forward neural networks, selecting an appropriate feedforward network architecture, and assessing the tradeoff between network complexity, training set size, and statistical reliability as measured by the probability of incorrect classification. We close with some suggestions, for improving the bounds that come from VapnikChervonenkis theory, that can narrow, but not close, the chasm between theory and practice. 1 Speculations on Neural Network Pattern Classifiers (1) The goal is to provide rapid, reliable classification of new inputs from a pattern source. Neural networks are appropriate as pattern classifiers when the pattern sources are ones of which we have little understanding, beyond perhaps a nonparametric statistical model, but we have been provided with classified samples of features drawn from each of the pattern categories. Neural networks should be able to provide rapid and reliable computation of complex decision functions. The issue in doubt is their statistical response to new inputs. (2) The pursuit of optimality is misguided in the context of Point (1). Indeed, it is unclear what might be meant by 'optimality' in the absence of a more detailed mathematical framework for the pattern source. (3) The well-known, oft-cited 'curse of dimensionality' exposed by Richard Bellman may be a 'blessing' to neural networks. Individual network processing nodes (e.g., linear threshold units) become more powerful as the number of their inputs increases. For a large enough number n of points in an input space of d dimensions, the number of dichotomies that can be generated by such a node grows exponentially in d. This suggests that, unlike all previous efforts at pattern classification that required substantial effort directed at the selection of low-dimensional feature vectors so as to make the decision rule calculable, we may now be approaching a",1990,NIPS,0.30000000000000004
The Devil and the Network: What Sparsity Implies to Robustness and Memory,"Robustness is a commonly bruited property of neural networks; in particular, a folk theorem in neural computation asserts that neural networks-in contexts with large interconnectivity-continue to function efficiently, albeit with some degradation, in the presence of component damage or loss. A second folk theorem in such contexts asserts that dense interconnectivity between neural elements is a sine qua non for the efficient usage of resources. These premises are formally examined in this communication in a setting that invokes the notion of the ""devil"" 1 in the network as an agent that produces sparsity by snipping connections. 1 ON REMOVING THE FOLK FROM THE THEOREM Robustness in the presence of component damage is a property that is commonly attributed to neural networks. The content of the following statement embodies this sentiment. Folk Theorem 1: Computation in neural networks is not substantially affected by damage to network components. While such a statement is manifestly not true in general-witness networks with ""grandmother cells"" where damage to the critical cells fatally impairs the computational ability of the network-there is anecdotal evidence in support of it in 1 Well, maybe an imp.",1990,NIPS,0.0
A Short-Term Memory Architecture for the Learning of Morphophonemic Rules,"Despite its successes, Rumelhart and McClelland's (1986) well-known approach to the learning of morphophonemic rules suffers from two deficiencies: (1) It performs the artificial task of associating forms with forms rather than perception or production. (2) It is not constrained in ways that humans learners are. This paper describes a model which addresses both objections. Using a simple recurrent architecture which takes both forms and ""meanings"" as inputs, the model learns to generate verbs in one or another ""tense"", given arbitrary meanings, and to recognize the tenses of verbs. Furthermore, it fails to learn reversal processes unknown in human language.",1990,NIPS,-1.0
Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches,Simple classical spin models well-known to physicists as the ANNNI and Heisenberg XY Models. in which long-range interactions occur in a pattern given by the Mexican Hat operator. can generate many of the structural properties characteristic of the ocular dominance columns and iso-orientation patches seen in cat and primate visual cortex.,1990,NIPS,0.0
Connectionist Approaches to the Use of Markov Models for Speech Recognition,"Previous work has shown the ability of Multilayer Perceptrons (MLPs) to estimate emission probabilities for Hidden Markov Models (HMMs). The advantages of a speech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. This paper presents results on the speaker-dependent portion of DARPA's English language Resource Management database. Results support the previously reported utility of MLP probability estimation for continuous speech recognition. An additional approach we are pursuing is to use MLPs as nonlinear predictors for autoregressive HMMs. While this is shown to be more compatible with the HMM formalism, it still suffers from several limitations. This approach is generalized to take account of time correlation between successive observations, without any restrictive assumptions about the driving noise.",1990,NIPS,-0.2
Subject-Dependent Co-Occurrence and Word Sense Disambiguation,"We describe a method for obtaining subject-dependent word sets relative to some (subjecO domain. Using the subject classifications given in the machine-readable version of Longman's Dictionary of Contemporary English, we established subject-dependent cooccurrence links between words of the defining vocabulary to construct these ""neighborhoods"". Here, we describe the application of these neighborhoods to information retrieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work.",1991,ACL,0.0
Non-Literal Word Sense Identification Through Semantic Network Path Schemata,"When computer programs disambiguate words in a sentence, they often encounter non-literal or novel usages not included in their lexicon. In a recent study, Georgia Green (personal communication) estimated that 17% to 20% of the content word senses encountered in various types of normal English text are not fisted in the dictionary. While these novel word senses are generally valid, they occur in such great numbers, and with such little individual frequency that it is impractical to explic i ty include them all within the lexicon. Instead, mechanisms are needed which can derive novel senses from existing ones; thus allowing a program to recognize a significant set of potential word senses while keeping its lexicon within a reasonable size.",1991,ACL,0.0
A Preference-first Language Processor: Integrating the Unification Grammar and Markov Language Model for Speech Recognition Applications,"A language processor is to find out a most promising sentence hypothesis for a given word lattice obtained from acoustic signal recognition. In this paper a new language processor is proposed, in which unification granunar and Markov language model are integrated in a word lattice parsing algorithm based on an augmented chart, and the island-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules. Test results""show that significant improvements in both correct rate of recognition and computation speed can be achieved .",1991,ACL,1.0
A Computational Model of Metaphor Interpretation,"Section 6 (""On Translation"") consists of a single paper by van der Korst in which he presents a FG MT system. The principles of the system are very simple. The predication underlying a linguistic expression is language-neutral; therefore the theory provides a ready-made interlingua. If it is possible to parse and to generate, then it is also possible to translate. Of course, this is an oversimplification, since different languages typically use different subsets of the set of possible predicates. Thus, paraphrasing relations between predications are necessary. Van der Korst provides a lot of useful examples to illustrate the problems and achievements of his system. The verdict: This is an important book, since it begins to sketch what a computational version of FG might look like. It is very important for people working in functional paradigms such as FG to bring their insights about language use to the design of NLP systems, which will have real users. However, the book is ultimately disappointing for a number of reasons. It has the feel of a collection of disparate papers that are united in their debt to Dik (1978) rather than by their participation in a coherent research program. The papers are inadequately cross-referenced and display many needless inconsistencies of style (e.g., ""PROLOG"" vs. ""Prolog'; endnotes vs. footnotes). The papers build very few bridges between computational FG and what is going on in the rest of NLP. As we have noted, some of the attempts to do so misfire. Perhaps most disappointing of all, the volume fails to raise what ought to be the most interesting question: what, if any, are the distinctive benefits of functional theories such as FG for NLP?",1991,CL,-1.0
A Computational Model of First Language Acquisition,"In a recent survey of early language acquisition, Gleitman, Gleitman, Landau, and Wanner (1988) cite Leonard Bloomfield (1933, p. 29) as remarking that ""language learning is doubtless the greatest intellectual feat any one of us is ever required to perform."" Given this, it is equally no small feat to attempt to build a computer model that does the same thing. Satake's book is one of but a handful of attempts in the computational linguistics tradition to take up this challengemsomewhat surprising given the vast range of linguistic and psychological literature on the subject. Perhaps it is because linguists and psychologists can try to digest just one piece of the acquisition puzzle, while a computational model must typically try to gobble a major chunk of language acquisition whole, or risk being called a mere toy. In this light, Satake should be congratulated for trying to present, in one brief volume, a computational model that attempts to handle facts about morpheme acquisition and intonation; varying word order across languages; verb subcategorization; and classic rule overgeneralization, while at the same time at least paying some attention to what psychologists know about child language. One then obviously runs the risk of stretching too thin, and in fact the volume under review runs far too short in large type. Readers looking for answers to these rich subjects mentioned just above will come away disappointed by a sketch that ultimately can only approximate what computer modeling did in this area more than ten years ago (work by Anderson 1977; Selfridge 1981; and Berwick 1979, 1985). The book exercises the model with a very limited range of sample sentences--just nine examples, with no recursion. More unfortunately, given the emphasis on free word order, no Japanese examples are included. The first quarter of the book is devoted to a rather thin outline of some of the basic psychological results on input available to the child and learnability theory, while the remainder is devoted to the three components of (sub)category generalization, a case analysis of the system working on the examples and a short study of over-regularization, and the use of teacher correction in a so-called ""production mode"" to repair mistakes. This last point is quite important, for Satake's intended novel contribution to this older literature is stated clearly at the outset: to build an empiricist model of acquisition that is cognitively faithful--that is, one where the structure of language is ""out there"" in the world and formed by inductive generalization, special properties of parental input (motherese), the order of examples (including negative examples), and the like rather than ""in there""--the child's head. Satake means this of course as the polar opposite of ""innate"" acquisition procedures, which assume a richly structured knowledge of language to begin with. (Satake labels these as ""passive"" acquisition models",1991,CL,0.0
Book Reviews: Functional Grammar and the Computer,"Linguistic theories can be divided into two varieties: those based on the form of linguistic expressions and those based on the function of linguistic expressions. Most presentday theories belong to the former variety. Because of their formal explicitness, the rule systems of these theories can be built into computer applications fairly straightforwardly, although they provide no indication of the contexts and ways in which they should be applied appropriately. Theories based on linguistic function address matters of appropriate usage but frequently fail to supply enough low-level formal detail to facilitate computer implementation. Halliday's Systemic (Functional) Grammar (Butler 1985) is an example of a functional theory that has served as the basis for some interesting computer systems, especially those concerned with text generation. This is probably less attributable to the functional credentials of Systemic Grammar than to its exceptionally useful descriptive tool, the system network (e.g., Mellish 1988). Connolly and Dik's volume presents the first major survey of NLP work based on another theory on functional principles, Dik's Functional Grammar (FG) (Dik 1978). FG views words as predicates. The lexicon consists of a list of predicate frames. A linguistic expression, i.e., a string of predicates, is analyzed in terms of an abstract underlying predication. Predicates and predications make no reference whatsoever to the surface forms of expressions. Predications are related to surface orderings by means of expression rules. These constitute the nearest thing FG has to a syntactic component. Functional Grammar and the Computer consists of six sections containing a total of 16 chapters. Section I is a good introduction by Dik to FG-based NLP and the papers in the collection. He locates the work reported in the book within the framework of a research program bearing the improbable name of FG*C*M*NLU (Functional Grammar Computational Model of the Natural Language User). I have no idea how to pronounce this. The book does not make clear whether FG*C*M*NLU is a single research project with coherent aims or simply the name by which all computational work is known within the FG community. A number of references suggest the former reading (e.g., Kahrel, p. 145; Connolly, p. 218) and this is made all the more plausible by the fact that 12 of the 15 authors are based in Amsterdam. However, many of the papers make no reference to FG*C*M*NLU and instead describe work carried out under rubrics such as ASCOT, LEXALYSE, LIKE, and LINKS. Section 2 (""On Generating'9 consists of two papers devoted to sentence generation by Samuelsdorff and Bakker. Samuelsdorff presents program examples written in PROLOG2, whereas all of the other authors who provide program examples use standard Edinburgh PROLOG. Sentence generation is achieved by applying expression rules to predications.",1991,CL,0.0
Erratum to: A Statistical Approach to Machine Translation,"In Section 6 of ""A statistical approach to machine translation"" (Computational Linguistics 16(2), 79-85), we reported the results of two experiments in which we estimated parameters of a statistical model of translation from English to French. In the first experiment, the English and French vocabularies each consisted of 9,000 common words, and the model parameters were estimated from 40,000 pairs of sentences 25 words or less in length. Words outside the 9,000-word vocabularies in these sentences were mapped to special unknown words. In the second experiment, the vocabularies were limited to 1,000 common English words and 1,700 common French words, and the model parameters were estimated from 117,000 pairs of sentences 10 words or less in length that were completely covered by the respective vocabularies. In Figures 4, 5, and 6 of the paper, we erroneously presented parameter estimates from the 1,000-word experiment, while claiming in the text that they were from the 9,000-word experiment. The parameter estimates for these two experiments differ considerably because of the restriction of the training corpus in the 1,000-word experiment to short, covered sentences. For example, the probability that hear is translated as bravo",1991,CL,0.0
"Chinese Number-Names, Tree Adjoining Languages, and Mild Context-Sensitivity","The Tree Adjoining Grammar formalism, both its singleas well as multiple-component versions, has recently received attention as a basis for the description and explication of natural language. We show in this paper that the number-name system of Chinese is generated neither by this formalism nor by any other equivalent or weaker ones, suggesting that such a task might require the use of the more powerful Indexed Grammar formalism. Given that our formal results apply only to a proper subset of Chinese, we extensively discuss the issue of whether they have any implications for the whole of that natural language. We conclude that our results bear directly either on the syntax of Chinese or on the interface between Chinese and the cognitive component responsible for arithmetic reasoning. Consequently, either Tree Adjoining Grammars, as currently defined, fail to generate the class of natural languages in a way that discriminates between linguistically warranted sublanguages, or formalisms with generative power equivalent to Tree Adjoining Grammar cannot serve as a basis for the interface between the human linguistic and mathematical faculties.",1991,CL,-0.5
Antilinguistics: A Critical Assessment of Modern Linguistic Theory and Practice,"One danger attendant upon reviewing a book as bad as this knuckle-headed assault on linguists and all they stand for is that one may inadvertently suggest by the length and vehemence of one's commentary that the book is worthy of extended discussion. Another is that by quoting the most strikingly absurd parts of the book the review might give an exaggerated sense of its entertainment value (this is not really a funny book, despite a few howlers). Yet perhaps a brief comment is called for, lest silence imply assent. I would not dissent from the claim that there is plenty to be criticized in the present state of linguistics. The unjustified pretentions of much generative grammar and the inadequacies of some of its woeful attempts at description merit harsh criticism-maybe even as harsh as the sort of ridicule and cruelty that I dispense elsewhere (Pullum 1991). But there is no utility in a purported challenge to a whole discipline written by someone who deals in naive panaceas and seems unable to understand the primary literature. The production of this book is bad enough to make one suspect an amateur job: the page layout is often bad, the style is often awkward, and there are signs of carelessness (for example, beyond p. 180 every page number the index gives is incorrect). But the content is far worse. Sophomoric blunders abound: underlying constituent order confused with what speakers ""start with ... in their heads"" and surface structure with ""what they actually say"" (p. 30); innate general language acquisition mechanisms confused with innate rules for English grammar (p. 46); uncomprehended formulae botched (the arrowless ""transformation"" on p. 23, for example); well-known names are wrong (""Deidre Wilson,"" pp. 28, 275; ""James Fodor,"" p. 195n; etc.); in a book on this level it is no surprise to find the old chestnut about the Eskimos' many words for snow turning up yet again (p. 221). Gethin argues that modern linguistics is a jumble of cabalistic nonsense, and can be swept away by the simple truth that the key to everything is meaning. Language acquisition is no puzzle: people learn meanings ""by observation and imitation,"" and they ""join individual meanings together so that they make larger meanings"" (p. 9). That is all one needs to know about language; but modern linguists are afflicted with a ""systematizing mania that pretends to discover new profundity in what everybody knows already"" (pp. 11-12); they fail to see that ""there is no such thing as structure in language"" (p. 93). ""There is no mystery"" (p. 108), there is only meaning. The targets of the book's ad hominem attacks include not only (of course) Noam Chomsky, but also an odd assortment of popularizers and interpreters. In fact, the bibliography is almost entirely restricted to secondary and tertiary sources like magazine articles, radio interviews, and pop science books, which exacerbates misreadings (see e.g.p. 187, where Gethin discusses a paraphrase by Jeremy Campbell of some remarks",1991,CL,0.0
As Time Goes By: Tense and Universal Grammar,"This book consists of an analysis, solidly within the framework of Government and Binding, of tense and its interaction with adverbials, temporal connectives, and complementation. There is both good news and bad news about the book; parts of it are carefully worked out and are therefore a valuable resource for the computational linguist, while other parts are less clearly argued. Like much current GB literature, Hornstein takes the goal of linguistic research to be the discovery of the nature of the innate language capacity; this is, of course, a laudable goal. Unfortunately, however, the argument that is presented takes a form that will no doubt prove bewildering to many. The argument, which pervades the book and forms the backbone of its first few chapters, resembles other arguments that have been presented within the same framework: a list of sentences is presented, some ungrammatical and starred; a formal model (one of many possible formal models) of some aspect of the structure of these sentences is presented, with rules for determining ill-formedness in the model and predicting ungrammaticality in the corresponding string; an argument is made that the nature of the formal model is such that it could not be acquired on the basis of primary data available to the child; and the claim is then made that the formal model that has been presented is (in fact, must be) a part of the innate linguistic competence that humans possess. There are several problems with this sort of argument. First, there are often a number of nonequivalent formal models that are consonant with the same set of data. Some of these models are more appealing than others; coming up with one model that covers the data does not in itself constitute proof that that particular model is the one that is innate and should be adopted. Indeed, an appeal to innateness has an insidious consequence: it essentially obviates the need for criteria for distinguishing between possible formal models of the same data. If any formal model can be claimed to be part of the innate language apparatus, there is no need to search for a simpler or cleaner or more parsimonious model than the one that has been proposed; any model is as good as any other, since all models can be innate. Second, the assumptions that are necessary to the claim that the model is unlearnable are not always thoroughly substantiated. In particular, Hornstein claims that ""of the data theoretically available to the child, it is likely that only the simple sentences can be absorbed . . . . The child.., is limited to an informationally restricted subset of the potentially relevant data"" (p. 2). This claim is made without citation of substantiating psycholinguistic or language acquisition literature; Hornstein stipulates, in effect, that the system he proposes is innate since it could not be learned on the basis of the subset of data he admits as relevant. Hornstein presents a 'neo-Reichenbachian' analysis of tense, by which he means past, present, and future tense as well as perfect aspect; other questions of the treatment",1991,CL,0.0
Books Received,"Books listed below that are marked with a t will be reviewed in a future issue. Authors and publishers who wish their books to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada MSS 1A4. All books received will be listed, but not all can be reviewed. Readers who wish to review books for the journal should write, outlining their qualifications, to the book review editor at the address above. Obviously, we cannot promise the availability of books in anyone's exact area of interest.",1991,CL,0.0
met*: A Method for Discriminating Metonymy and Metaphor by Computer,"The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences. In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate. Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly. Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter. The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5. Some examples of meta5's analysis of metaphor and metonymy are given. The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology.",1991,CL,0.0
Books Received,"Books listed below that are marked with a t have been selected for review in a future issue, and reviewers have been assigned to each. Authors and publishers who wish their books to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 1A4. All books received will be listed, but not all can be reviewed. Readers who wish to review books for the journal should write, outlining their qualifications, to the book review editor at the address above or send electronic mail to gh@cs.toronto.edu. Obviously, we cannot promise the availability of books in anyone's exact area of interest.",1991,CL,0.0
Benchmarking Feed-Forward Neural Networks: Models and Measures,"Existing metrics for the learning performance of feed-forward neural networks do not provide a satisfactory basis for comparison because the choice of the training epoch limit can determine the results of the comparison. I propose new metrics which have the desirable property of being independent of the training epoch limit. The efficiency measures the yield of correct networks in proportion to the training effort expended. The optimal epoch limit provides the greatest efficiency. The learning performance is modelled statistically, and asymptotic performance is estimated. Implementation details may be found in (Harney, 1992).",1991,NIPS,1.0
Fast Learning with Predictive Forward Models,"A method for transforming performance evaluation signals distal both in space and time into proximal signals usable by supervised learning algorithms, presented in [Jordan & Jacobs 90], is examined. A simple observation concerning differentiation through models trained with redundant inputs (as one of their networks is) explains a weakness in the original architecture and suggests a modification: an internal world model that encodes action-space exploration and, crucially, cancels input redundancy to the forward model is added. Learning time on an example task, cartpole balancing, is thereby reduced about 50 to 100 times.",1991,NIPS,0.6000000000000001
An Empirical Analysis of Terminological Representation Systems,"The family of terminological representation systems has its roots in the representation system KLONE. Since the development of this system more than a dozen similar representation systems have been developed by various research groups. These systems vary along a number of dimensions. In this paper, we present the results of an empirical analysis of six such systems. Surprisingly, the systems turned out to be quite diverse leading to problems when transporting knowledge bases from one system to another. Additionally, the runtime performance between different systems and knowledge bases varied more than we expected. Finally, our empirical runtime performance results give an idea of what runtime performance to expect from such representation systems. These findings complement previously reported analytical results about the computational complexity of reasoning in such systems.",1992,AAAI,0.0
Right Association Revisited,Consideration of when Right Association works and when it fails lead to a restatement of this parsing principle in terms of the notion of heaviness. A computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstances when RA is likely to make correct attachment predictions.,1992,ACL,0.0
Some Problematic Cases of VP Ellipsis,"It has been widely assumed that VP ellipsis is governed by an identity condition: the elided VP is interpreted as an identical copy of another expression in surrounding discourse. For example, Sag (76) imposes an identity condition on Logical Form representations of VP's. A basic feature of this account is the requirement that a syntactic VP be available as the antecedent. This requirement is reflected in most subsequent accounts as well. In this paper I examine three cases of VP ellipsis in which the antecedent cannot be identified with any VP. These cases, which are illustrated using naturallyoccurring examples, present a fundamental problem for any of the standard approaches. I will argue that they receive a natural treatment in the system I have developed, in which VP ellipsis is treated by storing VP meanings in a discourse model.",1992,ACL,0.30000000000000004
GPSM: A Generalized Probabilistic Semantic Model for Ambiguity Resolution,"In natural language processing, ambiguity resolution is a central issue, and can be regarded as a preference assignment problem. In this paper, a Generalized Probabilistic Semantic Model (GPSM) is proposed for preference computation. An effective semantic tagging procedure is proposed for tagging semantic features. A semantic score function is derived based on a score function, which integrates lexical, syntactic and semantic preference under a uniform formulation. The semantic score measure shows substantial improvement in structural disambiguation over a syntax-based approach.",1992,ACL,1.0
Estimating Upper and Lower Bounds on the Performance of Word-Sense Disambiguation Programs,"We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often enough so that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges.",1992,ACL,1.0
Incremental Dependency Parsing,"The paper introduces a dependency-based grammar and the associated parser and focusses on the problem of determinism in parsing and recovery from errors. First, it is shown how dependency-based parsing can be afforded, by taking into account the suggestions coming from other approaches, and the preference criteria for parsing are briefly addressed. Second, the issues of the interconnection between the syntactic analysis and the semantic interpretation in incremental processing are discussed and the adoption of a TMS for the recovery of the processing errors is suggested. T H E B A S I C P A R S I N G A L G O R I T H M The parser has been devised for a system that works on the Italian language. The structure that results from the parsing process is a dependency tree, that exhibits syntactic and semantic information. The dependency structure: The structure combines the traditional view of dependency syntax with the feature terms of the unification based formalisms (Shieber 86): single attributes (like number or tense) appear inside the nodes of the tree, while complex attributes (like grammatical relations) are realized as relations between nodes. The choice of a dependency structure, which is very suitable for free word order languages (Sgall et al. 86), reflects the intuitive idea of a language with few constraints on the order of legal constructions. Actually, the flexibility of a partially configurational language like Italian (that can be considered at an intermediate level between the totally configurational languages like English and the totally inflected free-ordered Slavonic languages) can be accounted for with a relaxation of the strong constraints posed by a constituency grammar (Stock 1989) or by constraining to a certain level a dependency grammar. Cases of topicalization, like un dolce di frutta ha ordinato il maestro a cake with fruits has ordered the teacher and in general all the five permutations of the ""basic"" (i.e. more likely) SVO structure of the sentence are so common in Italian, that it seems much more economical to express the syntactic knowledge in terms of dependency relations. Every node in the structure is associated with a word in the sentence, in such a way that the relation between two nodes at any level is of a head&modifier type. The whole sentence has a head, namely the verb, and its roles (the subj is included) are its modifiers. Every modifier in turn has a head (a noun, which can be a proper, common or pro-noun, for participants not marked by a preposition, a preposition, or a verb, in case of subordinate sentences not preceded by a conjunction) and further modifiers. Hence the dependency tree gives an immediate representation of the thematic structure of the sentence, thus being very suitable for the semantic interpretation. Such a structure also allows the application of the rules, based on grammatical relations, that govern complex syntactic phenomena, as revealed by the extensive work on Relational Grammar. The dependency grammar is expressed declaratively via two tables, that represent the relations of immediate dominance and linear order for pairs of categories. The constraints on the order between a head and one of its modifiers and between two modifiers of the same head are reflected by the nodes in the dependency structure. The formation of the complex structure that is associated with the nodes is accomplished by means of unification: the basic terms are originated by the lexicon and associated with the nodes. There exist principles that govern the propagation of the features in the dependency tree expressed as analogous conventions to GPSG ones. The incremental parser: In the system, the semantic, as well as the contextual and the anaphoric binding analysis, is interleaved with the syntactic parsing. The analysis is incremental, in the sense that it is carried out in a piecemeal strategy, by taking care of partial results too. In order to accomplish the incremental parsing and to build a dependency representation of the sentence, the linguistic knowledge of the two tables is",1992,ACL,0.5
Inside-Outside Reestimation From Partially Bracketed Corpora,"1. M O T I V A T I O N The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models (HMMs) (Jelinek et al., 1992). However, finite-state models cannot represent the hierarchical structure of natural language and are thus ill-suited to tasks in which that structure is essential, such as language understanding or translation. It is then natural to consider stochastic versions of more powerful grammar formalisms and their grammatical inference problems. For instance, Baker (1979) generalized the parameter estimation methods for HMMs to stochastic context-free grammars (SCFGs) (Booth, 1969) as the inside-outside algorithm. Unfortunately, the application of SCFGs and the original inside-outside algorithm to natural-language modeling has been so far inconclusive (Lari and Young, 1990; Jelinek et al., 1990; Lari and Young, 1991). Several reasons can be adduced for the difficulties. First, each iteration of the inside-outside algorithm on a grammar with n nonterminals may require O(n3[wl 3) time per training sentence w, 128 while each iteration of its finite-state counterpart training an HMM with s states requires at worst O(s2lwl) time per training sentence. That complexity makes the training of suffÉciently large grammars computationally impractical. Second, the convergence properties of the algorithm sharply deteriorate as the number of nonterminal symbols increases. This fact can be intuitively understood by observing that the algorithm searches for the maximum of a function whose number of local maxima grows with the number of nonterminals. Finally, while SCFGs do provide a hierarchical model of the language, that structure is undetermined by raw text and only by chance will the inferred grammar agree with qualitative linguistic judgments of sentence structure. For example, since in English texts pronouns are very likely to immediately precede a verb, a grammar inferred from raw text will tend to make a constituent of a subject pronoun and the following verb. We describe here an extension of the inside-outside algorithm that infers the parameters of a stochastic context-free grammar from a partially parsed corpus, thus providing a tighter connection between the hierarchical structure of the inferred SCFG and that of the training corpus. The algorithm takes advantage of whatever constituent information is provided by the training corpus bracketing, ranging from a complete constituent analysis of the training sentences to the unparsed corpus used for the original inside-outside algorithm. In the latter case, the new algorithm reduces to the original one. Using a partially parsed corpus has several advantages. First, the the result grammars yield constituent boundaries that cannot be inferred from raw text. In addition, the number of iterations needed to reach a good grammar can be reduced; in extreme cases, a good solution is found from parsed text but not from raw text. Finally, the new algorithm has bet ter t ime complexity when sufficient bracketing information is provided. 2 . P A R T I A L L Y B R A C K E T E D T E X T Informally, a partially bracketed corpus is a set of sentences annota ted with parentheses marking constituent boundaries that any analysis of the corpus should respect. More precisely, we s tar t f rom a corpus C consisting of bracketed strings, which are pairs e = (w,B) where w is a string and B is a bracketing of w. For convenience, we will define the length of the bracketed string c by Icl = Iwl. Given a string w = wl ..-WlM, a span of w is a pair of integers ( i , j ) with 0 < i < j g [w[, which delimits a substring iwj = wi+y . . .wj of w. The abbreviation iw will s tand for iWl~ I. A bracketing B of a string w is a finite set of spans on w ( that is, a finite set of pairs or integers (i, j ) with 0 g i < j < [w[) satisfying a consistency condition tha t ensures that each span (i, j ) can be seen as delimiting a string iwj consisting of a sequence of one of more. The consistency condition is simply tha t no two spans in a bracketing may overlap, where two spans (i, j ) and (k, l) overlap if either i < k < j < l or k < i < l < j . Two bracketings of the same string are said to be compatible if their union is consistent. A span s is valid for a bracketing B if {s} is compatible with B. Note that there is no requirement tha t a bracketing of w describe fully a constituent s t ructure of w. In fact, some or all sentences in a corpus may have empty bracketings, in which case the new algori thm behaves like the original one. To present the notion of compatibil i ty between a derivation and a bracketed string, we need first to define the span of a symbol occurrence in a context-free derivation. Let (w,B) be a bracketed string, and c~0 ==~ a l :=¢, . . . =~ c~m = w be a derivation of w for (S)CFG G. The span of a symbol occurrence in (~1 is defined inductively as follows: • I f j -m, c U = w E E*, and the span of wi in ~j is ( i 1, i). • I f j < m, then a j : flAT, aj+l = /3XI""'Xk')', where A -* X I "" . X k is a rule of G. Then the span of A in a j is ( i l , j k ) , where for each 1 < l < k, (iz,jt) is the span of Xl in a j+ l The spans in (~j of the symbol occurrences in/3 and 7 are the same as those of the corresponding symbols in ~ j+ l . A derivation of w is then compat ible with a bracketing B of w if the span of every symbol occurrence in the derivation is valid in B. 3 . G R A M M A R R E E S T I M A T I O N The inside-outside algori thm (Baker, 1979) is a reest imation procedure for the rule probabilit ies of a Chomsky normal-form (CNF) SCFG. It takes as inputs an initial CNF SCFG and a training corpus of sentences and it i teratively reest imates rule probabilities to maximize the probabil i ty tha t the g rammar used as a stochastic generator would produce the corpus. A reestimation algori thm can be used both to refine the parameter es t imates for a CNF SCFG derived by other means (Fujisaki et hi., 1989) or to infer a g rammar from scratch. In the lat ter case, the initial g r ammar for the inside-outside algor i thm consists of all possible CNF rules over given sets N of nonterrninals and E of terminals, with suitably assigned nonzero probabilities. In what follows, we will take N, ~ as fixed, n IN[, t = [El, and assume enumerat ions N {A1, . . . ,An} and E = {h i , . . . ,bt}, with A1 the g r a m m a r s ta r t symbol. A CNF SCFG over N , E can then be specified by the n~+ nt probabilities Bp,q,r of each possible binary rule Ap --* Aq Ar and Up,m of each possible unary rule Ap --* bin. Since for each p the parameters Bp,q,r and Up,rn are supposed to be the probabilities of different ways of expanding Ap, we must have for all 1 _< p _< n E Bp,q,r + E Up,m = 1 (7)",1992,ACL,0.0
"Computer Rules, Conversational Rules","There has been much controversy recently as to whether the rules of interaction discovered by conversation analysts are amenable to use by computers (Gilbert 1990; Hirst 1991; Luff, Gilbert, and Frohlich 1990). Button (1990) has argued that the rules of conversation are of a different ontological category than the rules used by computers, and that this means computers cannot be programmed to engage in conversation. Others (Fraser and Wooffitt 1990; Frohlich and Luff 1990; Gilbert, Wooffitt, and Fraser 1990) have argued to the contrary that the rules of conversation can be captured in a program, and indeed that some have been. I will argue for a third position. Button is right in his critique of existing attempts to import conversation analysis into computational linguistics and in his argument that there is a rule type mismatch. His arguments do not, however, show that computers cannot in principle be programmed to engage in conversation. I will argue by analogy to computer network protocols that an interactionist computational interpretation of the conversation analytical rules is possible, and that Button's critique can thereby be bypassed.",1992,CL,-1.0
A Problem for RST: The Need for Multi-Level Discourse Analysis,"Rhetorical Structure Theory (RST) (Mann and Thompson 1987), argues that in most coherent discourse, consecutive discourse elements are related by a small set of rhetorical relations. Moreover, RST suggests that the information conveyed in a discourse over and above what is conveyed in its component clauses can be der ived from the rhetorical relation-based structure of the discourse. A large number of natural language generation systems rely on the rhetorical relations defined in RST to impose structure on multi-sentential text (Hovy 1991; Knott 1991; Moore and Paris 1989; Rosner and Stede 1992). In addition, many descriptive studies of discourse have employed RST (Fox 1987; Linden, Cumming, and Martin 1992; Matthiessen and Thompson 1988). However , recent work by Moore and Paris (1992) noted that RST cannot be used as the sole means of controlling discourse structure in an interactive dialogue system, because RST representations provide insufficient information to suppor t the generation of appropriate responses to ""fol low-up questions."" The basic problem is that an RST representation of a discourse does not fully specify the intentional structure (Grosz and Sidner 1986) of that discourse. Intentional structure is crucial for responding effectively to questions that address a previous utterance: wi thout a record of what an utterance was intended to achieve, it is impossible to elaborate or clarify that utterance. 1 Further consideration has led us to conclude that the difficulty observed by Moore and Paris stems from a more fundamental problem with RST analyses. RST presumes that, in general, there will be a single, preferred rhetorical relation holding between consecutive discourse elements. In fact, as has been noted in other work on discourse structure (Grosz and Sidner 1986), discourse elements are related simultaneously on multiple levels. In this paper, we focus on two levels of analysis. The first involves the relation between the information conveyed in consecutive elements of a coherent discourse. Thus, for example, one utterance may describe an event that can be presumed to be the cause of another event described in the subsequent utterance. This causal relation is at what we will call the informational level. The second level of relation results from the fact that discourses are produced to effect changes in the mental state of the discourse participants. In coherent discourse, a speaker is carrying out a consistent plan to achieve the intended changes, and consecutive discourse elements are related to one another by means of the ways in which they participate in that plan. Thus, one utterance may be intended to increase the likelihood that the hearer will come to",1992,CL,0.0
Book Reviews: Literature and Cognition,"Literature and Cognition applies methods and ideas from computational linguistics and cognitive science to literature. It explores a wide range of topics, and is intended to be read by literary theorists as well as by computational linguists and cognitive scientists. From the perspective of computational linguistics, the largest contributions are to present Hobbs's previously published theory of discourse coherence in the context of an overall framework for discourse interpretation, and then use it to analyze a sonnet in great detail. (Hobbs also analyzes a novella in detail, but much less directly in terms of the theory.) These will be the focus of this review. The chapter presenting the overall framework (Chapter 3) is entitled ""A theory of discourse interpretation,"" but it outlines what would constitute such a theory, rather than presenting an actual theory itself. A major theme of the presentation is the author's view that a theory of discourse interpretation ""must first and foremost be a theory of how knowledge is used in solving the interpretation problems posed by the discourse"" (p. 41). Such a theory would include six subtheories. The first four are familiar: the knowledge-representation language; the encoding of background knowledge in this language; the ""deductive mechanism""; and syntactic and semantic ""translation,"" which produce logical forms. Notice that the word ""discourse"" is used with a very broad meaning in the chapter title. This is somewhat confusing, because it is used with a more narrow meaning in the discussions of the fifth and sixth subtheories. The fifth subtheory would specify the possible interpretations of a sentence, where an interpretation must solve each of the discourse problems of that sentence. Hobbs says:",1992,CL,0.0
Books Received,"Books listed below that are marked with a t have been selected for review in a future issue, and reviewers have been assigned to each. Authors and publishers who wish their books to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 1A4. All books received will be listed, but not all can be reviewed. Readers who wish to review books for the journal should write, outlining their qualifications, to the book review editor at the address above or send electronic mail to gh@cs.toronto.edu. Obviously, we cannot promise the availability of books in anyone's exact area of interest.",1992,CL,0.0
Book Reviews: English Word Grammar,"The interaction between theoretical and computational linguistics, while not often acknowledged, undoubtedly had some influence in the relative failure of the previous incarnations of Hudson's Word Grammar (also known as Dependency Grammar and Daughter-Dependency Grammar) to capture the imagination in the same way that the context-free phrase structure grammars of the 1980s did. Who needed nice theories of syntax that had relationships between words expressed with pretty drawings when one could have the infinitely more computable phrase structure rules? But Hudson was undeterred, and perhaps in the vein of the mountain coming to Mohammed, he has valiantly attempted in this volume to bring his theory of Word Grammar (WG) closer to computational linguistics and AI. This is no mean feat for someone whose background is firmly in theoretical linguistics, and a lack of familiarity with computational matters is sometimes painfully clean From the perspective of a computational linguist, some of the explanations and definitions are rather unnecessary, but it must be assumed that the book was primarily written with the theoretical linguist in mind. The theory of WG originally set out to rival phrase-structure theories of grammar, placing the emphasis on the word (hence the name Word Grammar) rather than the phrase, clause, or sentence. Such concepts played no part in the theory at all, which relied on dependencies between words (hence the original name of Dependency Grammar). Thus, a phrase such as black dogs eat cats was analyzed as having three dependency relations: between black and dogs, between eat and dogs and between eat and cats. The fact that Categorial Grammar has drawn much interest away from CF-PSGs may mean that Hudson's theory now has a more prominent role to play, and this book is certainly thought-provoking, if at times a little frustrating in the questions it asks without answering. The book is divided into two parts, the first seven chapters defining the theory, and the remaining seven providing an account of certain aspects of English, principally syntax with a little morphology and semantics. The theory has moved on quite substantially from previous versions, not only in its form, but in its coverage. While the original theory was predominantly a theory of syntax, the current version purports to be a theory of knowledge, both linguistic and nonlinguistic. That over half of the book is given over to a (fairly substantial) grammar of English, and is thus very linguistically dominated, is only partially countered by the examples in the theory section relating linguistic phenomena to nonlinguistic concepts that seem to require similar apparatus. These examples are interesting, and the broad argument that the ""bits of knowledge"" that make up our linguistic competence are only (possibly specialized) cases of our other knowledge is one that deserves closer inspection. It goes without saying that in a book of this size it would be impossible to cover the whole area in enough detail to permit definite conclusions about the",1992,CL,0.0
Letters to the Editor,"I do not believe that Geoffrey Pullum, in his attack on my Antilinguistics (Computational Linguistics, 17(2), 240) is trying to protect his livelihood. I hardly think he is in any danger there. But I do think he is protecting the intellectual pride of himself and his colleagues. That is reasonable and to be expected. What is not reasonable is that in doing this he does not report, let alone try to answer, a single one of my general arguments or, with one possible exception, any of my specific points. His method of review is to mock remarks that he has stripped of all their reasoned context, and to scorn conclusions without even hinting at the existence of the large amounts of evidence I present in justification. (Luckily for my confidence there have been other reviewers who have shown greater appreciation of my rational powers.) Most serious of all Pullum's mistakes is perhaps that several of his complaints are based on the very assumptions that I question in the book. His talk of Quirk's ""monumental descriptive work on modern English"" without mentioning any part of my case against it is one example. His contempt for my failure to master primary sources is another. I should probably have emphasized my argument about this more. But it is there, on pages 2, 195n, and 258. It is surely significant that Pullum uses p. 195n, not to explain my position, but to attack the book's poor production. He is quite right to criticize the mistakes in names (I apologize to the people concerned), the faulty index, and missing arrows. It was careless; I should have checked the proofs better. Pullum is careless himself. He misquotes a sentence from p. 56; and in the index it is only the proper names that have two figures too high (and from p. 170, not p. 180 as he states)--all the other entries are correct. In the only case where Pullum really attempts a proper discussion of the material to show that ""when by chance Gethin gets hold of the linguistic ball for a moment he unfailingly drops it"") his criticism is typical of the barren formality that I complain of: I should have located the ambiguity of Flying planes can be dangerous in the transitive/intransitive contrast in fly, not in -ing. He thus abandons reality, the words actually used, for the sake of an abstraction. And I am not simply insisting on a principle here, for in the process Pullum gets it wrong. The transitive/intransitive difference in the meanings of -ing is not the crucial one, and may not be there at all, as can be seen in, for example, The burning sun.../Burning wood (is wasteful), where (burn)ing is transitive in both cases, but has different meanings. At the same time I cannot think of any sentence where there would be any transitive/intransitive confusion through the use of an infinitive, indicative, or imperative. Can Pullum? It is sadly revealing that he suspects that my attack on Quirk's grammar is prompted by a desire to settle scores with prestigious linguists at British universities. I have no scores to settle with anyone. Is he so used to academic in-fighting that he cannot believe that I have no personal quarrel, only a general quarrel with the attitudes and assumptions, purposes and pretentions, methods and thinking, of academic social 'science' ? I am impatient because while social 'scientists' claim authority, they have failed, I believe, to deliver real results, and yet at the same time exercise intellectual dominance over the rest of the community. I repeat something I say in my book. If academic experts think their work has any importance, that it can affect people's lives in any way,",1992,CL,0.0
Reference and Computation,"Kronfeld's book attacks the problem of referring: How do speakers reveal what entities they are talking about? How do they choose particular referring expressions in their utterances? These questions are asked from the perspective of a plan-based theory of speech acts and communication. Utterances are viewed as acts that are intended by speakers to have certain effects on the addressees, and on the world more generally. Utterances are therefore to be planned in broadly the same way as other types of acts are. Kronfeld's embedding of referring in speech act theory rests partly on a Gricean formulation of the literal goal of a referring expression. The relationship of this goal to the looser notion of the discourse purpose of the expression is discussed. Another aspect of the speech act theme is the discussion of two ways in which a referring expression can be intended by a speaker to be relevant: functionally relevant or conversationally relevant. Functionally relevant expressions are used primarily to lead the addressee to identify an object. Conversationally relevant descriptions are those that are intended to focus the addressee on a specific aspect of an object. Such descriptions are related to a specific type of Gricean conversational implicature. Kronfeld carefully draws a distinction between the problem of referring and the philosophical problem of reference. The latter problem is cast as the question, ""How can thoughts (and sentences that articulate them) be about objects?"" (p. 13). Kronfeld covers quite a lot of ground on the philosophical problem of reference, as the problem of referring is somewhat dependent on it. Kronfeld concentrates throughout on reference to physical objects, and on referring expressions that are noun phrases. Furthermore, almost all the book is about definite descriptions, names and other expressions that refer to single, specific, physical objects, rather than to indefinite objects, several objects, or sets of objects. Kronfeld also stresses that he is not directly concerned with the anaphoric linking of pronouns to other noun phrases. He is directly concerned only with links between noun phrases and the world. In fact, much of the book is a defense of the descriptive approach to the philosophical problem of reference in thought and in language. In this approach, to refer to an object is essentially to have or invoke a mental representation of that object. The relationship between a sentence or thought and the objects it is about is that of denotation, which in turn is a function of descriptive content. The descriptive approach",1992,CL,0.0
"Computational Linguistics, Volume 18, Number 3, September 1992, Special Issue on Inheritance: II","Book R e v i e w s Towards a Theory of Cognition and Computing J. Gerard Wolff Papers in Laboratory Phonology I: Between the Grammar and Physics of Speech John Kingston and Mary E. Beckman, eds. S4mantique et Recherches Cognitives Francois Rastier Computational Morphology: Practical Mechanisms for the English Lexicon Graeme D. Ritchie, Graham J. Russell, Alan W. Black, and Stephen G. Pulman Intelligent User Interfaces Joseph W. Sullivan and Sherman W. Tyler, eds. Adaptive Parsing: Self-Extending Natural Language Interfaces Jill Fain Lehman Generalized LR Parsing Masaru Tomita, ed. Literature and Cognition Jerry R. Hobbs",1992,CL,0.0
Left-Corner Parsing and Psychological Plausibility,"It is well known that even extremely limited centerembedding causes people to have difficulty ill comprehension, but that leftand right-branching constractions produce no such effect. If the difficulty in comprehension is taken to be a result of processing load, as is widely assumed, then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model. On this basis, it has been ~rgued [A J91, JL83] that by identifying processing load with space utilization, we can rule out both top-down and bottom-up parsing as viable candidates for the human sentence processing mechanism, attd that left-corner parsing represents a plausible Mternative. Examining their arguments in detail, we find difficulties with each presentation. In this paper we revise the argument and validate its central claim. In so doing, we discover that the key distinction between the parsing methods is not the form of prediction (top-down vs. bottom-up vs. leftcorner), but rather the ability to iastantiate the operation of composition.",1992,COLING,-0.6000000000000001
Un Systeme Inferentiel Oriente Objet Pour Des Applications En Langues Naturelles,"Up to now, there is still no specific model for solving the problem of natured language representation and reasoning. In this paper, we propose an object oriented form,'dism for supporting knowledge representation, extraction and exploitation in tile context of natural language processing. In the natural language analysis, this system is situated after the morpbo-syntax and file linguistic semantics. It represents two classes of concepts: objects of discourse and action schemata, the former resulting from nominal syntngms and the latter from the 'processes'. We are concerned here just by the representation of objects. In the natural language discourse, manipulated objects ,are complex objects ~md the reasoning is by uature first inferential and then deductive. To lake into account this kind of reasoning we need a suitable representation: a model of inferential objects. The theoretical foundations of the proposed model are Lesniewski's logical systems: tile Calculus of Names and the Mereology. The former is based on a primitive lunctor called ""epsilon"" interpreted a.s is-a, the latter is based on a par t -o f relation which is called the ""ingredience"". The whole system is supported by these two primitives and theirs derived functions. The concepts of our model result from a collaboration between linguists and computer scientists. The main concepts are the intensional and extensional universes, notions and types. The possible thferenti,'d reasoning can be of different types : it can concern the status, the denominations, the structures or the ""fonctifs"" of the objects. Key-words : Knowledge Representation, Inferential Reasoning, Object Oriented Modelling, Natural Language Processing, Language Parsing and Understanding. RESUME Duns ce papier, nous proposons un lonnalisme orient6 objet pour la reprtsentation, I'extraction et l'exploitation des connaissances duns le contexte du traitement des langues natmelles. Duns un discours en laugue naturelle, les objets manipults sont des objets complexes et le raisonnemeut est avant tout de type inftrentiel awmt d'etre dtductif. Pour pouvoir tenir compte de ce type de raisounement, nous avons besoin d'uue reprdsentation idoine : un modtle d'objets inftrentiels. Les foudemeuts thtoriques de notre modtle sont les syst~mes logiques de Lesniewski : le Calcul des Noms et la Mtrtologie. Le premier repose sur ua fonctear primitif appel6 ""epsilon"" interprtt6 comme est-un, le second sur la relation partie-de appelte ""ringredience"". Les concepts de notre modtle sont le fruit d'une collaboration entre linguistes et informaticiens. Les principaux concepts sont les univers intensionnel et extensionnel, les notions et les types. Les raisonnements infdrentiels possibles sont de difftrentes sortes : ils peuvent porter sur le statut, les dtnominations, les structurels ou les fonctifs. Mots-clts : Repr6sentation des Conmtissances, Raisonnement Inftrentiel, Mod61isation Orientte Objet, Traitement de la Langue Naturelle, Analyse morphosyntaxique et Comprdhension du langage. 1 I N T R O D U C T I O N Le syst~me prtsent6 ici a pour but la reprtsentation, rextraction et rexploitation des connaissances dans le contexte du traitement automatique des langues. On salt [Berrendonner 89] que les raisonnements reprtsent~s duns des ""discours"" en langue naturelle ne sont que rarement dtductifs et sont le plus souvent inftrentiels. Pour pouvoir tenir compte de ces misonnements, nous avons besom d'une reprtsentation idoine. I1 n'existe pas en effet ~t l'heure actuelle de modble sptcifique pour r6soudre le probl/~me de la reprtsentafiou A~'Es DE COLING-92, NANTES, 23-28 At(It 1992 4 6 l PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 des connaissances et du raisonnement en langue naturelle. Dans ce document, nous d6crivons le formalisme de repr6sentatioo et certains raisonnements que notre syst~me autorise : c'est un module d'objets inf~rentiels. Ce module est lui-m~me fond6 sur les syst~mes logiques de Lesniewski [Lesniewski 89]. D,ans ces syst~mes, nous utilisons le Calcul des Noms (bas6 sur la primitive ¬£ : ""est-un/est-le"") et la M6rdologie (dont le fonctear de base est ""pattie-tout"", appel6 ingr6dience). En d6finitive, le module objet, et tout le syst~me reposent sur ces deux seules primitives et leurs d¬¢rivdes. Darts une chaine d'analyse du franqais, ce syst~me se situe apr~s la morpho-syntaxe et la s6mantique linguistique. II repr6sente deux families de concepts : les objets du discours, issus de cert~dns des syntagmes nominaux, et les sch6mas d'action qui sont issus des proc~s. Nous ne nous int6ressons ici qu'h la repr6sentation des objets du discours. Une premiere partie est consacr~e au module hun niveau conceptnel : nons y donnous une pr6sentation g6ndrale, suivie des concepts sur lesquels repose notre syst~me et enfin l'unit6 de repr6sentation de connaismmce choisie. Les bases logiques permettant la formalisation, ainsi que des caract6ristiques propres au module sont pr6sent6es dans une deuxi~me section. On donne un exemple de formalisation. Les troisi~me et quatri~me parties exposent l'organisation des connaissarlces et les raisonnements possibles sur cette connaissance. La derni~re pattie consacrCe aux teclmiques d'impl6mentation est suivie d'une conclusion. 2 LE MODELE CONCEPTUEL 2.1 Presentation g~n~rale Notre module r6sulte d'une collaboration entre linguistes et informaticiens, ll s'appuie sur certains r6sultats de la psychologie cognitive.",1992,COLING,0.5
Une ontologie du temps pour le langage naturel,We propose a new ontology for the time in natural language which provides the following,1992,COLING,1.0
The Typology of Unknown Words: An Experimental Study of Two Corpora,"Most current state-of-the-art natural language processing (NLP) systems, when presented with real-life texts, have problems recognizing each and every word present in the input. Depending on the application, the consequences can be severe. For example, in a machine translation system the quality of the processing may suffer and sometimes further processing may even be impossible.There are two main reasons why a word might not be recograzed and thus be considered unknown by the system:",1992,COLING,-0.5
Feature Structure Based Semantic Head Driven Generation,"This paper proposes a genera t ion me thod for fea ture-s t ructured)ased unificat ion g rammars . As comlx~red with fixed ~ri ty t e rm nota t ion , feature s t ruc tu re nota t ion is more tlexible for represent ing knowledge needed to genera te idiom~ttic s t ruc tu res as well as genem~l construct ions . The method enables feature s t rnc tu re retr ieval via nml t ip le indices. The indexing mechanism, when used with a semant ic head driven generat ion algor i thm, a t t a ins efficient genera t ion even when a large amoun t of genera t ion knowledge mus t be considered. Our method can produce all possi ble s t ruc tu res in parNlet , using s t ruc ture shar ing among ambiguous subs t ruc tures . 1 I n t r o d u c t i o n Pract icM generat ion sys tems mus t lnwe l inguis t ic knowledge of both specilic expressions like id ioms and generM g r a m m a t i c a l construct ions , ;rod t tmy should efgtciently produce sm'face s t r ings applying t h a t knowledge [[][2]. In order to satisfy the first requi rement , our sys tem employs a set of t rees anno ta t ed with fe,~ture s t ruc tures to represent genera t ion knowledge. l:;ach tree represents a t?agment of a syntact ic s t rnc ture , and is paired with a semant ic feature s t ructure . We can describe id iomat ic eons t ruc t ions , by making a tree which cont~tins lexical specifications and is paired with a specilie ra ther than general semaut ic s t ructure . Because feature s t ruc tu res allow par t ia l speei i ica t iom we can encode generat ion knowledge r ;mgiug over mul t ip le levels of genera l i ty in a. uniform way. l lowever , notice tha t this p roper ty will be res t r ic ted if we use DCG or (tixed ar i ty) t e rm nota t ion 1 Suppose there is a genera t ion knowledge s t ruc ture whose syn tac t i c par t is ""go on foot"". ' r im feat, tu'e s t ruc ture nota t ion of its semant ic par t will be sonmthing like: ~The flexibility of structure notation colnpated Lo tetln notation is also discussed il~ [4]. [ [Rein GO] [Agent ?agent [] ] [Instrument FOOT]]. while the t e rm nota t ion is : (1) i n s t rumen t (go (Agen t ) , foot) (2) These two no ta t ions seem to be equivalent , but there is a cruciN diflerence. A genera t ion knowledge s t ruc tu re conta in ing the fe~tture-based selnan t ics will still be unifiable even i f the semantic input to be unified contains additional material. Thus the knowledge s t ruc tu re will be discovered and i ts syntac t ic in format ion can he used for generat ion. By cont ras t , a te rm-based inpu t wi th additiona.1 e lements would not unify with the te rm-based semant ic s t ruc tu re shown above. It would thus be necessary to create add i t ional generat ion s t ruc tures conta in ing d is t inc t ( though par t ly overlN)ping) t e rm-based semantic s t ructures . Such addi t iona l s t ruc tures are red u n d a n t ~tn(l cause superfluous ou tpu t . For example , consider the a,ugmented feature s t ruc tu re (3). [ [Rein ~o] [Agent Ken] [Instrument FOOT] [Time I0 : OOmn] ] (3) i t will indeed nnify with (1) above. But termbased input semant ic s t ruc ture (4) will not unify with te rm-based semant ic s t ruc tu re (2). i n s t r u m e n t ( t i m e ( g o ( k e n ) , 10 :00am) , f o o t ) . (4) To unifv (2), semant ic ,structure (5) would a.lso be required. t i m e ( i n s t z u m e n t ( g o ( k e n ) , f o o t ) , 10:00ma) . (5) AcrEs DE COLING-92. NANTES. 23 28 AOt~q"" 1992 3 2 PROC. OI; COLING 92. NANTES. AUG. 23 28. 1992 For this reason, our generation knowledge consists of trees represented as feature structures. A tree can be substituted for a leaf node of asother tree to form a larger structure. Thus, tile tree can be regarded as a rule in a context-free feature-structure-based unification grammar. The second requirement for a generation system is efficient creation of syntactic structures. This is the main topic of this paper. Our system is based upon Semantic }lead Driven Generation [6], which is an efficient algorithm for unilication based formalisms. However, this algorithm requires some additional mechanisms to efficiently retrieve relevant generation knowledge, because feature structures can not be easily indexed. The algorithm presented here uses a nmltiple index network of feature structures to efficiently choose relevant generation knowledge from the knowledge base. The algorithm ""also uses an hypothetical node so as to efficiently maintain ambiguous structures during generation. 2 Phrase Descr ipt ion(PD) Generation knowledge is represented as a set of trees aunotated with feature structures, l,',ach tree is called a Phrase Description (PD). ALl example of a l)D is shown in Figure.1. Structure: (S AUX (NP PRON) VP) Annotation: (S [[syn [[cat S] [inv +]]1 [sem [[reln REQUEST] [agon *SP*] [recp *HR*] [obje ?ACTION]]]]) (AUX [[syn [[cat AUX] [lex ""would""] [v-morph PAST]]]]) (NP [[syn [[cat NP] [case NOM]]]]) (PRON [[syn [[cat PRON] [case NOM] [lex ""you""]]]]) (VP [[syn [[cat VP][v-morph BSE]]] [sem ?ACTION]J) Figure 1: an example of a PD A PD consists of two parts: a structure definition and feature structure annotation (Structure a.nd Annotation in Figure 1). The structure definition defines tile structure of a tree by using a list in which the first element corresl)onds to the mother node and tile rest of the elements correspond to daughters. l';ach daughter may t)e a tree rather than a sin> pie node. Acres DE COLING-92, NANTes, 23-28 AO6-r 1992 3 3 The annotation part specifies the feature structure of each symhol appearing in the structure definition. A feature structure description can contain tags or variables (symbols with ""?"" as a prefix in the figure), The scope of a tag in a PD is the entire PD. Each node should have a semmltic and syntactic feature structure. The semantic feature on the root node of a PD represents the semaattics of the PD; thus we call it the semantic structure of the PD. Although the description represents a tree, it is the same ms for a (partial) derivation structure of a unification-l)ased CFG, because tile current system does not allow adjoining operations. If the structure definition of every PD is restricted to mother-daughter relations only, the PD set is strictly equivalent to a unification-based CFG. 3 Generat ion Algor i thm Our algorithm is aal efficient extension of Semaattic Head Driven Generation. 3?he major extensions are: 1) it handles feature structures directly, and 2) it creates all possible phrase structures in parallel. These extensions are embodied mainly in the t 'l) activation and ambiguity handling mechanisms discussed in this section. 3.1 O v e r v i e w o f t h e a l g o r i t h m The main part of the generation process is expansion process, which iterates through expanding node selection, activation, prccombination, and application, using an e~Tmnding node agenda. Input to the process is a feature structure conraining syntactic, semantic and pragmatic features as an initial constraint on the root node. q'he Cxl)auding node agenda contains tim unlexicalized leaf nodes of the tree under creation. At the beginning of the process, it conta.ins only one node, which has the feature structure giveu as an initial constraint. The expanding node selection step picks up one node, say expanding node, from the agenda. If no node is picked ill) , the expaa~sinn process stops. The PD activation step activates all PD's whose senlantic strlLetures s~tlJs~tme the semantic structure of the expanding node. The precombination step makes PD sequences from activated PD's to satisfy some constraints. The application step instantiates the PD sequence(s) and applies it to tile expanding node. Paoc. oe COLING-92, NAm'ES, AU~. 23-28, 1992",1992,COLING,0.6000000000000001
A Three-level Revision Model for Improving Japanese Bad-styled Expressions,"This paper proposes a three-level revision model for improving badly-styled Japanese expressions, especially in the field of technical communication. The model is a mixture of the regeneration-based model and tile rewriting-based model. The first level divides tong sentences, while the second level improves several badly-styled expressions with iterative partial rewriting operations. The last level performs regeneration, in which word ordering and punctuation to reduce tile reading ambiguity are currently involvod. Expelimental results show that our model is effective in realizing practical revision support systems.",1992,COLING,0.9
Semantic Network Array Processor as a Massively Parallel Computing Platform for High Performance and Large-Scale Natural Language Processing,"This paper demonstrates the utility of the Semantic Network Array Processor (SNAP) as a massively parallel platform for high performance and large-scale natural language processing systems. SNAP is an experimental massively parallel machine which is dedicated to, but not limited to, the natural language processing ushag semantic networks. In designing the SNAP, we have investigated various natural language processing systems and theories to determine the scope of the hardware support and a set of micro-coded instructions to be provided. As a resuit, SNAP employs an extended markerpassing model and a dynamically modifiable network model. A set of primitive instructions is micro-coded to directly support a parallel marker-passing, bitoperations, numeric operations, network modifications, and other essential functions for natural language processing. This paper demonstrates the utility of SNAP for various paradigms of natural language processing. We have discovered that the SNAP provides milliseconds or microseconds performance on several important applicatious such as the memory-based parsing and translation, classificatlon-based parsing, and VLKB search. Also, we argue that there are numerous opportunities in the NLP community to take advantages of the comlmtational power of the SNAP. 1. I n t r o d u c t i o n In order to accomplish the high-performance natural language processing, we have designed a highly parallel machine called Semantic Network Array Processor (SNAP) [Lee and Moldovan, 1990]. The goal of our project is to develop and test the validity of the massively parallel machine for high performance and larg-scale natural language processing. Thus, the architecture of the SNAP was determined reflecting extensive analysis of basic operations essential to the ""This research is Bupported by the National Science Foundation under grant MIP-9009111 and MIP-9009109, and conducted as a part of IMPACT (InternationM Consortium for Massively Parallel Advanced Computing Technologies) Dan Moldovan Department of Electrical Engineering Systems University of Southern California Los Ange le s , C A 90089-1115 U.S.A. natural language processing. As a result of the investigation, we have decided to employ an extended marker-passing model and a dynamically modifiable network. Also, a set of primitive instructions is microcoded to directly support essential operations in natural language systems. Several approach can be taken to use SNAP as a platform for natural language processing systems. We can fully implement NLP system on SNAP, or we can speed up existing systems by implementing computationally expensive part on SNAP. We have hnplemented some of these approaches on SNAP, mid obtained extremely high performance (order of milliseconds for given tasks). In this paper, we describe the design philosophy and architecture of SNAP, and present several approaches toward high performance natural language processing systems on SNAP. 2. S N A P A r c h i t e c t u r e 2.1. Des ign P h i l o s o p h y of S N A P The Semantic Network Array Processor (SNAP) is a highly parallel array processor fully optindzed tbr semantic network processing with a marker-passing mechanism. The fundermental design decisions arc (1) a semantic network as a knowledge representation scheme, and (2) parallel marker-passing as an inference mechauism. First, the use of a semantic network as a represem tation scheme can be justified from the fact that most of the representation schemes of current AI and NLP theories (such as frame, feature structure, sort hierarchy, systemic choice network, neural network, etc.) can be mapped onto semantic networks. Also, tlmre are numbers of systems and models which directly use semantic networks [Sown, 1991]. Second, the use of marker-passing can be justified from several aspects. Obviously, there are many AI and NLP models which use some form of marker-passing as the central computing principle. For example, there are significant number of research being done on word-sense disambiguation as scene in Waltz and Pollack 1985] Itendler, 1988], [Hirst, 1986, [Charniak, 1983], [Tomabechi, 1987, etc. All of them assume passing of markers or values among nodes interconnected via some types of links. There are studies to handle syntactic conACRES DE COLING-92. NANaXS, 23-28 AO~"" 1992 8 1 3 Paoc. ov COLING-92. NAI, rVES, AUG. 23-28, 1992 tsmlttamtm4 ~ e t t m m a n t |NAP.1 ~ Cam~ oan@~a¢ c . J ~ t~ s s ^ p Figure I: SNAP-1 Architecture straints using some type of networks which can be mapped onto semantic networks. Recent studies on the Classification-Based Parsing [Kasper, 1989] and the Systemic Choice Network [Carpenter and Pollard~ 1991] assume hierarchical networks to represent varions linguistic constraints, and the search on these networks can be done by marker-passing. Also, there are more radical approaches to implement entire natural language systems using parallel marker-passing as seen in [Norvig, 1986], [Riesbeck and Martin, 1985], [Tomabechi, 1987], and [Kitano, 1991]. There are, however, differences in types of information carried in each marker-passing model. We will describe our design decisions later. As reported in [Evett, at. al., 1990], however, serial machines are not suitable for such processing because it causes performance degradation as a size of semantic network increases. There are clear needs for highly parallel machines. The rest of this section provides a brief overview of the SNAP architecture. 2.2. The A r c h i t e c t u r e SNAP consists of a processor array and an array controller (Figure 1). The processor array has processing cells which contain the nodes and hnks of a semantic network. The SNAP array consists of 160 processing elements each of which consists of a TMS320C30 DSP chip, local SRAM, etc. Each processing elements stores 1024 nodes which act as virtual processors. They are interconnected via a modified hypercube network. The SNAP controller interfaces the SNAP array with a SUN 3/280 host and broadcasts instructions to control the operation of the array. The instructions for the array are distributed through a global bus by the controller. Propagation of markers and the execution of other instructions can be proceased simultaneously. 2.3. Parallel M a r k e r P a s s i n g In the SNAP, content of the marker are: (1) bitvector, (2) address, and (3) numeric value (integer or floating point). In SNAP, the size of the marker is fixed. According to the classification in [Blelloch, 1986], our model is a kind of Finite Message Passing. There are types of marker-, or message-, pa~ing that propagates feature structures (or graphs)~ which are called Unbounded Message Passing. Although we have extended our marker-passing model from the traditional bit marker-passing to the complex markerpassing which carries bits, address, and numeric values, we decided not to carry unbounded messages. This is because propagation of feature structures and heavy symbolic operations at each PE are not practical assumptions to make, at least, on current massively parallel machines due to processor power, memory capacity on each PE, and the communication bottleneck. Propagation of feature structures would impose serious hardware design problems since the size of the message is unbounded, which means that the designer can not be sure if the local memory size is sufficient or not until the machine actually runs some applications. Also, PEa capable of performing operations to manipulate these messages (such as unification) would be large in physical size which causes assembly problems when thousands of processors are to be assembled into one machine. Since we decide not to support unbounded message passing, we decide to support functionalities attained by the unbounded message passing by other means such as sophisticated marker control rules, dynamic network modifications, etc. 2.4. Ins truct ion Sets A set of 30 high-level instructions specific to semantic network processing are implemented directly in hardware. These include associative search, marker setting and propagation, logical/arithmetic operations involving markers, create and delete nodes and relations, and collect a list of nodes with a certain marker set. Currently, the instruction set can be called from C language so that users can develop applications with an extended version of C language. From the programming level, SNAP provides dataparallel programming environment similar to C* of the Connection Machine ]Thinking Machines Corp., 1989], but specialized for semantic network processing with marker passing. Particularly important is the marker propagation rules. Several marker propagation rules are provided to govern the movement of markers. Marker propagation rules enables us to implement guided, or constraint, marker passing as well as unguided marker passing. This is done by specifying the type of links that markers can propagate. The following are some of the propagation rules of SNAP: e Seq(rl, r~): The Seq (sequence) propagation rule allows the marker to propagate through rl once then to r~. • Spread(rl,r2) : The Spread propagation rule allows the marker to travel through a chain of r l links and then r~ links. * Comb(rl,r~) : The Comb (combine) propagation rule allows the marker to propagate to all rl and r~ links without limitation. 2.5. K n o w l e d g e R e p r e s e n t a t i o n on S N A P SNAP provides four knowledge representation elements: node, link, node color and link value. These elements offer a wide range of knowledge representation schemes to be mapped on SNAP. On SNAP) a concept is represented by a node. A relation can be represented by either a node called relation node or AcrEs DE COLING-92, NANTES. 23-28 AOt)r 1992 8 1 4 PROC. OF COLING-92, NANTES. AUG. 23-28. 1992 a link between two nodes. The node color indicates the type of node. For example, when representing USC i s in Los Angeles and CW0 ie in Pi t t sbnrgh~ we may assign a relation node for IN. The IN node is shared by the two facts. In order to prevent the wrong interpretations such as USC in P i t t s b u r g h and CSll in Lea Angeles, we assigu I N # I and IN#2 to two distinct IN relations, and group the two relation nodes by a node color IN. Each lhlk has assigned to it a link value which indicates the strength of interconcepts relations. This link value supports probabilistic reasoning and connectionist-like processing. These four basic elements allow SNAP to support virtually any kind of graph-based knowledge representation formalisms such as KL-ONE [Braehman and Schmolze, 1985], Conceptual Graphs [Sown, 1984], KODIAK [Wilensky, 1987], etc. 3 . T h e M e m o r y B a s e d N a t u r a l L a n g u a g e P r o c e s s i n g Memory-baaed NLP is an idea of viewing NLP as a memory activity. For example, parsing is considered as a memoryosearch process which identifies similar eases in the past from the memory, and to provide interpretation based on the identified case. I t can be considered as an application of Memory-Baaed l~.easoning (MBR) [Stm~fill and Waltz, 1986] and CaseBased Reasoning (CBR) [Riesbeck and Schank, 1989] to NLP. This view~ however, counters to traditional idea to view NLP as arl extensive rule application process to build up meaning representation. Some models has been proposed in this direction, such as Direct Memory Access Parsing (DMAP) [Riesbeck and Martin, 1985] and q~DMDIALOO [Kitano, 1991]. For arguments concerning superiority of the metnory-based approach over the traditional approach, ace [Nagao, 1984], [Riesbeck and Martin, 1985], and [Sumita and ][ida, 1991]. DMSNAP is a SNAP implementation of the (I)DMDIALOG speech-to-speech dialogue translation system which is based on, in part, the memory-based approach. Naturally, it inherits basic ideas and mechanisms of the ~DMDIALOG system such as a memorybased approach to natural language processing and parallel marker-passing. Syntactic constraint network is introduced in DMSNAP whereas ODMDIALOG has been assuming unification operation to handle linguistic processing. DMSNAP consists of the nlemory network, syntactic constraint network, and markers to carry out inference. The memory network and the syntactic constraint network are compiled from a set of grammar rules written for DMSNAP. M e m o r y N e t w o r k on S N A P The major types of knowledge required for language translation in DMSNAP are: a lexicon, a concept type hierarchy, concept sequences, and syntactic constraints. Among them, the syntactic constraints are represented in the syntactic constraint network, and the rest of the knowledge is represented in the memory network. The memory network consists of various types of nodes such as concept sequence class (CSC), lexical item node* (LEX), concept nodes (CC) and others. Nodes are connected by a number of different links such as concept ahstraction links (ISA), expression links for both source language and target language (ENG and JPN), Role links (ROLE), constraint links (CONSTRAINT) , contextual llnk~ (CONTEXT) and others. A part of the menmry network is shown in Figure",1992,COLING,0.8
Multimodal Database Query,"The paper proposes a mult imodal interface for a real sales database application. We show how natural language processing may be integrated with a visual, direct manipulat ion method of database query, to produce a user interface which supports a flexible form of query specification, provides implicit guidance about the coverage of the linguistic component, and allows more focused discourse reference.",1992,COLING,0.6000000000000001
Multilinguisation d'un editeur de documents structures. Application a un dictionnaire trilingue,"R~sum~ Pour ""multilingualiser"" (et non simplement ""localiser"") Gif, un 6diteur de documents structures, nous avons d6fini un langage de transcription, appel6 langage E, analogue aux autres langages (S, Pet T) de Grif. E est utilis6 pour compl6ter la description structurale d'une classe de documents, 6crite en S, par une description ""linguistique"" concemant les syst~mes d'6cnture utilis6s clans les diff6rentes sous-structures des documents de la classe. Gr/tce ~ cette extension multilingue, on a pu construire une premi6re structure de dictionnmre trilingue chinois-fran~;ais-vietnamien, et l'utiliser sur un dictionnaire rEduit. Mots-el6s",1992,COLING,0.0
Shalt2- a Symmetric Machine Translation System with Conceptual Transfer,"Shal l2 is a knowledge-based machine translation system with a symmetric architecture. The grammar rules, mapping rules between syntactic and conceptual (semantic) representations, and transfer rules for conceptual paraphrasing are all bi-directional knowledge sources used by both a parser and a generator.",1992,COLING,0.0
Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases,"LEXTER is a software package for extracting terminology. A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated. To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages : analysis, parsing. In the first stage, LEXTER uses a base of rules designed to indentify frontier markers in view to analysing the texts and extracting maximallength noun phrases. In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units. In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach). I ) C o n s t i t u t i n g Constituting a terminology of a subject field, that is to say establishing a list of the terminological units that represent the concepts of this field, is an oft-encountered problem. For the Research Development Division of Electricit6 de France (French Electricity Board), this problem arose in the in formation documentation sector. An automatic indexing system, using different thesauri according to the application, has been operational for three years or more [Monteil 1990]. The terminologists and information scientists need a terminology a t e r m i n o l o g y extraction tool in order to keep these thesauri up to date in constantly changing fields and to create ""ex nihilo"" thesauri for new fields. This is the reason why the terminological extracting software, LEXTER, was developed, forming the first link in the chain that goes to make up the thesaurus. A corpus of frenchlanguage texts is fed into LEXTER, which gives out a list of likely terminological units, which are then passed on to art expert for validation. AUlXS DE COLING-92, NANTES, 23-28 AO~r 1992 9 7 7 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 2) What is a terminological unit ? The main aim here is not to provide a rigorous definition of what a terminological unit is, but rather to outline its essential features, and thus to justify the hypotheses (concerning the form of terminological units) on which LEXTER is based. Semantic function : the representation of the concept The first characteristic of the terminological unit is its function as the representation of a concept. The terminological unit plays this role of representa t ion in the f ramework of a terminology, which is the linguistic evidence of the organisation of a field of knowledge in the form of a ne twork of concepts; the terminological unit represents a concept, uniquely and completely, taken out of any textual context. The existence of this one-to-one relationship between a linguistic expression and an extra-linguistic object is, as we shall see, a situation which particulary concerns the terminological units. The appearance of a new terminological unit is most often a parallel process to that of the birth of the concept which it represents. This ""birth"" is marked by the consensus of a certain scientific community. This consensus is attested only when the occurrences of this linguistic expression, or term-to-be, shows a stable correlation to the same object in the subject field, uniquely and completely, in the writings of the agents of this scientific community. When this is the case, the object in question takes its place in the network describing the subject field, and the expression takes on the status of a terminological unit. This referential function is, for E. Benveniste, the ""synaptic"" mark of a syntagm [Benveniste 1966]. It is thus because occurrences in text of a terminological unit systematically refer to a concept, that a relationship of representation is established, out of any textual context, between the terminological unit and the concept. This underp ins the specif ic status of the terminological unit as opposed to that of the word in language, a status close to that of a descriptor in Information Science ([Le Guern 1984]). Syntactic form : synaptic composition We put forward the hypothesis that this function of representing the concept out of context puts a certain number of constraints on the form that terminological units may take on. It has been seen that the construction of terminological units obey well-known rules of syntactic formation, called synaptic composit ion ([Benveniste 1966]). For example : terminological units are noun phrases, generally made up of nouns and adjectives, and pratically never containing conjugated verbs; the prepositions used most often are ""de"" and ""~"", rarely followed by a determiner. To illustrate this, take the concept of a ""screen belonging to a portable computer"". Without going in to the linguistic phenomena behind this, it can be said that, in context, both the syntagms ""l 'rcran d'un ordinateur portable"" (""the screen of a portable computer"") and ""un 6cran d 'ordinateur portable"" (""a portable computer screen"") can refer generically to the concept. However, if one wished to represent this concept out of any textual context, the chances are that one would reject the expression ""rcran d'un ordinateur portable"", for wich the interpretation of the article ""un"" may be ambiguous, and accept the expression ""6cran d'ordinateur portable"", more naturally used in isolation and thus more suitable to go into a terminology. From these considerations on the form and the function of terminological units, two mains ideas are relevant to developing a computer NLsed system of terminology constitution : 1) It is possible to devise an extraction program solely based on syntactic data, since the grammatical form of terminological units is relatively predictable; 2) It is not possible to expect this program to extract terminological units and nothing else, given the basically referential semantic function of occurrences of terminological units : this means that the results obtained can only be considered, a priori, as likely terminological units. ACRES DE COL1NG-92, NAN'~S, 23-28 Aotrr 1992 9 7 8 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 3) How LEXTER works : analysis and parsing To detect terminological units, LEXTER takes the form of these units into consideration, and works in two phases : analysis and parsing. LEXTER treats categorized texts, which have been submitted to a morphological analysis : each word is tagged with its grammatical category (noun, verb, adjective, etc.). Figure 1 : Simplified example of how LEXTER works I (categorized) texts I UN TRAITEMENT DE TEXTE EST INSTALLE SUR LE DISQUE DUR DE LA STATION DE TRAVAIL",1992,COLING,0.0
A Knowledge-based Machine-aided System for Chinese Text Abstraction,Benjamin K Tsou Hing-cheung Ho Tom Bong-yeung Lai Caesar Suen Lun Hing-lung Lin City Polytechnic of Hong Kong,1992,COLING,0.0
TTP: A Fast and Robust Parser for Natural Language,"In this paper we describe TI~ , a fast and robust natural language parser which can analyze written text and generate regularized parse structures for sentences and phrases at the speed of approximately 0.5 sec/sentence, or 44 word per second. The parser is based on a wide coverage grammar for English, developed by the New York University's Linguistic String Project, and it uses the machine-readable version of the Oxford Advanced lw~arner's Dictionary as a source of its basic vocabulary. The parser operates on stochastically tagged text, and contains a powerful skip-and-fit recovery mechanism that allows it to deal with extra-grammatical input and to operate effectively under a severe time pressure. Empirical experiments, testing parser's speed and accuracy, were performed on several collections: a collection of technical abstracts (CACM-3204), a corpus of news messages (MUC-3), a selection from ACM Computer Library database, and a collection of Wall Street Journal articles, approximately 50 million words in total.",1992,COLING,0.7000000000000001
Logical Form of Hierarchical Relation on Verbs and Extracting it from Definition Sentences in a Japanese Dictionary,"We are studying how to extract hierarchical relation on verbs from definition sentences in a Japanese dictionary. The hierarchical relation on verbs has been dealt with as a binary relation on verbs, but it should be dealt with as logical relation on predicates. We will define the logical form of the hierarchical relation on verbs and then discuss which part of the syntactic structure of the definition sentence represents that relation. We will call the main predicate verb in this part the definition verb. Furthermore we will describe how to semiautomatically select the proper meaning of the definition verb and the proper correspondence between cases of an entry verb and the definition verb in order to extract the hierarchical relation as logical relation.",1992,COLING,0.30000000000000004
Two-Level Morphology with Composition,"(1) Lexical representations tend to be arbitrary. Because it is difficult to write and test two-level systems that map between pairs of radically dissimilar forms, lexical representations in existing two-level analyzers tend to stay close to the surface forms. This is not a problem for morphologically simple languages like English because, for most words, inflected forms are very similar to the canonical dictionary entry. Except for a small number of irregular verbs and nouns, it is not difficult to create a two-level description for English in which lexical forms coincide with the canonical citation forms found in a dictionary.",1992,COLING,0.0
Linguistic Knowledge Generator,"The difficulties in current NLP applications are seldom due to the lack of appropriate frameworks for encoding our linguistic or extra-linguistic knowledge, hut rather to the fact that we do not know in advance what actual znstances of knowledge should be, even though we know in advance what types of knowledge are required. It normally takes a long time and requires painful trial and error processes to adapt knowledge, for example, in existing MT systems in order to translate documents of a new text-type and of a new subject domain. Semantic classification schemes for words, for example, usually reflect ontologies of subject domains so that we cannot expect a single classification scheme to be effective across different domains. To treat different suhlanguages requires different word classification schemes. We have to construct appropriate schemes for given sublanguages from scratch [1]. It has also been reported that not only knowledge concerned with extra-linguistic domains but also syntactic knowledge, such as subcategorization frames of verbs (which is usually conceived as a par t of general language knowledge), often varies from one sublanguage to another [2]. Though re-usability of linguistic knowledge is currently and intensively prescribed [3], our contention is that the adaptation of existing knowledge requires processes beyond mere re-use. Tha t is,",1992,COLING,-0.30000000000000004
Analogy-- Watershed or Waterloo? Structural alignment and the development of connectionist models of analogy,"Neural network models have been criticized for their inability to make use of compositional representations. In this paper, we describe a series of psychological phenomena that demonstrate the role of structured representations in cognition. These findings suggest that people compare relational representations via a process of structural alignment. This process will have to be captured by any model of cognition, symbolic or subsymbolic.",1992,NIPS,0.30000000000000004
Weight Space Probability Densities in Stochastic Learning: II. Transients and Basin Hopping Times,"In stochastic learning, weights are random variables whose time evolution is governed by a Markov process. At each time-step, n, the weights can be described by a probability density function pew, n). We summarize the theory of the time evolution of P, and give graphical examples of the time evolution that contrast the behavior of stochastic learning with true gradient descent (batch learning). Finally, we use the formalism to obtain predictions of the time required for noise-induced hopping between basins of different optima. We compare the theoretical predictions with simulations of large ensembles of networks for simple problems in supervised and unsupervised learning. 1 Weight-Space Probability Densities Despite the recent application of convergence theorems from stochastic approximation theory to neural network learning (Oja 1982, White 1989) there remain outstanding questions about the search dynamics in stochastic learning. For example, the convergence theorems do not tell us to which of several optima the algorithm",1992,NIPS,0.0
Transient Signal Detection with Neural Networks: The Search for the Desired Signal,"Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (011 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.",1992,NIPS,-1.0
Metamorphosis Networks: An Alternative to Constructive Models,"Given a set oft raining examples, determining the appropriate number of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms-called metamorphosis algorithms-in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and interpolation of sparse training data.",1992,NIPS,0.30000000000000004
A Speech-First Model for Repair Detection and Correction,"Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or REPAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs. We test our acoustic-prosodic cues with other lexical cues to repair identification and find that precision rates of 89-93% and recall of 78-83% can be achieved, depending upon the cues employed, from a prosodically labeled corpus.",1993,ACL,0.7000000000000001
Transfers of Meaning,"In one form or another, the phenomena associated with ""meaning transfer"" have become central issues in a lot of recent work on semantics. Speaking very roughly, we can partition approaches to the phenomenon along two dimensions, which yield four basic points of departure. In the first two, people have considered transfer in basically semantic or linguistic terms. Some have concentrated on what we might call the paradigmatic aspects of transfer, focusing on the productive lexical processes that map semantic features into features for example, the ""grinding"" rule that applies to turn the names of animals into mass terms denoting their meat or fur. This the approach that's involved in most recent work on ""regular polysemy,"" ""systematic polysemy,"" and the like, for example by Apresjan, Ostler and Atkins, Briscoe and Copestake, Nunberg and Zaenen, Wilensky, Kilgarriff and a number of other people. Other people have emphasized the syncategorematic aspects of transfer; that is, the ways meaning shifts and specifications are coerced in the course of semantic composition. This is an approach that hass been developed in particular by James Pustejovsky and his collaborators, building on earlier work on type shifting. As opposed to these, there are conceptual and pragmatic approaches to transfer, which focus on the extralinguistic circumstances that license transfers of various types. Here again there are both paradigmatic and syncategorematic approaches, loosely speaking. The first is exemplified in a lot of recent work on metaphor by people associated with the ""cognitive linguistics"" school, which has focused chiefly on the relations between domains of experience that metaphor variously exploits and imputes. The second is represented by work on indirect speech within Gricean pragmatics, Relevance Theory, and the like, which has been chiefly concerned with specifying the conversational conditions that give rise to metaphor, irony, and analogous phenomena. Of course this categorization is somewhat factitious. The borders between these approaches are highly porous, and most work on transfer overlaps several of them. This is entirely appropriate, since these are in no sense competing theories or accounts of the phenomena. Transfer is clearly a linguistic process, and in many of its most important forms a lexical one. But it just as clearly has its basis in very general cognitive and communicative principles. And while it's reasonable that people should choose to focus on one or another of these considerations relative to their immediate interests, it is also useful to keep the Big Picture in mind, lest we inadvertently ascribe to one domain of explanation a responsibility that more properly belongs to another. This is the picture I want to sketch out in this talk. A comprehensive account of transfer has to make appeal to three different kinds of regularities or rules. The first are nonlinguistic: the correspondences between domains, real or imputed, that transfer invokes, and the communicative interests that may make these invocations useful or instructive they enable us to identify one thing in virtue of its relation to another, explain an abstract domain by reference to a concrete one, and so forth. Second, there is the repertory of general linguistic processes of transfer that exploit these correspondences and principles. By these I have in mind not traditional categories like metaphor, synecdoche, and m e t o n y m y distinctions that have basically to do with the kinds of domain correspondences that transfer exploits but the various types of operations that make possible typeshifting and sortal reassignment of expressions, syntactic recategorizations, and deferred indexical reference. These processes may cross-cut the types of domain correspondences that they exploit, and I'll show that we often find a single type of domain correspondence underlying two or more distinct semantic processes of transfer. Third, there are the language-specific instantiations of these operations, for example in the form of constructions or lexical rules that license particular types or",1993,ACL,0.30000000000000004
Metaphor and Cognition: An Interactionist Approach,"Bipin Indurkhya has written a wide-ranging and interesting work that is easy to read. Although Indurkhya's starting point is the puzzle of similarity-creating metaphors, the book is really about cognition and conceptual structure. In particular, he is concerned with the philosophical problem of reconciling the constructive nature of our concepts with the notion of a pre-existing mind-independent structure of reality. While the book covers a great deal of ground and is well worth reading, I feel that the basic theory is flawed in that it rests on a common philosophical view of meaning that has been considered inadequate.",1993,CL,-0.5
Issues in the choice of a source for Natural Language Generation,"The most vexing question in natural language generation is 'what is the source'-what do speakers start from when they begin to compose an utterance? Theories of generation in the literature differ markedly in their assumptions. A few start with an unanalyzed body of numerical data (e.g. Bourbeau et al. 1990; Kukich 1988). Most start with the structured objects that are used by a particular reasoning system or simulator and are cast in that system's representational formalism (e.g. Hovy 1990; Meteer 1992; R6sner 1988). A growing number of systems, largely focused on problems in machine translation or grammatical theory, take their input to be logical formulae based on lexical predicates (e.g. Wedekind 1988; Shieber et al. 1990). The lack of a consistent answer to the question of the generator's source has been at the heart of the problem of how to make research on generation intelligible and engaging for the rest of the computational linguistics community, and has complicated efforts to evaluate alternative treatments even for people in the field. Nevertheless, a source cannot be imposed by fiat. Differences in what information is assumed to be available, its relative decomposition when compared to the ""packaging"" available in the words or syntactic constructions of the language (linguistic resources), what amount and kinds of information are contained in the atomic units of the source, and what sorts of compositions and other larger scale organizations are possible--all these have an impact on what architectures are plausible for generation and what efficiencies they can achieve. Advances in the field often come precisely through insights into the representation of the source. Language comprehension research does not have this problem--its source is a text. Differences in methodology govern where this text comes from (e.g., single sentence vs. discourse, sample sentences vs. corpus study, written vs. spoken), but these aside there is no question of what the comprehension process starts with. Where comprehension ""ends"" is quite another matter. If we go back to some of the early comprehension systems, the end point of the process was an action, and there was linguistic processing at every stage (Winograd 1972). Some researchers, this author included, take the end point to be an elaboration of an already existing semantic model whereby some new individuals are added and new relations established between them and other individuals (Martin and Riesbeck 1986; McDonald 1992a). Today's dominant paradigm, however, stemming perhaps from the predominance of research on question-answering and following the lead of theoretical linguistics, is to take the end point to be a logical form: an expression that codifies the information in the text at a fairly shallow level, e.g., a first-order formula with content words mapped to predicates with the same spelling, and with individuals represented by quantified variables or constants.",1993,CL,0.0
Books Received,"Books listed below that are marked with a t have been selected for review in a future issue, and reviewers have been assigned to each. Authors and publishers who wish their books to be considered for review in Computational Linguistics should send a copy to the book review editor, Graeme Hirst, Department of Computer Science, University of Toronto, Toronto, Canada M5S 1A4. All books received will be listed, but not all can be reviewed. Readers who wish to review books for the journal should write, outlining their qualifications, to the book review editor at the address above or send electronic mail to gh@cs.toronto.edu. Obviously, we cannot promise the availability of books in anyone's exact area of interest.",1993,CL,0.0
Book Reviews: Text Generation and Systemic-Functional Linguistics: Experiences from English and Japanese,"This is an important book. On the surface, it is a survey and summary of work related to a major computational linguistics effort--the Penman project at the University of Southern California. Beneath the specific title and the apparently narrow subject matter, however, lies a general point about the field of computational linguistics: many important aspects of language are not addressed by the generative tradition that has dominated the field. This book aims to demonstrate that other types of linguistic description are available to serve as the basis for a computational linguistic treatment of these issues. As the second half of the book's title suggests, the research presented is based on M. A. K. Halliday's functional theory of grammar (see Winograd [1983, Chapter 6] for an excellent computationally oriented synopsis). Some confusion has surrounded Halliday's work and the notion of a functional grammar, as indicated by another recent review (Fraser 1991, pp. 104-106):",1993,CL,0.2
Book Reviews: Questions and Information Systems,"The handling of questions in a computational system falls into one of the more obvious convergences of natural language processing and human factors. The relationship of a user to an information system is fundamentally that of inquiring; at the same time, questions are asked in many different forms other than the simply interrogative, and much contextual knowledge must be maintained in order to understand what question is really being asked and what is the most informative answer. In Questions and Information Systems, Lauer, Peacock, and Graesser present 15 articles with the goal of presenting a wide, multi-disciplinary perspective on questioning. What is successful about the book, however, is not the breadth of perspective but rather the focus on descriptive models and the empirical studies that underlie them. The book is a compilation of amplified papers originally presented at a 1990 conference at Oakland University, Michigan. The articles in the book are organized along the following general topics: determining system information requirements (Chapters 25), design of computer interfaces (6-9), questions and large databases (10-12), and understanding complex decision processes (13-16). Regardless of the functional discussion, though, the articles in the book generally present ways of gathering empirical data on questioning behavior, and of representing this behavior in terms of question types. These two themes are the bases of the topics in the book. Both the approach toward data collection and the various representation schemes proposed are generally departures from the work of editor Graesser. In that sense, Chapter 9 (Graesser et al., ""Mechanisms that generate questions""), together with Chapter 12 (Graesser et al., ""Answering questions about information in databases"") state the fundamental principles of most of the work reported in the book. In Chapter 9, the authors zero in on a particular (empirically motivated) type of questioning, that of genuinely desiring to solicit new information. These ""inquiries"" are not always in the surface form of a question, of course, leading to an appeal to the Modern Language Philosophy concept of ""speech act"" (Searle 1969) to distinguish between the form of what is said and the intended effect on the hearer. The usage of ""speech act"" here is a bit less than rigorous, and that of ""taxonomy"" almost a misnomer, but the point is successfully conveyed that there are derivable, motivated categorizations of inquiries based on the mechanisms of generating inquiries (similar to what Searle would derive as perlocutionary force): correction of knowledge deficit, monitoring common ground, social coordination of action, and control of conversation and attention. In Chapter 12, Graesser et al. detail the QUEST model of question answering, intended to account for human question-answering behavior. Its major components",1993,CL,0.8
Letters to the Editor,"Adaptive Parsing I am concerned by a comment made in Julia Johnson's review of my book Adaptive Parsing (Kluwer Academic Publishers, 1992) in Computational Linguistics 18(3) (September 1992). In the review, Dr. Johnson poses a number of thought-provoking questions that underscore open issues in this research, and seems to have been, overall, a thoughtful and attentive reader. Toward the end of the review, however, she states, ""The performance improvements realized with adaptive parsing over a particular kernel grammar without adaptation were not strong."" This statement does not agree with the results in the book. As shown in the utility analysis on pages 194-200 and 207-210, performance of the system using the kernel grammar without adaptation gave an acceptance range from 7% to 24% of utterances; with adaptation, acceptance increased to 81% to 91%. I find it difficult to interpret this data as anything but a very strong performance improvement. Since the perceived usefulness of adaptation rests in great part on the performance improvements it affords over a static sublanguage, I am grateful for the opportunity to point out and correct this misperception.",1993,CL,0.0
Complexity Issues in Neural Computation and Learning,"The general goal of this workshop was to bring t.ogether researchers working toward developing a theoretical framework for the analysis and design of neural networks. The t.echnical focus of the workshop was to address recent. developments in understanding the capabilities and limitations of variolls modds for neural computation and learning. The primary topics addressed the following three areas: 1) Computational complexity issues in neural networks, 2) Complexity issues in learning, and 3) Convergence and numerical properties of learning algorit.hms. Other topics included experiment.al/simulat.ion results on neural llet.works, which seemed to pose some open problems in the areas of learning and generalizat.ion properties of feedforward networks.",1993,NIPS,-1.0
"How to Describe Neuronal Activity: Spikes, Rates, or Assemblies?","What is the 'correct' theoretical description of neuronal activity? The analysis of the dynamics of a globally connected network of spiking neurons (the Spike Response Model) shows that a description by mean firing rates is possible only if active neurons fire incoherently. If firing occurs coherently or with spatio-temporal correlations, the spike structure of the neural code becomes relevant. Alternatively, neurons can be gathered into local or distributed ensembles or 'assemblies'. A description based on the mean ensemble activity is, in principle, possible but the interaction between different assemblies becomes highly nonlinear. A description with spikes should therefore be preferred.",1993,NIPS,0.0
Robot Learning: Exploration and Continuous Domains,"The goal of this workshop was to discuss two major issues: efficient exploration of a learner's state space, and learning in continuous domains. The common themes that emerged in presentations and in discussion were the importance of choosing one's domain assumptions carefully, mixing controllers/strategies, avoidance of catastrophic failure, new approaches with difficulties with reinforcement learning, and the importance of task transfer.",1993,NIPS,0.0
Putting It All Together: Methods for Combining Neural Networks,"The past several years have seen a tremendous growth in the complexity of the recognition, estimation and control tasks expected of neural networks. In solving these tasks, one is faced with a large variety of learning algorithms and a vast selection of possible network architectures. After all the training, how does one know which is the best network? This decision is further complicated by the fact that standard techniques can be severely limited by problems such as over-fitting, data sparsity and local optima. The usual solution to these problems is a winner-take-all cross-validatory model selection. However, recent experimental and theoretical work indicates that we can improve performance by considering methods for combining neural networks.",1993,NIPS,-0.6000000000000001
Bayesian Backpropagation Over I-O Functions Rather Than Weights,"The conventional Bayesian justification of backprop is that it finds the MAP weight vector. As this paper shows, to find the MAP i-o function instead one must add a correction tenn to backprop. That tenn biases one towards i-o functions with small description lengths, and in particular favors (some kinds of) feature-selection, pruning, and weight-sharing.",1993,NIPS,0.0
Stability and Observability,"The theme was the effect of perturbations of the defining parameters of a neural network due to: 1) mea""urement"" (particularly with analog networks); 2) di""cretization due to a) digital implementation of analog nets; b) bounded-precision implementation of digital networks; or c) inaccurate evaluation of the transfer function(s}; 3) noise in or incomplete input and/or output of the net or individual cells (particularly with analog networks).",1993,NIPS,0.0
Emergence of Global Structure from Local Associations,"A variant of the encoder architecture, where units at the input and output layers represent nodes on a graph. is applied to the task of mapping locations to sets of neighboring locations. The degree to which the resuIting internal (i.e. hidden unit) representations reflect global properties of the environment depends upon several parameters of the learning procedure. Architectural bottlenecks. noise. and incremental learning of landmarks are shown to be important factors in maintaining topographic relationships at a global scale.",1993,NIPS,0.0
Functional Models of Selective Attention and Context Dependency,"This workshop reviewed and classified the various models which have emerged from the general concept of selective attention and context dependency, and sought to identify their commonalities. It was concluded that the motivation and mechanism of these functional models are ""efficiency"" and ''factoring'', respectively. The workshop focused on computational models of selective attention and context dependency within the realm of neural networks. We treated only ''functional'' models; computational models of biological neural systems, and symbolic or rule-based systems were omitted from the discussion.",1993,NIPS,0.0
On the Nature of Modal Truth Criteria in Planning,"Chapman‚Äôs paper, ‚ÄúPlanning for Conjunctive Goals,‚Äù has been widely acknowledged for its contribution toward understanding the nature of nonlinear (partial-order) planning, and it has been one of the bases of later work by others---but it is not free of problems. This paper addresses some problems involving modal truth and the Modal Truth Criterion (MTC). Our results are as follows: Even though modal duality is a fundamental axiom of classical modal logics, it does not hold for modal truth in Chapman‚Äôs plans; i.e., ‚Äúnecessarily p‚Äù is not equivalent to ‚Äúnot possibly lp.‚Äù Although the MTC for necessary truth is correct, the MTC for possible truth is incorrect: it provides necessary but insz#kient conditions for ensuring possible truth. Furthermore, even though necessary truth can be determined in polynomial time, possible truth is NP-hard. If we rewrite the MTC to talk about modal conditional truth (i.e., modal truth conditional on executability) rather than modal truth, then both the MTC for necessary conditional truth and the MTC for possible conditional truth are correct; and both can be computed in polynomial time.",1994,AAAI,-0.8
"Unclear Distinctions Lead to Unnecessary Shortcomings: Examining the Rule Versus Fact, Role versus Filler, and Type Versus Predicate Distinctions from a Connectionist Representation and Reasoning Perspective","This paper deals with three distinctions pertaining to knowledge representation, namely, the rules vs facts distinction, roles vs fillers distinction, and predicates vs types distinction. Though these distinctions may indeed have some intuitive appeal, the exact natures of these distinctions are not entirely clear. This paper discusses some of the problems that arise when one accords these distinctions a prominent status in a connectionist system by choosing the representational structures so as to reflect these distinctions. The example we will look at in this paper is the connectionist reasoning system developed by Ajjanagadde & Shastri(Ajjanagadde & Shastri 1991; Shastri & Ajjanagadde 1993). Their1 system performs an interesting class of inferences using activation synchrony to represent dynamic bindings. The rule/fact, role/filler, type/predicate distinctions figure predominantly in the way knowledge is encoded in their system. We will discuss some significant shortcomings this leads to. Then, we will propose a much more uniform scheme for representing knowledge. The resulting system enjoys some significant advantages over Ajjanagadde & Shastri‚Äôs system, while retaining the idea of using synchrony to represent bindings.",1994,AAAI,0.5
The First Law of Robotics (A Call to Arms),"Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of ‚Äúharm‚Äù? (2) How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov‚Äôs concern: society will reject autonomous agents unless we have some credible means of making them safe! The Three Laws of Robotics: A robot may not injure a human being, or, through inaction, allow a human being to come to harm. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Isaac Asimov (Asimov 1942): -.. Motiyation ~ In 1940, Isaac Asimov stated the First Law of Robotics, capturing an essential insight: an intelligent agent1 *We thank Steve Hanks, Nick Kushmerick, Neal Lesh, Kevin Sullivan, and Mike Williamson for helpful discussions. This research was funded in part by the University of Washington Royalty Research Fund, by Office of Naval Research Grants 90-J-1904 and 92-J-1946, and by National Science Foundation Grants IRI-8957302, IRI-9211045, and IRI-9357772. ‚ÄòSince the field of robotics now concerns itself primarily with kinematics, dynamics, path planning, and low level control issues, this paper might be better titled ‚ÄúThe First Law of Agenthood.‚Äù However, we keep the reference to ‚ÄúRobotics‚Äù as a historical tribute to Asimov. should not slavishly obey human commands its foremost goal should be to avoid harming humans. Consider the following scenarios: A construction robot is instructed to fill a pothole in the road. Although the robot repairs the cavity, it leaves the steam roller, chunks of tar, and an oil slick in the middle of a busy highway. A softbot (software robot 4 is instructed to reduce disk utilization below 90 o. It succeeds, but inspection reveals that the agent deleted irreplaceable I4TEX files without backing them up to tape. While less dramatic than Asimov‚Äôs stories, the sce_ . __ _-. narios illustrate his point: not all ways of satisfying a human order are equally good; in fact, sometimes it is better not to satisfy the order at all. As we begin to deploy agents in environments where they can do some real damage, the time has come to revisit Asimov‚Äôs Laws. This paper explores the following fundamental questions: How should one formalize the notion of ‚Äúharm‚Äù? We define dont-disturb and restoretwo domain-independent primitives that capture aspects of Asimov‚Äôs rich but informal notion of harm within the classical planning framework. How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? We leverage and extend the familiar mechanisms of planning with subgoal interactions (Tate 1977; Chapman 1987; McAllester & Rosenblitt 1991; Penberthy & Weld 1992) to detect potential harm in polynomial time. In addition, we explain how the agent can avoid harm using tactics such as confrontation and evasion (executing subplans to defuse the threat of harm). How should an agent resolve conflict between its goals and the need to avoid harm? We impose a strict hierarchy where dont-disturb constraints override planners goals, but restore constraints do not. When should an agent prevent a human from harming herself ? At the end of the paper, we show how our framework could be extended to partially address this question. 1042 Planning and Scheduling From: AAAI-94 Proceedings. Copyright ¬© 1994, AAAI (www.aaai.org). All rights reserved.",1994,AAAI,0.0
Experimentally Evaluating Communicative Strategies: The Effect of the Task,"Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents‚Äô resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents‚Äô resources and communicative strategies.",1994,AAAI,0.0
Decision-Theoretic Plan Failure Debugging and Repair,"A number of strategies exist for the recovery from execution-time plan failures. One manner in which these strategies difFer is the degree of dependence on the reliability and availability of the planner‚Äôs knowledge. The best strategy, however, may be dependent on a number of considerations, including the type of plan failure, the criticality of the failure, the availability of resources, and the reliability and availability of the knowledge involved in a given plan failure instance. We are examining a decision-theoretic approach to diagnose plan fakes and to dynamically select fkom multiple failure recovery strategies when an execution-time plan failure occurs. Existing failure recovery strategies generally classify, with assumed or proven certainty, the type of error that occurred during plan execution and then select a fixed strategy to recover from that error. Assumptions regarding the accuracy and completeness of the planner‚Äôs domain model vary. On one end are approaches that use deterministic heuristics or purely syntactic analyses to debug and repair. These approaches are efficient and require limited knowledge, but generally are limited in the level of diagnosis and repair they can perform. On the other end are logic-based approaches, that are robust, but are knowledge and resource intensive. Such approaches are not always feasible. Incomplete or uncertain knowledge of previous planning actions, as in multi-agent planners, prechrdes a complete logical analysis. Even when possible, the costs of collecting and reasoning with complete information may be intractable, especially given time pressures and other resource constraints. The goal of our work is the development of an approach that can intelligently select and apply ftilure recovery strategies that are appropriate to the situation and that can cope with uncertainty. There are three primary components of our research: diagnosis of plan failures, plan repair and planner modification. When a failure is detected, we use a",1994,AAAI,0.0
Preliminary Studies in Agent Design in Simulated Environments,"It is known that, in general, the point along the purely-reactive/classical-planning axis of the controller spectrum that is most appropriate for a particular environment/task (E/T) p air will be determined by characteristics of the environment, the agent‚Äôs perceptual and effectual capabilities, and the task. Instead of proposing another hybrid architecture, we want to determine criteria for determining which architectural compromise is best suited for a given E/T. Our goal is to understand relationships between E/T pairs and the agent architecture, so that we can predict the performance of the architecture under parametric variations of the environment and/or the architecture. This is a first step toward constructing methods for automatic synthesis of agents as in (Ros89). Our example of a domain where the choice of architectural basis is not so clear is the game of XChomp (programmed by Jerry J. Shekhel), a close relative of the commercial game PacMan. This domain allows for easy change of parameters to simulate a number of discrete combinatorial problem domains. Interesting characteristics of the game that make it different from the E/Ts considered in (AC87), (Bro86), (ChaSl), (Sch87) among others, are: There are no+EocaE taslcs. By non-local, we refer to not only spatial and temporal extents, but also universal quantification of parameters of the task. Hostile aspects of the environment may be temporarily made not only benign, but positive concrete goals; these conversions are under the control of the player. Classification of objects changes over time, complicating the decision of how to respond to such objects, as these are based on projections of possible futures. There is not much flexibility with respect to movement. Not only is movement within this environment restricted to the four cardinal directions, but it is a maze, so that in most locations, only two of those four may be used. When the cost of making mistakes is high, the extra effort to get it right the first time is (possibly) justified. There are multiple conflicting objectives. While having multiple objectives is not particularly novel, those in this environment have a nasty habit of pulling their acquisitions at cross purposes. Any designer must answer the following questions: On what informational basis does an agent make its action selection choice ? What aspects of the world does it perform forward projection on, what aspects does it sense, and what is the map from external and internal state of the agent to an action (or sequence) that maximizes the objective function of the agent? We need to be able to construct a solution and justify it using methods other than pointing to the constructed solution as an existence proof. This follows in the spirit of work done in (Hor93) for a mobile robot. To enable us to study these questions, we isolate a range of E/T combinations based on the XChomp game. We implement a range of controllers that exploit the information needed for ‚Äúoptimal‚Äù play and test their task performance experimentally. We then vary the performance requirements and environmental specifications in a form of perturbation analysis to determine how robust the agents are; and how to modify them to be effective in new situations. In addition, starting from a very simplified version of this E/T, we are developing a theoretical basis upon which to justify the agents we develop. This basis is expected to not only be used in determining how an agent should behave, but also what a designer should not be concerned about.",1994,AAAI,0.0
On the Relation between the Coherence and Foundations Theories of Belief Revision,"Two recent, papers, (Ggrdenfors 1990; Doyle 1992), try to assess the relative merits of the two main approaches to belief revision, the foundations and coherence theories, but leave open the question of the mathematical connections between them. We answer this question by showing that the foundations and coherence theories of belief revision are mathematically equivalent. The result also has consequences for nonmonotonic reasoning, as it, entails that Poole‚Äôs system of default, reasoning and Shoham‚Äôs preferential logic are expressively equivalent, in that they can represent the same set of non monotonic consequence relations.",1994,AAAI,0.0
Model-Based Sensor Diagnosis: When Monitoring Should be Monitored,"A complex industrial plant, such as a nuclear power plant, is monitored thanks to a number of sensors. The instrumentation may be itself a complex system liable to failures. We propose a model-based sensor diagnosis system which relies on the topological description of the plant and on a set of component models. This model implicitly conceals relations involving only sensor data. Such relations must always be verified if components behave normally; thus, the detection task consists of verifying these relations. So, this work is a first step in extending the scope of model-based diagnosis, since we question here the information stemming from the plant and normally considered as safe. As further studies, we wish to monitor this detection system itself; i.e., whenever the instrumentation is supposed to behave correctly, nonverified constraints point out to errors in the plant model. Questioning the model-based diagnosis A model-based diagnosis (Reiter 1987) relies on structural and behavioural knowledge and on observations. Observations in a plant stem from sensors. Since the instrumentation is liable to failures, sensor data are questionable. On the other hand, component models in a thermohydraulic circuit are very crude and the topological database describing the plant must be updated after each human intervention on the plant. So, when a model-based reasoning system provides a result, three assumptions must be taken into account: there is no sensor failure, component models are accurate enough, and the topological database rigorously describes the plant. Sensor failure in a thermohydraulic circuit Fluid behaviour is described by a set of equations of different kinds stemming from the plant model. As some of the variables are measured by sensors, we seek to exhibit, when they exist, algebraic relationships between them by eliminating the variables which are not measured. Such constraints should be verified at each step if every component behaves properly. Constraint violation is equivalent to a malfunction and is seen as a sensor failure. 1476 Student Abstracts Constraints can be found in two steps. First, a qualitative model of constraint existence is set up by means of structural analysis. Secondly, models are formally handled as according to the structural analysis results in order to establish the constraints on sensor data. The set of equations is turned into a structural matrix (Iwasaki & Simon 1986) in which each variable v is characterized with respect to each equation E by only two pieces of information: whether v is involved in E and whether E can be solved with respect to v. Constraints are found by triangulation of a part of this matrix. Further research direction Whereas the operator may check the installation thanks to the instrumentation, the present system aims at providing a diagnosis on the instrumentation itself, rather than on the installation. We wish to check the sensors with their own values. Sensor data, thus validated, may be used in other monitoring systems. Sensor diagnosis may be seen as a part of the diagnosis of a monitoring system. On the other hand, this sensor diagnosis system may itself be faulty, and should also be monitored. This system is based on four sources of knowledge and data, namely: the topological database describing the installation (TDB), the models library (ML), the research and generation algorithm (A), and sensors data. If no sensor is assumed to be faulty, then constraints violation is seen as a set of malfuctions of (TDB), (ML), or even (A).",1994,AAAI,0.30000000000000004
When the Best Move Isn‚Äôt Optimal: Q-learning with Exploration,"The most popular delayed reinforcement learning technique, Q-learning (Watkins 1989)) estimates the future reward expected from executing each action in every state. If these estimates are correct, then an agent can use them to select the action with maximal expected future reward in each state, and thus perform optimally. Watkins has proved that Q-learning produces an optimal policy (the function mapping states to actions) and that these estimates converge to the correct values given the optimal policy. However, often the agent does not follow the optimal policy faithfully the agent must also explore the world, taking suboptimal actions in order to learn more about its environment. The ‚Äúoptimal‚Äù policy produced by Q-learning is no longer optimal if its prescriptions are only followed occasionally. In many situations (e.g., dynamic environments), the agent never stops exploring. In such domains Q-learning converges to policies which are suboptimal in the sense that there exists a different policy which would achieve higher reward when combined with exploration. A bit of notation: &(z, a) is the expected future reward received after taking action a in state x. V(x) is the expected future reward received after starting in state x. 0 and v are used to denote the approximations kept by the algorithm. Each time the agent takes an action a moving it from state x to state y and generating a reward T, Q-learning updates the approximations according to the following rules:",1994,AAAI,-0.6000000000000001
Similarity-Based Estimation of Word Cooccurrence Probabilities,"In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ""eat a peach"" and ""eat a beach"" is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for est imat ing the probabili ty of such previously unseen word combinations using available information on ""most similar"" words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probabil i ty estimates for unseen word bigrams in a variant of Katz 's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.",1994,ACL,1.0
From Strings to Trees to Strings to Trees ... (Abstract),"A certain amount of structure is necessary simply because a clause may embed another clause, or one clause may attach to another clause or parts of it. Leaving this need of structure aside, the question then is how much structure should a (minimal) clause have? Grammar formalisms can differ significantly on this issue. Minimal clauses can be just strings, or words linked by dependencies (dependency trees), or with rich phrase structure trees, or with flat (one level) phrase structure trees (almost strings) and so on. How much hierarchical structure is needed for a minimal clause is still an open question, that is being debated heatedly. How are clauses put together? Are these operations more like string manipulations (concatenation, insertion, or wrapping, for example) or are they more like tree transformations (generalized transformations of the early transformational grammars, for example)? Curiously, the early transformational grammars, although clearly using tree transformations, actually formulated the transformations as pseudo string-like operations! More recent non-transformational grammars differ significantly with respect to their use of string rewriting or tree rewriting operations. Grammar formalisms differ with respect to their stringiness or treeness. Also during their evolution, they have gone back and forth between string-like and tree-like representations, often combining them in different ways. These swings are a reflection of the complex interplay between aspects of language structure such as constituency, dependency, dominance, locality of predicates and their arguments, adjacency, order, and discontinuity. We will discuss these issues in an informal manner, in the context of a range of formalisms.",1994,ACL,0.0
An Automatic Treebank Conversion Algorithm for Corpus Sharing,"An automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank. A new treebank associated with a different grammar can be generated automatically from the old one such that the information in the original treebank can be transformed to the new one and be shared among different research communities. The simple algorithm achieves conversion accuracy of 96.4% when tested on 8,867 sentences between two major grammar revisions of a large MT system.",1994,ACL,1.0
Book Reviews: The Digital Word: Text-Based Computing in the Humanities,"The sixteen essays assembled by Landow and Delany in this volume cover a broad array of topics connected by a concern with ""text-based computing"" that have applications to the ""humanities."" The unifying theme in the collection is, to a certain extent, an attempt to assess the impact that the rapid proliferation of networked text corpora and digital communications is having (or might have) in text-oriented disciplines. The editors suggest that networked computing facilitates analysis of very large corpora, or a docuverse (p. 23), which they contend is a vital component of research in the humanities. This does not mean that researchers can simply scale up their methods to meet the demands and possibilities of new technical capabilities. Rather, they suggest that humanists must",1994,CL,-1.0
Commentary on Bird and Klein,"Bird and Klein show us how various phonological constructs--feature geometry, the prosodic hierarchy, well-formedness constraints on strings of segments, templatic/autosegmental phonology, morphology-phonology interactions, and vowel-zero alternat i o n s m a y be treated in a rigorous fashion using the the formal resources of HPSG. To the incautious critic (or the skeptical phonologist) it may seem that they have given us merely a new notation in which to express conventional phonological analyses, such as Tranel's analysis of French schwa epenthesis or Goldsmith's textbook examples of Sierra Miwok. A central principle of Bird and Klein's approach that takes it beyond notation is Phonological Compositionality and the related concept of monotonicity, mentioned almost in passing in Section 3.3. Among the consequences of monotonicity are: (1) feature values may not be altered; (2) segments or other structural material may not be removed (i.e. delinking and restructuring, such as resyllabification, are prohibited); and (3) constraints are not extrinsically ordered; in fact, extrinsic rule ordering is inexpressible. An accommodation with orthodox generative phonology with regard to (1) may be found by employing underspecification, translating feature-changing rules into feature-filling constraints. Objections to extrinsic ordering have been raised at various times before, and many phonologists would like to be rid of it, so Bird and Klein's proposals regarding (3) are welcome. Many phonologists find (2) very unpalatable, however, despite the fact that Bird and Klein's example of French schwa insertion is also potentially applicable to some putative cases of deletion. A deletion rule operative in Welsh mutation, which removes / g / from the lexical representation of gardd in some environments, but leaves it present in the citation form, could be treated in HPSG phonology as consonant-zero alternation on a par with French schwa. This alternation is only treated as deletion because it is the noncitation form that lacks the initial consonant. Other apparent cases of deletion are more awkward for declarative approaches. In English trisyllabic shortening (e.g. profane --* profanity) and -ic shortening (e.g. t6ne t6nicity), orthodox analysis deletes a vowel slot and association lines incident to it. To treat these as instances of vowel-zero alternation would appear to require the representation of the stem to be sensitive to the presence of a very particular set of suffixes (-ic, -ity, etc.). HPSG phonology might either analyze these using different CV templates, like Sierra Miwok allomorphy, or reject the bisegmental analysis of phonological length, by treating shortening as the addition of a ""shortness"" feature (e.g. trisyllabic-shortening(V) --* I-long]). I do not expect either of these proposals to be popular. In any case, reconstructing morphophonological rules such as the above in declarative style will not satisfy some critics, for whom any suggestion of languagespecific rules is anathema.",1994,CL,0.0
Briefly Noted,"Most of the computer-based writing tools presently in use are quite dumb, relying on simple techniques to achieve their goals; however, development of intelligent writing aids is an important area in computational linguistics. Two recent books address many of the issues involved in the development of better writing tools. Computers and Writing: State of the Art (CWSA), and Computers and Writing: Issues and Implementations (CWII) contain papers from the Third International Conference on Computers and Writing and the Fourth Conference on Computers and the Writing Process, respectively. The second book contains selected papers, which were further developed by the authors for publication. It is not clear whether the papers in the first volume underwent the same selection process, but the time that elapsed between the conference and the publication of the book suggests that the papers might have been revised since their presentation at the conference. The papers cover a wide range of subtopics; as Sharples observes in his introduction, ""This volume is deliberately eclectic"" (CWII, p. 3). Research interests of participants include cognitive science, computer science, education, engineering, linguistics, and philosophy. The majority of the contributors are from the United Kingdom. CWII contains an introduction by the editor and 15 papers; CWSA contains a preface by Holt and 24 papers. Neither book has a concluding chapter to tie the strands together in any way, so the reader may go away with a sense of having been left hanging at the end. Perhaps the absence of concluding chapters is due to the difficulty of drawing any general conclusions from such a wide spectrum of topics. They range from designing writing tools to assessing educational tools, from observing writing practices to writing interactive fiction. Thus, many of the chapters might not be of interest to any one reader, unless that reader wants to gain an overview of the issues that are currently being explored. Most of the papers in the two books can be (very) loosely divided into three categories: the effects of the computer on writing practices; CAI for writing; and the development of computer writing tools. Neither of the books has a computational linguistics focus. About one-fifth of the papers in each describe the application of natural language processing techniques. For the most part, only current, reliable capabilities are applied to create working systems. I will briefly discuss a few of the papers that describe the implementation of NLP techniques, to give an idea of how they are being used. Kempen and Vosse (CWSA) present a ""language-sensitive"" text editor for Dutch. Their system focuses on detecting real-word spelling errors that are homophonous to the target word, as well as flagging inconsistent spellings and other orthographical errors. The text undergoes both word-level and sentence-level analyses. At the word level, triphone analysis is performed to select all possible homophonous word alternatives. Next, a shift-reduce parser analyzes each sentence. The parser, which contains about 500 augmented phrase structure rules, ignores aspects of syntactic structure that are not relevant to orthography. Feature matrices on the nonterminal symbols detect feature violations. The evaluation of the system is only preliminary, but it appears to be a practical solution to the problem of discovering many real-word errors using current parsing capabilities. Hoard, Wojcik, and Holzhauser (CWSA) report on a simplified English checker devel-",1994,CL,-1.0
Commentary on Kaplan and Kay,"Anyone with a fundamental interest in morphology and phonology, either from a scientific or a computational perspective, will want to study this long-awaited paper carefully. Kaplan and Kay (henceforth K&K) announce two goals: ""to provide the core of a mathematical framework for phonology"" and ""to establish a solid basis for computation in the domain of phonological and orthographic systems."" They show how the algebra of regular relations, with their corresponding automata, can be used to compile systems of phonological rules in the style of SPE, including directionality, optionality, and ordering. They sketch mechanisms for incorporating a lexicon and for dealing with exceptional forms, thus providing a complete treatment in a unified framework. This accomplishment in itself will not compel the attention of many working phonologists, who have found good reasons to replace the SPE framework (see Kenstowicz [1994] for a survey of modern practice), and whose efforts since 1975 have been aimed mainly at finding representational primitives to explain typological generalizations, support accounts of learning, generalization and change, and provide one end of the mapping between symbols and speech. In this effort, there has been little emphasis on SPE's goal of giving phonological descriptions an algorithmically specified denotation. Perhaps this paper, despite its superficial lack of connection to contemporary work in phonology, will set in motion a discussion that will ultimately redress the balance. On the computational side, practitioners of practical NLP will be happy to make extensive use of the algebra of regular relations, since it provides a truly elegant engineering solution to a wide range of problems. However, although direct interpretation of some simple FSTs can be efficient (e.g. Feigenbaum et al. 1991), and although Koskenniemi has documented efficient implementation techniques for his two-level systems, the overall architecture presented in this paper is not practically usable as written, because of either the size of the resulting automata or the time needed for (unwisely implemented) nondeterminism, or both. A range of well-known techniques enable programs based on the algebraic combination of (unary) FSAs to make efficient use of both time and space. Although these methods do not apply to FSTs in general, we may presume that K&K have developed analogous techniques for the crucial range of cases. With the growing interest in this technology, we can expect that either K&K will publish their work or others will recapitulate it, so that the algebra of regular relations can take its proper and prominent place in the toolkit of computational linguistics.",1994,CL,0.0
A Hierarchical Stochastic Model for Automatic Prediction of Prosodic Boundary Location,"Prosodic phrase structure provides important information for the understanding and naturalness of synthetic speech, and a good model of prosodic phrases has applications in both speech synthesis and speech understanding. This work describes a statistical model of an embedded hierarchy of prosodic phrase structure, motivated by results in linguistic theory. Each level of the hierarchy is modeled as a sequence of subunits at the next level, with the lowest level of the hierarchy representing factors such as syntactic branching and prosodic constituent length using a binary tree classification. A maximum likelihood solution for parameter estimation is presented, allowing automatic training of different speaking styles. For predicting prosodic phrase breaks from text, a dynamic programming algorithm is given for finding the maximum probability prosodic parse. Experimental results on a corpus of radio news demonstrate a high rate of success for predicting major and minor phrase boundaries from text without syntactic information (81% correct prediction with 4% false prediction).",1994,CL,1.0
Commentary on Kaplan and Kay,"To appreciate this article fully, it is essential to understand the historical context into which it fits, and which it has to some extent created. Although formally published for the first time here, it is already an extremely influential and classic piece of work. Finite-state machines, in one form or another, have been used for the description of natural language since the early 1950s, with the extension to transducers appearing in the 1960s. After Chomsky's stern condemnation of the adequacy of finite-state machines for describing sentence structures, they virtually disappeared from mainstream theoretical linguistics. Within computer science, they continued to be a standard formalism, although transducers were not accorded the same detailed algebraic attention as simple automata. Phonologists, meanwhile, were inventing a variety of rule mechanisms that were (with rare exceptions) only partly formalized. Superficially, most of these systems (as typified by those of Chomsky and Halle) appeared to have little to do with finite-state machines. Indeed, their notations tended to suggest that the rules had much more than finite-state power. Kaplan and Kay have integrated these two streams of work--algebraic treatment of automata in computer science, and phonologically-motivated formalisms within linguistics--and their results should feed back productively into both subfields. The framework they have established allows the comparison of different competing formalisms in a rigorous manner, and permits the exploration of the formal limitations or capabilities of rule notations that were previously more like expository devices than formally defined systems. What may not be clear to the casual reader is that this work has been developed over many years, and early versions of it have already escaped into the computational linguistics community in less prominent forums. In this way it has already affected the course of research into phonological/morphological formalisms. Perhaps the most notable (and in its turn, influential) development has been Koskenniemi's two-level morphology, which has been successfully applied to the morphology of a very wide number of languages. Koskenniemi's ideas are a direct development of Kaplan and Kay's, as explained in Section 7 of the paper here. The theory of regular relations and finite-state transducers should not be viewed as a mere re-formalisation of 1960s linguistics. As well as its relevance to the two-level model, Kaplan and Kay suggest that it may also throw light on the formal properties of autosegmental phonology. Although the ideas were first circulated about 15 years ago, they are still of central relevance to computational phonology today.",1994,CL,0.0
Briefly Noted,"The author hypothesizes that patterns of cohesion relations, displayed graphically, can illustrate the rudiments of texture. Thirty-five English texts taken from five genres (nonfiction, essays, biographies, novels, and short stories) are hand-labeled with three types of cohesion relations: definite articles, pronouns, and verbs with agent displacements (i.e., the verb's agent role is filled by a constituent not in the subject position). A text's ""cohesion map"" is created by assigning to each word a location on a two-dimensional grid corresponding to the word 's position in the text (roughly, each sentence corresponds to a row), and then drawing a line between the location of a cohesive element and the location of its original referent. The resulting map looks somewhat like a column of hanging pine-needle bunches; thus texts can be compared visually for properties such as burstiness, density, and connection span. Each kind of cohesive element is assigned its own map, although for one example all three cohesion maps are superimposed. The resulting maps seem to illustrate interesting differences among the texts. Unfortunately, the author neither describes how to analyze these maps nor explores the effects of multiple interacting types of cohesion elements. Instead, the analysis focuses on comparing text genres based on their overall ""relative cohesiveness."" The leap from elucidating a text's style or texture to comparing texts for relative cohesion on the basis of three syntactic cues (one of which tends to occur only rarely and almost never extra-sententiaUy) is not well justified by the early parts of the book. Perhaps for this reason, there are two significant problems with the way relative cohesiveness is computed. First, the author assumes that nonfiction text is less cohesive than other genres such as biographies and fiction (footnote, p. 55). By far the most frequently occurring of the three cohesion relations that the author examines is pronominal reference (p. 68); lexical cohesion relations are excluded. However, as the author points out, biographies and fiction tend to have many pronominal references, whereas nonfiction texts tend to have few pronominal references but many lexical cohesion relations. So in effect the result of the comparison is pre-determined. Despite these problems, the author concludes that the definitions and procedures used are satisfactory (p. 96). The second problem with this analysis is that the comparison is based on a ""cohesion index,"" which is determined by multiplying the average number of cohesive elements corresponding to a referent by the average distance between the elements and their referent. This number is meant to indicate the relative cohesiveness of a text, but does not discriminate between a referent that has only a few, distant references and a referent that has a large number of nearby references. Bearing in mind that this is crossdisciplinary work (the author apparently originates from a literary field), a reader of Computational Linguistics may be put off by outdated references to the artificial intelligence literature and weak discussions of computational issues in general. The author is to be commended for working with a large number of lengthy texts, a rare precedent in discourse analysis. The idea of graphically displaying the interactions of the syntactic cohesive elements is a useful one. The next important steps are to explore how to represent multiple interacting cue types, and how to analyze or interpret the resulting illustrations; this may lead to a better understanding of texture and cohesion in written texts. --Marti Hearst, University of California, Berkeley",1994,CL,0.0
"PRINCIPAR - An Efficient, Broad-coverage, Principle-based Parser","We present an efI]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It conrains a lexicon with over 90,000 entries, constructed automatically by applying a set of extraction and conversion rules to entries from machine readable dictionaries.",1994,COLING,0.5
XTAG System - A Wide Coverage Grammar for English,"This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English. The various components of the system are discussed and preliminary evaluation results from the parsing of various corpora are given. Results from the comparison of X3AG against the IBM statistical parser and the Alvey Natural Language Tool parser are also given. 1 I N T R O D U C T I O N XTAG is a large on-going project to develop a widecoverage grammar for English, based on the l,exicalized Tree Adjoining Grammar (I 3""AG) tbrmalism. LTAG is a lexicalized mildly-context sensitive tree rewriting system [Joshi et al., 1975; Schabes, 1990] that is closely related to Dependency Grammars and Categorial Grammars. Elementary trees in 13~AG provicle a larger domain of locality over which syntactic and semantic (predicate-argument) constraints are specified. XTAG also serves as an LTAG grammar development system consisting of a predictive left-to-right parser, an X-window interface, a roof phological analyzer, and a part-of-speech tagger (also referred to as simply 'tagger'). 2 S Y S T E M D E S C R I P T I O N Figure 1 shows the overall llow of the system when parsing a sentence. The input sentence is submitted to the Morphological Analyzer and the 3hgger. The morphological analyzer retrieves the morphological information for each individual word from the morphological database. This output is tiltered in the P.O.S Blender using the output of the trigram tagger to reduce the part-of-speech ambiguity of the *currently at BBN, Cambridge, MA, USA Input Sentence ~ ' p h Analy~ l_ 'l'~ger ,,__ _ _ ~ [ p.0.S Blender ~< 1 Ãß . . . . . . . i i _ _ / i ~ 1 % ~ TteeSdection , , ~ N y n O B ~ . . . . . . . . . . . . . . . . . . . . Derivation Structure Figure I : Overview of XTAG system words. The sentence, now annotated with part-ofspeech tags and morphological information for each word, is input to the Parser, which consults the syntactic database and tree database to retrieve the appropriate tree structures for each lexical item. A variety o[' heuristics arc used to reduce the number of trees selected. The parsertitan composes the structures to obtain the parse(s) of the sentence. 2.1 Morphological Analyzer The morphology database [Karp et al., 19921 was originally exlracted from 1979 edition of the Collins English Dictionary and Oxford Adwmced Learner's l)ictionary of Current English, and then cleaned up and auglnentcd by hand. It consists of approximately 317,000 inltected items, along with their root forms and intlectional intbrmalion (such as case, num-",1994,COLING,0.5
A System of Verbal Semantic Attributes Focused on the Syntactic Correspondence between Japanese and English,"This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases. These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures.",1994,COLING,0.5
A Reestimation Algorithm for Probabilistic ttecursive Transition Network,"Prob~bilistic l{,ecursive Tr~msition Network(Pl~TN) is an elevated version of t{51'N to model and process lan-. guages in stoch~st, ic parameters. The representation is a direct derivation front the H,TN and keeps much the spirit of ltidden Markov Model at the same tint(,. We present a reestimation algorithm ['or Ptl,TN that is ~ variation of Inside-Ontside algorithm that comput, es the vMues of the probabilistic parameters from sample sentences (parsed or unparsed). 1. ln trodu( : t ion In this pal)er , we introduce a network representa t ion, Probabilistic Recursive Transitio. Network tha t is directly derived fl'Oln R'CN and I tMM, and present an est imat ion algori thm lot tile probabilistic paraHteters. PR;12N is a ][]TN mJgmented with probabil i t ies in the transi t ions ~md states and with the lexical distr ibutions in the transi-tions, or is the Hidden Markov Model augmented with a stack tha t makes some traltsit ions deter ministic. The paramete.r esthnat ion of PI{;I'N is devel oped as a wu'iation of Ins ide ( )u t s ide algorithm. The hlsidc ()utside algori thm has becn applied e(,10t , I;o ~, ,.~* recently by Jelinek (1{t9{/) and ],ari (1991). The algori thm was first introduced by Baker in 1.979 and is the context free lmtguage version o[ Forward-.Backw~rd algori thm in IIid-. *This research is partly supported by KOSEF (Km:ea Science altd Teclntology l""oundation) under tit{= title ""A Study mt the Bnilding '[~echni(lues for [txdmst Km~wledge based Systems"" from 19911 through 1994. den Markov Models. Its theoret ical lbund~Ltion is laid by Baam aud Weh:h in the late 6l)'s, which in t a rn is a type of the F,M Mgorithm in statist ics (Rabiner, 1989). Kupiec (1991) introduced a trellis based es-. t imation Mgorithm of Hidden SCFG tha t ae commodates both ilnside-Outside ~dgorithm and l!brward-.Backward "",flgorithm. The meaning of our work can be sought from the use of more plain topology of I{TN, whereas Kupiec 's work is a unilied version of tbrward-.backword and Inside Outside ~lgorithms. Nonetheless, the implemen. rat ion of reest imat ion Mgorittun carries no more theoretical significance than the applicative efli ciency and variation for differing representat ions since B~ker first apt)lied it to CI""Gs. 2. Probab i l i s t i c R e c u r s i v e Trans i t ion N e t w o r k A probabilistic ff.l.'N (PRTN, hereafter) denoted by A is ~ 4 tuple. A is ~ t ransi t ion m~trix containing tr~n.sition probabili t ies, ~tnd 13 is aiL observation mat r ix containing probabi l i ty dis tr ibut ion of the words ob servable at each terminM transi t ion where row and column correspond to terminM transi t ions and a list of words respective, ly. F specilies the types of transit ions, and D2 denotes a stack. The first two model parameters are the same as tha t of I[MM, thus typed transi t ions and the existence of a stack art', what distinguishes I ' t tTN fl'om t[MM.",1994,COLING,0.1
A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm,We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pa~qs N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. q'he proposed Japanese morphological analyzer achieved 95. l% recall and 94.6% precision for open text when it was trained and tested on the ATI'¢ Corpus.,1994,COLING,1.0
A Modular Architecture for Constraint-Based Parsing,"This paper presents a framework and a system for implementing, comparing and analyzing parsers for some classes of Constraint-Based Grammars. The framework consists in a uniform theoretic description of parsing algorithms, and provides the structure for decomposing the system into logical components, with possibly several interchangeable implementations. Many parsing algorithms can be obtained by compositi(m of the modules of our system. Modularity is also ,~ way of achieving code sharing for the common parts of these various algorithms. Furthermore, tile design lielpi~ reusing the existing modules when implementing other algorithms. The system uses the flexible modularity provided by the programmifig languages hleool-90, 1)ased on a type system that ensures the safety of module composition.",1994,COLING,0.7000000000000001
Reverse Queries in DATR,"I)ATI{ is a declara t ive re.presentat ion language ti)r lex-. ical i i f formation and as such, fit prin(:iple, neul;ral with resl)(;ct; 1;o i)arl;icul&r l)rocessing st,rat,egies. Previous D A T R (:l)mt)iler/inl;erI)ret(!r sy,qt(!ms suppor t only one al:l:e.4s ,%rat,egy ~hnt, closely resembles the set, of inti~r-. otlce rllleS of the procedm:sd s(mumti(:s of ])A.Tli (Evmls & C,~tz(lar 1989a). In this i/al)er w(! present, an alt,ern;> 1;ivc access st,r~tl;egy ('ri:'uc'l'.s('. q'ucr!/ .stral, cgy ) ~br a himtrivial subsel; o f I ) A ' F ] / . 1 The Reverse Query Prob lem D A T R (Evans & Gazdm"" 1989@ has l)ecome. Olte of the iiiosl; widely used fornlatl languages tin' the I'(~l)t'ese.tll;;tt,ion of lexicad infornlat,ion. ! )N[ ' l l ~q)plil:ations ha.re been (h~velol)ed for a wide variety of lmlguages (including English, .lat/mmse , Kikuyu, Arabi(:, l,at,in, and others) ;rod mmly different; subdonudns of le, xical rel)resentat,ion, including inilect,ional morphology, undt~rspecification l)honltlogy, nlm-(:onca.t,enative morphophono l ogy, lexicaI senlanti(:s, and tone sys tems I. We presutlI)OSe tha t the reader of the llresenl; paper is [)mlilia.r wi th the basic Datur(!s of ])AT[/. as spe(:ilie(1 in Ewms & Gazda.r [ 1 9 8 9 @ 7'he ;all;(tu;tcy of st lexi(:on repr(~se.nt,;~t;ion fornmlisln depends basical ly on two ma jo r fact,ors: • it,s declarative c:cpres.sivenes.s: is the ff)rmalism, in prin(:iple, i:al)al)le of rel)resent,ing l;he phenomena in • This research was partly SUpl)orted by the (~ermau l,'e(h!ral Ministry of Heseareh and Technology (BMfI', project VEI~P,MOBIl,) at the University of l~ielelk~ld. I would like tel thank 1)afy(Id Gibbon for vely ttseful COllltHeltL8 o11 ali earlier draft of I;his paper. 1 See Cahill [[9.93], Gibbon [ [9!)2], (lazdm"" [I 9921, ;rod Kilbm'y [1992] for reeelll; I)ATR applicatious in Lhese areas. An informal introducl, ion I,o I)ATR is given in (lazdar [19!10]. 'l'he sl.andatd syntax and semantics of I)ATI{ is defined in I,iwms gz (~az(lar [198!)a, 19891)]. hul)lementation issues are discussed iu (:libbon & Almua [1991], Jenkins [1990], aud in Gibbon [19931 . M,)ser [I 992a, 1992b, 1992(:, 1992d] provides interesting insights into the fl~rmal properties of I)N['I/(see also the I)A'['I/ represen/ations of finil,e state allLomal.a, dilI'e~ent kiiMs of logics, regisl, er operations ere. in Evans & (l~z(la,' [1990], and l,;ml;er [1993]). Andry et al. [19931 describe how I)ATR can lm used in speech-oriented ~tl)l)lieal.ion.~;. qll(*,st,ion~ &lid does it allow lbr a.n explicit t,re;~t,lllonl; of generalisat,ions, subgene, ral isat ions, ;rod ex--",1994,COLING,0.5
An Efficient Syntactic Tagging Tool for Corpora,"A BSTRA CT The tree bank is an important resources tbr MT and linguistics researches, but it requires that large number of sentences be annotated with syntactic information. It is time consuming and troublesome, and dil'ficult to keep consistency, if' annotation is done manually. In this paper, wc presented a new technique for the semi-automatic tagging of Chinese tcxt. The system takes as input Chinese text, and outputs the syntactically tagged sentence(dependency tree). We use dependency grammar and employ a stack based sh i f t / r educe context-dependent parser as the tagging mechanism. The system works in human-machine cooperative way, in which the machine can acquire tagging rules from human intervention. The automation level can be improved step by step by accumulating rules during annotation. In addition, good consistency of tagging is guaranteed.",1994,COLING,0.6000000000000001
AUTOMATIC MODEL REFINEMENT - with an application to tagging,"Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic model, the Classitication and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement. Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust. This over-tuning phenomenon cannot be completely removed by crossvalidation process (i.e., pruning process). A probabilistic classification model based on the selected discriminative features is thtls proposed to use the training data more efficiently. In tagging the Brown Corpus, our probabilistic classification model reduces the error rate of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the un-",1994,COLING,1.0
NL Understanding with a Grammar of Constructions,"We present an approach to na tura l language unders tanding based on a computable grammar of const~ctions. A construetionconsists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key element of MINCAL, an implemented natural language speech-enabled interface to an online calendar system. Tile architecture has two key aspects: (a) the use of constructions, integrat ing descriptions of form, meaning an(t context into one whole; and (b) the separation of domain knowledge (about calendars) from application kno'wledgt; (about the part icular on-line calendar). 1 I n t r o d u c t i o n : a n o v e r v i e w o f t h e s y s t e m We present an approach to natural language unders tanding based on a computablc gTummar of constructions. A construction consists of a set of features of form and a description of meaning in a context. A grammar is a set of constructions. This kind of grammar is the key clement of MINCAI,, an imt)lemented natural language speech-enabled interface t() an on-line calendar system. The system consists of a NL grammar, a parser, an on-line calendar, a domain knowledge base (about dates, t imes and meetings), an application knowledge base (about the calendar), a speech recognizer,",1994,COLING,0.30000000000000004
Interference in Learning Internal Models of Inverse Dynamics in Humans,"Experiments were performed to reveal some of the computational properties of the human motor memory system. We show that as humans practice reaching movements while interacting with a novel mechanical environment, they learn an internal model of the inverse dynamics of that environment. Subjects show recall of this model at testing sessions 24 hours after the initial practice. The representation of the internal model in memory is such that there is interference when there is an attempt to learn a new inverse dynamics map immediately after an anticorrelated mapping was learned. We suggest that this interference is an indication that the same computational elements used to encode the first inverse dynamics map are being used to learn the second mapping. We predict that this leads to a forgetting of the initially learned skill.",1994,NIPS,-0.5
A Study of Parallel Perturbative Gradient Descent,"We have continued our study of a parallel perturbative learning method [Alspector et al., 1993] and implications for its implementation in analog VLSI. Our new results indicate that, in most cases, a single parallel perturbation (per pattern presentation) of the function parameters (weights in a neural network) is theoretically the best course. This is not true, however, for certain problems and may not generally be true when faced with issues of implementation such as limited precision. In these cases, multiple parallel perturbations may be best as indicated in our previous results.",1994,NIPS,-1.0
Hyperparameters Evidence and Generalisation for an Unrealisable Rule,"Using a statistical mechanical formalism we calculate the evidence, generalisation error and consistency measure for a linear perceptron trained and tested on a set of examples generated by a non linear teacher. The teacher is said to be unrealisable because the student can never model it without error. Our model allows us to interpolate between the known case of a linear teacher, and an unrealisable, nonlinear teacher. A comparison of the hyperparameters which maximise the evidence with those that optimise the performance measures reveals that, in the non-linear case, the evidence procedure is a misleading guide to optimising performance. Finally, we explore the extent to which the evidence procedure is unreliable and find that, despite being sub-optimal, in some circumstances it might be a useful method for fixing the hyperparameters.",1994,NIPS,0.30000000000000004
Compiling HPSG type constraints into definite clause programs,"We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs. This provides a clear and computationally useful correspondence between linguistic theories and their implementation. The compiler performs offline constraint inheritance and code optimization. As a result, we are able to efficiently process with HPSG grammars without haviog to hand-translate them into definite clause or phrase structure based systems. 1 I n t r o d u c t i o n The HPSG architecture as defined in (Pollard and Sag, 1994) (henceforth HPSGII) is being used by an increasing number of linguists, since the formally well-defined framework allows for a rigid and explicit formalization of a linguistic theory. At the same time, the feature logics which provide the formal foundation of HPSGII have been used as basis for several NLP systems, such as ALE (Carpenter, 1993), CUF (DSrre and Dorna, 1993), Troll (Gerdemann and King, 1993) or TFS (Emele and Zajac, 1990). These systems are at least partly intended as computational environments for the implementation of HPSG grammars. HPSG linguists use the description language of the logic to express their theories in the form of implicative constraints. On the other hand, most of the computational setups only allow feature descriptions as extra constraints with a phrase structure or definite clause based language. 1 From a computational point of view the latter setup has several advantages. It provides access to the pool of work done in the *The authors are listed alphabetically. 1One exception is the TFS system. However, the possibility to express recursive relations on the level of the description language leads to serious control problems in that system. area of natural language processing, e.g., to efficient control strategies for the definite clause level based on tabelling methods like Earley deduction, or different parsing strategies in the phrase structure setup. The result is a gap between the description language theories of HPSG linguists and the definite clause or phrase structure based NLP systems provided to implement these theories. Most grammars currently implemented therefore have no clear correspondence to the linguistic theories they originated from. To be able to use implemented grammars to provide feedback for a rigid and complete formalization of linguistic theories, a clear and computationMly useful correspondence has to be established. This link is also needed to stimulate further development of the computational systems. Finally, an HPSGII style setup is also interesting to model from a software engineering point of view, since it permits a modular development and testing of the grammar. The purpose of this paper is to provide the desired link, i.e., to show how a HPSG theory formulated as implicative constraints can be modelled on the level of the relational extension of the constraint language. More specifically, we define a compilation procedure which translates the type constraints of the linguistic theory into definite clauses runnable in systems such as Troll, ALE, or CUF. Thus, we perform constraint inheritance and code optimization off-line. This results in a considerable efficiency gain over a direct on-line treatment of type constraints as, e.g., in TFS. The structure of the paper is as follows: A short discussion of the logical setup for HPSGII provides the necessary formal background and terminology. Then the two possibilities for expressing a theory using the description language as in HPSGII or the relational level as in the computational architectures are introduced. The third section provides a simple picture of how HPSGII theories can be modelled on the relational level. This simple picture is then refined in the fourth section, where the compilation procedure and its implementation is discussed. A small example grammar is provided in the appendix.",1995,ACL,1.0
Knowledge-based Automatic Topic Identification,"As the first step in an automated text summarization algorithm, this work presents a new method for automatically identifying the central ideas in a text based on a knowledge-based concept counting paradigm. To represent and generalize concepts, we use the hierarchical concept taxonomy WordNet. By setting appropriate cutoff values for such parameters as concept generality and child-to-parent frequency ratio, we control the amount and level of generality of concepts extracted from the text. 1 1 I n t r o d u c t i o n As the amount of text available online keeps growing, it becomes increasingly difficult for people to keep track of and locate the information of interest to them. To remedy the problem of information overload, a robust and automated text summarizer or information extrator is needed. Topic identification is one of two very impor tant steps in the process of summarizing a text; the second step is summary text generation. A topic is a particular subject that we write about or discuss. (Sinclair et al., 1987). To identify the topics of texts, Information Retrieval (IR) researchers use word frequency, cue word, location, and title-keyword techniques (Paice, 1990). Among these techniques, only word frequency counting can be used robustly across different domains; the other techniques rely on stereotypical text structure or the functional structures of specific domains. Underlying the use of word frequency is the assumption that the more a word is used in a text, the more impor tant it is in that text. This method 1This research was funded in part by ARPA under order number 8073, issued as Maryland Procurement Contract # MDA904-91-C-5224 and in part by the National Science Foundation Grant No. MIP 8902426. recognizes only the literal word forms and nothing else. Some morphological processing may help, but pronominalization and other forms of coreferentiality defeat simple word counting. Furthermore, straightforward word counting can be misleading since it misses conceptual generalizations. For example: ""John bought some vegetables, fruit, bread, and milk."" What would be the topic of this sentence? We can draw no conclusion by using word counting method; where the topic actually should be: ""John bought some groceries."" The problem is that word counting method misses the impor tant concepts behind those words: vegetables, fruit, etc. relates to groceries at the deeper level of semantics. In recognizing the inherent problem of the word counting method, recently people have started to use artificial intelligence techniques (Jacobs and ttau, 1990; Mauldin, 1991) and statistical techniques (Salton et al., 1994; Grefenstette, 1994) to incorporate the sementic relations among words into their applications. Following this trend, we have developed a new way to identify topics by counting concepts instead of words. 2 T h e P o w e r o f G e n e r a l i z a t i o n In order to count concept frequency, we employ a concept generalization taxonomy. Figure 1 shows a possible hierarchy for the concept digital computer. According to this hierarchy, if we find iaptop and hand-held computer, in a text, we can infer that the text is about portable computers, which is their parent concept. And if in addition, the text also mentions workstation and mainframe, it is reasonable to say that the topic of the text is related to digital computer. Using a hierarchy, the question is now how to find the most appropriate generalization. Clearly we cannot just use the leaf concepts since at this level we have gained no power from generalization. On the other hand, neither can we use the very top concept everything is a thing. We need a method of identifying the most appropriate concepts somewhere in middle of the taxonomy. Our current solution uses",1995,ACL,0.5
The Effect of Pitch Accenting on Pronoun Referent Resolution,"By strictest interpretation, theories of both centering and intonational meaning fail to predict the existence of pitch accented pronominals. Yet they occur felicitously in spoken discourse. To explain this, I emphasize the dual functions served by pitch accents, as markers of both propositional (semantic/pragmatic) and attentional salience. This distinction underlies my proposals about the attentional consequences of pitch accents when applied to pronominals, in particular, that while most pitch accents may weaken or reinforce a cospecifier's status as the center of attention, a contrastively stressed pronominal may force a shift, even when contraindicated by textual features. I n t r o d u c t i o n To predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. This may be sufficient for written discourse. For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories. I investigate this via a phenomenon that, by the strictest interpretation of either centering or intonation theories, should not occur the case of pitch accented pronominals. Centering theories would be hard pressed to predict pitch accents on pronominals, on grounds of redundancy. To bestow an intonational marker of salience (the pitch accent) on a textual marker of salience (the pronominal) is unnecessarily redundant and especially when textual features correctly predict the focus of attention. Intonational theories would be similarly hard pressed, but on grounds of information quality and efficient use of limited resources. Given the serial and ephemeral nature of speech and the limits of working memory, it is most expedient to mark as salient the information-rich nonpronominals, rather than their semantically impoverished pronominal stand-ins. To do otherwise is an injudicious use of an attentional cue. However, when uttered with contrastive stress on the pronouns, (I) John introduced Bill as a psycholinguist and then HE insulted HIM. (after Lakoff, 1971) is felicitously understood to mean that after a slanderous introduction, Bill retaliated in kind against John. What makes (1) felicitous is tha t the pitch accents on the pronominals contribute at tentional information that cannot be gleaned from text alone. This suggests an attentional component to pitch accents, in addition to the propositional component explicated in Pierrehumbert and Hirschberg (1990). In this paper, I combine their account of pitch accent semantics with Grosz, Joshi and Weinstein's (1989) account of centering to yield insights into the phenomenon of pitch accented pronominals, and the attentional consequences of pitch accents in general. The relevant claims in PH90 and GJW89 are reviewed in the next two sections. P i t c h a c c e n t s e m a n t i c s A pitch accent is a distinctive intonational contour applied to a word to convey sentential stress (Bolinger, 1958; Pierrehumbert , 1980). PH90 catalogues six pitch accents, all combinations of high (H) and low (L) pitch targets, and structured as a main tone and an optional leading or trailing tone. The form of the accent L, H, L+H or H +L informs about the operation that would relate the salient i tem to the mutual beliefs 1 of the conversants; the main tone either commits (H*) or fails to commit 1 Mutual beliefs: propositions expressed or implied by the discourse, and which all conversants believe each other to accept as true and relevant same (Clark and Marshall, 1981).",1995,ACL,-0.8
A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances,"Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called stratified logic, that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world. For Frege (1892), Russell (1905), and Quine (1949) ""everything exists""; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994). We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts). 1. Most linguistic approaches account for the defeasibility of pragmatic inferences by analyzing them in a context that consists of all or some of the previous utterances, including the current one. Context (Karttunen, 1974; Kay, 1992), procedural rules (Gazdar, 1979; Karttunen and Peters, 1979), lexical and syntactic structure (Weischedel, 1979), intentions (Hirschberg, 1985), or anaphoric constraints (Sandt, 1992; Zeevat, 1992) decide what presuppositions or implicatures are projected as pragmatic inferences for the utterance that is analyzed. The problem with these approaches is that they assign a dual life to pragmatic inferences: in the initial stage, as members of a simple or complex utterance, they are defeasible. However, after that utterance is analyzed, there is no possibility left of cancelling that inference. But it is natural to have implicatures and presuppositions that are inferred and cancelled as a sequence of utterances proceeds: research in conversation repairs (I-Iirst et M., 1994) abounds in such examples. We address this issue in more detail in section 3.3. 2. One way of accounting for cancellations that occur later in the analyzed text is simply to extend the boundaries within which pragmatic inferences are evaluated, i.e., to look ahead a few utterances. Green (1992) assumes that implicatures are connected to discourse entities and not to utterances, but her approach still does not allow cancellations across discourse units. 3. Another way of allowing pragmatic inferences to be cancelled is to assign them the status of defeasible information. Mercer (1987) formalizes pre-",1995,ACL,0.7000000000000001
Letter to the Editor,"Unlike Sidner, who used thematic roles to order the Cf (or potential foci), Kameyama determined that surface grammatical functions mattered to centering. This directly influenced the view expressed in Grosz, Joshi, and Weinstein (unpublished manuscript 1986) about the importance of grammatical subjects (Grosz, personal communication), on which the centering algorithm used in Walker, Iida, and Cote (1994) is based.",1995,CL,0.0
An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities,"We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.",1995,CL,0.5
"Book Reviews: The Electronic Word: Democracy, Technology, and the Arts","This collection of essays, most of which have appeared in print before, bears the clear impress of the author's career as a historian of rhetoric who applied his scholarly labors to UCLA's composition program. Lanham, author of The Handlist of Rhetorical Terms (1991), The Motives of Eloquence (1976), and Style: An Anti-Textbook (1974), brings a valuable perspective to questions involving digital technology and culture. In attacking Daniel Boorstin's The Image, or, What Happened to the American Dream (1961) and Neil Postman's Amusing Ourselves to Death: Public Discourse in the Age of Show Business (1985), he points out that their pop ""arguments against electronic technology"" turn out ""to be variations on the traditional arguments developed against rhetoric by Platonic philosophy and Christian theology"" (p. 197), and he goes on to explain that ""the quarrel between the philosophers and rhetoricians constitutes the quarrel in Western culture"" (p. 202). According to Lanham, ""the deepest debates about TV, about the decline of the book, about the computer as Big Brother or little one"" (p. 203) simply repeat this millennia-old debate in which the opponents always speak past each other. Whereas the philosophers assume that all thought (and truth) exist essentially independent of language, media, or information technology, the rhetoricians, who begin with the assumption that the medium colors the message, outrage the philosophers by taking a playful, apparently nonserious, approach to language and learning. The particular relevance of this ancient debate to computing lies in Lanham's claim that ""the bit-mapped, graphics-based personal computer is .. . intrinsically a rhetorical device"" (p. 105). It is so in at least two ways. First, ""in its memory storage and retrieval, in its dynamic interactivity, in the dramatic rehearsal-reality it creates, in the way game and play are built into its motival structure, it expresses the rhetorical tradition just as the codex book embodies the philosophical tradition"" (pp. 105-106). Second, ""classical rhetoric, and hence all of classical education, was built on a single dominant exercise: modeling . . . . Declamatio, as the modeling of speeches came to be called, stood at the hub of Western education, just as computer modeling is coming to do today. The world of electronic text has reinstated this centrality of modeled reality"" (p. 47). Arguing that neither side has the complete answer, The Electronic Word locates the cultural and educational solution in alternating between both poles--though, as we shall see, Lanham does not have nearly enough to say about what such an oscillation might imply for a digital culture.",1995,CL,0.0
Book Reviews: Challenges in Natural Language Processing,"Prognostication is a dangerous business. Doing it in print is even more dangerous. Unless your predictions are luckily, spectacularly right, people in the future will read your milder (but correct) predictions with a yawn and, worse, your failures with amusement. The way to protect your dignity, of course, is not to make real predictions, but merely to discuss trends and to focus on issues that are unlikely to be decided for a long time. That is what happens in this book. A collection of papers from a workshop held at Bolt Beranek and Newman, Inc. (BBN) in Cambridge, Massachusetts, at the end of 1989, the book records the predictions, expectations, and trend analyses of a number of prominent researchers in the areas of computational linguistics, lexicography, phonology, and related areas. As explained in the preface, the purpose of the workshop was to ""discuss the most significant challenges and problems that will face the field of computational linguistics in the next two to ten years."" For those who care about the forces that largely shape developments in our field (i.e., politics and money), it will be evident that a second (if not first) purpose of the workshop was to argue before funding agents from (then) DARPA and (then) RADC that computational linguistics had not come to a relative standstill in the 1980s, but was still going strong, held significant promise, and was going to make great strides over the next two to ten years. That was five years ago. We are in an excellent position to judge the prognostications of the authors. In its overt purpose, the book does not succeed. In one way or another, the authors all show their caution and wisdom in avoiding anything other than general predictions and in sticking to the tried-and-true formula of describing hard problems and hinting at promising directions for solutions. But because the unsolved hard problems discussed in the papers remain unsolved hard problems five years later, they are by and large worth reading. However, the workshop did not fail in its covert purpose, since US funding for natural language processing remained strong over the past five years, a fact for which everyone in the field possibly owes the workshop organizers (the book editors) a debt of gratitude. The book is organized into six parts, each devoted to a theme: challenging problems, the lexicon, semantics and knowledge representation, discourse, speech, and in conclusion, a list of problems for the near future. Part I contains only one paper, by Madeleine Bates, Rusty Bobrow, and Ralph Weischedel (the workshop home team from BBN) and is a survey of written language processing from the BBN perspective. Mostly, the paper provides lists: the major ar-",1995,CL,0.0
Briefly Noted,"Verbmobil is a project sponsored by the German government, whose aim is to develop a translation system for face-to-face speech dialogue. This book is the report on a preliminary study of the project conducted by CSLI, Stanford University. The contents of the book are as follows: Chapter 1 contains introductory material; Chapter 2 and Chapter 3 are devoted to describing the current state-of-the-art of machine translation and speech recognition and speech synthesis technologies, respectively; and Chapter 4 reports a technical recommendation for the Verbmobil project based on the discussions in the preceding chapters. Chapter 2, which would be the most interesting for readers of Computational Linguistics, exhaustively covers various aspects of machine translation. First, the authors argue why machine translation is so difficult. They discuss the indeterminacy of language by referring to a number of examples: situatedness, mismatches between two languages in translation, and ambiguities of various levels. From these considerations, they suggest that good translation is not defined as preserving the meaning but as preserving the intention of the original (p. 27), sometimes by adding or deleting information (p. 26). Then, they review current technologies, historical perspectives, and theoretical issues such as syntax and grammar formalism. The most interesting and important discussion in this chapter is the section ""Translation Strategy."" First, the authors refer to the controversial argument concerning the comparison between the transfer approach and the interlingual approach, but they claim that ""the issue of interest is not whether to pursue a transfer or an interlingual approach; the issue is which levels of analysis are necessary, and how to arrive at a representation suitable for generation of a target text"" (p. 82). From this point, and by considering many examples of translation mismatch that imply the impossibility of a naive interlingual scheme, the authors propose a new architecture called ""translation by negotiation."" In this architecture, some interlingual representation is still supposed, but it does not require that an invariant representation that will be the same for the translation of a source sentence, or text, into any other language can be found at any time. There are three components: an analyzer, a generator, and a negotiator. The analyzer delivers to the negotiator an interlingual representation. The negotiator hands the interlingual representation to the generator. In some (or rare) cases, the generator can find the phrase in the target language that has exactly the same interlingual representation. In other cases, the generator reports to the negotiator error information such as the overspecification or underspecification of the input interlingual representation. Now, it is up to the negotiator to decide whether the error is fatal or not. When it is not so serious, the translation is accepted, but when it is serious, the negotiator has to solve it by some other means, such as referring to the context. In Chapter 2, the authors also discuss other topics: an overview of nonlinguistic translation approaches such as the stochastic approach and the example-based approach, and a comprehensive survey of current machine translation systems. These would also be quite informative to a reader. Chapter 3 is devoted to introducing speech-recognition technology and speechsynthesis technology, and Chapter 4 contains recommendations for the Verbmobil project. These might be less interesting for a reader of Computational Linguistics, so only a brief description is given below. In Chapter 3, general characteristics of speech are given, and current major speechrecognition technologies (the knowledgebased approach, the stochastic-based [HMMbased] approach, and the connectionismbased approach) are introduced, as well as a traditional template-based approach. These descriptions would be a good introduction for a reader who wants to see a general overview of speech-recognition technologies. As for speech synthesis, most pages are given",1995,CL,0.0
A Counterexample to Theorems of Cox and Fine,Cox‚Äôs well-known theorem justifying the use of probability is shown not to hold in finite domains. The counterexample also suggests that Cox‚Äôs assumptions are insufficient to prove the result even in infinite domains. The same counterexample is used to disprove a result of Fine on comparative conditional probability.,1996,AAAI,0.0
Formalizing Narratives Using Nested Circumscription,"The representation of narratives of actions and observations is a current issue in Knowledge Representation, where traditional plan-oriented treatments of action seem to fall short. To address narratives, Pinto and Reiter have extended Situation Calculus axioms, Kowalski and Sergot have introduced the Event Calculus in Logic Programming, and Baral et al. have defined the specification language C which allows to express actual and hypothetical situations in a uniform setting. The L entailment relation can formalize several forms of reasoning about actions and change. In this paper we illustrate a translation of L theories into Nested Abnormality Theories, a novel form of circumscription. The proof of soundness and completeness of the translation is the main technical result of the paper, but attention is also devoted to the features of Nested Abnormality Theories to capture commonsense reasoning in general and to clarify which assumptions a logical formalization forces upon a domain. These results also help clarifying the relationship between L and other recent circumscriptive formalizations for narratives, such as Miller and Shanahan‚Äôs. Content Areas Temporal Reasoning, Nonmonotonic Reasoning, Knowledge Representation.",1996,AAAI,0.5
On the Range of Applicability of Baker‚Äôs Approach to the Frame Problem,"We investigate the range of applicability of Baker‚Äôs approach to the frame problem using an action language. We show that for temporal projection and deterministic domains, Baker‚Äôs approach gives the intuitively expected results. Introduction Baker‚Äôs circumscriptive approach to the frame problem (Baker 1991) h as b een employed in several studies of reasoning about action (for instance, (Lifschitz 1991; Kartha 1993; Shanahan 1995)) because of its simplicity and its ability to deal with domain constraints. However, it has recently been pointed out (Crawford & Etherington 1992; Kartha 1994) that Baker‚Äôs approach may sometimes lead to unintuitive conclusions. This paper addresses the following question: Under what circumstances does Baker‚Äôs approach give intuitively correct conclusions? Of course, the question as phrased above is imprecisewe have not characterized what we mean by the ‚Äúintuitively correct‚Äù conclusions obtainable from the action domain under consideration. To do this, we follow the approach suggested in (Gelfond & Lifschitz 1993) of defining the syntax and semantics of an action language and identifying the ‚Äúintuitively correct‚Äù conclusions with those entailed by the encoding of the action domain in the language. So the question we address becomes: Can we identify classes of domains in an action language that can be faithfully encoded using Baker‚Äôs approach? We answer this question in the affirmative as follows. First we define the syntax and semantics of an action language AR-. Then we identify two classes of domains expressible in this language, present a translation that employs Baker‚Äôs approach and prove that the translation is sound and complete for these classes. As will be seen, these two classes correspond to deterministic and temporal projection action domains. The rest of the paper is organized as follows. We first define the action language dRand present a brief review of Baker‚Äôs approach. We then give a translation from ARand characterize the two classes of domains 664 Knowledge Representation for which the translation is sound and complete. We conclude by discussing related work and the significance of the results presented in this paper. The Language ARIn this section, we define the syntax and semantics of an action language AR. The language ARis a subset of the language d7& introduced in (Kartha & Lifschitz 1994)-the latter has one additional kind of propositions called release propositions. Syntax of ARFormulae and Propositions To be precise, ARis not a single language, but rather a family of languages. A particular language in this group is characterized by e a nonempty set of symbols, that are called Auent names, or Auents, m a subset of fluent names, that is called the frame, o a nonempty set of symbols, that are called action names, or actions. A formula is a propositional combination of fluents. There are three types of propositions in AR--value propositions, effect propositions and constraints. A value proposition is an expression of the form C after A, 0) where C is a formula, and ‚Äò7? is a string of actions. Informally, (1) asserts that C holds after the sequence of actions x is performed in the initial situation. For instance, OnRedBus after BuyTicket; GetOnBoard is a value proposition, where OnRedBus is a fluent and BuyTicket and GetOnBoard are actions. An effect proposition is an expression of the form A causes C if P, (2) where A is an action, and C and P are formulae. Intuitively, (2) asserts that A, if executed in a situation in which the precondition P is true, makes C true. For instance, BuyTicket causes HasTicket if -HasTicket From: AAAI-96 Proceedings. Copyright ¬© 1996, AAAI (www.aaai.org). All rights reserved. is an effect proposition, where BuyTicket is an action, and HasTicket is a fluent. Finally, a constraint is a proposition of the form",1996,AAAI,0.0
Symptom Management for Schizophrenic Agents,"Behavior-based paradigms are a promising avenue towards creating full-blown integrated autonomous agents. However, until now they have had a major stumbling block: programmers can create robust, subtle, and expressive behaviors, but the agent‚Äôs overall behavior gradually falls apart as these behaviors are combined. For small numbers of behaviors, this disintegration can be managed by the programmer, but as more behaviors are combined their interactions become so complex that they become at least time-consuming and at worst impossible to manage.",1996,AAAI,-1.0
On the Size of Reactive Plans,"One of the most widespread approaches to reactive planning is Schoppers‚Äô universal plans. We propose a stricter definition of universal plans which guarantees a weak notion of soundness not present in the original definition. Furthermore, we isolate three different types of completeness which capture different behaviours exhibited by universal plans. We show that universal plans which run in polynomial time and are of polynomial size cannot satisfy even the weakest type of completeness unless the polynomial hierarchy collapses. However, by relaxing either the polynomial time or the polynomial space requirement, the construction of universal plans satisfying the strongest type of completeness becomes trivial.",1996,AAAI,0.5
A Bias towards Relevance: Recognizing Plans where Goal Minimization Fails,"Domains such as multiple trauma management, in which there are multiple interacting goals that change over time, are ones in which plan recognition‚Äôs standard inductive bias towards a single explanatory goal is inappropriate. In this paper we define and argue for an alternative bias based on identifying contextually ‚Äúrelevant‚Äù goals. We support this claim by showing how a complementary planning system in TraumAID 2.0, a decision-support system for the management of multiple trauma, allows us to define a four-level scale of relevance and therefore, of measurable deviations from relevance. This in turn allows definition of a bias towards relevance in the incremental recognition of physician plans by TraumAID‚Äôs critiquing interface, TraumaTIQ.",1996,AAAI,0.0
Nearly Monotonic Problems: A Key to Effective FA/C Distributed Sensor Interpretation?,"The fesractioncslly-Qcczdrrcate, cooperative (FA/C) distributed problem-solving paradigm is one approach for organizing distributed problem solving among homogeneous, cooperating agents. A key assumption of the FA/C model has been that the agents‚Äô local solutions can substitute for the raw data in determining the global solutions. This is not the case in general, however. Does this mean that researchers‚Äô intuitions have been wrong and/or that FA/C problem solving is not likely to be effective ? We suggest that some domains have a characteristic that can account for the success of exchanging mainly local solutions. We call such problems nearly monotonic. This concept is discussed in the context of FA/C-based distributed sensor",1996,AAAI,0.0
Testing the Robustness of the Genetic Algorithm on the Floating Building Block Representation,"Recent studies on a floating building block representation for the genetic algorithm (GA) suggest that there are many advantages to using the floating representation. This paper investigates the behavior of the GA on floating representation problems in response to three different types of pressures: (1) a reduction in the amount of genetic material available to the GA during the problem solving process, (2) functions which have negative-valued building blocks, and (3) randomizing non-coding segments. Results indicate that the GA‚Äôs performance on floating representation problems is very robust. Significant reductions in genetic material (genome length) may be made with relatively small decrease in performance. The GA can effectively solve problems with negative building blocks. Randomizing non-coding segments appears to improve rather than harm GA performance. more fundamental AI problem is how ever more complex organisms can evolve, whether or not they are optimizers.",1996,AAAI,0.6000000000000001
Towards Testing the Syntax of Punctuation,"Little work has been done in NLP on the subject of punctuation, owing mainly to a lack of a good theory on which computational treatments could be based. This paper described early work in progress to try to construct such a theory. Two approaches to finding the syntactic function of punctuation marks are discussed, and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work. Suggestions are made for the use of these results, and for future work. 1 B a c k g r o u n d The field of punctuation has been almost completely ignored within Natural Language Processing, with perhaps the exception of the sentence-final full-stop (period). This is because there is no coherent theory of punctuation on which a computational treatment could be based. As a result, most contemporary systems simply strip out punctuation in input text, and do not put any marks into generated texts. Intuitively, this seems very wrong, since punctuation is such an integral part of many written languages. If text in the real world (a newspaper, for example) were to appear without any punctuation marks, it would appear very stilted, ambiguous or infantile. Therefore it is likely that any computational system that ignores these extra textual cues will suffer a degradation in performance, or at the very least a great restriction in the class of linguistic data it is able to process. Several studies have already shown the potential for using punctuation within NLP. Dale (1991) has * This work was carried out under an award from the (UK) ESRC. Thanks are also due to Lex Holt, Henry Thompson, Ted Briscoe and anonymous reviewers. shown the benefits of using punctuation in the fields of discourse structure and semantics, and Jones (1994) has shown in the field of syntax that using a grammar that includes punctuation yields around two orders of magnitude fewer parses than one which does not. Further work has been carried out in this area, particularly by Briscoe and Carroll (1995), to show more accurately the contribution that usage of punctuation can make to the syntactic analysis of text. The main problem with these studies is that there is little available in terms of a theory of punctuation on which computational treatments could be based, and so they have somewhat ad hoc, idiosyncratic treatments. The only account of punctuation available is that of Nunberg (1990), which although it provides a useful basis for a theory is a little too vague to be used as the basis of any implementation. Therefore it seems necessary to develop a new theory of punctuation, that is suitable for computational implementation. Some work has already been carried out, showing the variety of punctuation marks and their orthographic interaction (Jones, 1995), but this paper describes the continuation of this research to determine the true syntactic function of punctuation marks in text. There are two possible angles to the problem of the syntactic function of punctuation: an observational one, and a theoretical one. Both approaches were adopted, in order to be be able to evaluate the results of each approach against those of the other, and in the hope that the results of both approaches could be combined. Thus the approaches are described one after the other here. 2 C o r p u s b a s e d A p p r o a c h The best data source for observation of grammatical punctuation usage is a large, parsed corpus. It ensures a wide range of real language is covered, and because of its size it should minimise the effect of any",1996,ACL,-0.5
Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach,"In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET.",1996,ACL,1.0
Book Reviews: The Generative Lexicon,"The past decade has seen a tremendous explosion in interest both in practical structures for lexicons and in theories of lexical semantics. Pustejovsky's book represents an interesting attempt at providing a new and powerful lexical semantic theory, taking as its starting point a serious view of the generative nature of lexical compositionality. The heart of the attempt lies in treating word senses in context as the result of complex but well-defined functions of relatively simple and open basic lexical meanings. The author borrows views from traditional linguistics and from philosophy (in some cases, reaching as far back as Aristotle) to find categories that motivate different forms of combination in differing cases, attempting to provide a wide range of rich interpretations from a relatively simple and narrow set of initial constructs. The strength of the work lies in its combination of broad principles for interpretation with very specific details and mechanisms. The book suffers, however, from a number of weaknesses. The first, and most obvious, lies in the dismissive treatment of modern lexical semantics. The author argues over and over that the standard approach to lexical semantics consists of a simple enumeration of meanings, with mechanisms to select among them. This simply is not true, and has not been for a very long time. To cite a single example, the extensive work in the theory of lexical relations, in several of its incarnations, uses far more sophisticated and layered representations. Indeed, in a number of areas (for instance, his discussion of merology and the difficulties of part-whole relations, as well as all his discussions of taxonomic and inheritance issues), it would appear that work within the community that has been concentrating on lexical relations could contribute substantially to this effort. There is, ultimately, no need for every book to acknowledge every other theory going. But when a volume chooses to characterize file current state of the field, it should at least mention all the dominant theories of recent years. At a different level, the early chapters abound with examples of legal/illegal sentence pairs, intended to be motivating. Such pairs are, of course, common throughout the literature. Unfortunately, in this case, a large proportion of the ""illegal"" examples looked fine to me. This may seem like picking an unnecessary nit--and normally, I would reject it as such--but when the contrasts are presented as motivating fundamental parts of a theory of word meaning, it is particularly important that they be convincing, or that, at the least, they be presented with some evidence that some particular population found them convincing. A great many of Pustejovsky's arguments throughout rest on face validity. It is troubling, therefore, that his data do not always seem valid on their face. A separate problem, less to do with content than with style, lies in the volume's uncomfortable relationship with its audience. It is hard to see whom the author had in mind as he wrote. In the early pages, Pustejovsky discusses at tedious length such",1996,CL,-1.0
Assessing Agreement on Classification Tasks: The Kappa Statistic,"Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis.",1996,CL,-1.0
From Conceptual Time to Linguistic Time,"In this paper, we present a method for generating French texts conveying temporal information that integrates Discourse Representation Theory (DRT) and Systemic Grammar Theory. DRT is used to represent temporal information and an intermediate semantic level for the temporal localization expressed by temporal adverbial phrases and verb phrases. This representation is then translated into a syntactic form using Systemic Grammar Theory. We have implemented this method in a working prototype called Prdtexte.",1996,CL,0.30000000000000004
"Book Reviews: Speakers, Listeners, and Communication: Explorations in Discourse Analysis","Gillian Brown's book presents a careful argument for viewing the interpretation of referring expressions and definite expressions by listeners as a satisficing process. That is, listeners attempt to find an interpretation that is adequate for their present purposes, rather than one that is correct with respect to what the speaker might have intended. The author supports her claim with examples taken from a series of carefully controlled experiments that she and her colleagues have conducted over the last 14 years.",1996,CL,0.0
A Probabilistic Approach to Compound Noun Indexing in Korean Texts,In this paper we address the problem of compound noun indexing that is about segmenting or decomposing compound nouns into promising,1996,COLING,-1.0
Spoken-Language Translation Method Using Examples,"A certain recovery method is now under consideration: a re-entrizing model for phoneme candidates by means of searching the correct phonemes using modification depending on recognition error characteristics in an example-based framewbrk [Wakita95]. This approach provides a recovery effect in handling phoneme or syllable sequences, and the effect depends on the particular speakers because of individual error characteristics.",1996,COLING,0.30000000000000004
Inherited Feature-based Similarity Measure Based on Large Semantic Hierarchy and Large Text Corpus,"We describe a similarity calculation model called IFSM (Inherited Feature Similarity Measure) between objects (words/concepts) based on their common and distinctive features. We propose an implementation method for obtaining features based on abstracted triples extracted fi'om a large text eorpus utilizing taxonomical knowledge. This model represents an integration of traditional methods, i.e,. relation b~used sin> itarity measure and distribution based similarity measure. An experiment, using our new concept abstraction method which we <'all the fiat probability grouping method, over 80,000 surface triples, shows that the abstraction level of 3000 is a good basis for feature description.",1996,COLING,0.4
Extracting Word Correspondences from Bilingual Corpora Based on Word Co-occurrence Information,"A new method has been developed for extracting word correspondences from a bilingual corpus. First, the co-occurrence infi~rmation for each word in both languages is extracted li'om the corpus. Then, the correlations between the co-occurrence features of the words are calculated pairwisely with tile assistance of a basic word bilingual dictionary. Finally, the pairs of words with the highest correlations are output selectively. This method is applicable to rather small, unaligned corpora; it can extract correspondences between compound words as well as simple words. An experiment using bilingual patent-specification corpora achieved 28% recall and 76% precision; this demonstrates that the method effectively reduces the cost of bilingual dictionary augmentation.",1996,COLING,1.0
Identifying the Coding System and Language of On-line Documents on the Internet,"This paper proposes a new algorithm that simultaneously identifies the coding system and language of a code string fetched from the Internet, especially World-Wide Web. The algorithm uses statistic language models to select the correctly decoded string as well as to determine the language. The proposed algorithm covers 9 languages and 11 coding systems used in Eastern Asia and Western Europe. Experimental results show that the level of accuracy of our algorithm is over 95% for 640 on-line documents.",1996,COLING,1.0
Learning Bilingual Collocations by Word-Level Sorting,"This paper I)roposes ;t new tnethod for learning bilingual colloca, tions from sentence-aligned paralM corpora. Our method COml)ris('s two steps: (1) extracting llseftll word chunks (n-grmns) by word-level sorting and (2) constructing bilingua,l ('ollocations t)y combining the word-(;hunl(s a(-quired iu stag(' (1). We apply the method to a very ('hallenging text l)~tir: a stock market 1)ullet;in in Japanese and il;s abstract in En-glish. I)om;tin sl)ecific collocations are well captured ewm if they were not conta.ined in the dictionaric's of economic",1996,COLING,1.0
An Automatic Clustering of Articles Using Dictionary Definitions,"In this paper, we propose a statistical approach for clustering of artMes using on-line dictionary definitions. One of the characteristics of our approach is that every sense of word in ar tMes is automatically disambiguated using dictionary definitions. The other is that in order to cope with the problem of a phrasal lexicon, linking which links words with their semantically similar words in articles is introduced in our method. The results of experiments demonstrate the effectiveness of the proposed method.",1996,COLING,1.0
A Constraint-based Case Frame Lexicon,"We present a constraint-based case f lame lexicon arctfitecture fbr bi-directionM mapping between a syntactic case Dame and a semantic Dame. The lexicon uses a semantic sense as the basic unit and employs a multi-tiered constraint structure for the resolution of syntactic information into the appropriate senses and/or idiomatic usage. VMency changing transfbrmations such as morptnologieally marked passivized or causativized forms are handled via le:xical rules that manipulate case Dames templates. The system has been implemented in a typedfeature system and applied to Turkish. 1 I n t r o d u c t i o n -Recent adwmces in theoreticM and practical aspects of feature and constraint-based tbrmMisms for representing linguistic information have fostered research on the use of such formMisms in the design and implementat ion of computat ional lexicons (Briscoe el al., 1993). Case frame approach has been the representation of choice especially for languages with free constituent order, explicit case marking of noun phrases and embedded clauses filling nominal syntactic roles. The semantics of such syntactic role fillers are usually determined by their lexicM, semantic and morplmsyntactic properties, instead of position in the sentence. In this paper, we present an approach to building a constraint-based case Dame lexicon for use in natural language processing in Turkish. A number of observations tha.t we have made on Turkish tmve indicated that we have to go beyond the tradit ional transitive and intransitive distinction, and utilize a Damework where verb valence is considered as the obligatory co-existence of an arbi trary subset of possible arguments along with the obligatory exclusion of certain others, relative to a verb seuse. A d d i t i o n a l morphosyn t~c t i c , lexical and semantic selectional constrMnts are utilized to map a given syntactic argument structure to a specific verb sense. In recent years, there have been several studies on constrmnt-based lexicons. iR,ussell el al. (1993) propose an approach to multiple default inheritance tbr unification-based lexicon. In another study by Lascarides et el. (1995), an ordered approach to default unification is suggested, de Paiva (1993) tbrmalizes the system of well-fornmd typed feature struetures. In this study, type hierarchies and relations are mathematically defined. They also formalize unification and generalization operators between tin(; featm:e structures, along with defining well-formedness notion that we use in our system. 2 Representing Case Frame I n f o r m a t i o n In rlhu'kish, (and possibly in many other languages) verbs often convey several meanings (some totally unrelated) when they are used with subjects, objects, oblique objects, adverbiM adjuncts, with certain lexical, morphological, and semantic features, and co occurrence restrictions. In addition to the usual sense wu:iations due to selectional restrictions on verbal arguments, in most cases, the meaning conveyed by a. case Dante is idiomatic, with subtle constrMnts. For example, the Turkisln verb ye (cat), when used with a direct object noun phrase whose head is: 1. para (money), with no case or possessive markings and a lmman subject, means to accept bribe, 2. pare (money), wittn a non-human subject, means to cost a lol, 3. para (or any other NP whose head is ontologically IS-A money, e.g., dolar, mark, etc.) with obligatory accusative markilig ~md optional possessive meriting, means to spend *~IO~tCy~ 4. kafa (head) with obligatory accusative marking and no possessive marking, means to get mentally deranged, 5. hak (right) with optionM accusative and possessive markings, mearls to be unfair,",1996,COLING,0.4
Linguistic Indeterminacy as a Source of Errors in Tagging,"Most evaluations of part-of-speech tagging compare the utput of an automatic tagger to some established standard, define the differences as tagging errors and try to remedy them by, e.g., more training of the tagger. The present article is based on a manual analysis of a large number of tagging errors. Some clear patterns among the errors can be discerned, and the sources of the errors as well as possible alternative methods of remedy are presented and discussed. In particular are the problems with undecidable",1996,COLING,0.0
Human Language Technology can modernize writing and grammar instruction,"The recently published Survey of the State of the Art in Human Language Technology does not spend a single word on computer-aided language learning (CALL). Indeed, present-day CALL systems hardly employ Natural Language Processing (NLP) techniques. Reasons for this state of affairs are not hard to find. First of all, current language teaching methods tend to emphasize oral language skills, especially in second-language instruction. But automatic recognition of speech, in particular speech by non-natives, has only taken its first steps outside the laboratory; and many language teachers still judge synthesized speech of insufficient quality to serve as a model for language learners. Secondly, modern language pedagogy stresses communicative success rather than formal correctness. This, too, works against the profitable deployment of NLP tools in CALL because automatically generating non-trivial, communicatively interesting and instructive dialogues does not yet seem within reach of NLP technology, let alone the evaluation of student responses from the point of view of successful interpersonal communication. More congenial with these priorities were multimedia innovations. If anything has brought about a metamorphosis in second-language teaching practices, it was the introduction of affordable video, audio and other graphical and acoustic tools that, under the control of flexible software, can create an illusion of 'total immersion'--the supposedly ideal language learning situation. However, language proficiency includes more than conversational skills alone. Equally important are writing skills: orthography, formulating well-formed sentences, composing clear and well-organized texts. It is in the area of written language instruction that NLP technology can find a wealth of extremely valuable applications. Teaching firstand second-language writing skills is very laborintensive because teachers need to mark large numbers of test papers. NLP software holds the potential of alleviating this burden considerably, and even of outperforming teachers in the speed and quality of feedback to learners, and in the capability of generating well-targeted and attractive exercises. A particularly important reason why human language technology should begin to take written hmguage instruction seriously, derives from the following argument. Many instructional scientists subscribe to the view that language skills are best acquired in a situation similar to that of children learning their mother tongne. This explains not only the bias in favor of oral and conversational language skills in current hmguagc pedagogy but also the reluctance to work with explicit grammar rules. The negative attitude toward grammar is strengthened by the generally disappointing outcomes of grammarbased language teaching methods. However, these negative results may have a completely different origin. Although the reasoning that language acquisition can do without explicit rules may hold for oral language proficiency, there is no evidence that it generalizes to the acquisition of written language skills. If writing skills do require the application of explicit orthographic, morphological, syntactic, etc., rules by the learner, then antigrammar attitudes must be detrimental. Learners will be deprived of knowledge that in fact is essential to solving writing problems. More serious is the ensuing lack of interest in the improvement of grammar instruction methods. Taught by age-old methods, many learners only have an inkling of the meaning of important grammatical concepts. Grammar rules referencing these terms are hard to apply successfully in written composition. The response by most linguists and instructional scientists to this state of affairs has been misguided. Instead of initiating research into improved grammar teaching methods, they have tended to play down the importance of grammar rules and linguistic awareness in learning how to write.",1996,COLING,0.0
Error-tolerant Tree Matching,"This paper presents an efficient algor i thm for retrieving from a database of trees, all trees that match a given query tree appro,imately, that is, within a certain error tolerance. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. The algorithm has been implemented on SparcStations, and for large randomly generated synthetic tree databases (some having tens of thousands of trees) it can associatively search [or trees with a small error, in a mat te r of tenths of a second to few seconds. 1 I n t r o d u c t i o n Recent approaches in machine translation known as example-based translation rely on searching a database of previous translations of sentences or fragments of sentences, and composing a translation from the translations of any matching examples (Sato and Nagao, 1!)90; Nirenburg, Beale and l)omasnhev, 1994). The example database may consist, of paired text fragments, or trees as in Sat() and Nagao (1990). Most often, exact matches for new sentences or fragments will not be in the database, and one has to consider exampies that are ""similar"" to the sentence or fragment in question. This involves associatively searching through the database, tbr trees that are ""close"" to the query tree. This paper addresses the computat ional problem o[ retrieving trees that are close to a given query tree in terms of a certain distance metric. The paper first presents the approximate tree matching problem in an abstract setting and presents an algorithm for approximate associative tree matching. The Mgorithm relies on linearizing the trees and then representing the complete database of trees as a t r i e structure which can be efficiently searched. The problem then reduces to sequence correction problem akin to s tandard spelling correction problem. The trie is then used with an approximate finite state recognition algori thm close to a query tree. Following some experimental results from a number of synthetic tree databases, the paper ends with conclusions. 2 A p p r o x i m a t e T r e e M a t c h i n g In this paper we consider the problem of searching in a database of trees, all trees that are ""close"" to a given query tree, where closeness is defined in terms of an error metric. The trees tha t we consider have labeled terminal and non-terminal nodes. We assume that all immediate children of a given node have unique labels, and that a total ordering on these labels is defined. We consider two trees close if we can • add/delete a small number of leaves to / f rom one of the trees, and/or • change the label of a small number of leaves in one of the trees to get the second tree. A pair of such ""close"" trees is depicted in Fignre 1. 2.1 Linearizat ion of trees Before proceeding any fllrther we would like to define the terminology we will be using in the fob lowing sections: We identify each leaf node in a tree with an ordered vertex list (re, vl, v2, . . . , vd) where each vi is the label of a vertex from the root v0 to the leaf Vd at depth d, and :{'or i > 0, vi is the parent of vi+ L. A tree with n leaves is represented by a vertex list sequence. V L S = . V i , V ' e , . . . , 1⁄4 , where each V~. = v3o, v{, v~, v~, . •., va,;, corresponds to a vertex list for a leaf at level dj. This sequence is constructed by taking into account the total order on the labels at every level, that is, 17i is lexico.qraphically less than Vi+l, based on the total ordering of the vertex labels. For instance, the first tree in Fignre 1 would be represented by the vertex list sequence:",1996,COLING,0.5
Extraction of Lexical Translations from Non-Aligned Corpora,"A method for extracting lexical translations from non-aligned corpora is proposed to cope with the unavailability of large aligned corpus. The assumption that ""translations of two co-occurring words in a source language also co-occur in the target language"" is adopted and represented in the stochastic matrix formulation. The translation matrix provides the co-occurring information translated from the source into the target. This translated co-occurring information should resemble that of the original in the target when the ambiguity of the translational relation is resolved. An algorithm to obtain the best translation matrix is introduced. Some experiments were performed to evaluate the effectiveness of the ambiguity resolution and the refinement of the dictionary. 1 I n t r o d u c t i o n Alignment of corpora is now being actively studied to support example-based automatic translation and dictionary refinement. Focusing on the latter, in order to obtain lexical translations, the maximum likelihood method is applied to roughly aligned corpus. One of the problems of this method is that it needs a large amount of aligned corpus for training (Brown, 1993). When it exists, a qualified dictionary is also likely to exist, because it should have been created and used when the corpus in the source language was translated by hand to make the aligned corpus. There are few requirements to improve dictionaries in such a case. On the other hand, when a large amount of aligned corpus does not exist but only two independent corpora do, for example, the corpora between two 'not so international' *Author's current address: Department of Computer Science, Tokyo University of Agriculture and Technology. 2-24-16 Naka-machi, Koganei, Tokyo 184 JAPAN. languages or those in a constrained domain, the low quality dictionaries need to be improved. To make a new dictionary between two uncommon languages, it is often necessary to transform published dictionaries, one between the source and the international language, the other between the international and the target language. The problem in this process is to eliminate the irrelevant translations introduced by words with ambiguous meanings (Tanaka, 1994). This carl be thought of as choosing the translations from several candidates without aligned corpus. Note that adopting aligned corpus of insufficient size cause the same situation. We therefore propose a method to extract lexical translations using two corpora which are not aligned in the source and target language. Our method is proposed as the extension of the framework to solve the problem of choosing the translation according to the context. Thus, one of tile merits of our research is that two problems, looking for the translation according to the global and local context, are handled within the same framework. 2 A s s u m p t i o n a n d A m b i g u i t y",1996,COLING,1.0
Towards a Syntactic Account of Punctuation,"Little notice has been taken of punctuation in the field of natural language processing, chiefly due to the lack of any coherent theory on which to base implementations. Some work has been carried out concerning punctuation and parsing, but much of it seems to have been rather ad-hoc and performance-motivated. This paper describes the first step towards the construction of a theoretically-motivated account of punctuation. Parsed corpora are processed to extract punctuation patterns, which are then checked and generalised to a small set of General Punctuation Rules. Their usage is discussed, and suggestions are made for possible methods of including punctuation information in grammars. 1 I n t r o d u c t i o n Ititherto, the field of punctuation has been almost completely ignored within Natural Language Processing, with perhaps the single exception of the sentence-final full-stop (period). The reason for this non-treatment has been the lack of any coherent theory Of punctuation on which a computational treatment could be based. As a result, most contemporary systems simply strip out punctuation in input text, and do not put any marks into generated texts. Intuitively, this s~ems very wrong, since punctuation is such an integral part of many written languages. If text in the real world (a newspaper, for example) were to appear without any punctuation marks, it would appear very stilted, ambiguous or infantile. Therefore it is likely that any computational system that ignores these extra textual cues will suffer a degradation in performance, or at the very least a great restriction in the class of linguistic data it is able to process. Several studies have already shown the potential for using punctuation within NLP. Dale (1991) has shown the positive benefits of using punctuation ill the fields of discourse structure and semantics, suggesting that it can be used to indicate degrees of rhetorical balance and aggregation between juxtaposed elements, and also that in certain cases a punctuation mark can determine the rhetorical relations that hold between two elements. In the field of syntax Jones (1994) has shown, through a comparison of the performance of a grammar that uses punctuation and one which does not, that for the more complex sentences of real language, parsing with a punctuated grammar yields around two orders of magnitude fewer parses than parsing with an nnpunctuated grammar, and that additionally the punctuated parses better reflect the linguistic structure of the sentences. Briscoe and Carroll (1995) extend this work to show the real contribution that usage of punctuation can make to the syntactic analysis of text. They also point out some fundamental problems of the approach adopted by Jones (1994). If, based on the conclusions of these studies, we are to include punctuation in NLP systems it is necessary to have some theory upon which a treatment can be based. Thus far, the only account available is that of Nunberg (1990), which although it provides a useful basis for a theory is a little too vague to be used as the basis of' any implementation. In addition, the basic implementation of Nunberg's punctuation linguistics seems untenable, certainly on a computational level, since it stipulates that punctuation phenomena should be treated on a seperate level to the lexical words in the sentence (Jones, 1994). It is also the case that Nunberg's t reatment of punctuation is",1996,COLING,0.30000000000000004
Balancing Between Bagging and Bumping,"We compare different methods to combine predictions from neural networks trained on different bootstrap samples of a regression problem. One of these methods, introduced in [6] and which we here call balancing, is based on the analysis of the ensemble generalization error into an ambiguity term and a term incorporating generalization performances of individual networks. We show how to estimate these individual errors from the residuals on validation patterns. Weighting factors for the different networks follow from a quadratic programming problem. On a real-world problem concerning the prediction of sales figures and on the well-known Boston housing data set, balancing clearly outperforms other recently proposed alternatives as bagging [1] and bumping [8]. 1 EARLY STOPPING AND BOOTSTRAPPING Stopped training is a popular strategy to prevent overfitting in neural networks. The complete data set is split up into a training and a validation set . Through learning the weights are adapted in order to minimize the error on the training data. Training is stopped when the error on the validation data starts increasing. The final network depends on the accidental subdivision in training and validation set , and often also on the, usually random, initial weight configuration and chosen minimization procedure. In other words , early stopped neural networks are highly unstable: small changes in the data or different initial conditions can produce large changes in the estimate. As argued in [1 , 8], with unstable estimators it is advisable to resample, i.e ., to apply the same procedure several times using different subdivisions in training and validation set and perhaps starting from different initial RWCP: Real World Computing Partnership; SNN: Foundation for Neural Networks. Balancing Between Bagging and Bumping 467 configurations. In the neural network literature resampling is often referred to as training ensembles of neural networks [3, 6]. In this paper, we will discuss methods for combining the outputs of networks obtained through such a repetitive procedure. First, however, we have to choose how to generate the subdivisions in training and validation sets. Options are, among others, k-fold cross-validation, subsampling and bootstrapping. In this paper we will consider bootstrapping [2] which is based on the idea that the available data set is nothing but a particular realization of some probability distribution. In principle, one would like to do inference on this ""true"" yet unknown probability distribution. A natural thing to do is then to define an empirical distribution. With so-called naive bootstrapping the empirical distribution is a sum of delta peaks on the available data points, each with probability content l/Pdata with Pdata the number of patterns. A bootstrap sample is a collection of Pdata patterns drawn with replacement from this empirical probability distribution. Some of the data points will occur once, some twice and some even more than twice in this bootstrap sample. The bootstrap sample is taken to be the training set, all patterns that do not occur in a particular bootstrap sample constitute the validation set. For large Pdata, the probability that a pattern becomes part of the validation set is (1 l/Pdata)Pda.ta. ~ l/e ~ 0.368. An advantage of bootstrapping over other resampling techniques is that most statistical theory on resampling is nowadays based on the bootstrap. Using naive bootstrapping we generate nrun training and validation sets out of our complete data set of Pdata input-output combinations {iI', tl'}. In this paper we will restrict ourselves to regression problems with, for notational convenience, just one output variable. We keep track of a matrix with components q; indicating whether pattern p is part of the validation set for run i (q; = 1) or of the training set (qf = 0). On each subdivision we train and stop a neural network with one layer of nhidden hidden units. The output or of network i with weight vector w( i) on input il' reads",1996,NIPS,0.0
Limitations of Self-organizing Maps for Vector Quantization and Multidimensional Scaling,"The limitations of using self-organizing maps (SaM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SaM 's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined technique of online K-means clustering plus Sammon mapping of the cluster centroids. SaM are shown to perform significantly worse in terms of quantization error , in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems.",1996,NIPS,0.0
Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs,"In supervised learning there is usually a clear distinction between inputs and outputs inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can. learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing.",1996,NIPS,0.0
Why did TD-Gammon Work?,"Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games. We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro's program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself.",1996,NIPS,0.0
Retrieving Collocations by Co-Occurrences and Word Order Constraints,"In this paper, we describe a method for automatically retrieving collocations from large text corpora. This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations. Through the method, various range of collocations, especially domain specific collocations, are retrieved. The method is practical because it uses plain texts without any information dependent on a language such as lexical knowledge and parts of speech.",1997,ACL,0.9
A DP-based Search Using Monotone Alignments in Statistical Translation,"In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation lnodel, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order Hidden Markov model (HMM) as they are used successfully in speech recognition for the time alignment problem. Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated. The details of the search algorithm are described. Experiments on the EuTrans corpus produced a word error rate of 5.1(/~.. 1 O v e r v i e w : T h e S t a t i s t i c a l A p p r o a c h t o T r a n s l a t i o n The goal is the translation of a text given in some source language into a target language. We are given o J a source ( 'French') string fl = fl . . . fj . . . f . l , which is to be translated into a target ( 'English') string c~ = el...ei...el. Among all possible target strings, we will choose the one with the highest probability which is given by Bayes' decision rule (Brown e t al.. 1993): ,~ = argmax{P,'(e]~lfg~)} = argmax {P, ' (ef) . Pr(.f / lef)} Pr(e{) is the language model of the target language. whereas Pr(j'lale{) is the string translation model. The argmax operation denotes the search problem. In this paper, we address • the problem of introducing structures into the probabilistic dependencies in order to model the string translation probability Pr( f ] [e~). • the search procedure, i.e. an algorithm to perform the argmax operation in an efficient way. • transformation steps for both the source and the target languages in order to improve the translation process. The transformations are very much dependent on the language pair and the specific translation task and are therefore discussed in the context of the task description. We have to keep in mind that in the search procedure both the language and the translation model are applied after the text transformation steps. However, to keep the notation simple we will not make this explicit distinction in the subsequent exposition. The overall architecture of the statistical translation approach is summarized in Figure 1. 2 A l i g m n e n t M o d e l s A key issue in modeling the string translation probability Pr( f ( l e I) is the question of how we define the correspondence between the words of the target sentence and the words of the source sentence. In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (f j ,ei) for a given sentence pair [f( ; el]. We further constrain this model by assigning each source word to exactly one target word. Models describing these types of dependencies are referred to as alignrnen.t models (Brown e t al., 1993), (Dagan e ta] . . 1993). (Kay & R6scheisen, 1993). (Fung & Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotoue HMM based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be a,s follows. We use the symbol Pr(.) to denote general",1997,ACL,0.4
"Three Generative, Lexicalised Models for Statistical Parsing","In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).",1997,ACL,1.0
Learning Features that Predict Cue Usage,"Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations. Previous at tempts to devise rules for text generation were based on intuition or small numbers of constructed examples. We apply a machine learning program, C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 I n t r o d u c t i o n Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Millis, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research & Development Center tComputer Science Department, and Learning Research ~z Development Center tlntelllgent Systems Program cue occurrence and placement and specific rhetorical structures (RSsner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all of the factors thought to contribute to cue usage in all of the text. From their study, Moscr and Moore identified several interesting correlations between particular features and specific aspects of cue usage, and were able to test specific hypotheses from the hterature that were based on constructed examples. In this paper, we focus on cue occurrence and placement, and present an empirical study of the hypotheses provided by previous research, which have never been systematically evaluated with naturally occurring data. Wc use a machine learning program, C4.5 (Quinlan, 1993), on the tagged corpus of Moser and Moore to induce decision trees. The number of coded features and their interactions makes the manual construction of rules that predict cue occurrence and placement an intractable task. Our results largely confirm the suggestions from the hterature, and clarify them by highhghting the most influential features for a particular task. Discourse structure, in terms of both segment structure and levels of embedding, affects cue occurrence the most; intentional relations also play an important role. For cue placement, the most important factors are syntactic structure and segment complexity. The paper is organized as follows. In Section 2 we discuss previous research in more detail. Section 3 provides an overview of Moser and Moore's coding scheme. In Section 4 we present our learning experiments, and in Section 5 we discuss our results and conclude.",1997,ACL,0.7000000000000001
Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication,"Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing. We prove a dual result: CFG parsers running in time O([Gl[w[ 3-e) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O(m3-e/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 I n t r o d u c t i o n The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley's algorithm (Earley, 1970), both of which have a worst-case running time of O(gN 3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley's algorithm which runs in time O(gN3/log N). Valiant's parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is O(gM(N)), where M(rn) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time O(m3). There exist matrix multiplication algorithms with time complexity O(m3-J); for instance, Strassen's has a worstcase running time of O(m 2""sl) (Strassen, 1969), and the fastest currently known has a worst-case running time of O(m 2""376) (Coppersmith and Winograd, 1990). Unfortunately, the constants involved are so large that these fast algorithms (with the possible exception of Strassen's) cannot be used in practice. As matrix multiplication is a very well-studied problem (see Strassen's historical account (Strassen, 1990, section 10)), it is highly unlikely that simple, practical fast matrix multiplication algorithms exist. Since the best BMM algorithms all rely on general matrix multiplication 1, it is widely believed that there are no practical O(m 3-~) BMM algorithms. One might therefore hope to find a way to speed up CFG parsing without relying on matrix multiplication. However, we show in this paper that fast CFG parsing requires fast Boolean matrix multiplication in a precise sense: any parser running in time O(gN 3-e) that represents parse data in a retrieval-efficient way can be converted with little computational overhead into a O(m 3-e/3) BMM algorithm. Since it is very improbable that practical fast matrix multiplication algorithms exist, we thus establish one of the first nontrivial limitations on practical CFG parsing. 1The ""four Russians"" algorithm (Arlazarov et al., 1970), the fastest BMM algorithm that does not simply use ordinary matrix multiplication, has worst-case running time O(mS/log m). Our technique, adapted from that used by Sat ta (1994) for tree-adjoining grammar (TAG) parsing, is to show that BMM can be efficiently reduced to CFG parsing. Satta's result does not apply to CFG parsing, since it explicitly relies on the properties of TAGs that allow them to generate non-context-free languages. 2 D e f i n i t i o n s A Boolean matrix is a matrix with entries from the set {0, 1}. A Boolean matrix multiplication algorithm takes as input two m x m Boolean matrices A and B and returns their Boolean product A x B , which is the m × m Boolean matrix C whose entries c~j are defined by",1997,ACL,0.5
Current theories of centering for pronoun interpretation: a critical evaluation,"A central claim of centering theory (Grosz, Joshi, and Weinstein, 1995 henceforth GJW) is that certain entities ment ioned in an utterance are more central than others, and that this proper ty imposes constraints on a speaker 's use of different types of expressions to refer to them. To articulate some of these constraints, they define several fundamental centering concepts and propose rules based on them that should be followed by a speaker in producing coherent discourse. This work has led to several analyses employing centering theory and extensions of it, particularly in the area of p ronoun interpretation (Kameyama 1986; Brennan, Friedman, and Pollard 1987; Di Eugenio 1990, 1996; Walker, Iida, and Cote 1994; Strube and Hahn 1996, inter alia; see also citations within GJW, forthcoming papers in Walker, Joshi, and Prince in press, and psycholinguistic studies described in Hudson-D 'Zmura 1989, Gordon, Grosz, and Gilliom 1993, and Brennan 1995). 1 In this squib, we discuss some facets of the pronoun interpretation problem that motivate a centering-style analysis, and demonstra te some problems with a popular centering-based approach with respect to these motivations.",1997,CL,0.0
Word Sense Disambiguation Based on Structured Semantic Space,"In this paper, we propose a framework, structured semantic space, as a foundation for word sense disarnbiguation tasks, and present a strategy to identify the correct sense of a word in some context based on the space. The semantic space is a set of multidimensional real-valued vectors, which formally describe the contexts of words. Instead of locating all word senses in the space, we only make use of mono-sense words to outline it. We design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the nodes (sense clusters) corresponding with sets of similar senses in the dendrogram. Given a word in a particular context' the context would activate some clusters in the dendrogram, based on its similarity with the contexts of the words in the clusters, then the correct sense of the word could be determined by comparing its definitions with those of the words in the clusters.",1997,EMNLP,0.30000000000000004
Optimizing Initial Configurations of Neural Networks for the Task of Natural Language Learning,"One approach used to develop computer systems for natuidentifying phrases, e.g.<the boy> is a noun phrase; and (3) ral language processing (NLP) is that of Artificial Neural Netidentifying the relationships among phrases, e.g. <the boy> is works (NNs). Because of the large number of parameters a the agent of <runs>. This is similar to Jain (1991). NN has (e.g. network topology, learning algorithm, transfer The NN has 75 hidden nodes, divided into between 1 and functions) and the way they interact, finding optimal parame30 layers, as determined by the GA. The GA also determines ter values for any particular task can be extremely difficult. how these layers are connected to each other, and which Topology can greatly affect the performance of a NN. learning algorithm and transfer functions to use. Stolcke (1990) found that simple sentences could be proceThe NN is tested on a language of 508 sentences with three ssed by NNs with one hidden layer. Performance degraded, different complexity levels. The basic training set is 20% of however, when the NN was presented with embedded sententhe complete language. The GA makes three major decisions ces, for which more complex topologies were needed. What about training: (1) whether to train with sentences from all topology to use for any particular task is still an open quescomplexity levels at once, or with sentences from the simpler tion. Most decisions regarding NN topology are based on idelevel first and more complex sentences later, (2) whether to as of how the problem should be tackled. Jain (1991) used a train with sentences from all complexity levels in the same NN that first decomposed a sentence into phrases and later proportion, or with more sentences from one level or another; defined relationships between the phrases. Miikkulainen (3) whether the training set should be increased past the basic (1996) divided a NN into a parser, a segmenter, and a stack. 20% . If it is, the NN's fitness is decreased by a factor equal These NN vary in the number of layers, the number of nodes to the increase in the training set. in each layer, and how these layers are connected to each By studying the configurations chosen by the GA, I hope to other. identify which parameters are critical for the NLP task and Another aspect affecting performance is the corpus used to which values for these parameters produce the best perfortrain the NN. Nenov and Dyer (1994) found that training with mance. These results will provide a better understanding of individual words before showing sentences increased perforNN behavior and point to improved NN configurations. mance. Elman (1993) found that training a NN with simple sentences first and complex sentences later produced better results than presenting all sentences in one session. Although researchers have been successful in using NN for NLP, choosing an initial configuration is quite complex. An automated process that can find an optimal set of parameters would be helpful in designing such a system. In my research I use Genetic Algorithms (GAs) to find what NN parameter values produce better performance for a particular NL task. There is one input node for each word in the vocabulary. A sentence is presented to the NN one word at a time by activating the corresponding node at the input layer. The NN incrementally generates a description of the input by correctly identifying the parts of the sentence and how they relate to each other. For example, in the sentence <the boy runs> the NN should respond by (1) identifying each word in the sentence, e.g. <runs> is a movement verb; (2) References",1998,AAAI,0.5
The Branching Factor of Regular Search Spaces,"Many problems, such as the sliding-tile puzzles, generate search trees where different nodes have different numbers of children, in this case depending on the position of the blank. We show how to calculate the asymptotic branching factors of such problems, and how to efficiently compute the exact numbers of nodes at a given depth. This information is important for determining the complexity of various search algorithms on these problems. In addition to the sliding-tile puzzles, we also apply our technique to Rubik‚Äôs Cube. While our techniques are fairly straightforward, the literature is full of incorrect branching factors for these problems, and the errors in several incorrect methods are fairly subtle.",1998,AAAI,-1.0
Experimenting with Power Default Reasoning,"In this paper we explore the computational aspects of Propositional Power Default Reasoning (PDR), a form of non-monotonic reasoning which the underlying logic is Kleene‚Äôs 3-valued propositional logic. PDR leads to a concise meaning of the problem of skeptical entailment which has better complexity characteristics than the usual formalisms (co-NP(3)-Complete instead [[p-Complete). We take advantage of this in an implementation called powder to encode and solve hard graph problems and explore randomly generated instances of skeptical entailment.",1998,AAAI,0.0
Cooperating with People: The Intelligent Classroom,"People frequently complain that it is too difficult to figure out how to get computers to do what they want. However, with a computer system that actually tries to understand what its users are doing, people can interact in ways that are more natural to them. We have been developing a system, the Intelligent Classroom, that does exactly this. The Intelligent Classroom uses cameras and microphones to sense a speaker‚Äôs actions and then infers his intentions from those actions. Finally, it uses these intentions to decide what to do to best cooperate with the speaker. In the Intelligent Classroom, the speaker need not worry about how to operate the Classroom; he may simply go about his lecture and trust the Classroom to assist him at the appropriate moments.",1998,AAAI,0.5
A Script-Based Approach to Modifying Knowledge-Based Systems,"Modifying knowledge-based systems (KBSs) is a complex activity that needs to be performed very often. The occurrence of changes in a KBSs environment, requests for extending the system's functionality, and the debugging of inadequate knowledge are some of the events that demand modi cations to a KBSs. One of the di culties of KBS modi cations is that they might require the modi cation of several related portions of the system. Determining what portions have to be changed and how to change them requires a deep understanding of how the elements of the KBS interact. This requirement is especially hard for users when the rationale behind the design of a KBS or the details of its implementation are unknown.",1998,AAAI,-1.0
Do the Right Thing {\ldots} but Expect the Unexpected,"Dale and Reiter (1995) have recently discussed the nature of referring expression generation, focusing on the case of definite noun phrases. In particular, they consider Gricean approaches, whereby the speaker is supposed to take into account likely inferences by the hearer, in accord with Gricean maxims (Grice 1989), and select the generated NP accordingly, so as to avoid false or misleading inferences (Joshi 1982). They observe that previous accounts (including their own) have attempted to optimiz e the generated noun phrase, making it as brief as possible, within the constraints of accurately distinguishing the intended referent from any other candidate referents. For instance, consider a situation containing three animals: one small white cat and two dogs, one large and black and the other small and white. It is usually assumed that an optimal description of the first dog is either the large dog or the black dog, whereas the large black dog will be suboptimal, since it contains two adjectives where one will do; it is longer than strictly necessary, and suffers from a degree of redundancy (Dale 1992; Reiter 1990). However, Dale and Reiter argue that the previous algorithms proposed for this task are computationally inefficient, and that the task itself must be reconsidered. In particular, they suggest that there is substantial psycholinguistic evidence that people don't generate the shortest, most efficient NPs, and that this behavior is regarded as perfectly natural (see Levelt [1989] for a survey). Hence, generation algorithms need not optimize their descriptions either. Dale and Reiter go further; they state that:",1998,CL,-0.9
Letter to the Editor: Clues from the Depth Hypothesis: A Reply to Geoffrey Sampson's Review,"In linguistics it has not been possible to use the standard criteria and assumptions of science because the ancients placed our discipline not in the physical domain but in the logical domain where concepts and theories do not represent parts of the natural world. Many of the problems facing linguistics follow inevitably, for example the difficulties that linguistics experiences in agreeing on grammatical theory. One symptom is the long-standing difficulty in testing the depth hypothesis, which came out of early MT research. Sampson (1997) attempted recently to test the depth hypothesis by a computer analysis of a grammatically annotated corpus of English. It is shown that this attempted test and his attempt at defending the testability of the dept h hypothesis are invalid. But clues from the depth hypothesis have led to new foundations for general linguistics put forth in the book (Yngve 1996) that Sampson (1998) reviewed. This work reconstitutes linguistics in the physical domain where the criteria and assumptions of science can be applied. Sampson's review of this book contains a number of serious errors and inaccuracies.",1998,CL,-1.0
Book Review: Handbook of Standards and Resources for Spoken Language Systems,"This handbook is different from most books reviewed on these pages in that it is the result of, and a driving force in, a complex process of planned European-style international cooperation. Thus, it should be reviewed not only for content, but also for its role in this process. Let me start by briefly describing the cooperative process. EAGLES (Expert Advisory Group on Language Engineering Standards) is an initiative of the Commission of the European Union, whose purposes include producing specifications and guidelines, and encouraging cooperation between industry and academia, and between European countries. The initiative comprised five working groups, one of which--the Spoken Language Working Group, WG5---is responsible for the handbook under review.",1998,CL,0.0
Introduction to the Special Issue on Natural Language Generation,"There are two sides to natural language processing. On the one hand, work in natural language understanding is concerned with the mapping from some surface representation of linguistic material expressed as speech or text--to an underlying representation of the meaning carried by that surface representation. But there is also the question of how one maps from some underlying representation of meaning into text or speech: this is the domain of natural language generation. Whether our end-goal is the construction of artifacts that use natural languages intelligently, the formal characterization of phenomena in human languages, or the computational modeling of the human language processing mechanism, we cannot ignore the fact that language is both spoken (or written) and heard (or read). Both are equally large and important problems, but the literature contains much less work on natural language generation (NLG) than it does on natural language understanding (NLU). There are many reasons why this might be so, although clearly an important one is that researchers in natural language understanding in some sense start out with a more well-defined task: the input is known, and there is a lot of it around. This is not the case in natural language generation: there, it is the desired output that is known, but the input is an unknown; and while the world is awash with text waiting to be processed, there are fewer instances of what we might consider appropriate inputs for the process of natural language generation. For researchers in the field, this highlights the fundamental question that always has to be asked: What do we generate from? Despite this problem, the natural language generation community is a thriving one, with a research base that has been developing steadily--although perhaps at a slower pace because of the smaller size of the community--for just as long as work in natural language understanding. It should not be forgotten that much of NLP has its origins in the early work on machine translation in the 1950s; and that to carry out machine translation, one has to not only analyze existing texts but also to generate new ones. The early machine translation experiments, however, did not recognize the problems that give modern work in NLG its particular character. The first significant pieces of work in the field appeared during the 1970s; in particular, Goldman's work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and",1998,CL,0.0
Book Reviews: Corpus Linguistics,"The appearance of not one but two introductions to corpus linguistics within the same series shows the maturation and diversification of this fledgling subdiscipline within linguistics. McEnery and Wilson offer an overview or annotated report on work done within the computer-corpus research paradigm, including computational linguistics, whereas Barnbrook offers a guide or manual on the procedures and methodology of corpus linguistics, particularly with regard to machine-readable texts in English and to the type of results thereby generated. Whereas McEnery and Wilson recognize that the distinguishing features of corpus linguistics rest with its computer-aided empiricism, they are eager to line it up alongside cognitive rationalism in an effort to show the complementarity and interdependence of the two. As they argue, the advantages of a corpus-linguistics approach are that it is invariably systematic and rigorous, and that linguistics based on a corpus acts as a yardstick or control to linguistics based on artificial or introspective data. Of these current research paradigms, the authors' discussion offers fair and balanced criticism. In the central core of the book, McEnery and Wilson present overviews of the theory and practice of corpus linguistics, the relative merits of qualitative versus quanti-",1998,CL,0.0
Book Reviews: From Grammar to Science: New Foundations for General Linguistics,"Most readers will recognize what Yngve is complaining about. For many of us who see it as a serious problem, the response has been to become ""corpus linguists."" The implications of the phrase seem a little odd (surely we would not expect to find that geography, say, included a specialism of ""data geography"" alongside a larger band of geographers who based their theories on guesswork or ""intuition""?); but in practice ""corpus linguistics"" functions as a convenient shelter beneath which scientific linguistic research can proceed, undisturbed by the weird and wonderful things happening in areas of linguistics where people make their examples up out of their heads. To Yngve, this is an insufficiently radical response to the empiricism problem. Even corpus linguists routinely depend on concepts such as ""word,"" ""noun,"" ""sentence,"" and ""utterance,"" which have no clear correlates in observable reality. Yngve surveys the origins of grammatical thought in the classical tradition and points out, correctly, that the grammatical categories still used today derive largely from concepts developed in connection with universal, a priori laws of logic rather than with description of contingent, concrete realities. He therefore sees any discourse that uses these concepts as doomed to be unscientific. Yngve's solution is that we should ""abandon . . . logical-domain theories entirely and mov[e] to the physical domain."" We can observe the physical patterns of airwaves transmitted from speakers to hearers, as well as observing people as mobile bodies and the inanimate furniture of the environments in which speech occurs. Yngve outlines an abstract notation suitable to record lawlike relationships among properties of these physical complexes. ""Because this notation can be programmed on a computer it can be used to test large-scale . . . models""; and, because it refers to fundamental phenomena below the level of linguistic controversies, it ""can be freely shared among different linguists . . . . Gone will be the babel of arbitrary grammatical notations, each to be discarded in turn."" Yngve deserves credit for taking seriously the negative implications of these ideas for his own past work. By far Yngve's best-known contribution is his Depth Hypothesis (Yngve 1960, 1961), which identified an asymmetry in the incidence of left-branching and right-branching grammatical structures. In the present book, Yngve discusses this",1998,CL,0.0
Briefly Noted,"This book is a diverse collection of ten presentations given at an International Summer School on Information Extraction in Rome, 1997. The goal of information extraction (IE) is selective, task-driven interpretation of text narrative in order to fill out templates with information about a particular scenario. I was disappointed to find that only five of the articles were actually about IE research. The other half of the articles addressed issues peripheral to IE, such as information retrieval (IR) and text classification. The first two articles are by Yorick Wilks and by Ralph Grishrnan, who are prominent IE researchers in the UK and the US, respectively. Each gives a high-level discussion of IE, its successes and limitations. Wilks makes the observation that IE's strength comes from its modular architecture. Individual modules such as part-of-speech tagging or morphology analysis can be constructed and optimized independently and reused in a variety of applications. He sees the primary limitation of IE to be the template representation that restricts the type of information that can be extracted. Grishman describes the typical architecture of IE systems whose modules include lexical analysis, name recognition, shallow syntactic parsing, task-specific pattern matching, coreference analysis, event merging, and finally template generation. He identifies the main challenges to IE as the cost of adapting a system to a new domain or scenario and a ceiling on performance, which is closely related to the issue of knowledge acquisition and difficulty handling complex syntactic structures. Three other articles deal with more specialized topics within IE. Robert Gaizauskas, Kevin Humphreys, Saliha Azzam, and Yorick Wilks describe a system for multilingual IE with some language-independent modules that are indexed by language-specific lexicons. Roberto Basili and Maria Teresa Pazienza discuss corpus-driven lexical acquisition, in particular for the ""foreground"" lexicon of words that support a particular IE task. Branimir Boguraev and Christopher Kennedy present work in technicalterm recognition and how this can be a step towards document summarization. The remaining articles concern IR, text dassification, or heterogeneous database techniques, and are only tangentially related to IE. Gregory Grefenstette presents an NLPbased strategy for suggesting additional IR query terms to a user. Alan Smeaton gives a tutorial on uses of NLP in IR. Nicola Guarino discusses formal ontologies and how these can enhance IR with semantic matching. Filippo Neri and Lorenza Saitta give a tutorial on machine learning that briefly touches on text classification. Sophie Cluet describes database techniques for querying semi-structured Web pages.--Stephen Soderland, Children's Hospital, Seattle",1998,CL,0.0
Book Reviews: Computational and Conversational Discourse: Burning Issues-An Interdisciplinary Account,"The array of perspectives from which discourse processing is pursued can be mindboggling to students of discourse, both new and experienced. We are bombarded with concepts from various arenas that require sorting out: topic and focus, given and new, cohesion and coherence; the list goes on. Many questions loom for the computational linguistics (CL) researcher. What do notions such as topic and focus and given and new have to say about how discourses are interpreted, as modeled by formal approaches to coherence resolution? More generally, do works from traditionally functional perspectives contain deep insights, obscured by the lack of formal tools necessary to make them concrete, or does the articulate yet complex prose in these works obscure a lack of methodological rigor in their analyses? Is it time to follow other subdisciplines within CL and eschew theoretical work in favor of purely empirical approaches, or would such a move be wildly premature? Computational and Conversational Discourse, an outgrowth of a workshop entitled ""Burning Issues in Discourse"" held in April 1993 in Maratea, Italy, captures a snapshot of current discourse studies at the breadth necessary to begin addressing these questions.",1998,CL,0.0
Book Reviews: Linguistic Concepts and Methods in CSCW,"As computer science continues to study human language and its importance to system development, the field of linguistics will undoubtedly play a necessary and crucial role in ensuring that system design is sensitive to the intricacies and complexity of language in carrying out joint actions. To this end, Linguistic Concepts and Methods in CSCW provides us with perhaps our first detailed study of potential links between linguistics and the emerging field of computer-supported cooperative work (CSCW). Editors John H. Connolly and Lyn Pemberton have geared this collection of 15 chapters to those interested in either linguistics or CSCW, and the book begins with an excellent summary of both fields that could be used as stand-alone reading for any introductory course on linguistics or collaborative computing. Though many chapters of this book contain strong, productive links between the two disciplines, CSCW specialists may find certain chapters lacking in their applicability to the design of collaborative computing systems. As someone fairly well-grounded in CSCW research, I was confused about the focus of the first few chapters, which seemed to conflate CSCW and the general field of computer science. For example, in the second chapter, ""Linguistics and task analysis in computer supported cooperative work,"" Christine Cheepen and James Monaghan offer their analysis of business letters produced by a single author in a London solicitor's office as a predictive model for improving the reliability of speech-driven word processing programs. While this chapter is certainly interesting, speech recognition programs are usually intended for individual users composing texts in isolation, and it is not readily apparent from their discussion what application, if any, their research has to CSCW. In the third chapter, ""Spoken language and speech synthesis in computer supported cooperative work,"" Katherine Morton explores the role that computerized speech synthesis might play in CSCW design. But once again, this topic is never firmly related to how people use computer systems to coordinate joint activities. In her discussion, Morton argues that speech synthesis programs that accurately read textual :information could provide the advantages of speech by conveying pragmatic effect, while also taking advantage of narrow bandwidth requirements for transmission of textual information. Her bandwidth argument has some merits, but it seems dubious to me that synthetic speech generated from electronic text could convey much of the ""human-ness"" that Morton feels natural speech provides to communicative interaction. But as one continues through the book in a linear fashion, the link to CSCW becomes gradually more apparent, as in Stephanie Robertson's chapter, ""The contribution of genre to computer supported cooperative work."" Drawing on Swales's (1986) con-",1998,CL,0.0
Book Review: Vygotsky and Cognitive Science: Language and the Unification of the Social and Computational Mind,"William Frawley's original and wide-ranging new book embraces linguistics, artificial intelligence, philosophy, and psychology in seeking a reconciliation between contemporary cognitive science and Vygotskian psychology. Lev Semenovich Vygotsky (1896--1934) was a Russian psychologist who founded the sociocultural school of psychology, which is based upon the general law of cultural development. This states that ""Every function in the child's cultural development appears twice: first on the social level and later, on the individual level: first, between people (interpsychological), and then inside the child (intrapsychological)"" (Vygotsky 1978, p. 57). The proposed link between the external interpsychological activity and the intrapsychological processes within the mind allows the internalization of the higher mental processes from their social origins. This process of internalization is central to Vygotsky's work and is focused upon within Frawley's book. Internalization is not a simple ""transfer"" or ""copying"" process; the structure and functions of the process change during its internalization and lead to the formation of an ""internal plane of consciousness"" (Leont'ev 1979). The individual's psychological functioning that emerges from this process reflects the nature of the culture from which he or she was derived (Rogoff and Wertsch 1984). The processes of social interaction that are the point of contact between the intraand the interpsychological activity are mediated by sign systems such as language. Vygotsky places particular emphasis upon ""inner speech,"" which is described as ""thinking in pure meanings"" (Vygotsky 1986), as opposed to the embodiment of external speech in words. He describes it as a ""distinct plane of verbal thought,"" the ""next plane"" being thought itself (Vygotsky 1986). This process of inner speech is also of central importance to the arguments presented in Frawley's work. In order to understand the direction of Vygotsky's approach, it is also important to recognize that sociocultural psychology is developmental. Children interact with adults in a society within which a sign system is available and it is through this mediated social interaction that internalization takes place. The emphasis upon the internalization of culture via semiotic mediation suggests that the nature of the relationship between learning and development is particularly interesting. Vygotsky introduced his own approach to this relationship in the shape of the zone of proximal development. This represents the crystallization of the internalization process, with particular reference to the school-aged child, and describes the most fertile interactions that occur between members of an educational culture and a learner. In addition to its influence upon sociocultural psychology, Vygotsky's work is also at the heart of the theoretical framework that has been built around the concept of activity in Soviet psychology. This concept has subsequently been clarified and expanded by Leont'ev (in",1998,CL,0.0
Automatic Word Sense Discrimination,"This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.",1998,CL,0.5
Generalizing Case Frames Using a Thesaurus and the MDL Principle,"A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as ""cuts"" in the thesaurus tree, thus reducing the generalization problem to that of estimating a ""tree cut model"" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.",1998,CL,1.0
Similarity-based Word Sense Disambiguation,"We describe a method for automatic word sense disambiguation using a text corpus and a machinereadable dictionary (MRD). The method is based on word similarity and context similarity measures. Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words. The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD. A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context. Experiments show that this method can learn even from very sparse training data, achieving over 92% correct disambiguation performance.",1998,CL,0.8
Selective Sampling for Example-based Word Sense Disambiguation,"This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system.",1998,CL,1.0
Book Reviews: Text Databases: One Database Model and Several Retrieval Languages,"The development of adequate, sufficiently general models for text-dominated databases is a sticky problem, and one that is far from solved. With the increased use of corpora for their research, computational linguists have been forced in recent years to grapple with the organization of and access to large text databases, usually by developing application-specific, ad hoc systems barely grounded in database theory. The advent of digital libraries and the prospect of distributed research environments has generated more interest in the development of application-independent text database models, but few, if any, directly address the needs of corpus linguistics and natural language processing research. The work Doedens describes in this book is motivated by the need to accommodate the data in the Electronic Concordance Application (ECA) at the Vrije Universiteit Amsterdam. The data comprise the Hebrew Bible, together with annotation for morphology, part of speech, and syntactic structure. Consequently, although the text model and query languages he describes are generalizable to other texts, they were developed to directly address the representation and retrieval needs for linguistic data. The discussion and examples are therefore particularly relevant and accessible to computational linguists. In Chapter 2, Doedens outlines the properties of enriched text--such as hierarchical organization, recursively nested structures, variants and alternatives, and discontinuities--that make traditional database models (e.g., relational models) inadequate for their representation. He also describes the need for user access to different ""views"" of a text (e.g., as physical structure, logical structure, linguistic structure, etc.), a particularly thorny retrieval problem. The discussion could be more detailed, and it could be usefully generalized to other types of richly encoded text beyond those containing linguistic annotation. Nonetheless, this is one of the few places these special properties have been described and considered in developing text database models. The author provides a useful list of ""basic demands"" for text database models, and uses them to evaluate four previously developed models (two of them developed at the University of Waterloo, as an outgrowth of the New OED project). He gives cursory treatment to SGML as a text database model; given its increasingly widespread use, a more thorough discussion especially of its weaknesses--would be valuable. Chapter 3 outlines the Monads dot Feature (MdF) model for text databases, and Chapter 4 describes its implementation in the ECA database. The model itself seems",1998,CL,0.0
Dialogue Management in Vector-Based Call Routing,"This paper describes a domain independent, automatically trained call router which directs customer calls based on their response to an open-ended ""How may I direct your call?"" query. Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques. Based on the statistical discriminating power of the n-gram terms extracted from the caller's request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller's request and the destinations with which it is consistent. Our approach is domain independent and the training process is fully automatic. Evaluations over a financial services call center handling hundreds of activities with dozens of destinations demonstrate a substantial improvement on existing systems by correctly routing 93.8% of the calls after punting 10.2% of the calls to a human operator.",1998,COLING,1.0
Consonant Spreading in Arabic Stems,"This paper examines the phenomenon of consonant spreading in Arabic stems. Each spreading involves a local surface copying of an underlying consonant, and, in certain phonological contexts, spreading alternates productively with consonant lengthening (or gemination). The morphophonemic triggers of spreading lie in the patterns or even in the roots themselves, and the combination of a spreading root and a spreading pat tern causes a consonant to be copied multiple times. The interdigitation of Arabic stems and the realization of consonant spreading are formalized using finite-state morphotactics and variation rules, and this approach has been successfully implemented in a large-scale Arabic morphological analyzer which is available for testing on the Internet. 1 I n t r o d u c t i o n Most formal analyses of Semitic languages, including Arabic, defend the reality of abstract, unpronounceable morphemes called ROOTS, consisting usually of three, but sometimes two or four. consonants called RADICALS. The classic examples include k t b (~. ,D ~)1, appearing in a number of words having to do with writing, books, schools, etc.; and d r s ( ~ 9 z), appearing in words having to do with studying, learning, teaching, etc. Roots combine nonconcatenatively with PATTERNS to form STEMS, a process known informally as INTERDIGITATION o r INTERCALATION. W e shall look first at Arabic stems in general before examining GEMINATION and SPREADING, related phenomena wherein a single underlying radical is real~The Arabic-script examples in this paper were produced using the ArabTeX package for TEX and DTEX by Prof. Dr. Klaus Lagally of the University of Stut tgart . daras duris darn'as duruus diraasa(t) darraas madrasa( t ) madaar is madras iyy tadri is 'study' 'be studied' 'teach' 'lessons' 's tudy' 'eager student ' 'school' 'schools' 'scholastic' ' instruction' verb verb verb n o u n noun n o u n noun n o u n adj-like noun Figure 1: Some stems built on root d r s ized multiple times in a surface string. Semitic morphology, including stem interdigitation and spreading, is adequately and elegantly formalizable using finite-state rules and operations. 1.1 A r a b i c S t e m s The stems in Figure 12 share the d r s root morpheme, and indeed they are traditionally organized under a d r s heading in printed lexicons like the authoritative Dictionary of Modern Written Arabic of Hans Wehr (1979). A root morpheme like d r s interdigitates with a pattern morpheme, or, in some analyses. with a pat tern and a separate vocalization morpheme, to form abstract stems. Because interdigitation involves pat tern elements being inserted between the radicals of the root morpheme, Semitic stem formation is a classic example of non-concatenative morphotactics. Separating and identifying the component morphemes of words is of course the core task of morphological analysis for any language, and analyzing Semitic stems is a classic challenge 2The taa~ marbuu.ta, notated here as ( t ) , is the feminine ending pronounced only in certain environments. Long consonants and long vowels are indicated here with gemination.",1998,COLING,0.30000000000000004
Building Accurate Semantic Taxonomies Monolingual MRDs,"This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dict ionary (MRD). Our aim is to profi t from convent ional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE.",1998,COLING,0.8
Identifying Syntactic Role of Antecedent in Korean Relative Clause Using Corpus and Thesaurus Information,"This paper describes an approach to identifying the syntactic role of an antecedent in a Korean relative clause, which is essential to structural disambiguation and semantic analysis. In a learning phase, linguistic knowledge such as conceptual co-occurrence pat terns and syntactic role distribution of antecedents is extracted from a large-scale corpus. Then, in an application phase, the extracted knowledge is applied in determining the correct syntactic role of an antecedent in relative clauses. Unlike previous research based on co-occurrence pat terns at the lexical level, we represent co-occurrence pat terns with concept types in a thesaurus. In an experiment, the proposed method showed a high accuracy rate of 90.4% in resolving ambiguities of syntactic role determination of an-",1998,COLING,0.8
Word Clustering and Disambiguation Based on Co-occurrence Data,"We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages. We then coinbined this clustering method with the disamI)iguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994).",1998,COLING,1.0
Translating Idioms,"This paper discusses the t reatment of fixed word expressions developed for our ITS-2 FrenchEnglish translation system. This t reatment makes a clear distinction between compounds i.e. mult iword expressions of X¬∞-level in which the chunks are adjacent and idiomatic phrases i.e. mult iword expressions of phrasal categories, where the chunks are not necessarily adjacent. In our system, compounds are handled during the lexical analysis, while idioms are treated in the syntax, where they are t reated as ""specialized lexemes"". Once recognized, an idiom can be transfered according to the specifications of the bilingual dictionary. We will show several cases of transfer to corresponding idioms in the target language, or to simple lexemes. The complete system, including several hundreds of compounds and idioms can be consulted on the Internet (ht tp : / / lat l .unige.ch/i tsweb.html). 1 I n t r o d u c t i o n Multiword expressions (henceforth MWE), are known to constitute a serious problem for natural language processing (NLP) 1. In the case of translation, a proper t reatment of MWE is a fundamental requirement, as few customers would tolerate a literal translation of such common expressions as entrer en vigueur 'to come into effect', met t re en oeuvre 'to implement ' , faire preuve 'to show' or faire connaissance 'to meet '. "" I am grateful to Anne Vandeventer, Christopher Laenzlinger and Thierry Etchegoyhen for helpful comments. Part of the work described in this paper has been supported by a grant from CTI (grant no 2673.1). zCf. Abeill~ & Schabes (1989), Arnold et al. (1995), Laporte (1988), Schenk (1995), Stock (1989), among others. However, a simple glance at some of the current commercial translation systems shows that none of them can be said to handle MWEs in an appropriate fashion. As a mat te r of fact, some of them explicitely warn their users not to use multiword expressions. In this paper, we will first stress some fundamental properties of two classes of MWEs, c o m p o u n d s and i d i o m s , and then present the t reatment of idioms developed for our FrenchEnglish ITS-2 translation system (cf. Ramluckun & Wehrli, 1993). 2 Compounds and idioms A two-way part i t ion of MWEs in (i) compounds and (ii) idioms is both convenient and theoretically well-motivated 2. Compounds are defined as MWEs of X¬∞-level (ie. word level), in which the chunks are adjacent, as exemplified in (1), while ""idiomatic expressions"" correspond to MWEs of phrasal level, where chunks may not be adjacent, and may undergo various syntactic operations, as exemplified in (2-3). (1)a. pomme de terre 'potato ' b. ~ cause de 'because of' c. d~s lors que 'as soon as' The compounds given in (1) function, respectively, as noun, preposition and conjunction. They correspond to a single unit , both syntactically and semantically. In contrast, idiomatic expressions do not generally constitute fixed, closed syntactic units. They do, however, behave as semantic units. For instance the complex syntactic expression casser du sucre sur le dos de quelqu'un, literally break some sugar on ~This distinction between compounds and idioms is also discussed in Wehrli (1997)",1998,COLING,0.4
Long Distance Pronominalisation and Global Focus,"1) Out corpus of descriptive text contains a significant number of long-distance pronominal references (8.4% of the total). In order to account for how these pronouns are interpreted, we re-examine Grosz and Sidner's theory of the attentional state, and in particular the use of the global focus to supplement centering theory. Our corpus evidence concerning these long-distance pronominal references, as well as studies of the use of descriptions, proper names and ambiguous uses of pronouns, lead us to conclude that a discourse focus stack mechanism of the type proposed by Sidner is essential to account for the use of these referring expressions. We suggest revising the Grosz & Sidner fl'amework by allowing for the possibility that an entity in a focus space may have special status.",1998,COLING,0.0
Recognition of the Coherence Relation between Te-linked Clauses,"This paper describes a method for recognizing coherence relations between clauses which are linked by te in Japanese a translational equivalent of English and. We consider that the coherence relations are categories each of which has a prototype structure as well as the relationships among them. By utilizing this organization of the relations, we can infer an appropriate relation from the semantic structures of the clauses between which that relation holds. We carried out an experiment and obtained the correct recognition ratio of 82% for the 280 sentences.",1998,COLING,0.8
Spoken Dialogue Interpretation with the DOP Model,"We show how the DOP model can be used for fast and robust processing of spoken input in a practical spoken dialogue system called OVIS. OVIS, Openbaar Vervoer Informatie Systeem (""Public Transport Information System""), is a Dutch spoken language information system which operates over ordinary telephone lines. The prototype system is the immediate goal of the NWO 1 Priority Programme ""Language and Speech Technology"". In this paper, we extend the original DOP model to context-sensitive interpretation of spoken input. The system we describe uses the OVIS corpus (10,000 trees enriched with compositional semantics) to compute from an input word-graph the best utterance together with its meaning. Dialogue context is taken into account by dividing up the OVIS corpus into context-dependent subcorpora. Each system question triggers a subcorpus by which the user answer is analyzed and interpreted. Our experiments indicate that the context-sensitive DOP model obtains better accuracy than the original model, allowing for fast and robust processing of spoken input.",1998,COLING,1.0
A Stochastic Language Model using Dependency and its Improvement by Word Clustering,"In this paper, we present a stochastic language model for Japanese using dependency. The prediction unit in this model is all attribute of ""bunsetsu"". This is represented by the product of the head of content words and that of function words. The relation between the attributes of ""bunsetsu"" is ruled by a context-free grammar. The word sequences axe predicted from the attribute using word n-gram model. The spell of Unknow word is predicted using character n-grain model. This model is robust in that it can compute the probability of an arbitrary string and is complete in that it models from unknown word to dependency at the same time.",1998,COLING,1.0
On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora.,"This paper addresses the issue of Pos tagger evaluation. Such evaluation is usually performed by comparing the tagger outt)ut with a reference test corpus, which is ~msumed to be error-free. Current ly used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improve,nent given by a new system. The main conclusion is tha t a more rigorous testing experimentation set t ing/designing is needed to reliably evaluate and compare tagger accuracies. 1 Introduct ion and Motivat ion Par t of Speech (pos ) Tagging is a quite well defined NLP l>roblem, which consists of assigning to each word in a text the proper morphosyntaet ic tag for the given context. Although many words are ambiguous regarding their pos , in most cases they can be completely disambigua.ted taking into account an adequate context. Successful taggers have been built using several approaches, such ms statistical techniques, symbolic machine learning techniques, neural networks, etc. The accuracy reported by most current taggers ranges from 96-97% to almost 100~ in the linguistically motivated Constraint Grammar environment. Unfortunately, there have been very few direct comparisons of alternative taggers I on identical test data. However~ in most current papers it is argued that the performance of some taggers is bet ter than others as a result of some kind of indirect comparisons between them. We 1()no of the exceptions is the work by (Sanmelsson and Voutilainen, 1997), in which a very strict compm'ison bctwecn taggers is performed. think that there a.re a number of not enough controlled/considered factors tha t make these conchlsions dubious in most cases. In this direction, the present paper aims to point out some of tile difficulties arising when evaluating and comparing tagger performances against a reference test corpus, and to make. some criticism about common practices followed by the NLP re.searchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Training and test experiments are usually pertbrmed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments arc usually clone on corpora with the same characteristics as the training da ta -~usuMly a small fresh portion of the trMning corpusbut no serious a t t empts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computat ional effort space/ t ime complexi tyare usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple trial experiment with statistical tests of significance. For these reasons, this paper calls for a discussion on pos taggers evaluation, aiming to establish a more rigorous test experimentat ion setting/designing, indispensable to ext ract reliable conclusions. As a start ing point, we will focus only on how the noise in the test corpus can affect the obtained results.",1998,COLING,-1.0
A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis,"We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical-Functional Grammar (LFG). We start by summarizing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures. The resulting LFG-DOP model triggers a new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.",1998,COLING,-0.30000000000000004
An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines,"This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/fi 'om other agents in an efficient manner. From a programmers point of view. PSTFS provides a simple and unified mechanism for building high-level parallel NLP svstems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers. 1 I n t r o d u c t i o n The need for real-time NLP systems has been discussed for the last decade. The difficulty in implementing such a system is that people can not use sophisticated but computationally expensive methodologies. However, if we could provide an efficient tool/environment for developing parallel NLP systems, programmers would have to be less concerned about the issues related to efficiency of the system. This became possible due to recent developments of parallel machines with shared-memory architecture. We propose an efficient programming environment for developing parallel NLP systems on shared-memory parallel machines, called the Parallel Substrate for Typed Feature Structures (PSTFS). The environment is based on agentbased/object-oriented architecture. In other words, a system based on PSTFS has many computational agents running on different processors in parallel; those agents communicate with each other by using messages including TFSs. Tasks of the whole system, such as pars* This research is part ial ly founded by the project of JSPS(JSPS-RFTF96P00502). ] / Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter , 1992). • Efficient t reatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers ' point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describes the performance of PSTFS on our HPSG parsers.",1998,COLING,1.0
Experiments with Learning Parsing Heuristics,"Any large language processing software relies in its operation on heuristic decisions concerning the strategy of processing. These decisions are usually ""hard-wired"" into the software in the form of handcrafted heuristic rules, independent of the nature of the processed texts. We propose an alternative, adaptive approach in which machine learning techniques learn the rules from examples of sentences in each class. We have experimented with a variety of learning techniques on a representative instance of this problem within the realm of parsing. Our approach lead to the discovery of new heuristics that perform significantly better than the current hand-crafted heuristic. We discuss the entire cycle of application of machine learning and suggest a methodology for the use of machine learning as a technique for the adaptive optimisation of language-processing software.",1998,COLING,1.0
Word Sense Disambiguation using Optimised Combinations of Knowledge Sources,"Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which t)erforms word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subjec t /domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample. 1 I n t r o d u c t i o n This paper describes a system that integrates a nmnber of partial sources of information to perform word sense disambiguation (WSD) of content words in general text at a high level of accuracy. The methodology and evaluation of WSD are somewhat different from those of other NLP modules, and one can distinguish three aspects of this difference, all of which come down to evaluation problelns, as does so much in NLP these days. First, researchers are divided between a general inethod (that a t tempts to apply WSD to all the content words of texts, the option taken in this paper) and one that is applied only to a small trial selection of texts words (for example (Schiitze, 1992) (Yarowsky, 1995)). These researchers have obtained very high levels of success, in excess of 95%, close to the figures for other ""solved"" NLP modules, the issue being whether these small word sample methods and techniques will transfer to general WSD over all content words. Others, (eg. (Mahesh et al., 1997) (Harley and Glennon, 1997)) have pursued the general option on the grounds that it is the real task and should be tackled directly, but with rather lower success rates. The division between the approaches probably comes down to no more than the availability of gold s tandard text in sufficient quantities, which is more costly to obtain for WSD than other tasks. In this paper we describe a method we have used for obtaining more test material by transforming one resource into another, an advance we believe is unique and helpful in this impasse. However, there have also been deeper problems about evaluation, which has led sceptics like (Kil~ garriff, 1993) to question the whole WSD enterprise, for example that it is harder for subjects to assign one and only one sense to a word in context (and hence the produce the test material itself) than to perform other NLP related tasks. One of the present authors has discussed Kilgarriff 's figures elsewhere (Wilks, 1997) and argued that they are not, in fact, as gloomy as he suggests. Again, this is probably an area where there is an ""expertise effect"": some subjects can almost certainly make finer, more intersubjective, sense distinctions than others in a reliable way, just as lexicographers do. But there is another, quite different, source of unease about the evaluation base: everyone agrees that new senses appear in corpora that cannot be assigned to any existing dictionary sense, and this is an issue of novelty, not just one of the difi%ulty of discrimination. If that is the case, it tends to undermine the s tandard mark-up-model-andtes t methodology of most recent NLP, since it will not then be possible to mark up sense assignment in advance against a dictionary if new senses are present. We shall not tackle this difficult issue fllrther here, but press on towards experiment. 2 K n o w l e d g e S o u r c e s a n d W o r d S e n s e D i s a m b i g u a t i o n One further issue must be mentioned, because it is unique to WSD as a task and is at tile core of our approach. Unlike other welt-known NLP modules, WSD seems to be implementable by a number of apparent ly different information sources. All the following have been implemented as tile basis of experimental WSD at various times: part-of-speech, semantic preferences, collocating items or classes, thesanral or subject areas, dictionary definitions, synonym lists, among others (such as bilingual equivalents in parallel texts). These phenomena seem",1998,COLING,1.0
A Statistical Analysis of Morphemes in Japanese Terminology,"En este art~culo, esta informado el resultado de an~lisis estad~stico de la dinAmica de los e|ementos constitutivos de t~rminos japoneses. En t~rminos japoneses, la contribuciSn de |os elementos morfol6gicos es diferente segfin los tipos de origen (entre los elementos adoptados de lenguas occidentales y los elementos originMes incluso elementos adoptados de lengua china). Para analizar este punto, un m~todo cuantitativo esta applicado, que puede caracterizar propiamente la din£mica de los datos mofol6gicos de t~rminos en base a las muestras pequefias.",1998,COLING,0.0
Term-list Translation using Mono-lingual Word Co-occurrence Vectors,"A term-list is a list of content words that characterize a consistent text or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most 'coherent' combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations). 1 I n t r o d u c t i o n A list of content words, called a term-list, is widely used as a compact representation of documents in information retrieval and other document processing. Automatic translation of term-lists enables this processing to be cross-linguistic. This paper presents a new method for translating term-lists by using cooccurrence statistics in the target language. Although there is little study on automatic translation of term-lists, related studies are found in the area of target word selection (for content words) in conventional full-text machine translation (MT). Approaches for target word selection can be classified into two types. The first type, which has been adopted in many commercial MT systems, is based on hand assembled disambiguation rules, and/or dictionaries. The problem with this approach is that creating these rules requires much cost and that they are usually domain-dependent 1 The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This r e s e a r c h w a s done when the au thor w a s a t Center for the S tudy of Language and Information(CSLI) , Stanford University. 1In fact, this is par t ly shown by the fact tha t many MT s y s t e m s have subs t i tu tab le domain-dependen t (or ""user"" ) dict ionaries . relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses ""coherence scores"", which can capture associative relations between two words which do not co-occur in the training corpus. This paper is organized as follows, Section 2 describes the overall translation process. Section 3 presents a disambiguation algorithm, which is the core part of our translation Inethod. Section 4 and 5 give experimental results and discussion. 2 T e r m l i s t T r a n s l a t i o n Our term-list translation method consists of two steps called Dictionary Lookup and Disambiguation. 1. Dictionary Lookup: For each word in the given term-list, all the alternative translations are retrieved from a bilingual dictionary. A translation candidate is defined as a combination of one translation for each input word. For example, if the input term-list consists of two words, say Wl and w2, and their translation include Wll for Wl and W2a for w2, then (wu, w~a) is a translation candidate. If wl and w2 have two and three alternatives respectively then there are 6 possible translation candidates.",1998,COLING,1.0
A Flexible Example-Based Parser Based on the SSTC,"In this paper we sketch an approach for Natural Language parsing. Our approach is an example-based approach, which relies mainly on examples that already parsed to their representation structure, and on the knowledge that we can get from these examples the required information to parse a new input s e n t e n c e . In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrhzgs in the sentence and subtrees in the representation tree. In the process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base a bottom up approach. These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure a top down approach.",1998,COLING,0.0
Text Segmentation with Multiple Surface Linguistic Cues,"In general, a certain range of sentences in a text, is widely assumed to form a coherent unit which is called a discourse segment. Identifying the segment boundaries is a first step to recognize the structure of a text. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, though our experiments might be small-scale. We also present a method of training the weights for multiple linguistic cues automatically without the overfitting problem.",1998,COLING,0.8
A Structure-sharing Parser for Lexicalized Grammars,"In wide-coverage lexicalized grammars many of the elementary structures have substructures in common. This means that in conventional parsing algorithms some of the computation associated with different structures is duplicated. In this paper we describe a precompilation technique for such grammars which allows some of this computation to be shared. In our approach the elementary structures of the grammar are transformed into finite state automata which can be merged and minimised using standard algorithms, and then parsed using an automatonbased parser. We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar. 1 I n t r o d u c t i o n It is well-known that fully lexicalised grammar formalisms such as LTAG (Joshi and Schabes, 1991) are difficult to parse with efficiently. Each word in the parser's input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (VijayShanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate the processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vijay-Shanker (1997) addresses this problem from two different perspectives. Evans and Weir (1997) outline a technique for compiling LTAG grammars into automata which are then merged to introduce some sharing of structure. Chen and Vijay-Shanker (1997) use underspecified tree descriptions to represent sets of trees during parsing. The present paper takes the former approach, but extends our previous work by: • showing how merged automata can be minimised, so that they share as much structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; • providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of it 1 . 2 A u t o m a t o n b a s e d p a r s i n g Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; VijayShanker and Weir, 1993) maintain a pa r se table, a set of i t e m s corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for p a r s e r ac t ions . Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or 1However, due to lack of space, no proofs and only minimal informal descriptions are given in this paper.",1998,COLING,0.5
Modeling with Structures in Statistical Machine Translation,"Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based align: ment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and tile structures can be automatically acquired from parallel corpus. This new model achieved over 110% error reduction for our st)oken language translation task.",1998,COLING,1.0
Detecting Verbal Participation in Diathesis Alternations,We present a method for automatically identifying verbal participation in diathesis alternations. Automatically acquired subcategorization frames are compared to a hand-crafted classification for selecting candidate verbs. The minimum description length principle is then used to produce a model and cost for storing the head noun instances from a training corpus at the relevant argument slots. Alternating subcategorization frames are identified where the data from corresponding argument slots in the respective frames can be combined to produce a cheaper model than that produced if the data is encoded separately. I.,1998,COLING,0.0
An Estimate of Referent of Noun Phrases in Japanese Sentences,"In machine translation and man-machine dialogue, it is important to clarify' referents of noun phrases. We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors 1 of noun phrases. Since the Japanese language has no articles, it is difficult to decide whether a noun phrase has an antecedent or not. We had previously estimated the referential properties of noun phrases that correspond to articles by using clue words in the sentences (Murata and Nagao 1993). By using these referential properties, our system determined tile referents of noun phrases ill Japanese se,ltences. Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases. As a result, on training sentences we obtained a precision rate of 82% and a recall rate of 85% in the determination of the referents of noun phrases that have antecedents. On test sentences, we obtained a precision rate of 79% and a recall rate of 77%.",1998,COLING,0.7000000000000001
A Generative Lexicon Perspective for Adjectival Modification,This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon. It highlights the elements which can be borrowed from the GL and develops limitations and extensions. We show how elements of the Qualia structure can be incorporated into semantic composition rules to make explicit the semantics of the combination adjective + noun.,1998,COLING,0.5
Expérimentation en apprentissage d'heuristiques pour l'analyse syntaxique,"Les syst~mes ou programmes de traitement de la langue naturelle doivent prendre des drcisions quant au choix des meilleures stratrgies ou rrgles h appliquer en cours de rrsolution d 'un probl~me particulier. Pour un analyseur syntaxique constitu6 d 'une base de rrgles symboliques, le cas auquel nous nous intrressons ici, ces drcisions peuvent consister h srlectionner les rrgles ou l 'ordonnancement de celles-ci permettant de produire la plus rapide ou la plus prrcise analyse syntaxique pour un 6noncr, un type d ' rnonc6 ou m~me un corpus sprcifique. La complexit6 de telles bases de rrgles grammaticales et leurs subtilitrs computationnelles et linguistiques font en sorte que la prise de ces drcisions constitue un probl~me difficile. Nous nous sommes donc fix6 comme objectif de trouver des techniques qui permettraient d'apprendre des heuristiques performantes de prise de drcision afin de les incorporer ~ un analyseur syntaxique existant. Pour atteindre une telle adaptabilitr, nous avons adopt6 une approche d'apprentissage automatis6 supportre par l'utilisation de syst~mes de classification automatique.",1998,COLING,0.0
A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Texts,"We present an algorithm for bilingual word alignment that extends previous work by treating multi-word candidates on a par with single words, and combining some simple assumptions about the translation process to capture alignments for low frequency words. As most other alignment algorithms it uses cooccurrence statistics as a basis, but differs in the assumptions it makes about the translation process. The algorithm has been implemented in a modular system that allows the user to experiment with different combinations and variants of these assumptions. We give performance results from two evaluations, which compare well with results reported in the literature.",1998,COLING,1.0
Unlimited Vocabulary Grapheme to Phoneme Conversion for Korean TTS,"This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules. The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check. The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes. The phrase-break detector assigns phrase breaks using part-of-speech (POS) information. In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes. Graphemes within a morpheme are grouped into CCV patterns and converted into phonemes by the CCV conversion rules. The phoneme connectivity table supports grammaticality checking of the adjacent two phonetic morphemes. In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the graphemeto-phoneme conversion performance and 97.5% of the sentence conversion performance. The full Korean TTS system is now being implemented using this conversion method.",1998,COLING,1.0
On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora,"This paper addresses the issue of POS tagger evaluation. Such evaluation is usually performed by comparing the tagger output with a reference test corpus, which is assumed to be error-free. Currently used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system. The main conclusion is that a more rigorous testing experimentation setting/designing is needed to reliably evaluate and compare tagger accuracies. 1 I n t r o d u c t i o n and M o t i v a t i o n Part of Speech (POS) Tagging is a quite well defined NLP problem, which consists of assigning to each word in a text the proper morphosyntactic tag for the given context. Although many words are ambiguous regarding their POS, in most cases they can be completely disambiguated taking into account an adequate context. Successful taggers have been built using several approaches, such as statistical techniques, symbolic machine learning techniques, neural networks, etc. The accuracy reported by most current taggers ranges from 96-97% to almost 100% in the linguistically-motivated Constraint Grammar environment. Unfortunately, there have been very few direct comparisons of alternative taggers 1 on identical test data. However, in most current papers it is argued that the performance of some taggers is better than others as a result of some kind of indirect comparisons between them. We I One of the exceptions is the work by (Samuelsson and Voutilainen, 1997), in which a very strict comparison between taggers is performed. think that there are a number of not enough controlled/considered factors that make these conclusions dubious in most cases. In this direction, the present paper aims to point out some of the difficulties arising when evaluating and comparing tagger performances against a reference test corpus, and to make some criticism about common practices followed by the NLP researchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpusbut no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort -space/t ime complexityare usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance. For these reasons, this paper calls for a discussion on POS taggers evaluation, aiming to establish a more rigorous test experimentation setting/designing, indispensable to extract reliable conclusions. As a starting point, we will focus only on how the noise in the test corpus can affect the obtained results.",1998,COLING,-0.5
Machine Aided Error-Correction Environment for Korean Morphological Analysis and Part-of-Speech Tagging,Statistical methods require very large corpus with high quality. But building large and faultless annota ted corpus is a very difficult job. This paper proposes an efficient method to construct part-of-speech tagged corpus. A rulebased error correction method is proposed to find and correct errors semi-automatical ly by user-defined rules. We also make use of user's correction log to reflect feedback. Experiments were carried out to show the efficiency of error correction process of this workbench. The result shows that about 63.2 % of tagging errors can be corrected.,1998,COLING,1.0
Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model,"We present a novel OCR error correction method for languages without word delimiters that have a large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model. By using a statistical OCR model and character shape similarity, the proposed error corrector outperforms the previously published method. When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy.",1998,COLING,1.0
Keyword Extraction using Term-Domain Interdependence for Dictation of Radio News,"In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors shows term-domain interdependence and are used for selecting a suitable domain of each part of radio news.",1998,COLING,0.5
Linguistic Theory in Statistical Language Learning,"This article attempts to determine what elements of linguistic theory are used in statistical language learning, and why the extracted language models look like they do. The study indicates that some linguistic elements, such as the notion of a word, are simply too useful to be ignored. The second most important factor seems to be features inherited from the original task for which the technique was used, for example using hidden Markov models for partof-speech tagging, rather than speech recognition. The two remaining important factors are properties of the runtime processing scheme employing the extracted language model, and the properties of the available corpus resources to which the statistical learning techniques are applied. Deliberate attempts to include linguistic theory seem to end up in a fifth place.",1998,CoNLL,0.4
A Method of Incorporating Bigram Constraints into an LR Table and Its Effectiveness in Natural Language Processing,"In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table. Using a bigram LR table, it is possible for a GLR parser to make use of both big'ram and CFG constraints in natural language processing. Applying bigram LR tables to our GLR method has the following advantages: (1) Language models utilizing bigzam LR tables have lower perplexity than simple bigram language models, since local constraints (higram) and global constraints (CFG) are combined in a single bigram LR table. (2) Bigram constraints are easily acquired from a given corpus. Therefore data sparseness is not likely to arise. (3) Separation of local and global constraints keeps down the number of CFG rules. The first advantage leads to a reduction in complexity, and as the result, better performance in GLR parsing. Our experiments demonstrate the effectiveness of our method.",1998,CoNLL,1.0
Position Paper on Appropriate Audio/Visual Turing Test,"Dr. Hugh Loebner in his 1994 article in Communications [1] makes the statement regarding future LP competitions, ""the winner of the Loebner Grand Prize must develop a computer with associated hardware that can respond 'intelligently' to audio visual input in the manner that a human would .... Turing wrote, 'The question and answer method seems suitable for introducing any one of the fields of human endeavor that we wish to include.' Well, I would like to ask questions about images and pattern recognition. If the computer answers appropriately it is intelligent."" Some have said that requiring competitors to submit their programs to this kind of audio/visual interrogation is going overboard for what AI and robotics technology are prepared to offer [2]. The audio/visual requirement may prevent competitors from achieving the Loebner Grand Prize in the next ten years (yet I doubt it would have to take that long). However, observe how successful the scientists at Carnegie Mellon were able to be with the Navlab automated driving project in ten years [3]. Improved technologies for object and speech recognition (major component parts of the imagined program/hardware) are continuely being developed [4]. Little work has been done to consolidate neural recogition technologies and computational linguistics, to my knowledge [5]. Still, I believe that the most uncharted ground remains with determining sofrware requirements for passing the conventional Turing Test. The 'thinking' element which should allow a TT to be successfully passed is still largely unknown; i.e. the modeling of executive cognitive functions.",1998,CoNLL,0.0
The segmentation problem in morphology learning,"Recently there has been a large literature on various approaches to learning morphology, and the success and cognitive plausibility of different approaches (Rumelhart and McClelland (1986), MacWhinney and Leinbach (1991) arguing for connectionist models, Pinker and Prince (1988), Lachter and Bever (1988), Marcus et al. (1992) arguing against connectionist models, Ling and Marinov (1993), Ling (1994) using ID3/C4.5 decision trees, and Mooney and Califf (1995, 1996) using inductive logic programming/decision lists, among others). However except for a couple of forays into German this literature has been exclusively concerned with the learning of the English past tense. This has not worried some. Ling is happy to describe it as ""a landmark task"". But while the English past tense has some interesting features in its combination of regular rules with semi-productive strong verb patterns, it is in many other respects a very trivial morphological system reflecting the generally vestigal nature of inflectional morphology within modem English.",1998,CoNLL,0.0
An Empirical Approach to Text Categorization Based on Term Weight Learning,"In this paper) we propose a method for text categorizaLion task using term weight learning. In our approach, learning is to learn true keywords from the error of clustering results. Parameters of term weighting are then estimated so as to maximize the true keywords and minimize the other words in the text. The characteristic of our approach is that the degree of context dependency is used in order to judge whether a word in a text is a true keyvv·ord or not. The experiments using Wall Street Journal corpus demonstrate the effectiveness of the method.",1998,EMNLP,0.5
Aligning Clattses in Parallel Texts,"This paper describes a method for the automatic alignment of parallel texts at clause level. The method features statistical techniques coupled with shallow linguistic processing. It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus. Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories. Regular grammars functioning on tags, recognize clauses on both sides of the parallel text. A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths. Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment. 1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.",1998,EMNLP,1.0
Utilizing lime: Asynchronous Binding,"Historically, connectionist systems have not excelled at representing and manipulating complex structures. How can a system composed of simple neuron-like computing elements encode complex relations? Recently, researchers have begun to appreciate that representations can extend in both time and space. Many researchers have proposed that the synchronous firing of units can encode complex representations. I identify the limitations of this approach and present an asynchronous model of binding that effectively represents complex structures. The asynchronous model extends the synchronous approach. I argue that our cognitive architecture utilizes a similar mechanism.",1998,NIPS,0.2
Finding Parts in Very Large Corpora,"We present a method for extracting parts of objects from wholes (e.g. ""speedometer"" from ""car""). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.",1999,ACL,0.6000000000000001
A Decision-Based Approach to Rhetorical Parsing,"We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.",1999,ACL,1.0
Mixed Language Query Disambiguation,"We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only. A mixed language query consists of words in a primary language and a secondary language. Our method translates the query into monolingual queries in either language. Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor. Average query translation accuracy for the two features are 81.37% and 83.72%, compared to the baseline accuracy",1999,ACL,1.0
Lexical Semantics to Disambiguate Polysemous Phenomena of Japanese Adnominal Constituents,"We exploit and extend the Generative Lexicon Theory to develop a formal description of adnominal constituents in a lexicon which can deal with linguistic phenomena found in Japanese adnominal constituents. We classify the problematic behavior into ""static disambiguation"" and ""dynamic disambiguation"" tasks. Static disambiguation can be done using lexical information in a dictionary, whereas dynamic disambiguation requires inferences at the knowledge representation level. 1 I n t r o d u c t i o n Natural language processing must disambiguate polysemous constituents in the input sentences. A good description of information necessary for disambiguation in the lexicon is crucial in high quality NLP systems. This paper discusses the treatment of linguistic phenomena in Japanese adnominM constituents and it focuses on how to generate the same semantic representation from different syntactic structures, and how to generate different semantic representations from a semantically ambiguous sentence. We exploit and extend the Generative Lexicon Theory (Pustejovsky, 1995; Bouillon, 1996) to develop a formal description of adnominal constituents in a lexicon which can offer a solution to these problems. We classify the problematic behavior of Japanese adnominal constituents into ""static disambiguation"" and ""dynamic disambiguation"" tasks. Whereas static disambiguation can be done using the lexical information in a dictionary, dynamic disambiguation needs inferences at the knowledge representation level. This paper mainly discusses dynamic disambiguation. 2 C l a s s i f i c a t i o n o f t h e U s a g e o f J a p a n e s e A d n o m i n a l C o n s t i t u e n t s On consideration of the syntactic relations between adnominal constituents and their head nouns, we find that some adnominal constituents can appear both in the attributive and predicative positions (Sakuma, 1967; Martin, 1975; Makino and Tsutsui, 1986). However, some adjectives express different meanings when they appear in one or the other position and some adjectives can appear only in one of these two positions (Hashimoto and Aoyama, 1992). We have classified the semantic relations between adnominal constituents and their modified nouns, based on whether the paraphrasing from attributive position to predicative position is possible or not. There are three possibilities: (Type A ) A paraphrase can be made without changing the modifying relations semantically. Ad. + N , N $~ (ga) Ad. (N is Ad.) Ad. = Adnominal constituent N = Head noun of noun phrase which is modified by Ad. (Type B) A paraphrase can be made only when a noun is restricted by its context: the presence of modifiers or determiners, e.g., articles. A d . + N --* ~:¬¢)(sono) N F~ (wa) Ad. (that N is Ad.) (Type C) A paraphrase cannot be made at all, i.e., only the attributive position is available.",1999,ACL,0.5
Inducing a Semantically Annotated Lexicon via EM-Based Clustering,"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",1999,ACL,0.4
Learning to Recognize Tables in Free Text,"Many real-world texts contain tables. In order to process these texts correctly and extract the information contained within the tables, it is important to identify the presence and structure of tables. In this paper, we present a new approach that learns to recognize tables in free text, including the boundary, rows and columns of tables. When tested on Wall Street Journal news documents, our learning approach outperforms a deterministic table recognition algorithm that identifies tables based on a fixed set of conditions. Our learning approach is also more flexible and easily adaptable to texts in different domains with different table characteristics.",1999,ACL,1.0
A Syntactic Framework for Speech Repairs and Other Disruptions,"This paper presents a grammatical and processing framework for handling the repairs, hesitations, and other interruptions in natural human dialog. The proposed framework has proved adequate for a collection of human-human task-oriented dialogs, both in a full manual examination of the corpus, and in tests with a parser capable of parsing some of that corpus. This parser can also correct a pre-parser speech repair identifier resulting in a 4.8% increase in recall.",1999,ACL,1.0
Conceptions of limited attention and discourse focus,"Walker (1996) presents a cache model of the operation of attention in the processing of discourse as an alternative to the focus space stack that was proposed previously by Grosz and Sidner (Grosz 1977a; Grosz and Sidner 1986). In this squib, we present a critical analysis of the cache model and of Walker's supporting evidence from anaphora in discourses with interruptions and from informationally redundant utterances. We argue that the cache model is underdetermined in several ways that are crucial to a comparison of the two models and conclude that Walker has not established the superiority of the cache model. We also argue that psycholinguistic evidence does not support the cache model over the focus stack model.",1999,CL,-1.0
Letter to the Editor: Grammatical Depth: A Rejoinder,"I was a little startled to find Victor Yngve (1998) using my review of his book From Grammar to Science as a peg on which to hang a sustained critique of my article ""Depth in English grammar"" (Sampson 1997), an item that was mentioned only tangentially in the book review. My 1997 article tested Yngve's famous depth hypothesis of forty years ago against data of a kind richer than was available then. It showed that the hypothesis as formulated by Yngve was not precisely the correct generalization about English, but it also showed that there does exist an invariant quantitative property of English syntactic structures that is closely related to Yngve's hypothesis. Yngve regards this demonstration as empty, on the grounds that there are different schemes for representing English grammatical structure, and hence my finding could have been merely the result of an arbitrary choice of analyses. ""The fact that [Sampson] could not have anticipated the result he found does not validate the work as empirical scientific research; any nonsensical result would be equally unanticipated"" (Yngve 1998, p. 635). Of course there are alternative schemes of grammatical analysis. My article drew attention to this, and surmised that the finding might be robust with respect to choice of analytic scheme. (This surmise could prove mistaken, but Yngve does nothing to suggest that it was in fact mistaken.) The point that seems to be lost on Yngve, though, is that there was no a priori reason to expect the data to yield any result as specific and precise as the result that emerged. It was a surprise to me to find, long after publication of the SUSANNE Corpus, that its structures displayed a quantitative property whose distribution possesses so low a standard deviation round its mean as does the property of ""raw production-based sentence depth"" defined in Sampson (1997). Yngve seems to imagine that the team responsible for the SUSANNE Corpus brought about this outcome by making arbitrary analytic decisions. Protestations about our research ethics would be redundant here; we would not have known how to cook the books that way even if we wanted to. No member of the team that produced the SUSANNE Corpus was aware of the measure that later turned out to be highly invariant. (The team had scattered by the time I engaged in the research reported in Sampson [1997], so most members are very likely unaware of it even now.) If we had been conscious of it, I cannot imagine how we could have gone about forcing our analyses to conform to the quantitative invariant, while achieving consistency with published analytic guidelines defined in great detail and in entirely nonquantitative terms. All that effort, just to manufacture a basis for one 20-page journal article? I don't think so. The invariant was there in the language samples; we didn't put it there. On the wider issue, whether empirical science can--never mind should--be",1999,CL,0.0
Briefly Noted,"It may seem unusual for the author of a directly competing textbook (Covington 1994) to review this one---but in fact the competition is not head-on. My book introduces Prolog programmers to natural language, whereas this book introduces linguists to Prolog programming. As such, it helps solve the problem that there is no easy way for noncomputational linguists to get started in computing, and I may well use it in a course. The book covers most of Prolog but only a small and central part of computational linguistics, namely parsing. The first few chapters are a conventional introduction to Prolog except that the examples are chosen to be of interest to linguists (e.g., databases listing what language is spoken where) and little background in formal logic is presumed. Knowledge representation is introduced slowly to keep students from getting lost. The author demonstrates a sureness of touch derived, no doubt, from classroom experience. Compared to another competitor (Dougherty 1994), Matthews focuses more on Prolog and on parsing in general rather than a specific linguistic theory. The only section that may go awry in the classroom is that on structured objects (p. 61 ff.), where Prolog terms like language(uk) and queen(england) are described as involving ""functions."" Students are likely to think they are functions that return values--that queen(england) evaluates to something identifying the Queen of England. It does not; it's just a data structure with queen in the functor position and england in the argument position. I find that I have to emphasize this in the classroom. Still, this is only a small part of an otherwise fine presentation, and it may be that Matthews can steer his students clear of this misunderstanding. The latter part of the book introduces transition networks, DCGs, and (briefly) leftcomer and chart parsing. The presentation is clear but, of course, is not a complete course in computational linguistics. (Neither is my book or Dougherty's.) But it is enough to get students started, and it is probably as much as those without previous programming experience can be expected to absorb in a single course.--Michael A. Covington, University of Georgia",1999,CL,0.7000000000000001
Learning Transformation Rules to Find Grammatical Relations,"Grammatical relationships are an important level of natural language processing. We present a trainable approach to find these relationships through transformation sequences and-error-driven learning. Our approach finds grammatical relationships between core syntax groups and bypasses much of the parsing phase. On our training and test set, our procedure achieves 63.6% recall and 77.3% precision (f-score = 69.8).",1999,CoNLL,0.8
HMM Specialization with Selective Lexicalization,"We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. 'Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We perfor'med a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",1999,EMNLP,1.0
Automatically Merging Lexicons that have Incompatible Part-of-Speech Categories,"We present a new method to automatically merge lexicons that employ different incompatible POS categories. Such incompatibilities have hindered efforts to combine lexicons to maximize coverage with reasonable human effort. Given an ""original lexicon"", our method is able to merge lexemes from an ""additional lexicon"" into the original lexicon, converting lexemes from the additional lexicon with about 89% precision. This level of precision is achieved with the aid of a device we introduce called an anti-lexicon, which neatly summarizes all the essential information we need about the co-occurrence of tags and lemmas. Our model is intuitive, fast, easy to implement, and does not require heavy computational resources nor training corpus. l e m m a I tag apple INN boy NN calculate VB Example entries in Brill lexicon",1999,EMNLP,1.0
Rules and Similarity in Concept Learning,"This paper argues that two apparently distinct modes of generalizing concepts abstracting rules and computing similarity to exemplars should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes which rules are abstracted, how similarity is measured as well as why generalization should appear ruleor similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.",1999,NIPS,0.0
Acquisition in Autoshaping,"Quantitative data on the speed with which animals acquire behavioral responses during classical conditioning experiments should provide strong constraints on models of learning. However, most models have simply ignored these data; the few that have attempted to address them have failed by at least an order of magnitude. We discuss key data on the speed of acquisition, and show how to account for them using a statistically sound model of learning, in which differential reliabilities of stimuli playa crucial role.",1999,NIPS,0.2
Anchoring Symbols to Sensor Data: Preliminary Report,"Anchoring is the process of creating and maintaining the correspondence between symbols and percepts that refer to the same physical objects. Although this process must necessarily be present in any physically embedded system that includes a symbolic component (e.g., an autonomous robot), no systematic study of anchoring as a problem per se has been reported in the literature on intelligent systems. In this paper, we propose a domain-independent definition of the anchoring problem, and identify its three basic functionalities: find, reacquire, and track. We illustrate our definition on two systems operating in two different domains: an unmanned airborne vehicle for traffic surveillance; and a mobile robot for office navigation.",2000,AAAI,0.5
GeoRep: A Flexible Tool for Spatial Representation of Line Drawings,"A key problem in diagrammatic reasoning is understanding how people reason about qualitative relationships in diagrams. We claim that progress in diagrammatic reasoning is slowed by two problems: (1) researchers tend to start from scratch, creating new spatial reasoners for each new problem area, and (2) constraints from human visual processing are rarely considered. To address these problems, we created GeoRep, a spatial reasoning engine that generates qualitative spatial descriptions from line drawings. GeoRep has been successfully used in several research projects, including cognitive simulation studies of human vision. In this paper, we outline GeoRep‚Äôs architecture, explain the domainindependent and domain-specific aspects of its processing, and motivate the representations it produces. We then survey how GeoRep has been used in three different projects‚Äìa model of symmetry, a model of understanding juxtaposition diagrams of physical situations, and a system for reasoning about military courses of action. Introduction: How Diagrams Work Diagrams are ubiquitous. In daily communications, through sketches, maps, and figures, people use diagrams to convey information. Some diagrams depict intrinsically spatial domains, such as bus routes or furniture arrangements. Other diagrams use spatial concepts to compactly show more abstract relations, such as corporate hierarchies or data flow in a computer program. In all such domains, diagrams can be extremely effective. It is also true, however, that there is a keen difference between effective and ineffective diagrams. Small visual differences may distinguish a diagram that elucidates from one that confuses (Tufte, 1990). A key difference between good and bad diagrams is how well they utilize the kinds of qualitative spatial relations most easily perceived by the human visual system. In the best diagrams, these spatial relations support the conceptual relations the reader is meant to infer. For example, in a thermodynamics diagram, an arrow may indicate the direction of heat flow, with thicker arrows to indicate greater flow, or tapering arrows to indicate heat dissipation. Or, in a circuit diagram, wires Copyright ¬© 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. may be drawn so that related wires are adjacent and parallel, so they can be visually grouped. For this reason, to understand how diagrams work, we must show how diagrams use visual characteristics to support particular qualitative inferences. In the system described here, we model this process as an interaction between two representation levels: 1. A low-level, domain-independent representation which involves a representative set of primitive spatial relations. This level models human low-level vision. 2. A high-level, domain-specific representation that models visual skills for a particular domain. This level links lowlevel visual relations to a domain‚Äôs conceptual content. These two representation levels form the basis of GeoRep. GeoRep is an engine for building diagrammatic reasoners. GeoRep takes as input a line drawing, given as a set of primitive visual elements. From this drawing, GeoRep creates a predicate calculus representation of the drawing's visual relations. To perform this task, GeoRep, given the drawing, examines the primitive shapes in the figure, looking for a broad set of low-level visual relations. These relations are detected by a library of visual operations (assumed to be domain-independent) which partially cover the set of universal visual routines (Ullman, 1984). Next, GeoRep uses these relations, in combination with domain-dependent rules, to generate the second, domain-specific representation. GeoRep's two-level",2000,AAAI,0.4
Combining Classification and Temporal Learning,"This introduces TRACA (Temporal Reinforcement-learning and Classification Architecture), a connectionist learning system for solving problems in large state spaces. These types of problems, such as robot control, commonly include the presence of irrelevant attributes and hidden-state. TRACA is capable of dealing with both irrelevant information and hidden-state while addressing two common shortcomings of other learning systems. The first shortcoming is requiring a large number of training examples which is unrealistic for learning in the real world. The second is having to pre-determine or constrain network structure and size.",2000,AAAI,0.4
Solving Advanced Reasoning Tasks Using Quantified Boolean Formulas,"We consider the compilation of different reasoning tasks into the evaluation problem of quantified boolean formulas (QBFs) as an approach to develop prototype reasoning systems useful for, e.g., experimental purposes. Such a method is a natural generalization of a similar technique applied to NP-problems and has been recently proposed by other researchers. More specifically, we present translations of several well-known reasoning tasks from the area of nonmonotonic reasoning into QBFs, and compare their implementation in the prototype system QUIP with established NMRprovers. The results show reasonable performance, and document that the QBF approach is an attractive tool for rapid prototyping of experimental knowledge-representation systems.",2000,AAAI,0.5
Interfacing Issues for Information Extraction,"Traditional approaches to information extraction implicitly assume that many elements of the task are static ‚Äî the user‚Äôs query, and the description of domain and corpus, for example. We believe that in many real situations, however, this assumption does not hold and it is important to consider how the system could best support interaction with the user when the assumption breaks down. Current goals in the information extraction community are for the system to produce accurate results while being easy to retrain and port to a new domain. We seek to extend current approaches to handle dynamic elements of the problem. ‚ÄúEvolving queries‚Äù, discussed in the information retrieval (IR) literature, need to be supported by information extraction (IE) systems; IR and IE are both, after all, tools for gathering information from documents in response to a user query. When a casual user ‚Äî neither an expert in the use of the system, nor in the domain ‚Äî engages in any information gathering task, there will be an initial phase of investigation and discovery during which the user becomes familiar with the system, the domain, and the documents in the corpus and the user‚Äôs query may change over time or evolve. For example, a user may have a query about terrorist activities, asking for the names of perpetrators and the locations of targets; an interim system output prompts the user to refine the query, redefining terrorist activities as involving only a subset of weapons while generalizing to allow for additional (e.g., government) perpetrators. The query is not the only element that may change over time; certainly the domain evolves as additional documents are processed. As well, when the corpus is very large or dynamic (e.g., the Internet), the corpus itself may be seen as evolving ‚Äî rules for mapping text patterns to query items that apply at one time or for one portion of the corpus no longer apply for another. To provide more robust support for information extraction in a dynamic environment, we consider such issues as:",2000,AAAI,0.0
"Knowledge Representation on the Internet: Achieving Interoperability in a Dynamic, Distributed Environment","The Internet‚Äôs explosive growth is making it harder and harder to harness its potential. There is so much information available that users are frequently overwhelmed by information overload. Due to limitations in modern natural language processing, an important part of search involves keyword-based techniques, which tend to have poor precision and recall. Some systems use the format of a web page to extract information, but due to the changing nature of these pages, such systems are very fragile. It has been claimed that the Extensible Markup Language (XML) will solve these problems by replacing the presentation-oriented tags of HTML with contentspecific tags. While it is true that XML will be useful for data exchange and separating content from format, once XML is in widespread usage there will be significant interoperability problems. Unless all content providers agree on the same set of tags and the meanings of these tags it will be impossible to automatically integrate their information. The field of knowledge representation has studied techniques for storing, modifying and reasoning with complex information. A growing subfield of KR is the study of ontologies, which are reusable knowledge components. Recent research has shown that semantically marking up web pages using terms from an explicit ontology can greatly improve retrieval, integrate the data of many pages, and enable intelligent internet-based agents as well. However two characteristics of the Internet provide significant challenges for an ontology approach: it is ever changing and it is decentralized. Thus, it must be possible to adapt ontologies to meet existing needs in a timely fashion, but such changes must not have an adverse impact on the objects that depend on the ontology (i.e., those ontologies and web pages that use the ontology to define their terms). My work with SHOE, which stands for Simple HTML Ontology Extensions, has given me a lot of experience with the design and application of semantic markup languages for the Web. SHOE distinguishes between two types of web pages: ontologies and instances. A SHOE ontology describes a domain by defining categories, relations, inference rules, and other elements. Each",2000,AAAI,-0.30000000000000004
Using Existing Systems to Supplement Small Amounts of Annotated Grammatical Relations Training Data,"Grammatical relationships (GRs) form an important level of natural language processing, but di erent sets of GRs are useful for di erent purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. To boost the performance from using such a small training corpus on a transformation rule learner, we use existing systems that nd related types of annotations.",2000,ACL,0.0
The Order of Prenominal Adjectives in Natural Language Generation,The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe constraints which straddle the boundary between competence and performance. This paper describes and compares a number of statistical and machine learning techniques for ordering sequences of adjectives in the context of a natural language generation system.,2000,ACL,0.0
Panel: Computational Linguistics in India: An Overview,"In the anusaaraka systems, the load between the human reader and the machine is divided as follows: language-based analysis of the text is carried out by the machine, and knowledge-based analysis or interpretation is left to the reader. The machine uses a dictionary and grammar rules, to produce the output. Most importantly, it does not use world knowledge to interpret (or disambiguate), as it is an error prone task and involves guessing or inferring based on knowledge other than the text. Anusaaraka aims for perfect ""information preservation"". We relax the requirement that the output be grammatical. In fact, anusaaraka output follows the grammar of the source language (where the grammar rules differ, and cannot be applied with 100 percent confidence). This requires that the reader undergo a short training to read and understand the output.",2000,ACL,0.0
The Role of Centering Theory's Rough-Shift in the Teaching and Evaluation of Writing Skills,"Existing software systems for automated essay scoring can provide NLP researchers with opportunities to test certain theoretical hypotheses, including some derived from Centering Theory. In this study we employ ETS's e-rater essay scoring system to examine whether local discourse coherence, as de ned by a measure of Rough-Shift transitions, might be a signi cant contributor to the evaluation of essays. Our positive results indicate that Rough-Shifts do indeed capture a source of incoherence, one that has not been closely examined in the Centering literature. These results not only justify Rough-Shifts as a valid transition type, but they also support the original formulation of Centering as a measure of discourse continuity even in pronominal-free text.",2000,ACL,0.0
Book Reviews: Advances in Automatic Text Summarization,"It has been said for decades (if not centuries) that more and more information is becoming available and that tools are needed to handle it. Only recently, however, does it seem that a sufficient quantity of this information is electronically available to produce a widespread need for automatic summarization. Consequently, this research area has enjoyed a resurgence of interest in the past few years, as illustrated by a 1997 ACL Workshop, a 1998 AAAI Spring Symposium and in the same year SUMMAC: a TREC-like TIPSTER-funded summarization evaluation conference. Not unexpectedly, there is now a book to add to this list: Advances in Automatic Summarization, a collection of papers edited by Inderjeet Mani and Mark T. Maybury and published by The MIT Press. Half of it is a historical record: thirteen previously published papers, including classics such as Luhn's 1958 word-counting sentence-extraction paper, Edmundson's 1969 use of cue words and phrases, and Kupiec, Pedersen, and Chen's 1995 trained summarizer. The other half of the book holds new papers, which attempt to cover current issues and point to future trends. It starts with a paper by Karen Sp~irck Jones, which acts as an overall introduction. In it, the summarization process and the uses of summaries are broken down into their constituent parts and each of these is discussed (it reminded me of a much earlier Sp~rck Jones paper on categorization [1970]). Despite its comprehensiveness and authority, I must confess to finding this opener heavy going at times. The rest of the papers are grouped into six sections, each of which is prefaced with two or three well-written pages from the editors. These introductions contain valuable commentary on the coming papers--even pointing out a possible flaw in the evaluation part of one. The opening section holds three papers on so-called classical approaches. Here one finds the oft-cited papers of Luhn, Edmundson, and Pollock and Zamora. As a package, these papers provide a novice with a good idea of how basic summarization works. My only quibble was in their reproduction. In Luhn's paper, an article from Scientific American is summarized and it would have been beneficial to have this included in the book as well. Some of the figures in another paper contained very small fonts and were hard to read; fixing this for a future print run is probably worth thinking about. The next section holds papers on corpus-based approaches to summarization, starting with Kupiec et al.'s paper about a summarizer trained on an existing corpus of manually abstracted documents. Two new papers building upon the Kupiec et al. work follow this. Exploiting the discourse structure of a document is the topic of the next section. Of the five papers here, I thought Daniel Marcu's was the best, nicely describing summarization work so far and then clearly explaining his system, which is based on Rhetorical Structure Theory. The following section on knowledge-rich approaches to summarization covers such things as Wendy Lehnert's work on breaking",2000,CL,-0.5
Book Reviews: Lexical Semantics and Knowledge Representation in Multilingual Text Generation,"In contrast, a state-of-the-art natural language generation (NLG) system would likely be able to produce only one of them, modulo variations introduced by, for example, passivization, topicalization, or pronominalization. The problem is not simply to choose among paraphrases, but to be able to produce them to start with. In this book, an extended version of his dissertation, Stede provides the theoretical and practical means of endowing a text generator with such capability. He concentrates on tactical planning, namely, on choosing an appropriate verbalization for some content that another component of the NLG system (the text planner) has assembled in response to communicative goals. Stede's approach promotes lexicalization as the central step in this process and the lexicon itself as the link between the languageindependent domain model and the language-specific resources necessary to generate the surface sentence. Stede's ultimate goal is ambitious: he seeks to provide an architecture that can be used as is to perform generation of the same content in different languages--even lexical entries are to be reused when possible. In his view, multilingual generation, including the problem of language divergences, can be seen as a mere extension of the monolingual paraphrase task. Whereas the methods proposed are very compelling with respect to monolingual generation, it is not clear whether the multilingual goal has been fully achieved. I will come back to this point at the end of this review. The book is structured in three main parts:",2000,CL,0.0
Introduction to the Special issue on finite state methods in NLP,"The idea for this special issue came up during the preparations of the International Workshop on Finite-State Methods in Natural Language Processing, that was held at Bilkent University in Ankara, Turkey in the summer of 1998. The number of the submissions had exceeded our initial expectations and we were able to select quite a good set of papers from those submitted. Further, the workshop and the preceding tutorial by Kenneth Beesley, on finite-state methods, was attended by quite a large number of participants. This led us to believe that interest in the theory and applications of finitestate machinery was alive and well, and that some of the papers from this workshop along with further additional submissions could make a very good special issue for this journal. The five papers in this issue are the result of this process. The last decade has seen a quite a substantial surge in the use of finite-state methods in all aspects of natural language applications. Fueled by the theoretical contributions of Kaplan and Kay (1994), Mohri's recent contributions on the use of finite-state techniques in various NLP problems (Mohri 1996, 1997), the success of finite-state approaches especially in computational morphology, for example, Koskenniemi (1983), Karttunen (1983), and Karttunen, Kaplan, and Zaenen (1992), and, finally, the availability of state-of-the-art tools for building and manipulating large-scale finite-state systems (Karttunen 1993; Karttunen and Beesley 1992; Karttunen et al. 1996; Mohri, Pereira, and Riley 1998; van Noord 1999), recent years have seen many successful applications of finite-state approaches in tagging, spell checking, information extraction, parsing, speech recognition, and text-to-speech applications. This is a remarkable comeback considering that in the dawn of modern linguistics (Chomsky 1957), finitestate grammars were dismissed as fundamentally inadequate. As a result, most of the work in computational linguistics in the past few decades has been focused on far more powerful formalisms. Recent publications on finite-state technology include two collections of papers (Roche and Schabes 1997; Kornai 1999) with contributions covering a wide range of these topics. This special issue, we hope, will add to these contributions. The five papers in this collection cover many aspects of finite-state theory and applications. The papers Treatment of Epsilon Moves in Subset Construction by van Noord and Incremental Construction of Minimal Acyclic Finite-State Automata and Transducers by Daciuk, Watson, Watson, and Mihov, address two fundamental aspects in the construction of finite-state recognizers. Van Noord presents results for various methods for producing a deterministic automaton with no epsilon transitions from a nondeterministic automaton with a large number of epsilon transitions, especially those resulting from finite-state approximations of context-free and more powerful formalisms. Daciuk et al. present a new method for constructing minimal, deterministic, acyclic",2000,CL,0.0
Book Reviews: Syntactic Wordclass Tagging,"Part-of-speech (POS) tagging is one of the most popular and thoroughly researched tasks in the field of natural language processing, particularly since it is a prerequisite for a wide variety of more complex tasks. The book Syntactic Wordclass Tagging is a multiauthor collection of articles giving advice on how to use and implement a POS tagger. Part I of the book is entitled ""The User's View"" and is geared towards novices and researchers who are interested in the POS annotation that taggers produce. Part II, entitled ""The Implementer's View,"" is more technical and is written for researchers who want to understand the advantages of the various computational techniques used for POS tagging; it includes an introductory chapter by Hans van Halteren. After an introductory chapter by Atro Voutilainen, Part I begins with ""A Short History of Tagging,"" also by Voutilainen, which describes some of the notable developments in both data-driven and linguistic approaches to tagging. Then, in ""The Use of Tagging,"" Geoffrey Leech and Nicholas Smith argue that POS tagging is useful for almost every task in corpus linguistics, and has made its way into a number of practical applications as well, such as information retrieval, spelling correction, and machine-aided translation. Leech and Smith also point out that syntactic parsing is arguably the central task of natural language processing, since it is a prerequisite for any kind of semantic analysis of text, and claim that tagging, by being a prerequisite to parsing, is effectively an ""entry to the most central area of corpus processing"" (p. 27). Jan Cloeren then describes tagsets for wordclass annotation, and discusses the different levels of linguistic details--morphological, syntactic, semantic, discoursal-captured by various tagsets. He also discusses how certain phenomena, such as multiunit tokens, multitoken units, wordclass underspecification, and wordclass ambiguity, present challenges to the design of a tagset. He further describes a proposal by the Text Encoding Initiative (TEI) that specifies how to encode wordclass tags with SGML. ""Standards for Tagsets"" by Geoffrey Leech and Andrew Wilson discusses the guidelines for POS annotation standards developed by the Expert Advisory Group on Language Engineering Standards, or EAGLES. The chapter describes the obligatory, recommended, and optional attribute-value sets for describing wordclasses across languages and tasks. The intent is that the widely recognized attribute-value pairs (major parts of speech, gender, etc.) would be obligatory or recommended, whereas the task or language-specific attribute-value pairs would be optional. In ""Performance of Taggers,"" Hans van Halteren discusses the relationship between correctness, ambiguity, precision, and recall, which are the most commonly",2000,CL,0.0
Extracting the lowest-frequency words: pitfalls and possibilities,"s or to the complete newspaper corpus. This raises the quest ion of whether better results might have been obtained if the complete data sets had been used. In principle, more data might imply more power. At the same time, more data also entails the risk of more noise. At least for our af data, enlarging the complement leads to worse performance. When we allow any sentence that contains af in our analyses, F decreases f rom 0.31 to 0.23 for G 2. When we base the analyses on the complete newspaper corpus, F reduces further to 0.19. The reason for this decrease in performance is probably due to the W/C-ratio being very low for all practical w indow sizes, i.e., at the very left part of the saw-tooth-shaped pat tern characterizing Nsig as a function of W/C. Consequently, any low-frequency word is singled out as a significant i tem wheneve r it occurs at least once in the window. Given the Zipfian structure of word-f requency distributions, a great ma ny spurious low-frequency words are extracted. As ment ioned in the introduction, the received wisdom is that the windowing method is unreliable for events with a f requency of less than 5. By means of an analysis of the behavior of statistical tests for 2 x 2 contingency tables wi th sparse data, a me thod for opt imizing the use of these tests has been developed. We hope that this technique will prove to be useful for domains in which the extraction of low-probabili ty events is crucial.",2000,CL,0.4
Book Reviews: Extended Finite State Models of Language,"In this volume, AndrOs Kornai has put together a collection of articles that strongly argue his contention that finite-state approaches to natural language processing (NLP) are now part of the mainstream, both theoretically and computationally. The papers included were first presented at a 1996 workshop, held in Budapest as part of the European Conference on Artificial Intelligence (ECAI '96), and have been chosen for inclusion in this volume to complement previous works on finite-state approaches. Articles referring to the Xerox regular expression calculus and the AT&T Bell Labs system of weighted finite-state transducers that appeared in Roche and Schabes's 1997 Finite State Language Processing are followed up by new articles in this book. In addition this volume includes a CD-ROM, which, although it does not contain the Xerox and AT&T Bell Labs toolkits, does have source code and executables for several of the systems described in the book, including an older version of Bruce Watson's FIRE Lite toolkit for constructing and minimizing finite automata. The attraction of finite-state approaches to NLP is their speed and efficiency. The question is their adequacy--just how powerful a formal system is needed to describe natural language? Consider the following sentence:",2000,CL,0.0
Chunking with Maximum Entropy Models,"Maximum Entropy (MaxEnt) models (Jaynes, 1957) are exponential models that implement the intuition that if there is no evidence to favour one alternative solution above another, both alternatives should be equally likely. In order to accomplish this, as much information as possible about the process you want to model must be collected. This information consists of frequencies of events relevant to the process. The frequencies of relevant events are considered to be properties of the process. When building a model we have to constrain our attention to models with these properties. In most cases the process is only partially described. The MaxEnt framework now demands that from all the models that satisfy these constraints, we choose the model with the flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w]h), the MaxEnt solution has the form:",2000,CoNLL,0.0
Use of Support Vector Learning for Chunk Identification,"Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pat tern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pat tern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as follows:",2000,CoNLL,0.0
Single-Classifier Memory-Based Phrase Chunking,"In the shared task for CoNLL-2000, words and tags form the basic multi-valued features for predicting a rich phrase segmentation code. While the tag features, containing WSJ paxt-ofspeech tags (Marcus et al., 1993), have about 45 values, the word features have more than 10,000 values. In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features. We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a metalearning classifier architecture that could be expected to improve accuracy. Given this restriction we have explored the following:",2000,CoNLL,0.0
Generating Synthetic Speech Prosody with Lazy Learning in Tree Structures,"We present ongoing work on prosody prediction for speech synthesis. This approach considers sentences as tree structures and infers the prosody from a corpus of such structures using machine learning techniques. The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach. 1 I n t r o d u c t i o n Natural prosody production remains a problem in speech synthesis systems. Several automatic prediction methods have already been tried for this, including decision trees (Ross, 1995), neural networks (Traber, 1992), and HMMs (Jensen et al., 1994). The original aspect of our prediction approach is a tree structure representation of sentences, and the use of tree similarity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an at tempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorfet al., 1995). The prosodic information consists of ToBI labeling of accents and breaks (Silverman et al., 1992). The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project (Marcus et al., 1993). We firstly describe the tree structures defined for this work, then present the tree metrics that we are using, and finally discuss how they are manipulated to achieve the prosody prediction. 2 T r e e S t r u c t u r e s So far we have considered two types of structures in this work: a simple syntactic structure and a performance structure (Gee and Grosjean, 1983). Their comparison in use should provide some interesting knowledge about the usefulness or the limitations of the elements of information included in each one. 2.1 Syntactic Structure The syntactic structure considered is built exclusively from the syntactic parsing of the given sentences. This parsing, with the relative syntactic tags, constitute the backbone of the structure. Below this structure lie the words of the sentence, with their part-of-speech tags. Additional levels of nodes can be added deeper in the structure to represent the syllables of each word, and the phonemes of each syllable. The syntactic structure corresponding to the sentence ""Hennessy will be a hard act to follow"" is presented in Figure 1 as an example (the syllable level has been omit ted for clarity). 2.2 P e r f o r m a n c e Structure The performance structure used in our approach is a combination of syntactic and phonological informations. Its upper part is a binary tree where each node represents a break between the two parts of the sentence contained into the subtrees of the node. This binary structure defines a hierarchy: the closer to the root the node is, the more salient (or stronger) the break is.",2000,CoNLL,1.0
Experimenting with the Interaction between Aggregation and Text Structuring,"In natural language generation, different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter (Reiter, 1994). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence, and tries to explore what preferences exist among the factors related to the two tasks. The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text. 1 I n t r o d u c t i o n In automatic natural language generation (NLG), various versions of the pipeline architecture specified by Reiter and Dale ((Reiter, 1994) and (Reiter and Dale, 1997)) are usually adopted. They successfully modularise the generation problem, but fail to capture the complex interactions between different modules. Take aggregation as an example. It combines simple representations to form a complex one, which in the mean time leads to a shorter text as a whole. There is no consensus as to where aggregation should happen and how it is related to other generation processes ((Wilkinson, 1995) and (Reape and Mellish, 1999)). We think that the effect of aggregation spreads from text planning to sentence realisation. The task of text planning is to select the relevant information to be expressed in the text and organise it into a hierarchical structure which captures certain discourse preferences such as preferences for global coherence (e.g. the use of RST relations (Mann and Thompson, 1987)) and local coherence (e.g. center transitions as defined in Centering Theory (Grosz et al., 1995)). Aggregation affects text planning by taking away facts from a sequence featuring preferred center movements for subordination. As a result, the preferred center transitions in the sequence are cut off. For example, comparing the two descriptions of a necklace in Figure 1, 2 is less coherent than 1 because of the shifting from the description of the necklace to that of the designer. To avoid this side effect, aggregation should be considered in text planning, which might produce a different planning sequence. Aggregation is also closely related to the task of referring expression generation. A referring expression is used not only for identifying a referent, but also for providing additional information about the referent and expressing the speaker's emotional at t i tude toward the referent (Appelt, 1985). The syntactic form of a referring expression affects how much additional information can be expressed, but it can only be determined after sentence planning, when the ordering between sentences and sentence components has been decided. This demands that the factors relevant to referring expression generation and aggregation be considered at the same time rather than sequentially to generate referring expressions capable of serving multiple goals. In this paper, we are concerned with a specific type of aggregation called embedding, which shifts one clause to become a component within the structure of an NP in another clause. We focus on the interaction between maintaining local coherence and embedding, and describe how to capture this interaction as preferences among related factors. We believe that if these preferences are used properly, we would be able to generate more flexible texts without sacrificing quality. We implemented the preferences I. This necklace is in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance, this necklace has floral motifs. It was designed by Jessie King. King once lived in Scotland. 2. This necklace, which was designed by Jessie King, is in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance, this necklace has floral motifs. King once lived in Scotland. Figure 1: An aggregation example in an experimental generation system based on a Genetic Algori thm to produce museum descriptions, which describe museum objects on display. The result shows that the system can generate a number texts of similar qualities to human wri t ten texts. 2 E m b e d d i n g in a G A T e x t P l a n n e r To experiment with the interaction between maintaining local coherence and embedding, we adopt the text planner based on a Genetic Algorithm (GA) as described in (Mellish et al., 1998). The task is, given a set of facts and a set of relations between facts, to produce a legal rhetorical s t ructure tree using all the facts and some relations. A fragment of the possible input is given in Figure 2. A genetic algorithm is suitable for such a problem because the number of possible combinations is huge, the search space is not perfectly smooth and unimodal, and the generation task does not require a global opt imum to be found. The algorithm of (Mellish et al., 1998) is basically a repeated two step process first sequences of facts are generated by applying GA operators (crossover and mutation) and then the RS trees built from these sequences are evaluated. This provides a mechanism to integrate various planning factors in the evaluation function and search for the best combinations of them. To explore the whole space of embedding, we did not perform embedding on s t ructured facts or on adjacent facts in a linear sequence because these might restrict the possibilities and even miss out good candidates. Instead, we defined an operator called embedding mutation. It randomly selects two units (say Ui and Uk) mentioning a common entity from a sequence [U1,U2,...,Ui,...,Uk,...,Uu] to form a list [Ui,Uk] representing an embedding. The list substitutes the original unit Ui to produce a new sequence [U1,U2,...,[Ui,Uk],...,Un], which is then evaluated and ordered in the population. 3 C a p t u r i n g t h e I n t e r a c t i o n s a s P r e f e r e n c e s A key requirement of the GA approach is the ability to evaluate the quality of a possible solution. We claim that it is the relative preferences among factors ra ther than each individual factor that play the crucial role in deciding the quality. Therefore, if we can capture these preferences in a generation system properly, we would be able to produce coherent text. In this section, we first discuss the preferences among factors related to text planning, based on which those for embedding can be introduced. 3.1 P r e f e r e n c e s for global coherence Following the assumption of RST, a text is globally coherent if a hierarchical s t ructure like an RST tree can be constructed from the text. In addition to the semantic relations and the Joint relation 1 used in (Mellish et al., 1998), we assume a Conjunct or Disjunct relation between two facts with at least two identical components, so that semantic parataxis can be t reated as a combining operation on two subtrees connected by the relation. Embedding a Conjunct relation inside another semantic relation is not preferred because this could convey wrong information, for example, in Figure 3, 2 cannot be used to substi tute 1. Also a semantic relation is preferred to be used whenever possible. Here is the preferences concerning the use of relations, where ""A>B"" means that A is preferred over B: 1In (Mellish et al., 1998), a Joint r e l a t ion is used to connec t every two t ex t spans t h a t do no t have a n o r m a l s eman t i c re la t ion in between.",2000,NAACL,0.0
The use of error tags in ARTFL's Encyclopédie: Does good error identification lead to good error correction?,"Many corpora which are prime candidates for automatic error correction, such as the output of OCR software, and electronic texts incorporating markup tags, include information on which portions of the text are most likely to contain errors. This paper describes how the error markup tag <?> is being incorporated in the spell-checking of an electronic version of Diderot's Encyclopddie, and evaluates whether the presence of this tag has significantly aided in correcting the errors which it marks. Although the usefulness of error tagging may vary from project to project, even as the precise way in which the tagging is done varies, error tagging does not necessarily confer any benefit in attempting to correct a given word. It may, of course, nevertheless be useful in marking errors to be fixed manually at a later stage of processing the text.",2000,NAACL,0.0
Spelling and Grammar Correction for Danish in SCARRIE,"This paper reports on work carried out to develop a spelling and grammar corrector for Danish, addressing in particular the issue of how a form of shallow parsing is combined with error detection and correction for the treatment of context-dependent spelling errors. The syntactic grammar for Danish used by the system has been developed with the aim of dealing with the most frequent error types found in a parallel corpus of unedited and proofread texts specifically collected by the project's end users. By focussing on certain grammatical constructions and certain error types, it has been possible to exploit the linguistic 'intelligence' provided by syntactic parsing and yet keep the system robust and efficient. The system described is thus superior to other existing spelling checkers for Danish in its ability to deal with contextdependent errors. 1 I n t r o d u c t i o n In her much-quoted and still relevant review o f technologies for automatic word correction (Kukich, 1992), Kukich observes that ""research in context-dependent spelling correction is in its infancy"" (p. 429), and that the task of treating context-dependent errors is still an elusive one due to the complexity of the linguistic knowledge often necessary to analyse the context in sufficient depth to find and correct such errors. But progress in parsing technology and the growing speed of computers seem to have made the task less of a chimera. The '90s have in fact seen a renewed interest in grammar checking, and proposals have been made for systems covering English (Bernth, 1997) and other languages such as Italian (Bolioli et al., 1992), Spanish and Greek (Bustamante and Ldon, 1996), Czech (Holan et al., 1997) and Swedish (Hein, 1998). This paper describes the prototype of a spelling and grammar corrector for Danish which combines traditional spelling checking functionalities with the ability to carry out compound analysis and to detect and correct certain types of context-dependent spelling errors (hereafter simply ""grammar errors""). Grammar correction is carried out by parsing the text, making use of feature overriding and error weights to accommodate the errors. Although a full parse of each sentence is at tempted, the grammar has been developed with the aim of dealing only with the most frequent error types found in a parallel corpus of unedited and proofread texts specifically collected by the project's end users. By focussing on certain grammatical constructions and certain error types, it has been possible to exploit the linguistic 'intelligence' provided by syntactic parsing and yet keep the system robust and efficient. The system described is thus superior to other existing spelling checkers for Danish in its ability to deal with certain types of grammar errors. We begin by giving an overview of the system's components in Section 2. In Section 3 we describe the error types we want to deal with: Section 4 gives an overview of the grammar: in particular, the methods adopted for treating feature mismatches and structural errors are explained. Finally, in Section 5 evaluation results are presented and a conclusion is drawn. 2 T h e p r o t o t y p e The prototype is a system for high-quality proofreading for Danish which has been developed in the context of a collaborative EUproject 1. Together with the Danish prototype, 1Main contractors in the consortium were: WordFinder Software AB (Sweden), Center for",2000,NAACL,1.0
Acknowledgments in Human-Computer Interaction,"Acknowledgments are relatively rare in humancomputer interaction. Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-language interfaces are designed? We found that, given a simple spoken-language interface that provided opportunities for and responded to acknowle d g m e n t s , abou t h a l f of our sub j ec t s u s e d acknowledgments at least once and nearly 30% used them extensively during the interaction.",2000,NAACL,0.0
The Automatic Translation of Discourse Structures,"We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computat ional model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones. 1 M o t i v a t i o n Almost all current MT systems process text one sentence at a time. Because of this limited focus, MT systems cannot re-group and re-order the clauses and sentences of an input text to achieve the most natural rendering in a target language. Yet, even between languages as close as English and French, there is a 10% mismatch in number of sentences what is said in two sentences in one language is said in only one, or in three, in the other (Gale and Church, 1993). For distant language pairs, such as Japanese and English, the differences are more significant. Consider, for example, Japanese sentence (1), a word-by-word ""gloss"" of it (2), and a two-sentence translation of it that was produced by a professional translator (3).",2000,NAACL,1.0
Predicting Automatic Speech Recognition Performance Using Prosodic Cues,"In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the T O O T train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone. 1 I n t r o d u c t i o n One of the central tasks of the dialogue manager in most current spoken dialogue systems (SDSs) is error handling. The automatic speech recognition (ASR) component of such systems is prone to error, especially when the system has to operate in noisy conditions or when the domain of the system is large. Given that it is impossible to fully prevent ASR errors, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can take appropriate action, since users have considerable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user's input, reprompting for fresh input, or, in cases where many errors have occurred, changing the interaction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langkilde et al., 1999). Traditionally, the decision to rej ec t a recognition hypothesis is based on acoustic confidence score thresholds, which provide a reliability measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recognitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. ""a.m."" recognized as ""in the morning""). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies testing the hypothesis that prosodic features provide an important clue to ASR performance. We first present results comparing prosodic analyses of correctly and incorrectly recognized speaker turns in TOOT, an experimental SDS for obtaining train information over the phone. We then describe machine learning experiments based on these results that explore the predictive power of prosodic features alone and in combination with other automatically available information, including ASR confidence scores and recognized string. Our results indicate that there are significant prosodic differences between correctly and incorrectly recognized utterances. These differences can in fact be used to pre-",2000,NAACL,0.0
Whence Sparseness?,"It has been shown that the receptive fields of simple cells in VI can be explained by assuming optimal encoding, provided that an extra constraint of sparseness is added. This finding suggests that there is a reason, independent of optimal representation, for sparseness. However this work used an ad hoc model for the noise. Here I show that, if a biologically more plausible noise model, describing neurons as Poisson processes, is used sparseness does not have to be added as a constraint. Thus I conclude that sparseness is not a feature that evolution has striven for, but is simply the result of the evolutionary pressure towards an optimal representation.",2000,NIPS,-0.8
Stability and Noise in Biochemical Switches,"Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed.",2000,NIPS,0.0
Reinforcement Learning with Function Approximation Converges to a Region,"Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(O) and V(O); the latter algorithm was used in the well-known TD-Gammon program.",2000,NIPS,0.0
Exact Solutions to Time-Dependent MDPs,We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling.,2000,NIPS,-0.5
Occam's Razor,"The Bayesian paradigm apparently only sometimes gives rise to Occam's Razor; at other times very large models perform well. We give simple examples of both kinds of behaviour. The two views are reconciled when measuring complexity of functions, rather than of the machinery used to implement them. We analyze the complexity of functions for some linear in the parameter models that are equivalent to Gaussian Processes, and always find Occam's Razor at work.",2000,NIPS,0.0
Book Reviews: Pattern Grammar: A Corpus-Driven Approach to the Lexical Grammar of English,"In this book Hunston and Francis describe an approach to lexical and grammatical description that was used to produce two remarkable Collins COBU1LD reference works, Grammar Patterns 1: Verbs and Grammar Patterns 2: Nouns and Adjectives (Francis, Hunston, and Manning 1996, 1998). This approach uses large amounts of corpus data to make discoveries about lexical items and the specific phraseological and grammatical patterns in which they regularly occur. It is corpus-driven (as opposed to corpus-based) in the sense that corpus data are analyzed with minimal theoretical presuppositions about grammatical structure. The view of language that emerges from this work is one in which no strict distinction can be made between lexicon and grammar, since lexical items must be characterized in terms of their distributions in grammatical patterns, most patterns occur with particular classes of lexical items, and many patterns are, like lexical items, specific and conventional and must be learned. The importance of this general point for computational and theoretical linguists cannot, in the opinion of the reviewer, be overestimated. Those who need to be convinced of it would probably benefit from reading this book, but should be warned: this is neither a work of computational linguistics nor of formal linguistics. Rather, it presents practical lexicographic description and is intended largely for an applied-linguistics audience (e.g., teachers of English as a second language). Computational linguists who want a source of pattern descriptions for help in designing and implementing NLP applications would be better served by the reference works cited above (but should not expect to find formal descriptions there either). The book begins with a brief history of H&F's notion of pattern, which has its roots in the pedagogical language description of Hornby (1954) and Sinclair's (1991) work in corpus linguistics. Chapter 2 introduces patterns, mostly through examples. The patterns presented range from the very general, such as V n (a verb followed by a noun group), in which case they resemble basic phrase structure rules, to the quite specific-for example, it V n/amount before/since (i.e., the word it followed by a verb followed by a noun phrase expressing the semantic notion ""amount"" followed by the word before or since). Chapter 3 discusses methodological problems in identifying patterns. Chapters 4 and 5, the highlights of the book, discuss correlations between patterns and meaning, showing how clusters of semantically similar words tend to appear in the",2001,CL,-1.0
The Interaction of Knowledge Sources in Word Sense Disambiguation,"course(2) fulfils this criterion, course is modif ied by hilly which expects a noun of type noumovable s o l i d . However , course(2) is m a r k e d a b s t r a c t , which does not comply with this restriction. Therefore, a ssuming that run is being used in its second sense leads to a si tuation in which there is no set of senses which comply wi th all the restrictions placed on them; therefore run(l) is not the correct sense of run and the partial tagger marks this sense as wrong. This si tuation is represented b y the tree at the bo t tom left of Figure 4. The sense course(2) is not rejected at this point since it m a y be found to be acceptable in the configurat ion of senses of another sense of run. The algor i thm n o w assumes that run(2) is the correct sense. This implies that course(I) is the correct sense as it complies wi th the i n a n i m a t e restriction that that verb sense places on the direct object. As well as comply ing wi th the restriction imposed by run(2), course(I) also complies wi th the one imposed by hilly(i), since nonmovable s o l i d is s u b s u m e d by inan imate . Therefore, a s suming that the senses run(2) and",2001,CL,0.0
Book Reviews: Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language,"The strong connection between knowledge representation and reasoning and natural language is the main theme of this book. The book is divided into two parts. Part I, consisting of five original or updated papers, considers the connection from the viewpoint of knowledge for language. The six original or updated papers in Part II describe using language to gather or enrich knowledge. Most of the papers discuss significant software systems that are capable of dealing with a variety of corpora. A brief overview of the eleven papers will give some indication of the coverage of the book. The following five papers constitute Part I of the book. Lucja M. Iwafiska, in ""Natural language is a powerful knowledge representation system: The UNO system,"" argues for her conjecture that natural language is a powerful representational language which is particularly suitable for representing and using knowledge in notwell-formalized domains. David A. McAllester and Robert Givan, in ""Natural language syntax and first order logic,"" present a polynomial-time (incomplete) inference procedure for first-order logic given in a nonstandard syntactic representation that takes advantage of properties found in natural language syntax. David D. McDonald, in ""Issues in the representation of real texts: The design of KRISP,"" discusses the need for fundamental changes in our representational languages in order to deal with the analysis and synthesis of real texts in real time. Lenhart K. Schubert and Chung Hee Hwang, in ""Episodic logic meets Little Red Riding Hood-A comprehensive natural representation for language understanding,"" describe their logic for representing discourse content and linguistic and world knowledge that readily captures the meaning of the text and enables appropriate inferences to be made. Stuart C. Shapiro, in ""SNePS: A logic for natural language understanding and commonsense reasoning,"" describes a number of features that are incorporated in the latest version of SNePS. Although the papers in this part vary somewhat in what they espouse, the primary theme is the value of appropriate representation. It is this aspect which is the important contribution of this section of the book. Although the value of representation is obvious in McAllester and Givan's paper, the theme of their paper is somewhat different from that of the other four, which describe working natural language understanding or processing systems. Shapiro's paper is a terse list of some features found in SNePS 2.4, giving some motivation and a few examples. The remaining three (significantly longer) papers carefully motivate their respective positions and show the applicability of their representational languages with numerous examples.",2001,CL,0.0
Learning Computational Grammars,"This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use. We were interested in a more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars). We focused on syntax, esp. noun phrase (NP) syntax.",2001,CoNLL,0.0
Probabilistic Context-Free Grammars for Syllabification and Grapheme-to-Phoneme Conversion,"We investigated the applicability of probabilistic context-free grammars to syllabi cation and grapheme-to-phoneme conversion. The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries. However, our results indicate that the standard probability model does not solve grapheme-to-phoneme conversion su ciently although, we varied all free parameters of the probabilistic reestimation procedure.",2001,EMNLP,0.0
"Entropy and Inference, Revisited","We study properties of popular near‚Äìuniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam‚Äìstyle phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions. Learning a probability distribution from examples is one of the basic problems in data analysis. Common practical approaches introduce a family of parametric models, leading to questions about model selection. In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting ‚Äúphase space volume‚Äù automatically discriminates against models with larger numbers of parameters‚Äîhence the description of these volume terms as Occam factors [1, 2]. As we move from finite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum field theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3]. Further, at least under some conditions the relevant degree of smoothness can be determined self‚Äìconsistently from the data, so that we approach something like a model independent method for learning a distribution [4]. The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space. Here the probability distribution is just a list of numbers {qi}, i = 1, 2, ¬∑ ¬∑ ¬∑ ,K, where K is the number of bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to believe that any qi and qj should be similar. The task is to learn this distribution from a set of examples, which we can describe as the number of times ni each possibility is observed in a set of N = ‚àëK i=1 ni samples. This problem arises in the context of language, where the index i might label words or phrases, so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about similarity are consistent with the constraints of a metric space. Similarly, in bioinformatics the index i might label n‚Äìmers of the the DNA or amino acid sequence, and although most work in the field is based on metrics for sequence comparison one might like an alternative approach that does not rest on such assumptions. In the analysis of neural responses, once we fix our time resolution the response becomes a set of discrete ‚Äúwords,‚Äù and estimates of the information content in the response are determined by the probability distribution on this discrete space. What all of these examples have in common is that we often need to draw some conclusions with data sets that are not in the asymptotic limit N K. Thus, while we might use a large corpus to sample the distribution of words in English by brute force (reaching N K with K the size of the vocabulary), we can hardly do the same for three or four word phrases. In models described by continuous functions, the infinite number of ‚Äúpossibilities‚Äù can never be overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities may play this role. If we have a joint distribution of two variables, the analog of a smooth distribution would be one which does not have too much mutual information between these variables. Even more simply, we might say that smooth distributions have large entropy. While the idea of ‚Äúmaximum entropy inference‚Äù is common [5], the interplay between constraints on the entropy and the volume in the space of models seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly sensible, more or less uniform priors on the space of discrete probability distributions correspond to disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that reliable inference outside the asymptotic regimeN K requires a more uniform prior on the entropy, and we offer one way of doing this. While many distributions are consistent with the data when N ‚â§ K, we provide empirical evidence that this flattening of the entropic prior allows us to make surprisingly reliable statements about the entropy itself in this regime. At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform priors on the space of distributions. The natural ‚Äúuniform‚Äù prior is given by Pu({qi}) = 1 Zu Œ¥ ( 1 ‚àí K ‚àë",2001,NIPS,0.5
Classifying Single Trial EEG: Towards Brain Computer Interfacing,"Driven by the progress in the field of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming finger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100‚Äì230ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classification accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classifiers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).",2001,NIPS,0.5
Associative memory in realistic neuronal networks,"Almost two decades ago, Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2 , 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks experimental studies point in that direction [47], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The ""realistic"" feature that is probably hardest to capture is the steady firing at low rates the background state that is observed throughout the intact nervous system [813]. The reason it is difficult to build an attractor network that is stable at low firing rates, at least in the sparse coding limit, is as follows [2,3]: Attractor networks are constructed by strengthening recurrent connections among sub-populations of neurons. The strengthening must be large enough that neurons within a sub-population can sustain a high firing rate state, but not so large that the sub-population can be spontaneously active. This implies that the neuronal gain functions the firing rate of the post-synaptic neurons as a function of the average ‚Ä¢ http) / culture.neurobio.ucla.edu/ ""'pel firing rate of the pre-synaptic neurons must be sigmoidal: small at low firing rate to provide stability, high at intermediate firing rate to provide a threshold (at an unstable equilibrium), and low again at high firing rate to provide saturation and a stable attractor. In other words, a requirement for the co-existence of a stable background state and multiple attractors is that the gain function of the excitatory neurons be super linear at the observed background rates of a few Hz [2,3]. However and this is where the problem lies above a few Hz most realistic gain function are nearly linear or sublinear (see, for example, Fig. Bl of [14]). The superlinearity requirement rests on the implicit assumption that the activity of the sub-population involved in a memory does not affect the other neurons in the network. While this assumption is valid in the sparse coding limit , it breaks down in realistic networks containing both excitatory and inhibitory neurons. In such networks, activity among excitatory cells results in inhibitory feedback. This feedback, if powerful enough, can stabilize attractors even without a saturating nonlinearity, essentially by stabilizing the equilibrium (above considered unstable) on the steep part of the gain function. The price one pays, though, is that a reasonable fraction of the neurons must be involved in each of the memories, which takes us away from the sparse coding limit and thus reduces network capacity [15].",2001,NIPS,0.4
On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes,"We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observationwhich is borne out in repeated experimentsthat while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.",2001,NIPS,0.0
English Lexical Sample Task Description,"The English lexical sample task (adjectives and nouns) for SENSEVAL 2 was set up according to the same principles as for SENSEVAL1, as reported in (Kilgarriff and Rosenzweig, 2000). (Adjectives and nouns only, because the data preparation for the verbs lexical sample was undertaken alongside that for the English all-words task, and is reported in Palmer et al (this volume). All discussion below up to the Results section covers only adjectives and nouns.)",2001,SemEval,0.0
The Italian Lexical Sample Task,"In this paper we give an overall description of the Italian lexical sample task for SENSEV AL-2, together with some general reflections about on the one hand the overall task of lexical-semantic annotation and on the other about the adequacy of existing lexical-semantic reference resources.",2001,SemEval,1.0
"The SENSEVAL-2 Panel on Domains, Topics and Senses","An important aspect of sense disambiguation is the wider semantic space (domain, topic) in which the ambiguous word occurs. This may be most clearly illustrated by some cross-lingual examples, as they would appear in (machine) translation. Consider for instance the English word housing. In a more general ""sense"", this translates in German into Wohnung. In an engineering setting however it translates into Gehiiuse. Also verbs may be translated differently (i.e. have a different sense) according to the semantic space in which they occur. For instance, English warming up translates into erhitzen in a more general sense, but into aufwiinnen in the sports domain. Because of the apparent relevance then of domains or topics on sense disambiguation, a panel was organized at SENSEV AL-2 to discuss some current and previous work in this area. The paper presents a more extended overview based on the relevant literature, besides giving a summary of the discussion that developed after the panel presentations.",2001,SemEval,0.0
The Design of Collectives of Agents to Control Non-Markovian Systems,"The ‚ÄúCollective Intelligence‚Äù (COIN) framework concerns the design of collectives of reinforcement-learning agents such that their interaction causes a provided ‚Äúworld‚Äù utility function concerning the entire collective to be maximized. Previously, we applied that framework to scenarios involving Markovian dynamics where no re-evolution of the system from counter-factual initial conditions (an often expensive calculation) is permitted. This approach sets the individual utility function of each agent to be both aligned with the world utility, and at the same time, easy for the associated agents to optimize. Here we extend that approach to systems involving non-Markovian dynamics. In computer simulations, we compare our techniques with each other and with conventional ‚Äúteam games‚Äù We show whereas in team games performance often degrades badly with time, it steadily improves when our techniques are used. We also investigate situations where the system‚Äôs dimensionality is effectively reduced. We show that this leads to difficulties in the agents‚Äô ability to learn. The implication is that ‚Äúlearning‚Äù is a property only of high-enough dimensional systems.",2002,AAAI,-0.7000000000000001
Reviewing the Design of DAML+OIL: An Ontology Language for the Semantic Web,"In the current ‚ÄúSyntactic Web‚Äù, uninterpreted syntactic constructs are given meaning only by private off-line agreements that are inaccessible to computers. In the Semantic Web vision, this is replaced by a web where both data and its semantic definition are accessible and manipulable by computer software. DAML+OIL is an ontology language specifically designed for this use in the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The definition of DAML+OIL is now over a year old, and the language has been in fairly widespread use. In this paper, we review DAML+OIL‚Äôs relation with its key ingredients (XML, RDF, OIL, DAML-ONT, Description Logics), we discuss the design decisions and trade-offs that were the basis for the language definition, and identify a number of implementation challenges posed by the current language. These issues are important for designers of other representation languages for the Semantic Web, be they competitors or successors of DAML+OIL, such as the language currently under definition by W3C.",2002,AAAI,0.0
Disciple-RKF/COG: Agent Teaching by Subject Matter Experts,"We are addressing the knowledge acquisition bottleneck in the development of knowledge-based systems by elaborating the Disciple theory and methodology that enables subject matter experts to build such systems by themselves, with limited assistance from knowledge engineers (Tecuci 1998). The investigated solution consists of developing a very capable learning agent shell that can perform many of the functions of a knowledge engineer. As an expert system shell, the learning agent shell includes a general problem solving engine that can be reused for multiple applications. In addition, it includes a multistrategy learning engine for building its knowledge base (KB) which has two main components: an object ontology that defines the concepts from a specific application domain, and a set of task reduction rules expressed with these concepts. The subject matter expert and the agent engage into a mixed-initiative reasoning process during which the expert is teaching the agent his problem solving expertise, and the agent learns from the expert, building, verifying, and improving its KB. Over the years we have developed a series of increasingly more capable learning agent shells from the Disciple family. The most recent family member, DiscipleRKF/COG, represents a significant advancement over its predecessors. It implements a more powerful plausible version space representation that allows all the types of knowledge from the KB (not only the rules, but also the objects and the tasks) to be learned with similar methods. Moreover, the partially learned knowledge pieces are represented at several levels of formalization, from natural language to formal logic, facilitating expert-agent communication, mixed-initiative problem solving, and learning. As a consequence, Disciple-RKF/COG incorporates new tools that allow a subject matter expert to perform additional knowledge engineering tasks, such as scenario specification, modeling of his problem solving process, and task formalization. Disciple-RKF/COG was used and evaluated in several courses at the US Army War College, with very promising results, being made part of their regular syllabi.",2002,AAAI,0.0
Generating Trading Agent Strategies,"My thesis work concerns the generation of trading agent strategies‚Äîautomatically, semi-automatically, and manually. Automatic generation of an agent strategy means creating a system that can read the description of some mechanism (i.e., a game) and output a strategy for a participating agent‚Äîi.e., a mapping from percepts to actions in the environment defined by the mechanism. To make this more concrete, consider an extremely simple auction mechanism: a two-player first-price sealed-bid auction. This is a game in which two players each have one piece of private information‚Äîtheir valuations for the good being auctioned. Each agent also has a continuum of possible actions‚Äîits bid amount. The payoff to an agent is its valuation minus its bid, if its bid is highest, and zero otherwise. My current system can take such a game description and output the optimal strategy, i.e., the Nash equilibrium. (In this case, that strategy is to bid half of your valuation.) Existing game solvers (Gambit and Gala) can only solve games with an enumerated (finite) set of actions, and this limitation makes it impossible to even approximate (i.e., by discretizing) the solution to games with a continuum of actions because the size of the game tree quickly explodes. Of course, the optimal strategy for the first-price sealed-bid auction was computed before game solvers existed; however, my algorithm can automatically solve any of a class of games (with certain caveats) that current solvers can‚Äôt. In addition to this algorithm for exact solutions, I have an approximation algorithm using monte carlo simulation that can handle a more general class of games (e.g., arbitrary payoff functions and any number of players) albeit at high computational cost. Both of the above methods are only tractable for quite simple games. For example, almost any mechanism that involves iterated bidding and multiple auctions is likely not to be tractable for strictly game-theoretic analysis, regardless of whether exact or approximate solutions are sought. An example of such a mechanism that we are analyzing is a simultaneous ascending auction for scheduling resources among a group of agents. In this domain, every agent has certain preferences for acquiring time slots (say, for use of a resource in a factory) and simultaneous English auctions are held for every slot until bidding stops and the slots are allocated. Since this mechanism is too complicated for the fully",2002,AAAI,0.0
Introduction to the Special Issue on Summarization,"generation based on rhetorical structure extraction. In Proceedings of the International Conference on Computational Linguistics, Kyoto, Japan, pages 344–348. Otterbacher, Jahna, Dragomir R. Radev, and Airong Luo. 2002. Revisions that improve cohesion in multi-document summaries: A preliminary study. In ACL Workshop on Text Summarization, Philadelphia. Papineni, K., S. Roukos, T. Ward, and W-J. Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. Research Report RC22176, IBM. Radev, Dragomir, Simone Teufel, Horacio Saggion, Wai Lam, John Blitzer, Arda Çelebi, Hong Qi, Elliott Drabek, and Danyu Liu. 2002. Evaluation of text summarization in a cross-lingual information retrieval framework. Technical Report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, June. Radev, Dragomir R., Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies. In ANLP/NAACL Workshop on Summarization, Seattle, April. Radev, Dragomir R. and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469–500. Rau, Lisa and Paul Jacobs. 1991. Creating segmented databases from free text for text retrieval. In Proceedings of the 14th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, New York, pages 337–346. Saggion, Horacio and Guy Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational Linguistics, 28(4), 497–526. Salton, G., A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing & Management, 33(2):193–207. Silber, H. Gregory and Kathleen McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4), 487–496. Sparck Jones, Karen. 1999. Automatic summarizing: Factors and directions. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 1–13. Strzalkowski, Tomek, Gees Stein, J. Wang, and Bowden Wise. 1999. A robust practical text summarizer. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 137–154. Teufel, Simone and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4), 409–445. White, Michael and Claire Cardie. 2002. Selecting sentences for multidocument summaries using randomized local search. In Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July. Association for Computational Linguistics, New Brunswick, NJ, pages 9–18. Witbrock, Michael and Vibhu Mittal. 1999. Ultra-summarization: A statistical approach to generating highly condensed non-extractive summaries. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, pages 315–316. Zechner, Klaus. 2002. Automatic summarization of open-domain multiparty dialogues in diverse genres. Computational Linguistics, 28(4), 447–485.",2002,CL,0.0
A Method for Open-Vocabulary Speech-Driven Text Retrieval,While recent retrieval techniques do not limit the number of,2002,EMNLP,1.0
"Replay, Repair and Consolidation","A standard view of memory consolidation is that episodes are stored temporarily in the hippocampus, and are transferred to the neocortex through replay. Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation. However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing. We suggest and demonstrate two important computational roles associated with neocortical indices.",2002,NIPS,0.2
"""Name That Song!"" A Probabilistic Approach to Querying on Music and Text","We present a novel, flexible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.",2002,NIPS,1.0
One-Class LP Classifiers for Dissimilarity Representations,"Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical examples. These applications come from the areas of machine diagnostics, fault detection, illness identification or, in principle, refer to any problem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classifier for dissimilarity representations. By the use of linear programming an efficient one-class description can be found, based on a small number of prototype objects. This classifier can be made (1) more robust by transforming the dissimilarities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classifier by Campbell and Bennett is given.",2002,NIPS,0.7000000000000001
Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy,"We extend recent work on the connection between loopy belief propagation and the Bethe free energy. Constrained minimization of the Bethe free energy can be turned into an unconstrained saddle-point problem. Both converging double-loop algorithms and standard loopy belief propagation can be interpreted as attempts to solve this saddle-point problem. Stability analysis then leads us to conclude that stable fixed points of loopy belief propagation must be (local) minima of the Bethe free energy. Perhaps surprisingly, the converse need not be the case: minima can be unstable fixed points. We illustrate this with an example and discuss implications.",2002,NIPS,0.5
Modeling Midazolam's Effect on the Hippocampus and Recognition Memory,"The benz.odiaze:pine '~1idazolam causes dense,but temporary ~ anterograde amnesia, similar to that produced byhippocampal damage~Does the action of M'idazola:m on the hippocanlpus cause less storage, or less accurate storage, .of information in episodic. long-term menlory?\rVe used a sinlple variant of theREJv1. JD.odel [18] to fit data collected. by IIirsbnla.n~Fisher, .IIenthorn,Arndt} and Passa.nnante [9] on the effects of Midazola.m, study time~ and normative \vQrd... frequenc:y on both yes-no and remember-k.novv recognition m.emory. That a: simple strength. 'model fit well \\tas cont.rary to the expectations of 'flirshman et aLMore important,within the Bayesian based R.EM modeling frame\vork, the data were consistentw'ith the view that Midazolam causes less accurate storage~ rather than less storage, of infornlation in episodic mcm.ory..",2002,NIPS,0.0
Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories,"We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum description length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrinsic complexity of a concept (that is, its description -length) systematically influences its leamability. 1The Structure of Categories A number of different principles have been advanced to explain the manner in which humans learn to categorize objects. It has been variously suggested that the underlying principle might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e).",2002,NIPS,0.5
Automatic Acquisition and Efficient Representation of Syntactic Structures,"The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by using mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity. 1 Motivation Models dealing with the acquisition of syntactic knowledge are sharply divided into two classes, depending on whether they subscribe to some variant of the classical generative theory of syntax, or operate within the framework of ‚Äúgeneral-purpose‚Äù statistical or distributional learning. An example of the former is the model of [2], which attempts to learn syntactic structures such as Functional Category, as stipulated by the Government and Binding theory. An example of the latter model is Elman‚Äôs widely used Simple Recursive Network (SRN) [3]. We believe that polarization between statistical and classical (generative, rule-based) approaches to syntax is counterproductive, because it hampers the integration of the stronger aspects of each method into a common powerful framework. Indeed, on the one hand, the statistical approach is geared to take advantage of the considerable progress made to date in the areas of distributed representation, probabilistic learning, and ‚Äúconnectionist‚Äù modeling. Yet, generic connectionist architectures are ill-suited to the abstraction and processing of symbolic information. On the other hand, classical rule-based systems excel in just those tasks, yet are brittle and difficult to train. We present a scheme that acquires ‚Äúraw‚Äù syntactic information construed in a distributional sense, yet also supports the distillation of rule-like regularities out of the accrued statistical knowledge. Our research is motivated by linguistic theories that postulate syntactic structures (and transformations) rooted in distributional data, as exemplified by the work of Zellig Harris [1]. 2 The ADIOS model The ADIOS (Automatic DIstillation Of Structure) model constructs syntactic representations of a sample of language from unlabeled corpus data. The model consists of two elements: (1) a Representational Data Structure (RDS) graph, and (2) a Pattern Acquisition (PA) algorithm that learns the RDS in an unsupervised fashion. The PA algorithm aims to detect patterns ‚Äî repetitive sequences of ‚Äúsignificant‚Äù strings of primitives occurring in the corpus (Figure 1). In that, it is related to prior work on alignment-based learning [4] and regular expression (‚Äúlocal grammar‚Äù) extraction [5] from corpora. We stress, however, that our algorithm requires no pre-judging either of the scope of the primitives or of their classification, say, into syntactic categories: all the information needed for its operation is extracted from the corpus in an unsupervised fashion. In the initial phase of the PA algorithm the text is segmented down to the smallest possible morphological constituents (e.g., ed is split off both walked and bed; the algorithm later discovers that bed should be left whole, on statistical grounds).1 This initial set of unique constituents is the vertex set of the newly formed RDS (multi-)graph. A directed edge is inserted between two vertices whenever the corresponding transition exists in the corpus (Figure 2(a)); the edge is labeled by the sentence number and by its within-sentence index. Thus, corpus sentences initially correspond to paths in the graph, a path being a sequence of edges that share the same sentence number. mi ml mj mk i{j,k}l mh mn (a) mi ml j k ... (b) u v Figure 1: (a) Two sequences mi,mj ,ml and mi,mk,ml form a pattern ci{j,k}l . = mi, {mj ,mk},ml, which allows mj and mk to be attributed to the same equivalence class, following the principle of complementary distributions [1]. Both the length of the shared context and the cohesiveness of the equivalence class need to be taken into account in estimating the goodness of the candidate pattern (see eq. 1). (b) Patterns can serve as constituents in their own right; recursively abstracting patterns from a corpus allows us to capture the syntactic regularities concisely, yet expressively. Abstraction also supports generalization: in this schematic illustration, two new paths (dashed lines) emerge from the formation of equivalence classes associated with cu and cv . In the second phase, the PA algorithm repeatedly scans the RDS graph for Significant Patterns (sequences of constituents) (SP), which are then used to modify the graph (Algorithm 1). For each path pi, the algorithm constructs a list of candidate constituents, ci1, . . . , cik. Each of these consists of a ‚Äúprefix‚Äù (sequence of graph edges), an equivalence class of vertices, and a ‚Äúsuffix‚Äù (another sequence of edges; cf. Figure 2(b)). The criterion I ‚Ä≤ for judging pattern significance combines a syntagmatic consideration (the pattern must be long enough) with a paradigmatic one (its constituents c1, . . . , ck must have high mutual information): I (c1, c2, . . . , ck) = e ‚àí(L/k)2P (c1, c2, . . . , ck) log P (c1, c2, . . . , ck) Œ†j=1P (cj) (1) where L is the typical context length and k is the length of the candidate pattern; the probabilities associated with a cj are estimated from frequencies that are immediately available We remark that the algorithm can work in any language, with any set of tokens, including individual characters ‚Äì or phonemes, if applied to speech. Algorithm 1 PA (pattern acquisition), phase 2 1: while patterns exist do 2: for all path ‚àà graph do {path=sentence; graph=corpus} 3: for all source node ‚àà path do 4: for all sink node ‚àà path do {source and sink can be equivalence classes} 5: degree of separation = path index(sink) ‚àí path index(source); 6: pattern table ‚áê detect patterns(source, sink, degree of separation, equivalence table); 7: end for 8: end for 9: winner ‚áê get most significant pattern(pattern table); 10: equivalence table ‚áê detect equivalences(graph, winner); 11: graph ‚áê rewire graph(graph, winner); 12: end for 13: end while in the graph (e.g., the out-degree of a node is related to the marginal probability of the corresponding cj). Equation 1 balances two opposing ‚Äúforces‚Äù in pattern formation: (1) the length of the pattern, and (2) the number and the cohesiveness of the set of examples that support it. On the one hand, shorter patterns are likely to be supported by more examples; on the other hand, they are also more likely to lead to over-generalization, because shorter patterns mean less context. A pattern tagged as significant is added as a new vertex to the RDS graph, replacing the constituents and edges it subsumes (Figure 2). Note that only those edges of the multigraph that belong to the detected pattern are rewired; edges that belong to sequences not subsumed by the pattern are untouched. This highly context-sensitive approach to pattern abstraction, which is unique to our model, allows ADIOS to achieve a high degree of representational parsimony without sacrificing generalization power. During the pass over the corpus the list of equivalence sets is updated continuously; the identification of new significant patterns is done using thecurrent equivalence sets (Figure 3(d)). Thus, as the algorithm processes more and more text, it ‚Äúbootstraps‚Äù itself and enriches the RDS graph structure with new SPs and their accompanying equivalence sets. The recursive nature of this process enables the algorithm to form more and more complex patterns, in a hierarchical manner. The relationships among these can be visualized recursively in a tree format, with tree depth corresponding to the level of recursion (e.g., Figure 3(c)). The PA algorithm halts if it processes a given amount of text without finding a new SP or equivalence set (in real-life language acquisition this process may never stop). Generalization. A collection of patterns distilled from a corpus can be seen as an empirical grammar of sorts; cf. [6], p.63: ‚Äúthe grammar of a language is simply an inventory of linguistic units.‚Äù The patterns can eventually become highly abstract, thus endowing the model with an ability to generalize to unseen inputs. Generalization is possible, for example, when two equivalence classes are placed next to each other in a pattern, creating new paths among the members of the equivalence classes (dashed lines in Figure 1(b)). Generalization can also ensue from partial activation of existing patterns by novel inputs. This function is supported by the input module, designed to process a novel sentence by forming its distributed representation in terms of activities of existing patterns (Figure 6). These are computed by propagating activation from bottom (the terminals) to top (the patterns) of the RDS. The initial activities wj of the terminals cj are calculated given the novel input s1, . . . , sk as follows: wj = max m=1..k {I(sk, cj)} (2) PATTERN 230: the cat is {eat, play, stay} -ing s h o w E N D h e r in g p la y is P a m c a t th e B E G IN e a t 102: do you see the cat? 101: the cat is eating 103: are you sure? 101_1 101_2 101_3 101_4 101_5 101_6 Sentence Number Within-Sentence Index s ta y in g e a t is c a t th e p la y 101_1 101_6",2002,NIPS,0.6000000000000001
Dynamical Constraints on Computing with Spike Timing in the Cortex,"If the cortex uses spike timing to compute, the timing of the spikes must be robust to perturbations. Based on a recent framework that provides a simple criterion to determine whether a spike sequence produced by a generic network is sensitive to initial conditions, and numerical simulations of a variety of network architectures, we argue within the limits set by our model of the neuron, that it is unlikely that precise sequences of spike timings are used for computation under conditions typically found in the cortex.",2002,NIPS,0.0
A Minimal Intervention Principle for Coordinated Movement,"Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a ‚Äúminimal intervention‚Äù principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variability, as well as synergetic coupling among actuators‚Äîwhich is another unexplained empirical phenomenon.",2002,NIPS,-0.7000000000000001
An Impossibility Theorem for Clustering,"Although the study of clustering is centered around an intuitively compelling goal, it has been very difficult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the difficulty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these properties expose some of the interesting (and unavoidable) trade-offs at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median.",2002,NIPS,-0.5
Circuit Model of Short-Term Synaptic Dynamics,We describe a model of short-term synaptic depression that is derived from a silicon circuit implementation. The dynamics of this circuit model are similar to the dynamics of some present theoretical models of shortterm depression except that the recovery dynamics of the variable describing the depression is nonlinear and it also depends on the presynaptic frequency. The equations describing the steady-state and transient responses of this synaptic model fit the experimental results obtained from a fabricated silicon network consisting of leaky integrate-and-fire neurons and different types of synapses. We also show experimental data demonstrating the possible computational roles of depression. One possible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the resting potential.,2002,NIPS,0.4
How the Poverty of the Stimulus Solves the Poverty of the Stimulus,"Language acquisition is a special kind of learning problem because the outcome of learning of one generation is the input for the next. That makes it possible for languages to adapt to the particularities of the learner. In this paper, I show that this type of language change has important consequences for models of the evolution and acquisition of syntax. 1 The Language Acquisition Problem For both artificial systems and non-human animals, learning the syntax of natural languages is a notoriously hard problem. All healthy human infants, in contrast, learn any of the approximately 6000 human languages rapidly, accurately and spontaneously. Any explanation of how they accomplish this difficult task must specify the (innate) inductive bias that human infants bring to bear, and the input data that is available to them. Traditionally, the inductive bias is termed somewhat unfortunately ""Universal Grammar"", and the input data ""primary linguistic data"". Over the last 30 years or so, a view on the acquisition of the syntax of natural language has become popular that has put much emphasis on the innate machinery. In this view, that one can call the ""Principles and Parameters"" model, the Universal Grammar specifies most aspects of syntax in great detail [e.g. 1]. The role of experience is reduced to setting a limited number (30 or so) of parameters. The main argument for this view is the argument from the poverty of the stimulus [2]. This argument states that children have insufficient evidence in the primary linguistic data to induce the grammar of their native language. Mark Gold [3] provides the most well-known formal basis to this argument. Gold introduced the criterion ""identification in the limit"" for evaluating the success of a learning algorithm: with an infinite number of training samples all hypotheses of the algorithm should be identical, and equivalent to the target. Gold showed that the class of context-free grammars is not learnable in this sense by any algorithm from positive samples alone (and neither are other super'-jinite classes). This proof is based on the fact that no matter how many samples from an infinite language a learning algorithm has seen, the algorithm can not decide with certainty that the samples are drawn from the infinite language or from a finite language that contains all samples. Because natural languages are thought to be at least as complex as context-free grammars, and negative feedback is assumed to be absent in the primary linguistic data, Gold's analysis, and subsequent work in learn ability theory [1] , is usually interpreted as strong support for the argument from the poverty of the stimulus, and, in the extreme, for the view that grammar induction is fundamentally impossible (a claim that Gold would not subscribe to). Critics of this ""nativist"" approach [e.g. 4, 5] have argued for different assumptions on the appropriate grammar formalism (e.g. stochastic context-free grammars), the available primary data (e.g. semantic information) or the appropriate learnability criterion. In this paper I will take a different approach. I will present a model that induces context-free grammars without a-priori restrictions on the search space, semantic information or negative evidence. Gold's negative results thus apply. Nevertheless, acquisition of grammar is successful in my model, because another process is taken into account as well: the cultural evolution of language. 2 The Language Evolution Problem Whereas in language acquisition research the central question is how a child acquires an existing language, in language evolution research the central question is how this language and its properties have emerged in the first place. Within the nativist paradigm, some have suggested that the answer to this question is that Universal Grammar is the product of evolution under selection pressures for communication [e.g. 6]. Recently, several formal models have been presented to evaluate this view. For this paper, the most relevant of those is the model of Nowak et al. [7]. In that model it is assumed that there is a finite number of grammars, that newcomers (infants) learn their grammar from the population, that more successful grammars have a higher probability of being learned and that mistakes are made in learning. The system can thus be described in terms of the changes in the relative frequencies Xi of each grammar type i in the population. The first result that Nowak et al. obtain is a ""coherence threshold"". This threshold is the necessary condition for grammatical coherence in a population, i.e. for a majority of individuals to use the same grammar. They show that this coherence depends on the chances that a child has to correctly acquire its parents' grammar. This probability is described with the parameter q. Nowak et al. show analytically that there is a minimum value for q to keep coherence in the population. If q is lower than this value, all possible grammar types are equally frequent in the population and the communicative success in minimal. If q is higher than this value, one grammar type is dominant; the communicative success is much higher than before and reaches 100% if q = l. The second result relates this required fidelity (called qd to a lower bound (be) on the number of sample sentences that a child needs. Nowak et al. make the crucial assumption that all languages are equally expressive and equally different from each other. With that assumption they can show that be is proportional to the total number of possible grammars N. Of course, the actual number of sample sentences b is finite; Nowak et al. conclude that only if N is relatively small can a stable grammar emerge in a population. I.e. the population dynamics require a restrictive Universal Grammar. The models of Gold and Nowak et al. have in common that they implicitly assume that every possible grammar is equally likely to become the target grammar for learning. If even the best possible learning algorithm cannot learn such a grammar, the set of allowed grammars must be restricted. There is, however, reason to believe that this assumption is not the most useful for language learning. Language learning is a very particular type of learning problem, because the outcome of the learning process at one generation is the input for the next. The samples from which a child learns with its learning procedure, are therefore biased by the learning of previous generations that used the same procedure[8]. In [9] and other papers, Kirby, Hurford and students have developed a framework to study the consequences of that fact. In this framework, called the ""Iterated Learning Model"" (ILM), a population of individuals is modeled that can each produce and interpret sentences, and have a language acquisition procedure to learn grammar from each other. In the ILM one individual (the parent) presents a relatively small number of examples of form-meaning pairs to the next individual (the child). The child then uses these examples to induce his own granunar. In the next iteration the child becomes the parent, and a new individual becomes the child. This process is repeated many times. Interestingly, Kirby and Hurford have found that in these iterated transmission steps the language becomes easier and easier to learn, because the language adapts to the learning algorithm by becoming more and more structured. The structure of language in these models thus emerges from the iteration of learning. The role of biological evolution, in this view, is to shape the learning algorithms, such that the complex results of the iterated learning is biologically adaptive [10]. In this paper I will show that if one adopts this view on the interactions between learning, cultural evolution and biological evolution, the models such as those of Gold [3] and Nowak et al. [7] can no longer be taken as evidence for an extensive, innate pr~specification of human language. 3 A Simple Model of Grammar Induction To study the interactions between language adaptation and language acquisition, I have first designed a grammar induction algorithm that is simple, but can nevertheless deal with some non-trivial induction problems. The model uses context-free grammars to represent linguistic abilities. In particular, the representation is limited to grammars G where all rules are of one of the following forms: (1) A 1-+ t, (2) A 1-+ BC, (3) A 1-+ Bt. The nontenninals A, B, C are elements of the non-terminal alphabet Vnt , which includes the start symbol S. t is a string of tenninal symbols from the terminal alphabet Vt 1‚Ä¢ For determining the language L of a certain grammar G I use simple depth-first exhaustive search of the derivation tree. For computational reasons, the depth of the search is limited to a certain depth d, and the string length is limited to length l. The set of sentences (L' ~ L) used in training and in communication is therefore finite (and strictly speaking not context-free, but regular); in production, strings are drawn from a uniform distribution over L'. The grammar induction algorithm learns from a set of sample strings (sentences) that are provided by a teacher. The design of the learning algorithm is originally inspired by [11] and is similar to the algorithm in [12]. The algorithm fits within a tradition of algorithms that search for compact descriptions of the input data [e.g. 13, 14, 15]. It consists of three operations: Incorporation: extend the language, such that it includes the encountered string; if string s is not already part of the language, add a rule S 1-+ s to the grammar. INote that the restrictions on the rule-types above do not limit the scope of languages that can be represented (they are essentially equivalent to Chomsky Normal Form). They are, however, relevant for the language acquisition algorithm. Compression: substitute frequent and long substrings with a nonterminal, such that the gmmmar becomes smaller and the language remains unchangedj for every valid substring z of the right-hand sides of all rules, calculate the compression effect v(z) of substituting z with a nonterminal Aj replace all valid occurrences of the substring z, = arymaxzv(z) with A if v(z') > 0, and add a rule A f-+ Zl to the grammar. ""Valid substrings"" are those substrings which can be replaced while keeping all rules of the forms 13 described above. The compression effect is measured as the difference between the number of symbols in the grammar before and after the substitution. The compression step is repeated until the grammar does not change anymore. Generalization: equate two nonterminals, such that the grammar becomes smaller and the language laryerj for every combination of two nonterminals A and B (B :f S), calculate the compression effect v of equating A and B. Equate the combination (A',B') = arymaxABv(A,B) ifv(A',B') > OJ i.e. replace all occurrences of B with A. The compression effect is measured as the difference between the number of symbols before and after replacing and deleting redundant rules. The generalization step is repeated until the grammar does not change anymore. 4 Learnable and U nlearnable Classes The algorithm described above is implemented in C++ and tested on a variety of target grammars2 ‚Ä¢ I will not present a detailed analysis of the learning behavior here, but limit myself to a simple example that shows that the algorithm can learn some (recursive) grammars, while it can not learn others. The induction algorithm receives three sentences (abed, abcabcd, abcabcabcd). The incorporation, compression (repeated twice) and generalization steps yield subsequently the following grammars: (a) Incorporation (b) Compression (c) Generalization S f-+ abed S f-+ Yd S f-+ Xd S f-+ abcabcd S f-+ Xd S f-+ Xabcd S f-+ abcabcabcd S f-+ Xabcd X f-+ XX X f-+ yy X f-+ abc Y f-+ abc In (b) the substrings ""abcabc"" and ""abc"" are subsequently replaced by the nonterminals X and Y. In (c) the non-terminals X and Y are equated, which leads to the deletion of the second rule in (b). One can check that the total size of the grammar reduces from 24, to 19 and further down to 16 characters. From this example it is also clear that learning is not always successful. Any of the three grammars above ¬´a) and (b) are equivalent) could have generated the training data, but with these three input strings the algorithm always yields grammar (c). Consistent with Gold's general proof [3], many target grammars will never be learned correctly, no matter how many input strings are generated. In practice, each finite set of randomly generated strings from some target grammar, might yield a different result. Thus, for some number of input strings T, some set of target grammars are always acquired, some are never acquired, and some are some of the time acquired. H we can enumerate all possible grammars, we can describe this with a matrix Q, where each entry Qij describes the probability that the algorithm learning from sample strings from a target grammar i, will end up with grammar 2The source code is available at http://wvv.ling.ed.ac . uk/ """" j elle of type j. Qii is the probability that the algorithm finds the target grammar. To make learning successful, the target grammars that are presented to the algorithm have to be biased. The following section will show that for this we need nothing more than to assume that the output of one learner is the input for the next. 5 Iterated Learning: the Emergence of Learnability To study the effects of iterated learning, we extend the model with a population structure. In the new version of the model individuals (agents, that each represent a generation) are placed in a chain. The first agent induces its grammar from a number E of randomly generated strings. Every subsequent agent (the child) learns its grammar from T sample sentences that are generated by the previous one (the parent). To avoid insufficient expressivenes:,;, we al:,;o extend the generalization step with a check if the number EG of different strings the grammar G can recognize is larger than or equal to E. If not, E EG random new strings are generated and incorporated in the grammar. Using the matrix Q from the previou:,; section, we can formalize this iterated learning model with the following general equation, where Xi is the probability that grammar i is the grammar of the current generation:",2002,NIPS,0.0
ACL-03 Interactive Posters and Demonstrations,"On behalf of the program committee for ACL-2003 Interactive Posters/Demonstrations Sessions, it is my great pleasure to present you with this proceedings. Though ACL occasionally have had demonstration sessions, this will be the first attempt for ACL to have this kind of sessions, which aim to provide researchers or developers of natural language technologies with a generous environment for presentation and discussion of their works. Each paper receives an opportuity to give a five-minute oral preview presentatin and a poster or poster with demo presentation for two and half hours. To this attempt, there were initially 62 submissions, one paper was withdrawn during the review process, and we could accept only 28 papers. One paper was withdrawn after the selection and we finally have 27 papers, which you find in this volume.",2003,ACL,0.0
Introduction to the Special Issue on the Web as Corpus,"The Web is immense, free, and available by mouse click. It contains hundreds of billions of words of text and can be used for all manner of language research. The simplest language use is spell checking. Is it speculater or speculator? Google gives 67 for the former (usefully suggesting the latter might have been intended) and 82,000 for the latter. Question answered. Language scientists and technologists are increasingly turning to the Web as a source of language data, because it is so big, because it is the only available source for the type of language in which they are interested, or simply because it is free and instantly available. The mode of work has increased dramatically from a standing start seven years ago with the Web being used as a data source in a wide range of research activities: The papers in this special issue form a sample of the best of it. This introduction to the issue aims to survey the activities and explore recurring themes. We first consider whether the Web is indeed a corpus, then present a history of the theme in which we view the Web as a development of the empiricist turn that has brought corpora center stage in the course of the 1990s. We briefly survey the range of Web-based NLP research, then present estimates of the size of the Web, for English and for other languages, and a simple method for translating phrases. Next we open the Pandora’s box of representativeness (concluding that the Web is not representative of anything other than itself, but then neither are other corpora, and that more work needs to be done on text types). We then introduce the articles in the special issue and conclude with some thoughts on how the Web could be put at the linguist’s disposal rather more usefully than current search engines allow.",2003,CL,0.0
Dynamic Integration of Distributed Semantic Services: Infrastructure for Process Queries and Question Answering,"The DARPA IXO mission is to develop “systems for real-time sensing, exploitation, and decision making in a rich tactical environment”. The mission includes the development of individual technologies for sensors, sensor exploitation and command/control as well as the technology of information integration. Our research focuses on how to integrate distributed services in a dynamic networked environment to support IXO applications. This dynamic networked environment should include the following capabilities (DARPA, 2002):",2003,NAACL,0.0
A Robust Retrieval Engine for Proximal and Structural Search,"Katsuya Masuda† Takashi Ninomiya†‡ Yusuke Miyao† Tomoko Ohta†‡ Jun’ichi Tsujii†‡ † Department of Computer Science, Graduate School of Information Science and Technology, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan ‡ CREST, JST (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan {kmasuda,ninomi,yusuke,okap,tsujii}@is.s.u-tokyo.ac.jp",2003,NAACL,0.5
Demonstration of the CROSSMARC System,"Vangelis Karkaletsis , Constantine D. Spyropoulos , Dimitris Souflis , Claire Grover , Ben Hachey , Maria Teresa Pazienza , Michele Vindigni , Emmanuel Cartier , José Coch  Institute for Informatics and Telecommunications, NCSR “Demokritos”  vangelis, costass @iit.demokritos.gr Velti S.A. Dsouflis@velti.net Division of Informatics, University of Edinburgh  grover, bhachey @ed.ac.uk D.I.S.P., Universita di Roma Tor Vergata  pazienza, vindigni @info.uniroma2.it  Lingway  emmanuel.cartier, Jose.Coch @lingway.com",2003,NAACL,0.0
A Model for Learning the Semantics of Pictures,"We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.",2003,NIPS,1.0
Sensory Modality Segregation,"Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the ‚Äúvisual input‚Äù. We explain this finding in terms of the statistical structure in sensory inputs.",2003,NIPS,0.2
No Unbiased Estimator of the Variance of K-Fold Cross-Validation,"Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as con¬£rmed by numerical experiments.",2003,NIPS,0.0
Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons,"We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.",2003,NIPS,1.0
Human and Ideal Observers for Detecting Image Curves,"This paper compares the ability of human observers to detect target image curves with that of an ideal observer. The target curves are sampled from a generative model which specifies (probabilistically) the geometry and local intensity properties of the curve. The ideal observer performs Bayesian inference on the generative model using MAP estimation. Varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics, in particular those observed on natural shapes. Experiments are performed with data on both rectangular and hexagonal lattices. Our results show that human observers‚Äô performance approaches that of the ideal observer and are, in general, closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves. This suggests a bias of human observers towards straight curves and natural statistics.",2003,NIPS,0.0
Ambiguous Model Learning Made Unambiguous with 1/f Priors,"What happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data? In a Bayesian model-learning framework the answer depends on the prior expectations of the dynamics of the model parameter that is to be inferred from the data. Local time constraints on the priors are insufficient to pick one interpretation over another. On the other hand, nonlocal time constraints, induced by a 1/f noise spectrum of the priors, is shown to permit learning of a specific model parameter even when there are infinitely many equally plausible interpretations of the data. This transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system, allowing the use of powerful statistical mechanical methods to uncover the transition from indeterminate to determinate model learning.",2003,NIPS,0.0
Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks,"Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difficulties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system; with sufficiently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective.",2003,NIPS,0.2
Flexible Decision-Making in Sequential Auctions,"It is quite common that items are sold sequentially in a series of auctions. For example, in the Seattle Fur Exchange, approximately eighty percent of pelts are auctioned sequentially (Lambson & Thurston 2003). On eBay, there are thousands of identical or similar items that are sold sequentially. A sequential auction is a combination of individual auctions. Each individual auction, which we call a component auction, can be any type of auction, such as a first-price, sealed-bid auction. In this thesis, I assume that there are q items for sale in a sequence of auctions, K. There are n agents, n > q, competing for these q items. Each agent has single-unit demand and will leave if it wins one item. Agents have incomplete information; each agent knows its own true valuation; however, it knows only a distribution function of other agents‚Äô valuations. After each auction, all bidders‚Äô bids are revealed, a practice which is common on eBay and other e-commerce sites. The intention of decision-making in sequential auctions is to find the optimal solutions for agents in the sequence of auctions as a whole. One myopic approach is to treat sequential auctions as a collection of independent auctions. However, as pointed out by Engelbrecht-Wiggans and Weber (Engelbrecht-Wiggans & Weber 1979), this kind of approach may be inappropriate in the general case. A better approach is to model sequential auctions as a game and find the equilibrium strategies. There is a voluminous theoretical literature on finding equilibria in sequential auctions. However, these models lack flexibility and a re-analysis is necessary even for a slightly different model. On the other hand, the vast number of trading opportunities and the increasingly fluid markets bolster the need for automated trading support in the form of trading agents‚Äîsoftware programs that participate in electronic markets on behalf of a user. Simple bidding tools, like eSnipe1 and AuctionBlitz2, enable bidders to automate submission of bids. However, these tools lack the sophistication that bidders require when faced with a plethora of auctions possibly occurring in a sequence. The need for economic ef-",2004,AAAI,0.0
Modeling Choices in Quasigroup Completion: SAT Versus CSP,"We perform a systematic comparison of SAT and CSP models for a challenging combinatorial problem, quasigroup completion (QCP). Our empirical results clearly indicate the superiority of the 3D SAT encoding (Kautz et al. 2001), with various solvers, over other SAT and CSP models. We propose a partial explanation of the observed performance. Analytically, we focus on the relative conciseness of the 3D model and the pruning power of unit propagation. Empirically, the focus is on the role of the unit-propagation heuristic of the best performing solver, Satz (Li & Anbulagan 1997), which proves crucial to its success, and results in a significant improvement in scalability when imported into the CSP solvers. Our results strongly suggest that SAT encodings of permutation problems (Hnich, Smith, & Walsh 2004) may well prove quite competitive in other domains, in particular when compared with the currently preferred channeling CSP models.",2004,AAAI,0.0
WordNet::Similarity ‚Äî Measuring the Relatedness of Concepts,"WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity or relatedness between a pair of concepts (or word senses). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.",2004,AAAI,0.0
Learning and Inferring Transportation Routines,This paper introduces a hierarchical Markov model that can learn and infer a user‚Äôs daily movements through the community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user‚Äôs mode of transportation or her goal. We apply Rao-Blackwellised particle filters for efficient inference both at the low level and at the higher levels of the hierarchy. Significant locations such as goals or locations where the user frequently changes mode of transportation are learned from GPS data logs without requiring any manual labeling. We show how to detect abnormal behaviors (e.g. taking a wrong bus) by concurrently tracking his activities with a trained and a prior model. Experiments show that our model is able to accurately predict the goals of a person and to recognize situations in which the user performs unknown activities.,2004,AAAI,0.7000000000000001
TransType2 - An Innovative Computer-Assisted Translation System,"TT2 is an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions. Different versions of the system are being developed for English, French, Spanish and German by an international team of researchers from Europe and Canada. Two professional translation agencies are currently evaluating successive prototypes.",2004,ACL,1.0
Automatic Acquisition of English Topic Signatures Based on a Second Language,"We present a novel approach for automatically acquiring English topic signatures. Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it. Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation. Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web. We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results.",2004,ACL,1.0
Briefly Noted,"Alan Turing begins his 1950 Mind article, “Computing Machinery and Intelligence,” with the following straightforward pronouncement: “I propose to consider the question ‘Can machines think?’ ” He quickly (too quickly?) argues for “replacing” the original question with another “which is closely related to it and is expressed in relatively unambiguous words.” (Please note the vagueness of the claim; what is not vague is that it is not a claim of identity.) Turing goes on to describe the new form of the “problem” in terms of an “imitation game,” played with three people, a man A, a woman B, and an interrogator C, who may be of either sex. They are placed in separate rooms, with the only form of communication between them being teleprinter (or, to be au courant, instantmessenger clients on their desktops). C is to ask them questions or otherwise engage them in conversation, with the object of the game being for C, who knows them only by the labels X and Y, to determine which of them is a man and which a woman. Turing then proceeds to offer his replacement(s) for the original question:",2004,CL,0.0
Attribute-Based and Value-Based Clustering: An Evaluation,"In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all.",2004,EMNLP,0.0
WordNet::Similarity - Measuring the Relatedness of Concepts,"WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related.",2004,NAACL,1.0
Hierarchical Clustering of a Mixture Model,"In this paper we propose an efficient algorithm for reducing a large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original model; this is achieved by clustering (grouping) the components. The method minimizes a new, easily computed distance measure between two Gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters, avoiding the need for explicit resampling of datapoints. We demonstrate the method by performing hierarchical clustering of scenery images and handwritten digits.",2004,NIPS,0.5
Outlier Detection with One-class Kernel Fisher Discriminants,"The problem of detecting ‚Äúatypical objects‚Äù or ‚Äúoutliers‚Äù is one of the classical topics in (robust) statistics. Recently, it has been proposed to address this problem by means of one-class SVM classifiers. The main conceptual shortcoming of most one-class approaches, however, is that in a strict sense they are unable to detect outliers, since the expected fraction of outliers has to be specified in advance. The method presented in this paper overcomes this problem by relating kernelized one-class classification to Gaussian density estimation in the induced feature space. Having established this relation, it is possible to identify ‚Äúatypical objects‚Äù by quantifying their deviations from the Gaussian model. For RBF kernels it is shown that the Gaussian model is ‚Äúrich enough‚Äù in the sense that it asymptotically provides an unbiased estimator for the true density. In order to overcome the inherent model selection problem, a cross-validated likelihood criterion for selecting all free model parameters is applied.",2004,NIPS,0.5
An Auditory Paradigm for Brain-Computer Interfaces,"Motivated by the particular problems involved in communicating with ‚Äúlocked-in‚Äù paralysed patients, we aim to develop a braincomputer interface that uses auditory stimuli. We describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences. Using Support Vector Machine classification and Recursive Channel Elimination on the independent components of averaged eventrelated potentials, we show that an untrained user‚Äôs EEG data can be classified with an encouragingly high level of accuracy. This suggests that it is possible for users to modulate EEG signals in a single trial by the conscious direction of attention, well enough to be useful in BCI.",2004,NIPS,0.5
Joining forces to resolve lexical ambiguity: East meets West in Barcelona,"This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model, this paper focuses solely on the joint contributions to the ”Swat-HK” effort.",2004,SemEval,1.0
The University of Amsterdam at Senseval-3: Semantic roles and Logic forms,We describe our participation in two of the tasks organized within Senseval-3: Automatic Labeling of Semantic Roles and Identification of Logic Forms in English.,2004,SemEval,1.0
Question Answering as Question-Biased Term Extraction: A New Approach toward Multilingual QA,"This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE). This new QBTE approach liberates QA systems from the heavy burden imposed by question types (or answer types). In conventional approaches, a QA system analyzes a given question and determines the question type, and then it selects answers from among answer candidates that match the question type. Consequently, the output of a QA system is restricted by the design of the question types. The QBTE directly extracts answers as terms biased by the question. To confirm the feasibility of our QBTE approach, we conducted experiments on the CRL QA Data based on 10-fold cross validation, using Maximum Entropy Models (MEMs) as an ML technique. Experimental results showed that the trained system achieved 0.36 in MRR and 0.47 in Top5 accuracy.",2005,ACL,0.9
Word Sense Disambiguation vs. Statistical Machine Translation,"We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-ofthe-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone. Error analysis suggests several key factors behind this surprising finding, including inherent limitations of current statistical MT architectures.",2005,ACL,-1.0
Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars,"Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.",2005,ACL,1.0
CL Research's Knowledge Management System,"CL Research began experimenting with massive XML tagging of texts to answer questions in TREC 2002. In DUC 2003, the experiments were extended into text summarization. Based on these experiments, The Knowledge Management System (KMS) was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration. KMS has been extended to include web question answering, both general and topic-based summarization, information extraction, and document exploration. The document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation. As development of KMS has continued, user modeling has become a key research issue: how will different users want to use the information they identify.",2005,ACL,0.0
ACL Lifetime Achievement Award: Some Points in a Time,"This article offers a personal perspective on the development of language and information processing over the last half century, focusing on the use of statistical methods. Introduced, with computers, in the 1950s, these have not always been highly regarded, but were revived in the 1990s. They have proved effective in more ways than might have been expected, and encourage new thinking about what language and information processing involve.",2005,CL,0.0
Squibs and Discussions: Evaluating Discourse and Dialogue Coding Schemes,Agreement statistics play an important role in the evaluation of coding schemes for discourse and dialogue. Unfortunately there is a lack of understanding regarding appropriate agreement measures and how their results should be interpreted. In this article we describe the role of agreement measures and argue that only chance-corrected measures that assume a common distribution of labels for all coders are suitable for measuring agreement in reliability studies. We then provide recommendations for how reliability should be inferred from the results of agreement statistics.,2005,CL,0.0
Squibs and Discussions: Real versus Template-Based Natural Language Generation: A False Opposition?,"Natural language generation (NLG) systems are sometimes partitioned into applicationdependent systems which lack a proper theoretical foundation, on the one hand, and theoretically well-founded systems which embody generic linguistic insights, on the other. Template-based systems are often regarded as automatically falling into the first category. We argue against this view. First, we describe the received view of both template-based and ‘‘standard’’ NLG systems (section 2). Then we describe a class of recent template-based systems (section 3) that will serve as a basis for a comparison between template-based and other NLG systems with respect to their potential for performing NLG tasks (section 4). We ask what the real difference between templatebased and other systems is and argue that the distinction between the two is becoming increasingly blurred (section 5). Finally, we discuss the implications of engineering shortcuts (Mellish 2000) and corpus-based methods (section 6).",2005,CL,-0.1
A Joint Model for Semantic Role Labeling,"We present a semantic role labeling system submitted to the closed track of the CoNLL-2005 shared task. The system, introduced in (Toutanova et al., 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. We also describe experiments aimed at increasing the robustness of the system in the presence of syntactic parse errors. Our final system achieves F1-Measures of 76.68 and 78.45 on the development and the WSJ portion of the test set, respectively.",2005,CoNLL,1.0
MBOI: Discovery of Business Opportunities on the Internet,"We propose a tool for the discovery of business opportunities on the Web, more specifically to help a user find relevant call for tenders (CFT), i.e. invitations to contractors to submit a tender for their products/services. Simple keyword-based Information Retrieval do not capture the relationships in the data, which are needed to answer the complex needs of the users. We therefore augment keywords with information extracted through natural language processing and business intelligence tools. As opposed to most systems, this information is used at all stages in the back-end and interface. The benefits are twofold: first we obtain higher precision of search and classification, and second the user gains access to a deeper level of information. Two challenges are: how to discover new CFT and related documents on the Web, and how to extract information from these documents, knowing that the Web offers no guarantee on the structure and stability of those documents. A major hurdle to the discovery of new documents is the poor degree of “linkedness” between businesses, and the open topic area, which makes topic-focused Web crawling (Aggarwal et al., 2001) unapplicable. To extract information, wrappers (Soderland, 1999), i.e. tools that can recognise textual and/or structural patterns, have limited success because of the diversity and volatility of Web documents. Since we cannot assume a structure for documents, we exploit information usually contained in CFTs: contracting authority, opening/closing date, location, legal notices, conditions of submission, classification, etc. These can appear marked up with tags or as free-text. A first type of information to extract are the socalled named entities (Maynard et al., 2001), i.e. names of people, organisations, locations, time or quantities. To these standard entities we add some application-specific entities such as FAR (regulation number), product dimensions, etc. To extract named entities we use Nstein NFinderTM, which uses a combination of lexical rules and a dictionary. More details about the entities, statistics and results can be found in (Paradis and Nie, 2005a). We use another tool, Nstein NconceptTM, to extract concepts, which capture the “themes” or “relevant phrases” in a document. NConcept uses a combination of statistics and linguistic rules. As mentioned above, CFTs not only contains information about the subject of the tender, but also procedural and regulation information. We tag passages in the document as “subject” or “non-subject”, according to the presence or absence of the most discriminant bigrams. Some heuristics are also applied to use the “good predictors” such as URL and money, or to further refine the non-subject passages into “regulation”. More details can be found in (Paradis and Nie, 2005b). Another information to extract is the industry or service, according to a classification schema such as NAICS (North American Industry Classification System) or CPV (Common Procurement Vocabulary). We perform multi-schema, multi-label classification, which facilitates use across economic zones (for instance, an American user may not be familiar with CPV, a European standard) and confusion over schemas versions (NAICS version 1997/Canada vs. NAICS version 2002). Our classifier is a simple Naive Bayes, trained over 20,000 documents gathered from an American Government tendering site, FBO (Federal Business Opportunities). Since we have found classification to be sensitive to the pres-",2005,EMNLP,0.9
Using Question Series to Evaluate Question Answering System Effectiveness,"The original motivation for using question series in the TREC 2004 question answering track was the desire to model aspects of dialogue processing in an evaluation task that included different question types. The structure introduced by the series also proved to have an important additional benefit: the series is at an appropriate level of granularity for aggregating scores for an effective evaluation. The series is small enough to be meaningful at the task level since it represents a single user interaction, yet it is large enough to avoid the highly skewed score distributions exhibited by single questions. An analysis of the reliability of the per-series evaluation shows the evaluation is stable for differences in scores seen in the track. The development of question answering technology in recent years has been driven by tasks defined in community-wide evaluations such as TREC, NTCIR, and CLEF. The TREC question answering (QA) track started in 1999, with the first several editions of the track focused on factoid questions. A factoid question is a fact-based, short answer question such as How many calories are there in a Big Mac?. The track has evolved by increasing the type and difficulty of questions that are included in the test set. The task in the TREC 2003 QA track was a combined task that contained list and definition questions in addition to factoid questions (Voorhees, 2004). A list question asks for different instances of a particular kind of information to be returned, such as List the names of chewing gums. Answering such questions requires a system to assemble an answer from information located in multiple documents. A definition question asks for interesting information about a particular person or thing such as Who is Vlad the Impaler? or What is a golden parachute?. Definition questions also require systems to locate information in multiple documents, but in this case the information of interest is much less crisply delineated. Like the NTCIR4 QACIAD challenge (Kato et al., 2004), the TREC 2004 QA track grouped questions into series, using the series as abstractions of information-seeking dialogues. In addition to modeling a real user task, the series are a step toward incorporating context-processing into QA evaluation since earlier questions in a series provide some context for the current question. In the case of the TREC series, each series contained factoid and list questions and had the target of a definition associated with it. Each question in a series asked for some information about the target. In addition, the final question in each series was an explicit “other” question, which was to be interpreted as “Tell me other interesting things about this target I don’t know enough to ask directly”. This last question was roughly equivalent to the definition questions in the",2005,EMNLP,0.0
Differentiating Homonymy and Polysemy in Information Retrieval,"Recent studies into Web retrieval have shown that word sense disambiguation can increase retrieval effectiveness. However, it remains unclear as to the minimum disambiguation accuracy required and the granularity with which one must define word sense in order to maximize these benefits. This study answers these questions using a simulation of the effects of ambiguity on information retrieval. It goes beyond previous studies by differentiating between homonymy and polysemy. Results show that retrieval is more sensitive to polysemy than homonymy and that, when resolving polysemy, accuracy as low as 55% can potentially lead to increased performance.",2005,EMNLP,0.5
Fixing two weaknesses of the Spectral Method,"We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting suboptimal cuts with local search, we recommend the adoption of flow-based rounding. The second weakness is that for many ‚Äúpower law‚Äù graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see figure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method‚Äôs quotient-style objective function does encourage balance, can be fixed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro.",2005,NIPS,0.5
Fast Gaussian Process Regression using KD-Trees,"The computation required for Gaussian process regression with n training examples is about O(n) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression.",2005,NIPS,1.0
Noise and the two-thirds power Law,"The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in handor joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system.",2005,NIPS,0.0
Learning Influence among Interacting Markov Chains,"We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model.",2005,NIPS,0.7000000000000001
Exploring GnuGo's Evaluation Function with a SVM,"This ongoing research project investigates articulatory feature (AF) classification using multiclass support vector machines (SVMs). SVMs are being constructed for each AF in the multi-valued feature set (Table 1), using speech data and annotation from the IFA Dutch ‚ÄúOpen-Source‚Äù (van Son et al. 2001) and TIMIT English (Garofolo et al. 1993) corpora. The primary objective of this research is to assess the AF classification performance of different multiclass generalizations of the SVM, including one-versus-rest, one-versus-one, Decision Directed Acyclic Graph (DDAG), and direct methods for multiclass learning. Observing the successful application of SVMs to numerous classification problems (Bennett and Campbell 2000), it is hoped that multiclass SVMs will outperform existing state-of-the-art AF classifiers. One of the most basic challenges for speech recognition and other spoken language systems is to accurately map data from the acoustic domain into the linguistic domain. Much speech processing research has approached this task by taking advantage of the correlation between phones, the basic units of speech sound, and their acoustic manifestation (intuitively, there is a range of sounds that humans would consider to be an ‚Äúe‚Äù). The mapping of acoustic data to phones has been largely successful, and is used in many speech systems today. Despite its success, there are drawbacks to using phones as the point of entry from the acoustic to linguistic domains. Notably, the granularity of the ‚Äúphoneticsegmental‚Äù model, in which speech is represented as a series of phones, makes it difficult to account for various subphone phenomena that affect performance on spontaneous speech. Researchers have pursued an alternative approach to the acoustic-linguistic mapping through the use of articulatory modeling. This approach more directly exploits the intimate relation between articulation and acoustics: the state of one‚Äôs speech articulators (e.g. vocal folds, tongue) uniquely determines the parameters of the acoustic speech signal. Unfortunately, while the mapping from articulator to acoustics is straightforward, the problem of recovering the state of the articulators from an acoustic speech representation, acoustic-to-articulatory inversion, poses a formidable challenge (Toutios and Margaritis 2003). Nevertheless, re-",2006,AAAI,0.0
Turing‚Äôs Dream and the Knowledge Challenge,"There is a set of clear-cut challenges, all centering around knowledge, that have received insufficient attention in AI, and whose solution could bring the realization of Turing‚Äôs dream ‚Äì the dream of a machine we can talk with just like a person, and which is therefore (at least) our intellectual equal. These challenges have to do with the representation of linguistically expressible knowledge, the role of knowledge in language understanding, the use of knowledge for several sorts of commonsense reasoning, and knowledge accumulation. Concerning the last topic, I briefly present preliminary results of some of our recent efforts to extract ‚Äúshallow‚Äù general knowledge about the world from large text corpora. Hi-fidelity Representation of Linguistically Expressible Knowledge ‚ÄúLanguage is fundamental to our ability to think; it is ‚Äòmore or less synonymous with symbolic thought,‚Äô...‚Äù ‚Äì Donald Johanson, Sci. Am. 1998, expounding on Ian Tattersall‚Äôs Becoming Human Humans acquire large amounts of knowledge about the world from verbal expressions of such knowledge, and in turn, are able to communicate much of their internalized knowledge in language. It stands to reason, therefore, that our ‚Äòmentalese‚Äô must match the expressive resources of language ‚Äì and that artificial agents competent in language will also require internal representations no less expressive than language. My collaborators and I have argued for many years (e.g., (Schubert & Hwang 1989; 2000; Schubert 2000)) that a representation capable of supporting natural language and commonsense reasoning in machines must allow not only for predication, logical connectives, and ‚àÄ/‚àÉ-quantifiers ‚Äì the devices of classical logic ‚Äì but also the following: ‚Ä¢ generalized quantifiers such as most or often; e.g., ‚ÄúMost of the trick-or-treaters who came to our door received several candy bars‚Äù; though set-theoretic paraphrases are possible, inference is more straightforward if such quantifiers are directly allowed for; ‚Ä¢ event/situation reference; e.g., (following the previous sentence) ‚ÄúThis nearly emptied our bowl of treats by 7:30 Copyright c ¬© 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. pm‚Äù; note the anaphoric reference to the collection of ‚Äòreceiving events‚Äô, and the temporal modification of the ‚Äònearly-emptying event‚Äô; ‚Ä¢ modification (of predicates and sentences); examples are ‚Äúnearly‚Äù in the previous sentence, verbs like ‚Äúbecome‚Äù and ‚Äúpose as‚Äù (with a predicate complement), and sentence adverbials such as ‚Äúperhaps‚Äù and ‚Äúaccording to reliable sources‚Äù; many such examples require an intensional semantics, i.e., one that does not reduce meaning to reference (extension) in the world; ‚Ä¢ reification (of predicates and sentences); examples are ‚ÄúDomestic dogs evolved from wolves and African wild dogs‚Äù (nominal predicate reification), ‚ÄúComposing music requires problem solving, but is deeply gratifying‚Äù (VP predicate reification), ‚ÄúThat Turing was brilliant is beyond doubt‚Äù (proposition reification), ‚ÄúFor Turing to make a mistake was unusual‚Äù (event-type reification), and ‚ÄúWho murdered Montague remains a mystery‚Äù (question reification); again intensionality is involved in many such examples; ‚Ä¢ metric/comparative attributes; e.g., ‚ÄúThousands of men were employed for 20 years to build the Great Pyramid, which rose to a height of 485 feet‚Äù; ‚ÄúThe frame of the bed is wider than the door is high‚Äù; ‚Ä¢ uncertainty and genericity; e.g., ‚ÄúIf John receives a job offer from North Central Positronics, he will probably accept it‚Äù; ‚ÄúElementary school children are usually bused to school‚Äù; ‚ÄúDogs bark‚Äù; ‚Ä¢ metalinguistic capabilities; e.g., ‚ÄúWhat is the sum of binary numbers 111 and 1, expressed as a binary number?‚Äù; ‚ÄúCan you finish the opening sentence of Hamlet‚Äôs soliloquy, ‚ÄòTo be or not to be, ...‚Äô?‚Äù; It is possible to contrive FOPC paraphrases by hand for some of the above examples, but doing so automatically is deeply problematic, and probably impossible in some cases (e.g., for some examples of modification, reification, genericity, and metalanguage). Our Episodic Logic (EL) representation (Schubert & Hwang 2000) covers many of the above constructs and implements them in the EPILOG system (Schaeffer et al. 1993; Schubert & Hwang 2000). Also (Fox & Lappin 2004; 2005) provide a two-level representation (PTCT ‚Äì",2006,AAAI,0.0
Machine Reading,"Over the last two decades or so, Natural Language Processing (NLP) has developed powerful methods for low-level syntactic and semantic text processing tasks such as parsing, semantic role labeling, and text categorization. Over the same period, the fields of machine learning and probabilistic reasoning have yielded important breakthroughs as well. It is now time to investigate how to leverage these advances to understand text.",2006,AAAI,0.0
Further Investigations into Regular XORSAT,"Recent years have witnessed rapid progress both in the foundations of and in applying state-of-art solvers for the propositional satisfiability problem (SAT). The study of sources for hard SAT instances is motivated by the need for interesting benchmarks for solver development and on the other hand by theoretical analysis of different proof systems. In this respect satisfiable instance families are especially interesting. In contrast to unsatisfiable instance families, there are few theoretical results for satisfiable formulas (Alekhnovich, Hirsch, & Itsykson); for the successful DPLL method, restricted heuristics need to be considered. While real-world problems serve as best benchmark instances in many sense, such instances are typically very large and unavailable in abundance. More ‚Äúartificial‚Äù empirically hard satisfiable CNF families include (see references therein for more) regular random k-SAT (Boufkhad et al.), encodings of quasi-group completion (Achlioptas et al. 2000), XORSAT models inspired by statistical physics (Ricci-Tersenghi, Weight, & Zecchina 2001; Jia, Moore, & Selman 2005), and the regular XORSAT model (Haanp√§√§ et al. 2006) motivated by expansion properties of random regular bipartite graphs. Experimental comparison with other available generators for notably hard satisfiable 3-CNF formulas shows that the regular XORSAT model gives extremely hard instances for state-of-the art clausal SAT solvers (Haanp√§√§ et al. 2006). In this paper we generalize the regular XORSAT model for k > 3, and investigate how this relates to the hardness of the instances. By increasing the degree of the underlying regular constraint graphs, we observe a sharp increase in problem difficulty with respect to the number of variables, motivating further analysis of regular XORSAT.",2006,AAAI,0.0
Soft Syntactic Constraints for Word Alignment through Discriminative Training,"Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.",2006,ACL,0.30000000000000004
Unsupervised Analysis for Decipherment Problems,"We study a number of natural language decipherment problems using unsupervised learning. These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation. Straightforward unsupervised learning techniques most often fail on the first try, so we describe techniques for understanding errors and significantly increasing performance.",2006,ACL,-0.2
MT Evaluation: Human-Like vs. Human Acceptable,"We present a comparative study on Machine Translation Evaluation according to two different criteria: Human Likeness and Human Acceptability. We provide empirical evidence that there is a relationship between these two kinds of evaluation: Human Likeness implies Human Acceptability but the reverse is not true. From the point of view of automatic evaluation this implies that metrics based on Human Likeness are more reliable for system tuning. Our results also show that current evaluation metrics are not always able to distinguish between automatic and human translations. In order to improve the descriptive power of current metrics we propose the use of additional syntax-based metrics, and metric combinations inside the",2006,ACL,0.1
Coreference Handling in XMG,"We claim that existing specification languages for tree based grammars fail to adequately support identifier managment. We then show that XMG (eXtensible MetaGrammar) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design. 1 Specifying tree-based grammars Whilst the development of standard unificationbased grammars is well supported by the design of formalisms such as PATR-II, Ale or TDL (Krieger and Schafer, 1994), the situation is less well established for Tree-Based Grammars such as Tree Adjoining Grammars (Joshi and Schabes, 1997), Tree Description Grammars (Kallmeyer, 1996) or Interaction Grammars (Perrier, 2003). Roughly, two main types of specification formalism for Tree-Based Grammars can be distinguished: formalisms based on tree fragments and non monotonic inheritance and formalisms based on tree descriptions and monotonic inheritance. The tree fragment approach is advocated in (Evans et al., 1995) which proposes to encode lexicalised TAGs using the DATR representation language1. In this approach, tree fragments are combined within a non monotonic inheritance hierarchy. Furthermore, new fragments can be derived from existing ones by means of lexical rules. This first approach suffers from the procedural character of non-monotonic inheritance. In specifying the grammar, the grammar writer must keep A tree based approach is also used in(Becker, 2000) but this time in combination with metarules. In that particular approach, procedural aspects also come into play as the order in which metarules apply affect the results. in mind the order in which non-monotonic statements have been made so as to be able to predict how explicit statements interact with defaults and non-monotonic inheritance in determining the final output. When developing a large coverage grammar, this rapidly become extremely cumbersome. Moreover, as (Candito, 1996) remarks, nonmonotonicity may result in an information loss which makes it impossible to express the relation existing for instance between an active object and the corresponding passive subject. The approach based on tree descriptions (often called, the metagrammar approach) obviates the procedural character of the non-monotonic approach by taking tree descriptions rather than trees to be the basic units (Candito, 1996; Xia et al., 1999; Vijay-Shanker and Schabes, 1992). In essence, tree fragments are described using tree descriptions and tree descriptions are combined through conjunction or inheritance. The idea is that the minimal models satisfying the resulting descriptions are TAG elementary trees. In some cases, lexical rules are also used to derive new trees from existing ones. One main drawback with this second type of approach concerns the management of node identifiers. Either nodes are represented by nameless variables and node identification is forced by well-formedness constraints e.g., wff-constraints on trees and wff-constraints given by the input tree description (cf. e.g., (Duchier and Gardent, 1999)) or nodes are named and nodes with identical names are forced to denote the same entity. The first option is unrealistic when developing a large core grammar as it is easy to omit a necessary constraint and thereby permit overgeneration (the description will be satisfied by more trees than intended). The second option greatly degrades",2006,ACL,-0.30000000000000004
Towards A Modular Data Model For Multi-Layer Annotated Corpora,"In this paper we discuss the current methods in the representation of corpora annotated at multiple levels of linguistic organization (so-called multi-level or multi-layer corpora). Taking five approaches which are representative of the current practice in this area, we discuss the commonalities and differences between them focusing on the underlying data models. The goal of the paper is to identify the common concerns in multi-layer corpus representation and processing so as to lay a foundation for a unifying, modular data model.",2006,ACL,0.0
The Effect of Corpus Size in Combining Supervised and Unsupervised Training for Disambiguation,"We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment. The supervised component is Collins’ parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning.",2006,ACL,0.0
Improving QA Accuracy by Question Inversion,"This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions. We define the notion of an inverted question, and show that by requiring that the answers to the original and inverted questions be mutually consistent, incorrect answers get demoted in confidence and correct ones promoted. Additionally, we show that lack of validation can be used to assert no-answer (nil) conditions. We demonstrate increases of performance on TREC and other question-sets, and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours.",2006,ACL,0.8
Morphological Richness Offsets Resource Demand - Experiences in Constructing a POS Tagger for Hindi,"In this paper we report our work on building a POS tagger for a morphologically rich languageHindi. The theme of the research is to vindicate the stand thatif morphology is strong and harnessable, then lack of training corpora is not debilitating. We establish a methodology of POS tagging which the resource disadvantaged (lacking annotated corpora) languages can make use of. The methodology makes use of locally annotated modestly-sized corpora (15,562 words), exhaustive morpohological analysis backed by high-coverage lexicon and a decision tree based learning algorithm (CN2). The evaluation of the system was done with 4-fold cross validation of the corpora in the news domain (www.bbc.co.uk/hindi). The current accuracy of POS tagging is 93.45% and can be further improved. 1 Motivation and Problem Definition Part-Of-Speech (POS) tagging is a complex task fraught with challenges like ambiguity of parts of speech and handling of “lexical absence” (proper nouns, foreign words, derivationally morphed words, spelling variations and other unknown words) (Manning and Schutze, 2002). For English there are many POS taggers, employing machine learning techniques like transformation-based error-driven learning (Brill, 1995), decision trees (Black et al., 1992), markov model (Cutting et al. 1992), maximum entropy methods (Ratnaparkhi, 1996) etc. There are also taggers which are hybrid using both stochastic and rule-based approaches, such as CLAWS (Garside and Smith, 1997). The accuracy of these taggers ranges from 93-98% approximately. English has annotated corpora in abundance, enabling usage of powerful data driven machine learning methods. But, very few languages in the world have the resource advantage that English enjoys. In this scenario, POS tagging of highly inflectional languages presents an interesting case study. Morphologically rich languages are characterized by a large number of morphemes in a single word, where morpheme boundaries are difficult to detect because they are fused together. They are typically free-word ordered, which causes fixed-context systems to be hardly adequate for statistical approaches (Samuelsson and Voutilainen, 1997). Morphology-based POS tagging of some languages like Turkish (Oflazer and Kuruoz, 1994), Arabic (Guiassa, 2006), Czech (Hajic et al., 2001), Modern Greek (Orphanos et al., 1999) and Hungarian (Megyesi, 1999) has been tried out using a combination of hand-crafted rules and statistical learning. These systems use large amount of corpora along with morphological analysis to POS tag the texts. It may be noted that a purely rule-based or a purely stochastic approach will not be effective for such",2006,ACL,0.8
Stochastic Iterative Alignment for Machine Translation Evaluation,"A number of metrics for automatic evaluation of machine translation have been proposed in recent years, with some metrics focusing on measuring the adequacy of MT output, and other metrics focusing on fluency. Adequacy-oriented metrics such as BLEU measure n-gram overlap of MT outputs and their references, but do not represent sentence-level information. In contrast, fluency-oriented metrics such as ROUGE-W compute longest common subsequences, but ignore words not aligned by the LCS. We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches. We compare SIA with existing metrics, and find that it outperforms them in overall evaluation, and works specially well in fluency evaluation.",2006,ACL,1.0
A Finite-State Model of Human Sentence Processing,"It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model. We show that a simple computational model (a bigram part-of-speech tagger based on the design used by Corley and Crocker (2000)) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data. We use two modes of evaluation: one that relies on comparison with a control sentence, paralleling practice in human studies; another that measures probability drop in the disambiguating region of the sentence. Both are surprisingly good indicators of the processing difficulty of garden-path sentences. The sentences tested are drawn from published sources and systematically explore five different types of ambiguity: previous studies have been narrower in scope and smaller in scale. We do not deny the limitations of finite-state models, but argue that our results show that their usefulness has been underestimated.",2006,ACL,0.2
Left-to-Right Target Generation for Hierarchical Phrase-Based Translation,"We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form. The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system.",2006,ACL,1.0
A FrameNet-Based Semantic Role Labeler for Swedish,"We present a FrameNet-based semantic role labeling system for Swedish text. As training data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task.",2006,ACL,1.0
Empirical Lower Bounds on the Complexity of Translational Equivalence,"This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why “syntactic” constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06",2006,ACL,-0.5
Briefly Noted,"Intensional logic (IL) and its application to natural language, which the present monograph addresses, was first developed by Richard Montague in the late 1960s (e.g., Montague 1970a, 1970b). Through the efforts of (especially) Barbara Partee (e.g., Partee 1975, 1976), and Richmond Thomason, who edited the posthumous collection of Montague’s works (Thomason 1974), this became the main framework for those who aspired to a formal semantic theory for natural language, and these included computational linguists as early as Jerry Hobbs in the late 1970s (e.g., Hobbs and Rosenschein 1977). In fact, until the advent of the current interest in statistical linguistics with its own conception of what semantics is, IL, or some variant of it, was perhaps the main theory of semantics within computational linguistics generally. And within current computational semantics it still is. But over the years, philosophers, linguists, and computational linguists have noted a variety of shortcomings in Montague’s version of IL. Montague defined intensions as functions from possible worlds to extensions in that world. But this had the effect of making logically equivalent expressions have the same intension, thus leading to the problem of “logical omniscience” (believing/knowing all the logical consequences of what is believed/known). Montague had based his IL on Church’s simple theory of types (Church 1940), supplemented with intensions of each type. But this implies that each natural language item accepts only arguments of some one fixed type. However, this is not true for natural language, where conjunctions, verbs, and pretty much any functional term that accepts arguments at all can accept arguments of different types. (For example, and can accept arguments that are of the sentence type, of the verb phrase type, of the adjective type, etc.; and indeed, it can accept arguments of differing types in its different argument places.) Much study has gone into techniques for changing types of arguments so as to accommodate this phenomenon. One result of this has been to make the semantic structure of all complex items be binary—a consequence that seems otherwise to be without justification. And finally, Montague’s version of IL is higher order, allowing quantification over entities and functions of any level. This results in the system not having a recursively enumerable set of theorems, and there has always been a desire to have a more restricted system for representing natural language. The present monograph addresses all these problematic issues. It develops a fine-grained account of intensions from which an account of a first-order property theory is extracted. A “polymorphic” type structure is developed and the resulting system is claimed “to permit us to achieve expressive power comparable to a higher-order system like IL while remaining within the formal limits of a firstorder system.” Although there are many quite difficult concepts introduced in this monograph, in the main it is very clearly written. If the claims made by the authors for their new intensional language are borne out by further research, then this is a very important addition to the literature on the foundations of semantics for natural language.—Francis Jeffry Pelletier, Simon Fraser University",2006,CL,0.0
Last Words: Mark-up Barking Up the Wrong Tree,"The interest in machine-learning methods to solve natural-language-understanding problems has led to the use of textual annotation as an important auxiliary technique. Grammar induction based on annotation has been very successful for the Penn Treebank, where a corpus of English text was annotated with syntactic information. This shining example has inspired a plethora of annotation efforts: corpora are annotated for ‘coreference’, for animacy, for expressions of opinions, for temporal dependencies, for the estimated duration of the activities that the expressions refer to, and so on. It is not clear though that these efforts are bound to repeat the success of the Penn Treebank. The circumstances in which the Penn Treebank project was executed are vastly different from those in which most annotation tasks take place. First, the annotation was a linguistic task and one about which there is reasonable agreement. People might quibble about the way to represent certain constituent structure distinctions in English, but they do, in general, not disagree about the distinctions themselves; and if you don’t like the Treebank as is, you can translate it into your favorite format. Second, the work was done by advanced students who understood the task and were supervised by specialists in the field. Third, this was not done in a hurry. The project started in 1989 and the corpora are still maintained and the annotations improved. About the only thing that this project has in common with the bulk of annotation tasks is that the annotators were not very well paid. Currently, we see annotation schemas being developed for phenomena that are much less well understood than constituent structure. In workshops and conferences we hear lively discussions about interannotator agreement, about tools to make the annotation task easier, about how to cope with multiple annotations of the same text, about the development of international standards for annotation schemes in specific subdomains, and, most importantly, about the statistical models that can be built once the annotations are in place. One thing that is much less discussed is whether the annotation indeed helps isolate the property that motivated it in the first place. This is not the same as interannotator agreement. For interannotator agreement, it suffices that all annotators do the same thing. But even with full annotator agreement it is not sure that the task captures what was initially intended. Assume that I want to mark all the entities in a text that refer to the same entity with the same number and I tell my annotators “Whenever you see the word Chicago, give it the same number”: I’ll get great interannotator agreement with that guideline but it is debatable whether I will realize my proclaimed aim of classifying references to one and the same entity in the outside world. Presumably, I would like to catch all the references to the city of Chicago, but Chicago pizza is made and sold all over the United States, and the relation",2006,CL,-0.5
"Book Review: Memory-Based Language Processing, by Walter Daelemans and Antal van den Bosch","ion is introduced. Since MBLP does not abstract over the training data, it is called a lazy learning approach. Rule induction, in contrast, learns rules and does not go back to the actual training data during classification. ∗ A shorter version of this review will be published in German in the journal Linguistische Berichte. Computational Linguistics Volume 32, Number 4 The book consists of 7 chapters. Chapter 1 situates memory-based language processing firmly in the domain of empirical approaches to NLP. Empirical approaches became attractive in the early 1990s, replacing knowledge-based approaches to a high degree. Daelemans and van den Bosch argue that in the range of empirical approaches, memory-based learning offers the advantage over statistical approaches that it does not abstract over low-frequency events. Such low-frequency events are necessary in processing natural language problems because they often describe exceptions or subregularities. The chapter also introduces the major concepts of MBLP and provides an intuitive example from linguistics: PP attachment. Chapter 2 locates central concepts of MBLP in neighboring areas of research: In linguistics, the idea of processing by analogy to previous experience is a well-known concept. Psycholinguistics often uses exemplar-based approaches or, more recently, hybrid approaches that combine rules with exceptions. Applications of memory-based principles can be found in explanation-based machine translation (Nagao 1984) and data-oriented parsing (Bod 1998). Chapter 3 gives a simultaneous introduction to memory-based learning and TiMBL, the Tilburg implementation of the method. This strategy of combining theory and practice gives the reader an impression of the importance of selecting optimal parameter settings for different problems. The application of TiMBL is demonstrated on the example of plural formation in German. The chapter ends with the introduction of evaluation methodology and TiMBL’s built-in evaluation functions. Chapter 4 describes the application of TiMBL to two more complex linguistic examples: grapheme to phoneme conversion and morphological analysis. In order to find optimal solutions for these problems, two algorithms that deviate from the standard memory-based learning algorithm are introduced: IGTREE and TRIBL. IGTREE is a decision tree approximation, which bases the comparison of an example to others on a small number of feature comparisons. TRIBL is a hybrid model between the standard memory-based learning algorithm, IB1, and IGTREE. Both modifications reduce memory requirements and processing time during classification, but they may also affect classification accuracy. Unfortunately, the presentation of the first example suffers from unreadable phonetic transcriptions throughout the chapter. Whereas Chapter 4 analyzes linguistic problems, which are easily described in terms of classification, chapter 5 approaches a problem of sequence learning: partial parsing. For this task, phrase and clause boundaries must be found. In order to apply classification methods to sequence learning, the problem must be redefined as assigning tags to words or word combinations, so-called IOB tagging (Ramshaw and Marcus 1995). This tagging provides information as to whether a word constitutes a boundary or not. One advantage of using MBLP for such problems lies in the fact that different types of information, including long-distance information, can be included without modification of the original algorithm. In Chapter 6, Daelemans and van den Bosch investigate the difference between lazy and eager learning. As noted earlier, TiMBL is a typical example of lazy learning since it does not abstract from the training data. RIPPER (Cohen 1995), the other classifier used in this chapter, is a typical eager learning approach: It is a rule-induction algorithm, which displays the opposite behavior to TiMBL: a complex learning strategy and simple, efficient classification. The results presented in this chapter show that deleting examples from the training data is harmful for classification, supporting the hypothesis that lazy learning has a fitting bias for natural language problems. However, this seems to be a little too straightforward. Here, one would expect a reference to the findings of Daelemans and Hoste (2002), which show that parameter and feature",2006,CL,-0.30000000000000004
Briefly Noted,"Spoken dialogue systems have received increased interest because they are potentially much more natural and powerful methods of communicating with machines than are current graphics-based interfaces. Wired for Speech presents basic research in the psychological and sociological aspects of voice synthesis and recognition. Its major lesson is that people attribute human characteristics to spoken dialogue systems for reasons related to human evolution. But although it contains interesting basic research, the book is mainly aimed at giving technological or marketing advice to those seeking to use voice interfaces when creating commercial applications. The book is oriented around a series of simple experiments designed to show just how pervasive psychological and social influences can be on the opinions and behaviors of people confronted with voice interfaces. Each chapter describes a basic research hypothesis, introduces an experiment to test it, and discusses its implications for designing voice interfaces: gender, personality, accent, ethnicity, emotion, number of distinct voices, use of “I” by the system, voices in concert with faces, mixed synthetic and recorded voices, context, and the effects of errors in human–computer cooperation. Although Wired for Speech is very accessible, especially to the non-scientist, it is written with an unusual bibliography style for an academic book: All references and details are given in a notes section at the end of the book, making up one third of the content. This narrative exposition style will probably not satisfy either type of reader: Scientists will be frustrated at the imprecision in argumentation, lack of detail in the book itself, and continually having to refer to the notes. The lack of detail also prevents the book from serving as a reference work. Meanwhile those needing advice when implementing voice interfaces will be puzzled at references to Chomsky, Grice, and Zemlin. Thus the book seems to suffer from not knowing its audience well, which is odd as this is precisely the lesson that the book tries to impart. Another complaint from the scientific point of view is the obsession this particular book has with casting every experimental result as advice for the business and marketing side of voice interfaces, typically concentrating on Web-based e-marketing examples such as buying books on line. Most of the experiments have sample sizes of between 40 and 50, but the authors seem ready to invite multibillion-dollar businesses to immediately base their deployed systems on these results. Finally, and fundamentally, this book represents research on psychology and sociology, and the impact of these approaches on the interactions between people and machines. It contains very little linguistic content (for instance, text-to-speech system (TTSs) are described only in terms of their ability to modify pitch, pitch range, volume, etc.) and some of the linguistic detail is wrong, such as the discussion on dialects in the chapter on accent, race, and ethnicity, as well as lexical alignment. Wired for Speech may hold a strong place in the literature on practical advice for human– computer interaction, but computational linguistics readers should get this book only if they want to do research on implementations of spoken dialogue systems and be aware of potential complications when designing experiments that will use such systems.— Charles Callaway, University of Edinburgh [M. Lothaire (pseud.)]",2006,CL,-0.5
Squibs and Discussions: WordNet Nouns: Classes and Instances,"If you were to say “Women are numerous,” you would not wish to imply that any particular woman is numerous. Instead, you would probably mean something like “The class of women contains numerous instances.” To say, on the other hand, “Rosa Parks is numerous,” would be nonsense. Whereas the noun woman denotes a class, the proper noun Rosa Parks is an instance of that class. As Quirk et al. (1985, page 288) point out, proper nouns normally lack number contrast. This important distinction between classes and instances underlies the present discussion of WordNet nouns. Some nouns are understood to refer to classes; membership in those classes determines the semantic relation of hyponymy that is basic for the organization of nouns in WordNet (WN). Other nouns, however, are understood to refer to particular individuals. In many cases the distinction is clear, but not always. The distinction to be discussed here is between words ordinarily understood as referring to classes and words ordinarily understood as referring to particular individuals and places. In the literature on knowledge representation, the classic discussion of this distinction is provided by Woods (1975). The distinction was not drawn in initial versions of WN (Miller 1990; Fellbaum 1998), which used the “is a” relation in both cases. That is to say, both “A heroine is a woman” and “Rosa Parks is a woman” were considered to be occurrences of the “is a” relation and were encoded in the WN database in the same manner. Requests to incorporate a distinction between classes and instances have come from ontologists, among others. In their discussion of WN, for example, Gangemi et al. (2001) and Oltramari et al. (2002) complain about the confusion between individuals and concepts. They suggest that if there was an “instance of” relation, they could distinguish between a concept-to-concept relation of subsumption and an individual-to-concept relation of instantiation. That is, essentially, the suggestion we follow in the present work, but in some cases the distinction was not easy to draw. Incorporating this distinction was resisted at first because WN was not initially conceived as an ontology, but rather as a description of lexical knowledge. WN includes verbs, adjectives, and adverbs in addition to nouns. Although no ontology was intended, the organization of nouns in WN bore many similarities to an ontology. As the importance of ontology became more apparent, requests to convert the WN noun hierarchy could no longer be ignored. Version 2.1 of WN takes a step in that direction: the Tops file is reorganized to have a single unique beginner: entity. In a reasonable ontology, however, all terms might be expected to conform to the membership relation of set theory and would not contain individuals or placenames. The confounding of classes and instances in WN posed a problem. The obvious way to solve",2006,CL,0.0
A Mission for Computational Natural Language Learning,"In this presentation, I will look back at 10 years of CoNLL conferences and the state of the art of machine learning of language that is evident from this decade of research. My conclusion, intended to provoke discussion, will be that we currently lack a clear motivation or “mission” to survive as a discipline. I will suggest that a new mission for the field could be found in a renewed interest for theoretical work (which learning algorithms have a bias that matches the properties of language?, what is the psycholinguistic relevance of learner design issues?), in more sophisticated comparative methodology, and in solving the problem of transfer, reusability, and adaptation of learned knowledge.",2006,CoNLL,-0.30000000000000004
"LingPars, a Linguistically Inspired, Language-Independent Machine Learner for Dependency Treebanks","This paper presents a Constraint Grammarinspired machine learner and parser, Ling­ Pars, that assigns dependencies to morpho­ logically annotated treebanks in a functioncentred way. The system not only bases at­ tachment probabilities for PoS, case, mood, lemma on those features' function probabili­ ties, but also uses topological features like function/PoS n-grams, barrier tags and daughter-sequences. In the CoNLL shared task, performance was below average on at­ tachment scores, but a relatively higher score for function tags/deprels in isolation suggests that the system's strengths were not fully exploited in the current architecture.",2006,CoNLL,1.0
Investigating Lexical Substitution Scoring for Subtitle Generation,"This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms. In particular, we examine this problem in the setting of subtitle generation and evaluate state of the art scoring methods that predict the validity of a given substitution. The paper evaluates two context independent models and two contextual models. The major findings suggest that distributional similarity provides a useful complementary estimate for the likelihood that two Wordnet synonyms are indeed substitutable, while proper modeling of contextual constraints is still a challenging task for future research.",2006,CoNLL,0.0
Automatic Recognition of Personality in Conversation,"The identification of personality by automatic analysis of conversation has many applications in natural language processing, from leader identification in meetings to partner matching on dating websites. We automatically train models of the main five personality dimensions, on a corpus of conversation extracts and personality ratings. Results show that the models perform better than the baseline, and their analysis confirms previous findings linking language and personality, while revealing many new linguistic and prosodic markers.",2006,NAACL,0.8
Semantic Back-Pointers from Gesture,"Although the natural-language processing community has dedicated much of its focus to text, faceto-face spoken language is ubiquitous, and offers the potential for breakthrough applications in domains such as meetings, lectures, and presentations. Because spontaneous spoken language is typically more disfluent and less structured than written text, it may be critical to identify features from additional modalities that can aid in language understanding. However, due to the long-standing emphasis on text datasets, there has been relatively little work on nontextual features in unconstrained natural language (prosody being the most studied non-textual modality, e.g. (Shriberg et al., 2000)). There are many non-verbal modalities that may contribute to face-to-face communication, including body posture, hand gesture, facial expression, prosody, and free-hand drawing. Hand gesture may be more expressive than any non-verbal modality besides drawing, since it serves as the foundation for sign languages in hearing-disabled communities. While non-deaf speakers rarely use any such systematized language as American Sign Language (ASL) while gesturing, the existence of ASL speaks to the potential of gesture for communicative expressivity. Hand gesture relates to spoken language in several ways:",2006,NAACL,0.0
Creating a Test Collection for Citation-based IR Experiments,"We present an approach to building a test collection of research papers. The approach is based on the Cranfield 2 tests but uses as its vehicle a current conference; research questions and relevance judgements of all cited papers are elicited from conference authors. The resultant test collection is different from TREC’s in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information. The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology. We describe properties of our queries and relevance judgements, and demonstrate the use of the test collection in an experimental setup. One potentially problematic property of our collection is that queries have a low number of relevant documents; we discuss ways of alleviating this.",2006,NAACL,0.4
Semantic role labeling of nominalized predicates in Chinese,"Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as training and test data for the semantic role labeling systems. However, it is wellknown that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns generally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experiments on nominalized predicates in Chinese, using a newly completed corpus, the Chinese Nombank. We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.",2006,NAACL,0.2
Unlimited vocabulary speech recognition for agglutinative languages,"It is practically impossible to build a word-based lexicon for speech recognition in agglutinative languages that would cover all the relevant words. The problem is that words are generally built by concatenating several prefixes and suffixes to the word roots. Together with compounding and inflections this leads to millions of different, but still frequent word forms. Due to inflections, ambiguity and other phenomena, it is also not trivial to automatically split the words into meaningful parts. Rule-based morphological analyzers can perform this splitting, but due to the handcrafted rules, they also suffer from an out-of-vocabulary problem. In this paper we apply a recently proposed fully automatic and rather language and vocabulary independent way to build subword lexica for three different agglutinative languages. We demonstrate the language portability as well by building a successful large vocabulary speech recognizer for each language and show superior recognition performance compared to the corresponding word-based reference systems.",2006,NAACL,0.7000000000000001
Automatic Spoken Document Processing for Retrieval and Browsing,"Ever increasing computing power and connectivity bandwidth together with falling storage costs is resulting in overwhelming amounts of multimedia data being produced, exchanged, and stored. One key application area in this realm is the search and retrieval of spoken audio documents. As storage becomes cheaper, the availability and usefulness of large collections of spoken documents is limited strictly by the lack of adequate technology to exploit them. Manually transcribing speech is expensive and sometimes outright impossible due to privacy concerns. This leads us to exploring an automatic approach to searching and navigating spoken document collections. This tutorial will present an overview of speech transcription, indexing, and search technologies for spoken documents, with an emphasis on a corpus containing recorded academic lectures. The tutorial will point out general problems in this area and suggest possible solutions. Included in the tutorial will be a discussion of scenarios and previous projects in the area of spoken document retrieval, issues of automatic transcription of long audio files, and techniques for the indexing and retrieval of spoken audio files.",2006,NAACL,0.2
Efficient Algorithms for Richer Formalisms: Parsing and Machine Translation,"My PhD research has been on the algorithmic and formal aspects of computational linguistics, esp. in the areas of parsing and machine translation. I am interested in developing efficient algorithms for formalisms with rich expressive power, so that we can have a better modeling of human languages without sacrificing efficiency. In doing so, I hope to help integrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. Among other projects, I have been working on kbest parsing, synchronous binarization, and syntaxdirected translation.",2006,NAACL,1.0
"Role of Local Context in Automatic Deidentification of Ungrammatical, Fragmented Text","Deidentification of clinical records is a crucial step before these records can be distributed to non-hospital researchers. Most approaches to deidentification rely heavily on dictionaries and heuristic rules; these approaches fail to remove most personal health information (PHI) that cannot be found in dictionaries. They also can fail to remove PHI that is ambiguous between PHI and non-PHI. Named entity recognition (NER) technologies can be used for deidentification. Some of these technologies exploit both local and global context of a word to identify its entity type. When documents are grammatically written, global context can improve NER. In this paper, we show that we can deidentify medical discharge summaries using support vector machines that rely on a statistical representation of local context. We compare our approach with three different systems. Comparison with a rulebased approach shows that a statistical representation of local context contributes more to deidentification than dictionaries and hand-tailored heuristics. Comparison with two well-known systems, SNoW and IdentiFinder, shows that when the language of documents is fragmented, local context contributes more to deidentification than global context.",2006,NAACL,0.4
"From Pipedreams to Products, and Promise!","This demonstration provides a historical perspective of a number of research and commercial systems in Spoken Language Technology over the past 20+ years. A series of chronologically ordered video clips from many sources will be presented to illustrate the many steps and the tremendous progress that has been achieved over the years. The clips themselves are drawn from diverse academic and commercial research labs, product presentations, and user applications. All show systems being demonstrated or in actual use. Over 20 different laboratory systems, products, and companies are represented in this collection of video materials. Each of the clips has previously been shown publicly. The present selection primarily focuses on speech and natural language systems for speech recognition and synthesis. Additional contributions to this collection are welcome. 1. Project Description Preparations of these materials are being done in conjunction with the History of Speech and Language Technology Project being conducted by Saras Institute, in affiliation with the Dibner Institute for the History of Science and Technology, at MIT (Cambridge, MA). The overall mission for this project is to collect, preserve, and make readily available information about significant research discoveries, technical achievements, and business developments in speech and language technology. For further information on this project, please go to www.SarasInstitute.org. Work on this project is on-going. Additional contributions of relevant materials are welcome in the area of Spoken Language Technology, including speech and natural language systems and applications incorporating speech recognition, speech synthesis, interactive dialogue, information retrieval, machine translation, multimodal interfaces, etc. Please contact us at HistSpch@mit.edu if you have materials you would like to contribute or with any inquiries, updates, corrections, and suggestions.",2006,NAACL,0.0
Online Clustering of Moving Hyperplanes,"We propose a recursive algorithm for clustering trajectories lying in multiple moving hyperplanes. Starting from a given or random initial condition, we use normalized gradient descent to update the coefficients of a time varying polynomial whose degree is the number of hyperplanes and whose derivatives at a trajectory give an estimate of the vector normal to the hyperplane containing that trajectory. As time proceeds, the estimates of the hyperplane normals are shown to track their true values in a stable fashion. The segmentation of the trajectories is then obtained by clustering their associated normal vectors. The final result is a simple recursive algorithm for segmenting a variable number of moving hyperplanes. We test our algorithm on the segmentation of dynamic scenes containing rigid motions and dynamic textures, e.g., a bird floating on water. Our method not only segments the bird motion from the surrounding water motion, but also determines patterns of motion in the scene (e.g., periodic motion) directly from the temporal evolution of the estimated polynomial coefficients. Our experiments also show that our method can deal with appearing and disappearing motions in the scene.",2006,NIPS,1.0
Blind Motion Deblurring Using Image Statistics,"We address the problem of blind motion deblurring from a single image, caused by a few moving objects. In such situations only part of the image may be blurred, and the scene consists of layers blurred in different degrees. Most of of existing blind deconvolution research concentrates at recovering a single blurring kernel for the entire image. However, in the case of different motions, the blur cannot be modeled with a single kernel, and trying to deconvolve the entire image with the same kernel will cause serious artifacts. Thus, the task of deblurring needs to involve segmentation of the image into regions with different blurs. Our approach relies on the observation that the statistics of derivative filters in images are significantly changed by blur. Assuming the blur results from a constant velocity motion, we can limit the search to one dimensional box filter blurs. This enables us to model the expected derivatives distributions as a function of the width of the blur kernel. Those distributions are surprisingly powerful in discriminating regions with different blurs. The approach produces convincing deconvolution results on real world images with rich texture.",2006,NIPS,0.7000000000000001
Learning annotated hierarchies from relational data,"The objects in many real-world domains can be organized into hierarchies, where each internal node picks out a category of objects. Given a collection of features and relations defined over a set of objects, an annotated hierarchy includes a specification of the categories that are most useful for describing each individual feature and relation. We define a generative model for annotated hierarchies and the features and relations that they describe, and develop a Markov chain Monte Carlo scheme for learning annotated hierarchies. We show that our model discovers interpretable structure in several real-world data sets.",2006,NIPS,0.5
Modeling Human Motion Using Binary Latent Variables,"We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued ‚Äúvisible‚Äù variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture. Website: http://www.cs.toronto.edu/‚àºgwtaylor/publications/nips2006mhmublv/",2006,NIPS,0.5
Inducing Metric Violations in Human Similarity Judgements,"Attempting to model human categorization and similarity judgements is both a very interesting but also an exceedingly difficult challenge. Some of the difficulty arises because of conflicting evidence whether human categorization and similarity judgements should or should not be modelled as to operate on a mental representation that is essentially metric. Intuitively, this has a strong appeal as it would allow (dis)similarity to be represented geometrically as distance in some internal space. Here we show how a single stimulus, carefully constructed in a psychophysical experiment, introduces l2 violations in what used to be an internal similarity space that could be adequately modelled as Euclidean. We term this one influential data point a conflictual judgement. We present an algorithm of how to analyse such data and how to identify the crucial point. Thus there may not be a strict dichotomy between either a metric or a non-metric internal space but rather degrees to which potentially large subsets of stimuli are represented metrically with a small subset causing a global violation of metricity.",2006,NIPS,-0.6000000000000001
Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods,"We propose a highly efficient framework for kernel multi-class models with a large and structured set of classes. Kernel parameters are learned automatically by maximizing the cross-validation log likelihood, and predictive probabilities are estimated. We demonstrate our approach on large scale text classification tasks with hierarchical class structure, achieving state-of-the-art results in an order of magnitude less time than previous work.",2006,NIPS,1.0
User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation,"We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer. We grounded our work on the empirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based Information Retrieval can only deal successfully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference.",2007,ACL,-0.1
Instance-based Evaluation of Entailment Rule Acquisition,"Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.",2007,ACL,0.5
Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization,"The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance. We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization: raw frequency (word probability) and log-likelihood ratio. We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input. We also find that LLR is more suitable for query-focused summarization since, unlike raw frequency, it is more sensitive to the integration of the information need defined by the user.",2007,ACL,0.4
Using Error-Correcting Output Codes with Model-Refinement to Boost Centroid Text Classifier,"In this work, we investigate the use of error-correcting output codes (ECOC) for boosting centroid text classifier. The implementation framework is to decompose one multi-class problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier. However, this kind of decomposition incurs considerable bias for centroid classifier, which results in noticeable degradation of performance for centroid classifier. In order to address this issue, we use Model-Refinement to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance.",2007,ACL,0.8
Instance Weighting for Domain Adaptation in NLP,"Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.",2007,ACL,0.8
A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation,"Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics.",2007,ACL,0.7000000000000001
Using Mazurkiewicz Trace Languages for Partition-Based Morphology,"Partition-based morphology is an approach of finite-state morphology where a grammar describes a special kind of regular relations, which split all the strings of a given tuple into the same number of substrings. They are compiled in finite-state machines. In this paper, we address the question of merging grammars using different partitionings into a single finite-state machine. A morphological description may then be obtained by parallel or sequential application of constraints expressed on different partition notions (e.g. morpheme, phoneme, grapheme). The theory of Mazurkiewicz Trace Languages, a well known semantics of parallel systems, provides a way of representing and compiling such a description. 1 Partition-Based Morphology Finite-State Morphology is based on the idea that regular relations are an appropriate formalism to describe the morphology of a natural language. Such a relation is a set of pairs, the first component being an actual form called surface form, the second component being an abstract description of this form called lexical form. It is usually implemented by a finitestate transducer. Relations are not oriented, so the same transducer may be used both for analysis and generation. They may be non-deterministic, when the same form belongs to several pairs. Furthermore, finite state machines have interesting properties, they are composable and efficient. There are two main trends in Finite-State Morphology: rewrite-rule systems and two-level rule systems. Rewrite-rule systems describe the morphology of languages using contextual rewrite rules which are easily applied in cascade. Rules are compiled into finite-state transducers and merged using transducer composition (Kaplan and Kay, 1994). The other important trend of Finite-State Morphology is Two-Level Morphology (Koskenniemi, 1983). In this approach, not only pairs of lexical and surface strings are related, but there is a one-to-one correspondence between their symbols. It means that the two strings of a given pair must have the same length. Whenever a symbol of one side does not have an actual counterpart in the other string, a special symbol 0 is inserted at the relevant position in order to fulfill the same-length constraint. For example, the correspondence between the surface form spies and the morpheme concatenation spy+s is given as follows: s p y 0 + s s p i e 0 s Same-length relations are closed under intersection, so two-level grammars describe a system as the simultaneous application of local constraints. A third approach, Partition-Based Morphology, consists in splitting the strings of a pair into the same number of substrings. The same-length constraint does not hold on symbols but on substrings. For example, spies and spy+s may be partitioned as follows: s p y + s s p ie s The partition-based approach was first proposed by (Black et al., 1987) and further improved by (Pulman and Hepple, 1993) and (Grimley-Evans et al.,",2007,ACL,0.0
Multimedia Blog Creation System using Dialogue with Intelligent Robot,"A multimedia blog creation system is described that uses Japanese dialogue with an intelligent robot. Although multimedia blogs are increasing in popularity, creating blogs is not easy for users who lack highlevel information literacy skills. Even skilled users have to waste time creating and assigning text descriptions to their blogs and searching related multimedia such as images, music, and illustrations. To enable effortless and enjoyable creation of multimedia blogs, we developed the system on a prototype robot called PaPeRo. Video messages are recorded and converted into text descriptions by PaPeRo using continuous speech recognition. PaPeRo then searches for suitable multimedia contents on the internet and databases, and then, based on the search results, chooses appropriate sympathetic comments by using natural language text retrieval. The retrieved contents, PaPeRo's comments, and the video recording on the user's blog is automatically uploaded and edited. The system was evaluated by 10 users for creating travel blogs and proved to be helpful for both inexperienced and experienced users. The system enabled easy multimediarich blog creation and even provided users the pleasure of chatting with PaPeRo.",2007,ACL,0.5
Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition,"Speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary (OOV) ratio. Earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages. This paper compares various vocabulary decomposition approaches to open vocabulary speech recognition, using Estonian speech recognition as a benchmark. Comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items. A large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate, while the unsupervised morphology discovery method Morfessor Baseline gives marginally weaker results. Only the Morfessor-based approach is shown to adequately scale to smaller vocabulary sizes.",2007,ACL,0.0
"Briefly Noted: Lingvisticeskie problemy komp'juternoj morfologii [Linguistic Issues in Computational Morphology], by S. A. Koval'","This book’s stated goal is to suggest ways to optimize the incorporation of linguistic knowledge about morphology into computer systems. The book is composed of two quite different parts. Part I (roughly half of the book) defines computational linguistics as a field. Part II presents a clear introduction to computational morphology, primarily using examples from Russian. In addition to basal concepts, this part presents a history of morphological analysis systems for Russian; various implementation options for practical systems; an inventory of phenomena that, although important for descriptive and theoretical morphology, have traditionally been less important in computational applications; and ongoing areas of research. The final chapter argues that a core, general-purpose model for Russian morphology is needed, and that its development should be largely driven by linguistic research. In the Afterword, the author emphasizes his belief that approaching computational morphology as “a linguistic, not a technical discipline” is the most “fruitful” approach, because the real object of research is language, not computer systems (p. 131). The book seems most suited to linguists or students of linguistics who are unacquainted with computational morphology.— Marjorie McShane, University of Maryland at Baltimore County",2007,CL,0.0
Squibs and Discussions: Measuring Word Alignment Quality for Statistical Machine Translation,"Automatic word alignment plays a critical role in statistical machine translation. Unfortunately, the relationship between alignment quality and statistical machine translation performance has not been well understood. In the recent literature, the alignment task has frequently been decoupled from the translation task and assumptions have been made about measuring alignment quality for machine translation which, it turns out, are not justified. In particular, none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation performance. This paper explains this state of affairs and presents steps towards measuring alignment quality in a way which is predictive of statistical machine translation performance.",2007,CL,-0.30000000000000004
Letter to the Editor,"In her review of our book Memory-based Language Processing, which appeared in Computational Linguistics, Issue 32(4), Sandra Kübler comments on the illegibility of the phonetic font. Unfortunately, she had an early copy; the book was reprinted as soon as this problem was noticed, and Cambridge University Press has offered to replace any remaining misprinted copies with reprints upon request. This applies to copies with incorrectly overlapping phonetic symbols on pages 28, 59–63, and 108. They may be sent to Helen Barton, 3rd Floor, The Edinburgh Building, Cambridge University Press, Shaftesbury Road, Cambridge, UK, CB2 2RU. Postage will also be reimbursed if a receipt for the cost is provided.",2007,CL,0.0
Last Words: Computational Linguistics: What About the Linguistics?,"Three times a year I get my copy of a wholly respectable mainstream linguistics journal. Its scholarly articles are rich in examples from varied languages, and alongside these detailed analyses it advances theoretical claims and counterclaims. Its many reviews point to much more of the same. But this journal content is of interest here for another reason than these scholarly ones. First, references to computing are conspicuous by their absence. Just occasionally, grammar types or semantic models appear that have computational connections, for example in a shared view of feature sets; and there are references to computational corpus analysis, though more often in reviews than in major articles. Very very occasionally, there are articles that are more manifestly computational ones, for instance in applying unification to syntactic structures. But in general, the notion that computation in a serious sense, not just as some highly abstract grounding or, maybe, politically correct meta-reference, has something important to say to linguistics never figures. Does this matter? Specifically, whether it matters to the linguists or not, does it matter to us (computational types)? In a world of proliferating specialist journals and, increasingly, conference proceedings, why worry about the lack of computational reference in mainstream linguistics journals? Being computational is not the only way of being legitimate in any field, but there are plenty of publishing venues for the computational. So it need not be a criticism of a linguistics journal like the one I get that it does not have more on computational linguistics, or natural language processing, or natural language engineering, or human language technology. We may find the lack of computational reference a surprise, but even if there are a lot of non-computational linguists out there (as there indubitably are), why should this be a problem? The linguists may argue they have things we ought to learn from them, and it’s up to us to bridge the gap. We may argue just the reverse. But in my view there is a deeper problem. This has to do with the fact that as far as I can tell, the journal I have referred to, and other linguistics journals, are dominated by some Chomskyan paradigm. But does it matter that the linguistic zeitgeist is Chomskyan? Further, does it matter precisely which Chomskyan paradigm: older or newer, broader or more specific, pure or modified?",2007,CL,0.0
Using RBMT Systems to Produce Bilingual Corpus for SMT,"This paper proposes a method using the existing Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Machine Translation (SMT) system. We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus. With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus. In our experiments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora. We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora. The interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual corpus.",2007,CoNLL,1.0
Frustratingly Hard Domain Adaptation for Dependency Parsing,We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline.,2007,CoNLL,-0.1
Incremental Generation of Plural Descriptions: Similarity and Partitioning,"Approaches to plural reference generation emphasise descriptive brevity, but often lack empirical backing. This paper describes a corpus-based study of plural descriptions, and proposes a psycholinguisticallymotivated algorithm for plural reference generation. The descriptive strategy is based on partitioning and incorporates corpusderived heuristics. An exhaustive evaluation shows that the output closely matches human data.",2007,CoNLL,0.7000000000000001
Large Margin Synchronous Generation and its Application to Sentence Compression,"This paper presents a tree-to-tree transduction method for text rewriting. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.",2007,CoNLL,1.0
"Log-Linear Models of Non-Projective Trees, $k$-best MST Parsing and Tree-Ranking","We present our system used in the CoNLL 2007 shared task on multilingual parsing. The system is composed of three components: a k-best maximum spanning tree (MST) parser, a tree labeler, and a reranker that orders the k-best labeled trees. We present two techniques for training the MST parser: tree-normalized and graphnormalized conditional training. The treebased reranking model allows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system.",2007,CoNLL,1.0
Factored Translation Models,"We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level — may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.",2007,CoNLL,1.0
Exploiting Acoustic and Syntactic Features for Prosody Labeling in a Maximum Entropy Framework,"In this paper we describe an automatic prosody labeling framework that exploits both language and speech information. We model the syntactic-prosodic information with a maximum entropy model that achieves an accuracy of 85.2% and 91.5% for pitch accent and boundary tone labeling on the Boston University Radio News corpus. We model the acousticprosodic stream with two different models, one a maximum entropy model and the other a traditional HMM. We finally couple the syntactic-prosodic and acousticprosodic components to achieve significantly improved pitch accent and boundary tone classification accuracies of 86.0% and 93.1% respectively. Similar experimental results are also reported on Boston Directions corpus.",2007,NAACL,1.0
"Introduction to Classification: Likelihoods, Margins, Features, and Kernels","Statistical methods in NLP have exploited a variety of classification techniques as core building blocks for complex models and pipelines. In this tutorial, we will survey the basic techniques behind classification. We first consider the basic principles, including the principles of maximum likelihood and maximum margin. We then discuss several core classification technologies: naive Bayes, perceptrons, logistic regression, and support vector machines. The discussion will include the key optimization ideas behind their training and the empirical trade-offs between the various classifiers. Finally, we consider the extension to kernels and kernelized classification: what can kernels offer and what is their cost? The presentation is targeted to NLP researchers new to these methods or those wanting to understand more about how these techniques are interconnected.",2007,NAACL,0.0
A Semi-Automatic Evaluation Scheme: Automated Nuggetization for Manual Annotation,"In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.",2007,NAACL,0.5
Analysis of Summarization Evaluation Experiments,"The goals of my dissertation are: 1) to propose a French terminology for the presentation of evaluation results of automatic summaries, 2) to identify and describe experimental variables in evaluations of automatic summaries, 3) to highlight the most common tendencies, inconsistencies and methodological problems in summarization evaluation experiments, and 4) to make recommendations for the presentation of evaluation results of automatic summaries. In this paper, I focus on the second objective, i.e. identifying and describing variables in summarization evaluation experiments.",2007,NAACL,-0.2
Learning to Find Transliteration on the Web,"This prototype demonstrate a novel method for learning to find transliterations of proper nouns on the Web based on query expansion aimed at maximizing the probability of retrieving transliterations from existing search engines. Since the method we used involves learning the morphological relationships between names and their transliterations, we refer to this IR-based approach as morphological query expansion for machine transliteration. The morphological query expansion approach is general in scope and can be applied to translation and transliteration, but we focus on transliteration in this paper. Many texts containing proper names (e.g., “The cities of Mesopotamia prospered under Parthian and Sassanian rule.”) are submitted to machine translation services on the Web every day, and there are also service on the Web specifically target transliteration of proper names, including CHINET (Kwok et al. 2005) ad Livetrans (Lu, Chien, and Lee 2004). Machine translation systems on the Web such as Yahoo Translate (babelfish.yahoo.com) and Google Translate (translate.google.com/translate_t.g) typically use a bilingual dictionary that is either manually compiled or learned from a parallel corpus. However, such dictionaries often have insufficient coverage of proper names and technical terms, leading to poor translation due to out of vocabulary problem. The OOV problems of machine translation or cross language information retrieval can be handled more effectively by learning to find transliteration on the Web. Consider Sentence 1 containing three place names. 1. The cities of Mesopotamia prospered under Parthian and Sassanian rule. 2. 城市繁榮下parthian 達米亞、sassanian統",2007,NAACL,0.6000000000000001
Analysis of Morph-Based Speech Recognition and the Modeling of Out-of-Vocabulary Words Across Languages,"We analyze subword-based language models (LMs) in large-vocabulary continuous speech recognition across four “morphologically rich” languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. By estimating n-gram LMs over sequences of morphs instead of words, better vocabulary coverage and reduced data sparsity is obtained. Standard word LMs suffer from high out-of-vocabulary (OOV) rates, whereas the morph LMs can recognize previously unseen word forms by concatenating morphs. We show that the morph LMs generally outperform the word LMs and that they perform fairly well on OOVs without compromising the accuracy obtained for in-vocabulary words.",2007,NAACL,0.2
Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming,"Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.",2007,NAACL,1.0
A Log-Linear Block Transliteration Model based on Bi-Stream HMMs,"We propose a novel HMM-based framework to accurately transliterate unseen named entities. The framework leverages features in letteralignment and letter n-gram pairs learned from available bilingual dictionaries. Letter-classes, such as vowels/non-vowels, are integrated to further improve transliteration accuracy. The proposed transliteration system is applied to out-of-vocabulary named-entities in statistical machine translation (SMT), and a significant improvement over traditional transliteration approach is obtained. Furthermore, by incorporating an automatic spell-checker based on statistics collected from web search engines, transliteration accuracy is further improved. The proposed system is implemented within our SMT system and applied to a real translation scenario from Arabic to English.",2007,NAACL,1.0
Can Semantic Roles Generalize Across Genres?,"PropBank has been widely used as training data for Semantic Role Labeling. However, because this training data is taken from the WSJ, the resulting machine learning models tend to overfit on idiosyncrasies of that text’s style, and do not port well to other genres. In addition, since PropBank was designed on a verb-by-verb basis, the argument labels Arg2 Arg5 get used for very diverse argument roles with inconsistent training instances. For example, the verb “make” uses Arg2 for the “Material” argument; but the verb “multiply” uses Arg2 for the “Extent” argument. As a result, it can be difficult for automatic classifiers to learn to distinguish arguments Arg2-Arg5. We have created a mapping between PropBank and VerbNet that provides a VerbNet thematic role label for each verb-specific PropBank label. Since VerbNet uses argument labels that are more consistent across verbs, we are able to demonstrate that these new labels are easier to learn.",2007,NAACL,0.6000000000000001
Are Some Speech Recognition Errors Easier to Detect than Others?,"This study investigates whether some speech recognition (SR) errors are easier to detect and what patterns can be identified from those errors. Specifically, SR errors were examined from both nonlinguistic and linguistic perspectives. The analyses of non-linguistic properties revealed that high error ratios and consecutive errors lowered the ease of error detection. The analyses of linguistic properties showed that ease of error detection was associated with changing parts-of-speech of reference words in SR errors. Additionally, syntactic relations themselves and the change of syntactic relations had impact on the ease of error detection.",2007,NAACL,0.0
Simultaneous Identification of Biomedical Named-Entity and Functional Relation Using Statistical Parsing Techniques,"In this paper we propose a statistical parsing technique that simultaneously identifies biomedical named-entities (NEs) and extracts subcellular localization relations for bacterial proteins from the text in MEDLINE articles. We build a parser that derives both syntactic and domain-dependent semantic information and achieves an F-score of 48.4% for the relation extraction task. We then propose a semi-supervised approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2%. Our key contributions are: learning from noisy data, and building an annotated corpus that can benefit relation extraction research.",2007,NAACL,1.0
Statistical Language Models for Information Retrieval,"Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only achieve superior empirical performance, but also facilitate parameter tuning and provide a more principled way, in general, for modeling various kinds of complex and non-traditional retrieval problems. The purpose of this tutorial is to systematically review the recent progress in applying statistical language models to information retrieval with an emphasis on the underlying principles and framework, empirically effective language models, and language models developed for non-traditional retrieval tasks. Tutorial attendees can expect to learn the major principles and methods of applying statistical language models to information retrieval, the outstanding problems in this area, as well as obtain comprehensive pointers to the research literature. The tutorial should appeal to both people working on information retrieval with an interest in applying more advanced language models and those who have a background on statistical language models and wish to apply them to information retrieval. Attendees will be assumed to know basic probability and statistics. The outline of the tutorial is as follows:",2007,NAACL,0.0
Competition Adds Complexity,"Empirical studies have documented cases of belief polarization, where two people with opposing prior beliefs both strengthen their beliefs after observing the same evidence. Belief polarization is frequently offered as evidence of human irrationality, but we demonstrate that this phenomenon is consistent with a fully Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the set of Bayesian models that we consider. Suppose that Carol has requested a promotion at her company and has received a score of 50 on an aptitude test. Alice, one of the company‚Äôs managers, began with a high opinion of Carol and became even more confident of her abilities after seeing her test score. Bob, another manager, began with a low opinion of Carol and became even less confident about her qualifications after seeing her score. On the surface, it may appear that either Alice or Bob is behaving irrationally, since the same piece of evidence has led them to update their beliefs about Carol in opposite directions. This situation is an example of belief polarization [1, 2], a widely studied phenomenon that is often taken as evidence of human irrationality [3, 4]. In some cases, however, belief polarization may appear much more sensible when all the relevant information is taken into account. Suppose, for instance, that Alice was familiar with the aptitude test and knew that it was scored out of 60, but that Bob was less familiar with the test and assumed that the score was a percentage. Even though only one interpretation of the score can be correct, Alice and Bob have both made rational inferences given their assumptions about the test. Some instances of belief polarization are almost certain to qualify as genuine departures from rational inference, but we argue in this paper that others will be entirely compatible with a rational approach. Distinguishing between these cases requires a precise normative standard against which human inferences can be compared. We suggest that Bayesian inference provides this normative standard, and present a set of Bayesian models that includes cases where polarization can and cannot emerge. Our work is in the spirit of previous studies that use careful rational analyses in order to illuminate apparently irrational human behavior (e.g. [5, 6, 7]). Previous studies of belief polarization have occasionally taken a Bayesian approach, but often the goal is to show how belief polarization can emerge as a consequence of approximate inference in a Bayesian model that is subject to memory constraints or processing limitations [8]. In contrast, we demonstrate that some examples of polarization are compatible with a fully Bayesian approach. Other formal accounts of belief polarization have relied on complex versions of utility theory [9], or have focused on continuous hypothesis spaces [10] unlike the discrete hypothesis spaces usually considered by psychological studies of belief polarization. We focus on discrete hypothesis spaces and require no additional machinery beyond the basics of Bayesian inference. We begin by introducing the belief revision phenomena considered in this paper and developing a Bayesian approach that clarifies whether and when these phenomena should be considered irrational. We then consider several Bayesian models that are capable of producing belief polarization and illustrate them with concrete examples. Having demonstrated that belief polarization is compatible",2007,NIPS,0.0
DIFFRAC: a discriminative and flexible framework for clustering,"We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).",2007,NIPS,-1.0
GYDER: Maxent Metonymy Resolution,"Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don’t consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive, and argue that metonymy resolution needs more features.",2007,SemEval,-1.0
OE: WSD Using Optimal Ensembling (OE) Method,"Optimal ensembling (OE) is a word sense disambiguation (WSD) method using word-specific training factors (average positive vs negative training per sense, posex and negex) to predict best system (classifier algorithm / applicable feature set) for given target word. Our official entry (OE1) in Senseval-4 Task 17 (coarse-grained English lexical sample task) contained many design flaws and thus failed to show the whole potential of the method, finishing -4.9% behind top system (+0.5 gain over best base system). A fixed system (OE2) finished only -3.4% behind (+2.0% net gain). All our systems were 'closed', i.e. used the official training data only (average 56 training examples per each sense). We also show that the official evaluation measure tends to favor systems that do well with high-trained words.",2007,SemEval,-0.4
SemEval-2007 Task 12: Turkish Lexical Sample Task,"This paper presents the task definition, resources, and the single participant system for Task 12: Turkish Lexical Sample Task (TLST), which was organized in the SemEval-2007 evaluation exercise. The methodology followed for developing the specific linguistic resources necessary for the task has been described in this context. A language-specific feature set was defined for Turkish. TLST consists of three pieces of data: The dictionary, the training data, and the evaluation data. Finally, a single system that utilizes a simple statistical method was submitted for the task and evaluated.",2007,SemEval,0.0
USYD: WSD and Lexical Substitution using the Web1T corpus,"This paper describes the University of Sydney’s WSD and Lexical Substitution systems for SemEval-2007. These systems are principally based on evaluating the substitutability of potential synonyms in the context of the target word. Substitutability is measured using Pointwise Mutual Information as obtained from the Web1T corpus. The WSD systems are supervised, while the Lexical Substitution system is unsupervised. The lexical sample sub-task also used syntactic category information given from a CCG-based parse to assist in verb disambiguation, while both WSD tasks also make use of more traditional features. These related systems participated in the Coarse-Grained English All-Words WSD task (task 7), the Lexical Substitution Task (task 10) and the English Lexical Sample WSD sub-task (task 17).",2007,SemEval,0.0
UMND2 : SenseClusters Applied to the Sense Induction Task of Senseval-4,"SenseClusters is a freely–available open– source system that served as the University of Minnesota, Duluth entry in the SENSEVAL-4 sense induction task. For this task SenseClusters was configured to construct representations of the instances to be clustered using the centroid of word cooccurrence vectors that replace the words in an instance. These instances are then clustered using k–means where the number of clusters is discovered automatically using the Adapted Gap Statistic. In these experiments SenseClusters did not use any information outside of the raw untagged text that was to be clustered, and no tuning of the system was performed using external corpora.",2007,SemEval,0.0
Semantical Considerations on Dialectical and Practical Commitments,"This paper studies commitments in multiagent systems. A dialectical commitment corresponds to an agent taking a position about a putative fact, including for the sake of argument. A practical commitment corresponds to an agent being obliged to another to bring about a condition. Although commitments have been used in many works, an adequate formal semantics and axiomatization for them does not yet exist. This paper presents a logic of commitments that illustrates the commonalities and differences of the two kinds of commitments. In this manner, it generalizes the developments of previous papers, precisely delineates the meanings of commitments, and identifies important postulates used informally or semiformally in previous work. This paper considers ‚Äúsocial‚Äù commitments as introduced in (Singh 1991): by one agent to another, not of an agent to itself. Commitments help formalize a variety of interactive, loosely contractual, settings especially including argumentation and business protocols. Despite several formalizations that use commitments, there is surprisingly little work treating them as an abstraction in their own right. With few exceptions (reviewed in the last section), existing work has generally not emphasized the model-theoretic semantics of commitments as such, concentrating on ways of reasoning with or using them. It was a sensible research strategy to first establish that commitments were a useful concept. However, now that the case for commitments has been made well, further progress is hampered by the lack of a clear modeltheoretic semantics. For example, tools for designing correct protocols or verifying the interoperability or compliance of agents would rely upon a precise notion of what it means for an agent to be committed, which unfortunately is lacking. Analyses of commitments range in complexity from obligations to extensive conglomerates of social expectations and obligations. Following (Singh 1999), this paper takes a middle ground, erring perhaps toward simplicity. A commitment here is somewhat like a directed obligation, but one that arises in a context, and which can be manipulated in standardized ways. This paper doesn‚Äôt discuss context and manipulation, but its semantics provides a basis for specifying them precisely. Richer notions are readily accommodated in this approach. For example, Castelfranchi (1995) Copyright c ¬© 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. requires that a commitment be explicitly accepted by its creditor. This is reasonable in some applications but not others. We can easily define additional concepts that combine multiple instances of the more basic kinds of commitments studied here to achieve the various intuitive requirements of Castelfranchi and other researchers. Dialectical and Practical Commitments Commitments fall into two main varieties. Dialectical or dialogical commitments (Norman et al. 2004) reflect positions taken in dialogue or argumentation. By contrast, practical commitments reflect promises made during negotiation or trade. Thus dialectical commitments are about what holds and practical commitments about what is to be done. For example, a stock quote may be a dialectical commitment about the price; or a practical commitment to sell at the specified price. The two commitments may go together but not necessarily so. This paper lays the groundwork for formulating any such constraints as needed for different applications. Some key patterns of reasoning arise in dealing with commitments. Examples of natural reasoning patterns during modeling include (Ex 1) if a pharmacy commits to delivering medicines if the customer pays and shows a prescription, then once the customer shows the prescription, the pharmacy is committed to delivering medicines if the customer pays; (Ex 2) if a merchant commits to a customer to ship goods and commits to the same customer to send warranty paperwork, then the merchant commits to the customer to ship goods and send warranty paperwork; (Ex 3) a commitment that if the light is on, the light will be on would not be meaningful. We capture these patterns below as postulates B2, B5, and B8, respectively (along with several other postulates). How commitments relate to time is important. Dialectical commitments are about claims staked now (even if about future conditions) whereas practical commitments are about actions to be performed or conditions to be brought about in the future. Thus, postulates that hold for one kind of commitment can fail for the other kind. Examples Ex 1 and Ex 2 above would hold for dialectical but fail for practical commitments unless we impose additional constraints: because conjunction means that the conditions must hold simultaneously. Clearly, the above examples illustrate practical commitments. Therefore, we identify constraints under which various postulates hold for both kinds of commitments. Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (2008)",2008,AAAI,0.2
CIGAR: Concurrent and Interleaving Goal and Activity Recognition,"In artificial intelligence and pervasive computing research, inferring users‚Äô high-level goals from activity sequences is an important task. A major challenge in goal recognition is that users often pursue several high-level goals in a concurrent and interleaving manner, where the pursuit of goals may spread over different parts of an activity sequence and may be pursued in parallel. Existing approaches to recognizing multiple goals often formulate this problem either as a single-goal recognition problem or in a deterministic way, ignoring uncertainty. In this paper, we propose CIGAR (Concurrent and Interleaving Goal and Activity Recognition) a novel and simple two-level probabilistic framework for multiple-goal recognition where we can recognize both concurrent and interleaving goals. We use skip-chain conditional random fields (SCCRF) for modeling interleaving goals and we model concurrent goals by adjusting inferred probabilities through a correlation graph, which is a major advantage in that we are able to reason about goal interactions explicitly through the correlation graph. The two-level framework also avoids the high training complexity when modeling concurrency and interleaving together in a unified CRF model. Experimental results show that our method can effectively improve recognition accuracies on several real-world datasets collected from various wireless and sensor networks.",2008,AAAI,1.0
Computational Influence for Training and Entertainment,"An interactive narrative is an education, training, or entertainment experience. There are two qualities that make an experience an interactive narrative. To understand those, let us clarify the meanings of interactive and narrative. Interactive: capable of acting on or influencing each other; and Narrative: presentation of events in a purposeful sequence. Thus, the qualities that define an interactive narrative are autonomy for players to act and intent of the system for the player to experience a narrative prescribed by an author. This conception of interactive narrative is broad and encompasses many types of entertainment and training experiences. One characteristic that sets it apart from other experiences and common games like Chess is that authorial intent often requires the player‚Äôs experience to be dramatic or adhere to some aesthetic. There is a tension between the systematic control required to ensure the intent of the narrative and the player autonomy required for interactivity‚Äî the player-driven exploration that results in an interactive quality is a potential threat to the narrative. Therefore, AI researchers in the field of interactive narrative are interested in balancing the conflicting requirements of autonomy and authorial intent. It is in this area that both I and many interactive narrative researchers have focused their efforts (Roberts and Isbell 2008). While many of the existing research projects have met with success in their own right, they have also uncovered limitations in the state of the art. I plan to complete my dissertation by addressing some of these limitations. The tools of artificial intelligence and machine learning have often been applied in domains where the limits of human ability are stretched. For example, sophisticated AI algorithms retrieve information from the internet, filter spam from email inboxes, and aid doctors in diagnosing illnesses. In each of these cases, a technically-minded AI expert has created a technique to solve a class of problems and has put the power of that technology in the hands of a practitioner. Similarly, I plan to develop AI technologies for computerbased gaming and simulation that when given to game designers will ease their authorial burden and increase their expressive power. The fundamental question I plan to an-",2008,AAAI,0.0
Game Theory Pragmatics: A Challenge for AI,"Game theory has been playing an increasingly visible role in computer science in general and AI in particular, most notably in the area of multiagent systems. I briefly list the areas where most of the action has been in the past decade or so. I then suggest that going forward, the most dramatic interaction between computer science and game theory ‚Äì with a special role for AI ‚Äì could be around what might be called game theory pragmatics.",2008,AAAI,0.0
How Good is Almost Perfect?,"Heuristic search using algorithms such as A and IDA is the prevalent method for obtaining optimal sequential solutions for classical planning tasks. Theoretical analyses of these classical search algorithms, such as the well-known results of Pohl, Gaschnig and Pearl, suggest that such heuristic search algorithms can obtain better than exponential scaling behaviour, provided that the heuristics are accurate enough. Here, we show that for a number of common planning benchmark domains, including ones that admit optimal solution in polynomial time, general search algorithms such as A must necessarily explore an exponential number of search nodes even under the optimistic assumption of almost perfect heuristic estimators, whose heuristic error is bounded by a small additive constant. Our results shed some light on the comparatively bad performance of optimal heuristic search approaches in ‚Äúsimple‚Äù planning domains such as GRIPPER. They suggest that in many applications, further improvements in run-time require changes to other parts of the search algorithm than the heuristic estimator.",2008,AAAI,0.30000000000000004
Robustness and Generalization of Role Sets: PropBank vs. VerbNet,"This paper presents an empirical study on the robustness and generalization of two alternative role sets for semantic role labeling: PropBank numbered roles and VerbNet thematic roles. By testing a state–of–the–art SRL system with the two alternative role annotations, we show that the PropBank role set is more robust to the lack of verb–specific semantic information and generalizes better to infrequent and unseen predicates. Keeping in mind that thematic roles are better for application needs, we also tested the best way to generate VerbNet annotation. We conclude that tagging first PropBank roles and mapping into VerbNet roles is as effective as training and tagging directly on VerbNet, and more robust for domain shifts.",2008,ACL,0.0
A Discriminative Latent Variable Model for Statistical Machine Translation,"Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.",2008,ACL,0.6000000000000001
Forest Reranking: Discriminative Parsing with Non-Local Features,"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.",2008,ACL,1.0
Interactive Visualization for Computational Linguistics,"Interactive information visualization is an emerging and powerful research technique that can be used to understand models of language and their abstract representations. Much of what computational linguists fall back upon to improve NLP applications and to model language “understanding” is structure that has, at best, only an indirect attestation in observable data. An important part of our research progress thus depends on our ability to fully investigate, explain, and explore these structures, both empirically and relative to accepted linguistic theory. The sheer complexity of these abstract structures, and the observable patterns on which they are based, usually limits their accessibility — often even to the researchers creating or attempting to learn them. To aid in this understanding, visual ‘externalizations’ are used for presentation and explanation — traditional statistical graphs and custom-designed illustrations fill the pages of ACL papers. These visualizations provide post hoc insight into the representations and algorithms designed by researchers, but visualization can also assist in the process of research itself. There are special statistical methods, falling under the rubric of “exploratory data analysis,” and visualization techniques just for this purpose, in fact, but these are not widely used or even known in CL. These techniques offer the potential for revealing structure and detail in data, before anyone else has noticed them. When observing natural language engineers at work, we also notice that, even without a formal visualization background, they often create sketches to aid in their understanding and communication of complex structures. These are ad hoc visualizations, but they, too, can be extended by taking advantage of current information visualization research. This tutorial will enable members of the ACL community to leverage information visualization theory into exploratory data analysis, algorithm design, and data presentation techniques for their own research. We draw on fundamental studies in cognitive psychology to introduce ‘visual variables’ — visual dimensions on which data can be encoded. We also discuss the use of interaction and animation to enhance the usability and usefulness of visualizations. Topics covered in this tutorial include a review of information visualization techniques that are applicable to CL, pointers to existing visualization tools and programming toolkits, and new directions in visualizing CL data and results. We also discuss the challenges of evaluating visualizations, noting differences from the evaluation methods traditionally used in CL, and discuss some heuristic approaches and techniques used for measuring insight. Information visualizations in CL research can also be measured by the impact they have on algorithm and data structure design. Information visualization is also filled with opportunities to make more creative visualizations that benefit from the CL community’s deeper collective understanding of natural language. Given that most visualizations of language are created by researchers with little or no linguistic expertise, we’ll cover some open and very ripe possibilities for improving the state of the art in text-based visualizations.",2008,ACL,0.0
Dimensions of Subjectivity in Natural Language,"Current research in automatic subjectivity analysis deals with various kinds of subjective statements involving human attitudes and emotions. While all of them are related to subjectivity, these statements usually touch on multiple dimensions such as non-objectivity1, uncertainty, vagueness, non-objective measurability, imprecision, and ambiguity, which are inherently different. This paper discusses the differences and relations of six dimensions of subjectivity. Conceptual and linguistic characteristics of each dimension will be demonstrated under different contexts.",2008,ACL,0.0
Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries,"Automatic summarization evaluation is critical to the development of summarization systems. While ROUGE has been shown to correlate well with human evaluation for content match in text summarization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE. In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization. Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries.",2008,ACL,-0.2
Kernels on Linguistic Structures for Answer Extraction,"Natural Language Processing (NLP) for Information Retrieval has always been an interesting and challenging research area. Despite the high expectations, most of the results indicate that successfully using NLP is very complex. In this paper, we show how Support Vector Machines along with kernel functions can effectively represent syntax and semantics. Our experiments on question/answer classification show that the above models highly improve on bag-of-words on a TREC dataset.",2008,ACL,0.9
Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation,"In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task.",2008,ACL,0.2
Enriching Morphologically Poor Languages for Statistical Machine Translation,"We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For English–Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%.",2008,ACL,1.0
Building Practical Spoken Dialog Systems,"This tutorial will give a practical description of the free software Carnegie Mellon Olympus 2 Spoken Dialog Architecture. Building real working dialog systems that are robust enough for the general public to use is difficult. Most frequently, the functionality of the conversations is severely limited down to simple question-answer pairs. While offthe-shelf toolkits help the development of such simple systems, they do not support more advanced, natural dialogs nor do they offer the transparency and flexibility required by computational linguistic researchers. However, Olympus 2 offers a complete dialog system with automatic speech recognition (Sphinx) and synthesis (SAPI, Festival) and has been used, along with previous versions of Olympus, for teaching and research at Carnegie Mellon and elsewhere for some 5 years. Overall, a dozen dialog systems have been built using various versions of Olympus, handling tasks ranging from providing bus schedule information to guidance through maintenance procedures for complex machinery, to personal calendar management. In addition to simplifying the development of dialog systems, Olympus provides a transparent platform for teaching and conducting research on all aspects of dialog systems, including speech recognition and synthesis, natural language understanding and generation, and dialog and interaction management. The tutorial will give a brief introduction to spoken dialog systems before going into detail about how to create your own dialog system within Olympus 2, using the Let's Go bus information system as an example. Further, we will provide guidelines on how to use an actual deployed spoken dialog system such as Let's Go to validate research results in the real world. As a possible testbed for such research, we will describe Let's Go Lab, which provides access to both the Let's Go system and its genuine user population for research experiments.",2008,ACL,0.0
Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations,"Finding temporal and causal relations is crucial to understanding the semantic structure of a text. Since existing corpora provide no parallel temporal and causal annotations, we annotated 1000 conjoined event pairs, achieving inter-annotator agreement of 81.2% on temporal relations and 77.8% on causal relations. We trained machine learning models using features derived from WordNet and the Google N-gram corpus, and they outperformed a variety of baselines, achieving an F-measure of 49.0 for temporals and 52.4 for causals. Analysis of these models suggests that additional data will improve performance, and that temporal information is crucial to causal relation identification.",2008,ACL,1.0
Introduction to Computational Advertising,"Web advertising is the primary driving force behind many Web activities, including Internet search as well as publishing of online content by third-party providers. Even though the notion of online advertising barely existed a decade ago, the topic is so complex that it attracts attention of a variety of established scientific disciplines, including computational linguistics, computer science, economics, psychology, and sociology, to name but a few. Consequently, a new discipline — Computational Advertising — has emerged, which studies the process of advertising on the Internet from a variety of angles. A successful advertising campaign should be relevant to the immediate user’s information need as well as more generally to user’s background and personalized interest profile, be economically worthwhile to the advertiser and the intermediaries (e.g., the search engine), as well as be aesthetically pleasant and not detrimental to user experience.",2008,ACL,0.0
Assessing Dialog System User Simulation Evaluation Measures Using Human Judges,"Previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems’ logs. However, the validity of these automatic measures has not been fully proven. In this study, we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures. We observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives. However, the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models. When building prediction models of human judgments using previously proposed automatic measures, we find that we cannot reliably predict human ratings using a regression model, but we can predict human rankings by a ranking model.",2008,ACL,-0.1
A Logical Basis for the D Combinator and Normal Form in CCG,"The standard set of rules defined in Combinatory Categorial Grammar (CCG) fails to provide satisfactory analyses for a number of syntactic structures found in natural languages. These structures can be analyzed elegantly by augmenting CCG with a class of rules based on the combinator D (Curry and Feys, 1958). We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG’s rule base (Baldridge, 2002). We also show how Eisner’s (1996) normal form constraints follow from this logic, ensuring that the D rules do not lead to spurious ambiguities.",2008,ACL,-0.30000000000000004
Finding Contradictions in Text,"Detecting conflicting statements is a foundational text understanding task with applications in information analysis. We propose an appropriate definition of contradiction for NLP tasks and develop available corpora, from which we construct a typology of contradictions. We demonstrate that a system for contradiction needs to make more fine-grained distinctions than the common systems for entailment. In particular, we argue for the centrality of event coreference and therefore incorporate such a component based on topicality. We present the first detailed breakdown of performance on this task. Detecting some types of contradiction requires deeper inferential paths than our system is capable of, but we achieve good performance on types arising from negation and antonymy.",2008,ACL,0.5
Better Alignments = Better Translations?,"Automatic word alignment is a key step in training statistical machine translation systems. Despite much recent work on word alignment methods, alignment accuracy increases often produce little or no improvements in machine translation quality. In this work we analyze a recently proposed agreement-constrained EM algorithm for unsupervised alignment models. We attempt to tease apart the effects that this simple but effective modification has on alignment precision and recall trade-offs, and how rare and common words are affected across several language pairs. We propose and extensively evaluate a simple method for using alignment models to produce alignments better-suited for phrase-based MT systems, and show significant gains (as measured by BLEU score) in end-to-end translation systems for six languages pairs used in recent MT competitions.",2008,ACL,1.0
Recent Improvements in the CMU Large Scale Chinese-English SMT System,"In this paper we describe recent improvements to components and methods used in our statistical machine translation system for ChineseEnglish used in the January 2008 GALE evaluation. Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach.",2008,ACL,1.0
Analyzing the Errors of Unsupervised Learning,"We identify four types of errors that unsupervised induction systems make and study each one in turn. Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.",2008,ACL,0.8
Parsing Noun Phrase Structure with CCG,"Statistical parsing of noun phrase (NP) structure has been hampered by a lack of goldstandard data. This is a significant problem for CCGbank, where binary branching NP derivations are often incorrect, a result of the automatic conversion from the Penn Treebank. We correct these errors in CCGbank using a gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%.",2008,ACL,0.8
Construct State Modification in the Arabic Treebank,"Earlier work in parsing Arabic has speculated that attachment to construct state constructions decreases parsing performance. We make this speculation precise and define the problem of attachment to construct state constructions in the Arabic Treebank. We present the first statistics that quantify the problem. We provide a baseline and the results from a first attempt at a discriminative learning procedure for this task, achieving 80% accuracy.",2008,ACL,0.5
ModelTalker Voice Recorder-An Interface System for Recording a Corpus of Speech for Synthesis,"We will demonstrate the ModelTalker Voice Recorder (MT Voice Recorder) – an interface system that lets individuals record and bank a speech database for the creation of a synthetic voice. The system guides users through an automatic calibration process that sets pitch, amplitude, and silence. The system then prompts users with both visual (text-based) and auditory prompts. Each recording is screened for pitch, amplitude and pronunciation and users are given immediate feedback on the acceptability of each recording. Users can then rerecord an unacceptable utterance. Recordings are automatically labeled and saved and a speech database is created from these recordings. The system’s intention is to make the process of recording a corpus of utterances relatively easy for those inexperienced in linguistic analysis. Ultimately, the recorded corpus and the resulting speech database is used for concatenative synthetic speech, thus allowing individuals at home or in clinics to create a synthetic voice in their own voice. The interface may prove useful for other purposes as well. The system facilitates the recording and labeling of large corpora of speech, making it useful for speech and linguistic research, and it provides immediate feedback on pronunciation, thus making it useful as a clinical learning tool. 1 Demonstration 1.1 MT Voice Recorder Background While most of us are familiar with the highly intelligible but somewhat robotic sound of synthetic speech, for the approximately 2 million people in the United States with a limited ability to communicate vocally (Matas et al., 1985), these synthetic voices are inadequate. The restricted number of available voices lack the personalization they desire. While intelligibility is a priority for these individuals, almost equally important is the naturalness and individuality one associates with one’s own voice. Individuals with difficulty speaking can be any age, gender, and from any part of the country, with regional dialects and idiosyncratic variations. Each individual deserves to speak with a voice that is not only intelligible, but uniquely his or her own. For those with degenerative diseases such as Amyotrophic Lateral Sclerosis (ALS), knowing they will be losing the voice that has become intricately associated with their identity is not only traumatic to the individual but to family and friends as well. A form of synthesis that incorporates the qualities of individual voices is concatenative synthesis. In this type of synthesis, units of recorded speech are appended. By using recorded speech, many of the voice qualities of the person recording the speech remain in the resulting synthetic voice. Different synthesis systems append different sized",2008,ACL,0.0
Last Words: What's the Future for Computational Linguistics,"You are reading the last issue of Computational Linguistics that will appear in printed hardcopy form. The beginning of 2009 heralds a new era for the journal in at least two major respects: As of the first issue of volume 35, Computational Linguistics will be published only electronically, and it will be open access. As editor, I’d like to take this opportunity to use these last words that will tumble from the presses to provide some explanation of the changes afoot at the journal, and to offer some thoughts on where this might take us in the future.",2008,CL,0.0
Erratum: Dependency Parsing of Turkish,"In Section 5 of the article “Dependency Parsing of Turkish” by Gülşen Eryiğit, Joakim Nivre, and Kemal Oflazer (September 2008, Vol. 34, No. 3: 357–389), some abbreviations were misinterpreted during the copyediting process. The third sentence of Section 5.2 should be as follows: “We use an unlexicalized feature model where the parser uses only the minor part-of-speech category (as POS) and dependency type of tokens (as DEP) and compare the results with the probabilistic parser.” The first sentence of the second paragraph of Section 5.2.1 should start as follows: “We take the minor part-of-speech category. . . .” The “POS” abbreviation used on page 20 should be read as “minor part-ofspeech,” and the “POS” abbreviations on pages 21, 26, 27, and 28 should be read as “part-of-speech.”",2008,CL,-1.0
Fast Mapping in Word Learning: What Probabilities Tell Us,"Children can determine the meaning of a new word from hearing it used in a familiar context—an ability often referred to as fast mapping. In this paper, we study fast mapping in the context of a general probabilistic model of word learning. We use our model to simulate fast mapping experiments on children, such as referent selection and retention. The word learning model can perform these tasks through an inductive interpretation of the acquired probabilities. Our results suggest that fast mapping occurs as a natural consequence of learning more words, and provides explanations for the (occasionally contradictory) child experimental data.",2008,CoNLL,0.0
Applying Sentence Simplification to the CoNLL-2008 Shared Task,"Our submission to the CoNLL-2008 shared task (Surdeanu et al., 2008) focused on applying a novel method for semantic role labeling to the shared task. Our system first simplifies each sentence to be labeled using a set of hand-constructed rules; the weights of the system are trained on semantic role labeling data to generate simplifications which are as useful as possible for semantic role labeling. Our system is only a semantic role labeling system, and thus did not receive a score for Syntactic Dependencies (or, by extension, a score for the complete problem). Unlike most systems in the shared task, our system took constituency parses as input. On the subtask of semantic dependencies, our system obtained an F1 score of 76.17, the highest in the open task. In this paper we give a high-level overview of the sentence simplification system, and discuss and analyze the modifications to this system required for the CoNLL-2008 shared task. 1 Sentence Simplification The main technical interest of our method is a sentence simplification system. This system is described in depth in (Vickrey and Koller, 2008); for lack of space, we omit many details, including a discussion of related work, from this paper. Current semantic role labeling systems rely primarily on syntactic features in order to identify and classify roles. Features derived from a syntactic parse of the sentence have proven particularly useful (Gildea and Jurafsky, 2002). For example, the syntactic subject of “eat” is nearly always the c © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. S",2008,CoNLL,1.0
Collective Semantic Role Labelling with Markov Logic,"This paper presents our system for the Open Track of the CoNLL 2008 Shared Task (Surdeanu et al., 2008) in Joint Dependency Parsing1 and Semantic Role Labelling. We use Markov Logic to define a joint SRL model and achieve a semantic F-score of 74.59%, the second best in the Open Track.",2008,CoNLL,1.0
Discriminative Learning of Syntactic and Semantic Dependencies,"A Maximum Entropy Model based system for discriminative learning of syntactic and semantic dependencies submitted to the CoNLL-2008 shared task (Surdeanu, et al., 2008) is presented in this paper. The system converts the dependency learning task to classification issues and reconstructs the dependent relations based on classification results. Finally F1 scores of 86.69, 69.95 and 78.35 are obtained for syntactic dependencies, semantic dependencies and the whole system respectively in closed challenge. For open challenge the corresponding F1 scores are 86.69, 68.99 and 77.84.",2008,CoNLL,1.0
Adapting a Lexicalized-Grammar Parser to Contrasting Domains,"Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue. In this paper we demonstrate that a CCG parser can be adapted to two new domains, biomedical text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation.",2008,EMNLP,0.6000000000000001
A Simple and Effective Hierarchical Phrase Reordering Model,"While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).",2008,EMNLP,0.7000000000000001
LTAG Dependency Parsing with Bidirectional Incremental Construction,"In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set.",2008,EMNLP,1.0
N-gram Weighting: Reducing Training Data Mismatch in Cross-Domain Language Model Estimation,"In domains with insufficient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora. Since the ngrams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on features derived from readily available segmentation and metadata information for each corpus. Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task.",2008,EMNLP,1.0
A Generative Model for Parsing Natural Language to Meaning Representations,"In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.",2008,EMNLP,1.0
Word Sense Disambiguation Using OntoNotes: An Empirical Study,"The accuracy of current word sense disambiguation (WSD) systems is affected by the fine-grained sense inventory of WordNet as well as a lack of training examples. Using the WSD examples provided through OntoNotes, we conduct the first large-scale WSD evaluation involving hundreds of word types and tens of thousands of sense-tagged examples, while adopting a coarse-grained sense inventory. We show that though WSD systems trained with a large number of examples can obtain a high level of accuracy, they nevertheless suffer a substantial drop in accuracy when applied to a different domain. To address this issue, we propose combining a domain adaptation technique using feature augmentation with active learning. Our results show that this approach is effective in reducing the annotation effort required to adapt a WSD system to a new domain. Finally, we propose that one can maximize the dual benefits of reducing the annotation effort while ensuring an increase in WSD accuracy, by only performing active learning on the set of most frequently occurring word types.",2008,EMNLP,0.7000000000000001
Bridging Lexical Gaps between Queries and Questions on Large Online Q&A Collections with Compact Translation Models,"Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.",2008,EMNLP,-0.1
"It's a Contradiction - no, it's not: A Case Study using Functional Relations","Contradiction Detection (CD) in text is a difficult NLP task. We investigate CD over functions (e.g., BornIn(Person)=Place), and present a domain-independent algorithm that automatically discovers phrases denoting functions with high precision. Previous work on CD has investigated hand-chosen sentence pairs. In contrast, we automatically harvested from the Web pairs of sentences that appear contradictory, but were surprised to find that most pairs are in fact consistent. For example, “Mozart was born in Salzburg” does not contradict “Mozart was born in Austria” despite the functional nature of the phrase “was born in”. We show that background knowledge about meronyms (e.g., Salzburg is in Austria), synonyms, functions, and more is essential for success in the CD task.",2008,EMNLP,1.0
Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms,"B is the de facto standard for evaluation and development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments.",2008,EMNLP,0.2
Automatic induction of FrameNet lexical units,"Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNetbased models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined.",2008,EMNLP,0.7000000000000001
Query-Focused Summaries or Query-Biased Summaries?,"In the context of the Document Understanding Conferences, the task of Query-Focused Multi-Document Summarization is intended to improve agreement in content among humangenerated model summaries. Query-focus also aids the automated summarizers in directing the summary at specific topics, which may result in better agreement with these model summaries. However, while query focus correlates with performance, we show that highperforming automatic systems produce summaries with disproportionally higher query term density than human summarizers do. Experimental evidence suggests that automatic systems heavily rely on query term occurrence and repetition to achieve good performance.",2009,ACL,-0.4
Improving Tree-to-Tree Translation with Packed Forests,"Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically. The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees.",2009,ACL,0.8
Correlating Human and Automatic Evaluation of a German Surface Realiser,"We examine correlations between native speaker judgements on automatically generated German text against automatic evaluation metrics. We look at a number of metrics from the MT and Summarisation communities and find that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements. In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the human judgements and the automatic metrics was quite weak.",2009,ACL,0.0
Co-Training for Cross-Lingual Sentiment Classification,"The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.",2009,ACL,1.0
Cross-Domain Dependency Parsing Using a Deep Linguistic Grammar,"Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain. In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar. The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.†",2009,ACL,0.8
Computational Modeling of Human Language Acquisition,"The nature and amount of information needed for learning a natural language, and the underlying mechanisms involved in this process, are the subject of much debate: is it possible to learn a language from usage data only, or some sort of innate knowledge and/or bias is needed to boost the process? This is a topic of interest to (psycho)linguists who study human language acquisition, as well as computational linguists who develop the knowledge sources necessary for largescale natural language processing systems. Children are a source of inspiration for any such study of language learnability. They learn language with ease, and their acquired knowledge of language is flexible and robust. Human language acquisition has been studied for centuries, but using computational modeling for such studies is a relatively recent trend. However, computational approaches to language learning have become increasing popular, mainly due to the advances in developing machine learning techniques, and the availability of vast collections of experimental data on child language learning and child-adult interaction. Many of the existing computational models attempt to study the complex task of learning a language under the cognitive plausibility criteria (such as memory and processing limitations that humans face), as well as to explain the developmental patterns observed in children. Such computational studies can provide insight into the plausible mechanisms involved in human language acquisition, and be a source of inspiration for developing better language models and techniques.",2009,ACL,0.0
Directional Distributional Similarity for Lexical Expansion,"Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion.",2009,ACL,0.8
From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression?,"Most previous studies on meeting summarization have focused on extractive summarization. In this paper, we investigate if we can apply sentence compression to extractive summaries to generate abstractive summaries. We use different compression algorithms, including integer linear programming with an additional step of filler phrase detection, a noisychannel approach using Markovization formulation of grammar rules, as well as human compressed sentences. Our experiments on the ICSI meeting corpus show that when compared to the abstractive summaries, using sentence compression on the extractive summaries improves their ROUGE scores; however, the best performance is still quite low, suggesting the need of language generation for abstractive summarization.",2009,ACL,0.30000000000000004
The Modulation of Cooperation and Emotion in Dialogue: The REC Corpus,"In this paper we describe the Rovereto Emotive Corpus (REC) which we collected to investigate the relationship between emotion and cooperation in dialogue tasks. It is an area where still many unsolved questions are present. One of the main open issues is the annotation of the socalled “blended” emotions and their recognition. Usually, there is a low agreement among raters in annotating emotions and, surprisingly, emotion recognition is higher in a condition of modality deprivation (i. e. only acoustic or only visual modality vs. bimodal display of emotion). Because of these previous results, we collected a corpus in which “emotive” tokens are pointed out during the recordings by psychophysiological indexes (ElectroCardioGram, and Galvanic Skin Conductance). From the output values of these indexes a general recognition of each emotion arousal is allowed. After this selection we will annotate emotive interactions with our multimodal annotation scheme, performing a kappa statistic on annotation results to validate our coding scheme. In the near future, a logistic regression on annotated data will be performed to find out correlations between cooperation and negative emotions. A final step will be an fMRI experiment on emotion recognition of blended emotions from face displays.",2009,ACL,1.0
Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study,"Manually annotated corpora are valuable but scarce resources, yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards. This seems to be a great waste of human efforts, and it would be nice to automatically adapt one annotation standard to another. We present a simple yet effective strategy that transfers knowledge from a differently annotated corpus to the corpus with desired annotation. We test the efficacy of this method in the context of Chinese word segmentation and part-of-speech tagging, where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese. Experiments show that adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank results in significant improvements in both segmentation and tagging accuracies (with error reductions of 30.2% and 14%, respectively), which in turn helps improve Chinese parsing accuracy.",2009,ACL,0.9
The Contribution of Linguistic Features to Automatic Machine Translation Evaluation,"A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested. However, n-gram based metrics are still today the dominant approach. The main reason is that the advantages of employing deeper linguistic information have not been clarified yet. In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics. We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics. Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance.",2009,ACL,0.9
"Who, What, When, Where, Why? Comparing Multiple Approaches to the Cross-Lingual 5W Task","Cross-lingual tasks are especially difficult due to the compounding effect of errors in language processing and errors in machine translation (MT). In this paper, we present an error analysis of a new cross-lingual task: the 5W task, a sentence-level understanding task which seeks to return the English 5W's (Who, What, When, Where and Why) corresponding to a Chinese sentence. We analyze systems that we developed, identifying specific problems in language processing and MT that cause errors. The best cross-lingual 5W system was still 19% worse than the best monolingual 5W system, which shows that MT significantly degrades sentence-level understanding. Neither source-language nor targetlanguage analysis was able to circumvent problems in MT, although each approach had advantages relative to the other. A detailed error analysis across multiple systems suggests directions for future research on the problem.",2009,ACL,-1.0
Dependency Grammar Induction via Bitext Projection Constraints,"Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.",2009,ACL,0.9
"Book Review: Learning Machine Translation by Cyril Goutte, Nicola Cancedda, Marc Dymetman, and George Foster (editors)","Attending recent computational linguistics conferences, it is hard to ignore the phenomenal amount of research devoted to statistical machine translation (SMT). Driven by the wide availability of open-source translation systems, corpora, and evaluation tools, a research area that was once the preserve of large research groups has become accessible to those of more modest resources. Although the current state-of-the-art SMT systems have matured into robust commercial systems, capable of providing reasonable quality translations for a variety of domains, they remain limited by naive modeling assumptions and a heavy reliance on heuristics. These limitations have led researchers to ask the question of whether the adoption of techniques from the machine learning literature could allow more complex translations to be modeled effectively. As such, this book, focused on the application of machine learning to SMT, is particularly timely in capturing the current interest of the machine translation community. Learning Machine Translation is presented in two parts. The first, titled “Enabling Technologies,” focuses on research peripheral to machine translation. Topics covered include the acquisition of parallel corpora, cross-language named-entity processing, and language modeling. The second part covers core machine translation system building, presenting a number of approaches applying discriminative machine learning techniques within a SMT decoder. Much of the content of the book arose from the Machine Learning for Multilingual AccessWorkshop held at the Neural Information Processing conference in 2006. As SMT is not a frequent topic at that conference, the bridging of research from the mainstream machine learning community with research on MT is particularly promising. A fine example of this cross-over is Chapter 9, “Kernel-Based Machine Translation,” in which a novel approach to estimating translation models is presented. However, this promise is not entirely fulfilled, as some contributions either fail to make use of machine learning or are somewhat obscure, unlikely to impact on the mainstream SMT community.",2009,CL,-0.4
Unsupervised Type and Token Identification of Idiomatic Expressions,"Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them. They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics. In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition. Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text. We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context.",2009,CL,0.6000000000000001
An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems,"There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.",2009,CL,-0.5
Lexical Patterns or Dependency Patterns: Which Is Better for Hypernym Extraction?,"We compare two different types of extraction patterns for automatically deriving semantic information from text: lexical patterns, built from words and word class information, and dependency patterns with syntactic information obtained from a full parser. We are particularly interested in whether the richer linguistic information provided by a parser allows for a better performance of subsequent information extraction work. We evaluate automatic extraction of hypernym information from text and conclude that the application of dependency patterns does not lead to substantially higher precision and recall scores than using lexical patterns.",2009,CoNLL,0.0
Investigating Automatic Alignment Methods for Slide Generation from Academic Papers,"In this paper we investigate the task of automatic generation of slide presentations from academic papers, focusing initially on slide to paper alignment. We compare and evaluate four different alignment systems which utilize various combinations of methods used widely in other alignment and question answering approaches, such as TF-IDF term weighting and query expansion. Our best aligner achieves an accuracy of 75% and our findings show that for this application, average TF-IDF scoring performs more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance.",2009,CoNLL,-0.4
Joint Inference for Natural Language Processing,"of the Invited Talk In recent decades, researchers in natural language processing have made great progress on welldefined subproblems such as part-of-speech tagging, phrase chunking, syntactic parsing, named-entity recognition, coreference and semantic-role labeling. Better models, features, and learning algorithms have allowed systems to perform many of these tasks with 90% accuracy or better. However, success in integrated, end-to-end natural language understanding",2009,CoNLL,-0.2
Interactive Feature Space Construction using Semantic Information,"Specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classifiers. Effectively incorporating information regarding semantically related words into the feature space is known to produce robust, accurate classifiers and is one apparent motivation for efforts to automatically generate such resources. However, naive incorporation of this semantic information may result in poor performance due to increased ambiguity. To overcome this limitation, we introduce the interactive feature space construction protocol, where the learner identifies inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources. We demonstrate effectiveness on an entity and relation extraction system including both performance improvements and robustness to reductions in annotated data.",2009,CoNLL,0.7000000000000001
A Comparison of Model Free versus Model Intensive Approaches to Sentence Compression,"This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it compares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). It is found that a model free approach significantly outperforms T3 on the particular data we created from the Internet. We also discuss what might have caused T3’s poor performance.",2009,EMNLP,-0.2
Bilingual dictionary generation for low-resourced language pairs,"Bilingual dictionaries are vital resources in many areas of natural language processing. Numerous methods of machine translation require bilingual dictionaries with large coverage, but less-frequent language pairs rarely have any digitalized resources. Since the need for these resources is increasing, but the human resources are scarce for less represented languages, efficient automatized methods are needed. This paper introduces a fully automated, robust pivot language based bilingual dictionary generation method that uses the WordNet of the pivot language to build a new bilingual dictionary. We propose the usage of WordNet in order to increase accuracy; we also introduce a bidirectional selection method with a flexible threshold to maximize recall. Our evaluations showed 79% accuracy and 51% weighted recall, outperforming representative pivot language based methods. A dictionary generated with this method will still need manual post-editing, but the improved recall and precision decrease the work of human correctors.",2009,EMNLP,0.9
Feasibility of Human-in-the-loop Minimum Error Rate Training,"Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU.",2009,EMNLP,1.0
Semi-Supervised Learning for Semantic Relation Classification using Stratified Sampling Strategy,"This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification. First, the training data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set. We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance. Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself. This suggests that a proper sampling strategy is critical in semi-supervised learning.",2009,EMNLP,0.9
EEG responds to conceptual stimuli and corpus semantics,"Mitchell et al. (2008) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fMRI. This could be a very powerful technique for evaluating conceptual models extracted from corpora; however, fMRI is expensive and imposes strong constraints on data collection. Following on experiments that demonstrated that EEG activation patterns encode enough information to discriminate broad conceptual categories, we show that corpus-based semantic representations can predict EEG activation patterns with significant accuracy, and we evaluate the relative performance of different corpus-models on this task.",2009,EMNLP,0.6000000000000001
Phrase Dependency Parsing for Opinion Mining,"In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing.",2009,EMNLP,1.0
Effective Use of Linguistic and Contextual Information for Statistical Machine Translation,"Current methods of using lexical features in machine translation have difficulty in scaling up to realistic MT tasks due to a prohibitively large number of parameters involved. In this paper, we propose methods of using new linguistic and contextual features that do not suffer from this problem and apply them in a state-ofthe-art hierarchical MT system. The features used in this work are non-terminal labels, non-terminal length distribution, source string context and source dependency LM scores. The effectiveness of our techniques is demonstrated by significant improvements over a strong baseline. On Arabic-to-English translation, improvements in lower-cased BLEU are 2.0 on NIST MT06 and 1.7 on MT08 newswire data on decoding output. On Chinese-to-English translation, the improvements are 1.0 on MT06 and 0.8 on MT08 newswire data.",2009,EMNLP,0.9
"It's Not You, it's Me: Detecting Flirting and its Misperception in Speed-Dates","Automatically detecting human social intentions from spoken conversation is an important task for dialogue understanding. Since the social intentions of the speaker may differ from what is perceived by the hearer, systems that analyze human conversations need to be able to extract both the perceived and the intended social meaning. We investigate this difference between intention and perception by using a spoken corpus of speed-dates in which both the speaker and the listener rated the speaker on flirtatiousness. Our flirtationdetection system uses prosodic, dialogue, and lexical features to detect a speaker’s intent to flirt with up to 71.5% accuracy, significantly outperforming the baseline, but also outperforming the human interlocuters. Our system addresses lexical feature sparsity given the small amount of training data by using an autoencoder network to map sparse lexical feature vectors into 30 compressed features. Our analysis shows that humans are very poor perceivers of intended flirtatiousness, instead often projecting their own intended behavior onto their interlocutors.",2009,EMNLP,0.6000000000000001
Empirical Exploitation of Click Data for Task Specific Ranking,"There have been increasing needs for task specific rankings in web search such as rankings for specific query segments like long queries, time-sensitive queries, navigational queries, etc; or rankings for specific domains/contents like answers, blogs, news, etc. In the spirit of ”divide-andconquer”, task specific ranking may have potential advantages over generic ranking since different tasks have task-specific features, data distributions, as well as featuregrade correlations. A critical problem for the task-specific ranking is training data insufficiency, which may be solved by using the data extracted from click log. This paper empirically studies how to appropriately exploit click data to improve rank function learning in task-specific ranking. The main contributions are 1) the exploration on the utilities of two promising approaches for click pair extraction; 2) the analysis of the role played by the noise information which inevitably appears in click data extraction; 3) the appropriate strategy for combining training data and click data; 4) the comparison of click data which are consistent and inconsistent with baseline function.",2009,EMNLP,0.0
Mining Search Engine Clickthrough Log for Matching N-gram Features,"User clicks on a URL in response to a query are extremely useful predictors of the URL’s relevance to that query. Exact match click features tend to suffer from severe data sparsity issues in web ranking. Such sparsity is particularly pronounced for new URLs or long queries where each distinct query-url pair will rarely occur. To remedy this, we present a set of straightforward yet informative query-url n-gram features that allows for generalization of limited user click data to large amounts of unseen query-url pairs. The method is motivated by techniques leveraged in the NLP community for dealing with unseen words. We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing “form” tend to lead to clicks on URLs containing “pdf”. We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value < 0.0001 level over a strong baseline with exact match clickthrough features.",2009,EMNLP,0.8
STAT: Speech Transcription Analysis Tool,"The Speech Transcription Analysis Tool (STAT) is an open source tool for aligning and comparing two phonetically transcribed texts of human speech. The output analysis is a parameterized set of phonological differences. These differences are based upon a selectable set of binary phonetic features such as [voice], [continuant], [high], etc. STAT was initially designed to provide sets of phonological speech patterns in the comparisons of various English accents found in the Speech Accent Archive http://accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, forensic linguistics, and speech recognition.",2009,NAACL,0.8
Multi-scale Personalization for Voice Search Applications,"Voice Search applications provide a very convenient and direct access to a broad variety of services and information. However, due to the vast amount of information available and the open nature of the spoken queries, these applications still suffer from recognition errors. This paper explores the utilization of personalization features for the post-processing of recognition results in the form of n-best lists. Personalization is carried out from three different angles: short-term, long-term and Web-based, and a large variety of features are proposed for use in a log-linear classification framework. Experimental results on data obtained from a commercially deployed Voice Search system show that the combination of the proposed features leads to a substantial sentence error rate reduction. In addition, it is shown that personalization features which are very different in nature can successfully complement each other.",2009,NAACL,0.4
The independence of dimensions in multidimensional dialogue act annotation,"This paper presents empirical evidence for the orthogonality of the DIT multidimensional dialogue act annotation scheme, showing that the ten dimensions of communication which underlie this scheme are addressed independently in natural dialogue.",2009,NAACL,1.0
Assessing and Improving the Performance of Speech Recognition for Incremental Systems,"In incremental spoken dialogue systems, partial hypotheses about what was said are required even while the utterance is still ongoing. We define measures for evaluating the quality of incremental ASR components with respect to the relative correctness of the partial hypotheses compared to hypotheses that can optimize over the complete input, the timing of hypothesis formation relative to the portion of the input they are about, and hypothesis stability, defined as the number of times they are revised. We show that simple incremental post-processing can improve stability dramatically, at the cost of timeliness (from 90% of edits of hypotheses being spurious down to 10% at a lag of 320ms). The measures are not independent, and we show how system designers can find a desired operating point for their ASR. To our knowledge, we are the first to suggest and examine a variety of measures for assessing incremental ASR and improve performance on this basis.",2009,NAACL,1.0
The Role of Implicit Argumentation in Nominal SRL,"Nominals frequently surface without overtly expressed arguments. In order to measure the potential benefit of nominal SRL for downstream processes, such nominals must be accounted for. In this paper, we show that a state-of-the-art nominal SRL system with an overall argument F1 of 0.76 suffers a performance loss of more than 9% when nominals with implicit arguments are included in the evaluation. We then develop a system that takes implicit argumentation into account, improving overall performance by nearly 5%. Our results indicate that the degree of implicit argumentation varies widely across nominals, making automated detection of implicit argumentation an important step for nominal SRL.",2009,NAACL,0.6000000000000001
Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition,"We analyze the recognition errors made by a morph-based continuous speech recognition system, which practically allows an unlimited vocabulary. Examining the role of the acoustic and language models in erroneous regions shows how speaker adaptive training (SAT) and discriminative training with minimum phone frame error (MPFE) criterion decrease errors in different error classes. Analyzing the errors with respect to word frequencies and manually classified error types reveals the most potential areas for improving the system.",2009,NAACL,-0.2
A Fully Unsupervised Word Sense Disambiguation Method Using Dependency Knowledge,"Word sense disambiguation is the process of determining which sense of a word is used in a given context. Due to its importance in understanding semantics of natural languages, word sense disambiguation has been extensively studied in Computational Linguistics. However, existing methods either are brittle and narrowly focus on specific topics or words, or provide only mediocre performance in real-world settings. Broad coverage and disambiguation quality are critical for a word sense disambiguation system. In this paper we present a fully unsupervised word sense disambiguation method that requires only a dictionary and unannotated text as input. Such an automatic approach overcomes the problem of brittleness suffered in many existing methods and makes broad-coverage word sense disambiguation feasible in practice. We evaluated our approach using SemEval 2007 Task 7 (Coarse-grained English All-words Task), and our system significantly outperformed the best unsupervised system participating in SemEval 2007 and achieved the performance approaching top-performing supervised systems. Although our method was only tested with coarse-grained sense disambiguation, it can be directly applied to fine-grained sense disambiguation.",2009,NAACL,0.8
An Iterative Reinforcement Approach for Fine-Grained Opinion Mining,"With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently. Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context. From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem. More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information. The experimental results demonstrate that our approach outperforms the template extraction based approaches.",2009,NAACL,0.8
Improved Syntactic Models for Parsing Speech with Repairs,"This paper introduces three new syntactic models for representing speech with repairs. These models are developed to test the intuition that the erroneous parts of speech repairs (reparanda) are not generated or recognized as such while occurring, but only after they have been corrected. Thus, they are designed to minimize the differences in grammar rule applications between fluent and disfluent speech containing similar structure. The three models considered in this paper are also designed to isolate the mechanism of impact, by systematically exploring different variables.",2009,NAACL,1.0
Pronunciation Modeling in Spelling Correction for Writers of English as a Foreign Language,"We propose a method for modeling pronunciation variation in the context of spell checking for non-native writers of English. Spell checkers, typically developed for native speakers, fail to address many of the types of spelling errors peculiar to non-native speakers, especially those errors influenced by differences in phonology. Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by Toutanova and Moore (2002), which includes models for both orthography and pronunciation. The pronunciation variation modeling is shown to improve performance for misspellings produced by Japanese writers of English.",2009,NAACL,0.9
Search Engine Adaptation by Feedback Control Adjustment for Time-sensitive Query,"We propose a new method to rank a special category of time-sensitive queries that are year qualified. The method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher. Our method, which is based on feedback control theory, uses ranking errors to adjust the search engine behavior. For this purpose, we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents. Our method was tested on a commercial search engine. The experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed.",2009,NAACL,1.0
Bayesian Belief Polarization,"Empirical studies have documented cases of belief polarization, where two people with opposing prior beliefs both strengthen their beliefs after observing the same evidence. Belief polarization is frequently offered as evidence of human irrationality, but we demonstrate that this phenomenon is consistent with a fully Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the set of Bayesian models that we consider. Suppose that Carol has requested a promotion at her company and has received a score of 50 on an aptitude test. Alice, one of the company‚Äôs managers, began with a high opinion of Carol and became even more confident of her abilities after seeing her test score. Bob, another manager, began with a low opinion of Carol and became even less confident about her qualifications after seeing her score. On the surface, it may appear that either Alice or Bob is behaving irrationally, since the same piece of evidence has led them to update their beliefs about Carol in opposite directions. This situation is an example of belief polarization [1, 2], a widely studied phenomenon that is often taken as evidence of human irrationality [3, 4]. In some cases, however, belief polarization may appear much more sensible when all the relevant information is taken into account. Suppose, for instance, that Alice was familiar with the aptitude test and knew that it was scored out of 60, but that Bob was less familiar with the test and assumed that the score was a percentage. Even though only one interpretation of the score can be correct, Alice and Bob have both made rational inferences given their assumptions about the test. Some instances of belief polarization are almost certain to qualify as genuine departures from rational inference, but we argue in this paper that others will be entirely compatible with a rational approach. Distinguishing between these cases requires a precise normative standard against which human inferences can be compared. We suggest that Bayesian inference provides this normative standard, and present a set of Bayesian models that includes cases where polarization can and cannot emerge. Our work is in the spirit of previous studies that use careful rational analyses in order to illuminate apparently irrational human behavior (e.g. [5, 6, 7]). Previous studies of belief polarization have occasionally taken a Bayesian approach, but often the goal is to show how belief polarization can emerge as a consequence of approximate inference in a Bayesian model that is subject to memory constraints or processing limitations [8]. In contrast, we demonstrate that some examples of polarization are compatible with a fully Bayesian approach. Other formal accounts of belief polarization have relied on complex versions of utility theory [9], or have focused on continuous hypothesis spaces [10] unlike the discrete hypothesis spaces usually considered by psychological studies of belief polarization. We focus on discrete hypothesis spaces and require no additional machinery beyond the basics of Bayesian inference. We begin by introducing the belief revision phenomena considered in this paper and developing a Bayesian approach that clarifies whether and when these phenomena should be considered irrational. We then consider several Bayesian models that are capable of producing belief polarization and illustrate them with concrete examples. Having demonstrated that belief polarization is compatible",2009,NIPS,0.0
Which graphical models are difficult to learn?,"We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).",2009,NIPS,-1.0
Complexity of Decentralized Control: Special Cases,"The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases. Some reductions in complexity have been achieved by exploiting independence relations in some models. We show that these results are somewhat limited: when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.",2009,NIPS,-1.0
Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning,"A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-specific knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence ‚Äì the absence of a sentence ‚Äì when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on how the learning problem is presented.",2009,NIPS,0.0
No evidence for active sparsification in the visual cortex,"The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.",2009,NIPS,-0.8
Accurate Context-Free Parsing with Combinatory Categorial Grammar,"The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.",2010,ACL,0.8
On the Computational Complexity of Dominance Links in Grammatical Formalisms,"Dominance links were introduced in grammars to model long distance scrambling phenomena, motivating the definition of multiset-valued linear indexed grammars (MLIGs) by Rambow (1994b), and inspiring quite a few recent formalisms. It turns out that MLIGs have since been rediscovered and reused in a variety of contexts, and that the complexity of their emptiness problem has become the key to several open questions in computer science. We survey complexity results and open issues on MLIGs and related formalisms, and provide new complexity bounds for some linguistically motivated restrictions.",2010,ACL,-0.2
Tackling Sparse Data Issue in Machine Translation Evaluation,We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.,2010,ACL,0.5
A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices,"Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector",2010,ACL,1.0
"Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information","This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.",2010,ACL,-0.8
Hard Constraints for Grammatical Function Labelling,"For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.",2010,ACL,0.1
Cognitively Plausible Models of Human Language Processing,"We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.",2010,ACL,-0.8
Generating Image Descriptions Using Dependency Relational Patterns,"This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an image’s location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.",2010,ACL,1.0
Complexity Assumptions in Ontology Verbalisation,"We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology develop-",2010,ACL,-0.1
Wide-Coverage NLP with Linguistically Expressive Grammars,"In recent years, there has been a lot of research on wide-coverage statistical natural language processing with linguistically expressive grammars such as Combinatory Categorial Grammars (CCG), Head-driven Phrase-Structure Grammars (HPSG), Lexical-Functional Grammars (LFG) and Tree-Adjoining Grammars (TAG). But although many young researchers in natural language processing are very well trained in machine learning and statistical methods, they often lack the necessary background to understand the linguistic motivation behind these formalisms. Furthermore, in many linguistics departments, syntax is still taught from a purely Chomskian perspective. Additionally, research on these formalisms often takes place within tightly-knit, formalismspecific subcommunities. It is therefore often difficult for outsiders as well as experts to grasp the commonalities of and differences between these formalisms.",2010,ACL,-0.5
Importance of Linguistic Constraints in Statistical Dependency Parsing,"Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for",2010,ACL,-0.2
Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study,"Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search.",2010,ACL,0.0
From Structured Prediction to Inverse Reinforcement Learning,"Machine learning is all about making predictions; language is full of complex rich structure. Structured prediction marries these two. However, structured prediction isn’t always enough: sometimes the world throws even more complex data at us, and we need reinforcement learning techniques. This tutorial is all about the how and the why of structured prediction and inverse reinforcement learning (aka inverse optimal control): participants should walk away comfortable that they could implement many structured prediction and IRL algorithms, and have a sense of which ones might work for which problems.",2010,ACL,0.0
Coreference Resolution with Reconcile,"Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference resolution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.",2010,ACL,0.7000000000000001
Exemplar-Based Models for Word Meaning in Context,"This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.",2010,ACL,1.0
On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds,"An important relation in information extraction is the part-whole relation. Ontological studies mention several types of this relation. In this paper, we show that the traditional practice of initializing minimally-supervised algorithms with a single set that mixes seeds of different types fails to capture the wide variety of part-whole patterns and tuples. The results obtained with mixed seeds ultimately converge to one of the part-whole relation types. We also demonstrate that all the different types of part-whole relations can still be discovered, regardless of the type characterized by the initializing seeds. We performed our experiments with a state-ofthe-art information extraction algorithm.",2010,ACL,-0.9
Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.,"Researchers in textual entailment have begun to consider inferences involving downward-entailing operators, an interesting and important class of lexical items that change the way inferences are made. Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality collection of negative polarity items (NPIs). However, English is one of the very few languages for which such a list exists. We propose the first approach that can be applied to the many languages for which there is no pre-existing high-precision database of NPIs. As a case study, we apply our method to Romanian and show that our method yields good results. Also, we perform a cross-linguistic analysis that suggests interesting connections to some findings in linguistic typology.",2010,ACL,0.6000000000000001
Last Words: What Computational Linguists Can Learn from Psychologists (and Vice Versa),"Sometimes I am amazed by how much the field of computational linguistics has changed in the past 15 to 20 years. In the mid 1990s, I was working at a research institute where language and speech technologists worked in relatively close quarters. Speech technology seemed on the verge of a major breakthrough; this was around the time that Bill Gates was quoted in Business Week as saying that speech was not just the future of Windows, but the future of computing itself. At the same time, language technology was, well, nowhere. Bill Gates certainly wasn’t championing language technology in those days. And while the possible applications of speech technology seemed endless (who would use a keyboard in 2010, when speech-driven user interfaces would have replaced traditional computers?), the language people were thinking hard about possible applications for their admittedly somewhat immature technologies. Predicting the future is a tricky thing. No major breakthrough came for speech technology—I am still typing this. However, language technology did change almost beyond recognition. Perhaps one of the main reasons for this has been the explosive growth of the Internet, which helped language technology in two different ways. On the one hand it instigated the development and refinement of techniques needed for searching in document collections of unprecedented size, on the other it resulted in a large increase of freely available text data. Recently, language technology has been particularly successful for tasks where huge amounts of textual data is available to which statistical machine learning techniques can be applied (Halevy, Norvig, and Pereira 2009). As a result of these developments, mainstream computational linguistics is now a successful, application-oriented discipline which is particularly good at extracting information from sequences of words. But there is more to language than that. For speakers, words are the result of a complex speech production process; for listeners, they are what starts off the similarly complex comprehension process. However, in many current applications no attention is given to the processes by which words are produced nor to the processes by which they can be understood. Language is treated as a product not as a process, in the terminology of Clark (1996). In addition, we use language not only as a vehicle for factual information exchange; speakers may realise all sorts of other intentions with their words: They may want to convince others to do or buy something, they may want to induce a particular emotion in the addressee, and so forth. These days, most of computational linguistics (with a few notable exceptions, more about which subsequently) has little to",2010,CL,-0.1
Commentary and Discussion: Reply to Rao et al. and Lee et al.,"In the last issue of this journal, I presented a piece that called into question some of the techniques reported in two papers in high-profile journals that purported to provide statistical evidence for the linguistic status of some ancient symbol systems (Sproat 2010a). Not surprisingly, the authors of those two papers took issue with a number of my claims, and have requested the opportunity to respond. The two responses, taken together, are rather lengthy and as a result it is not possible, given limitations of space, for me to address each and every one of their criticisms. I will therefore focus on what I consider to be the most important objections.",2010,CL,0.0
"Commentary and Discussion: A Response to Richard Sproat on Random Systems, Writing, and Entropy","In his article “Ancient symbols and computational linguistics” (Sproat 2010), Professor Sproat raised two concerns over a method that we have proposed for analyzing small data sets of symbols using entropy (Lee, Jonathan, and Ziman 2010): first, that the method is unable to detect random but non-equiprobable systems; and second, that it misclassifies kudurru texts. We address these concerns in the following response.",2010,CL,0.0
Briefly Noted: Essential Programming for Linguistics,"This book is of only marginal use as a Perl primer for linguists or as a university textbook. Thick with explicatory text in 10-point Minion font, some chapters, such as that on references and modules, are consequently very difficult to work through, and are written in sometimes abstruse language (“The other thing is something to do with how we use functions,” p. 55), which further impedes full comprehension. The book’s exercises become increasingly arduous (number 24, for example, asks the reader to build a basic concordancer using the localizer “my,” which is not explained until 20 pages later), and readers must therefore resort to downloading Weisser’s solutions and trying to decipher the code themselves. Furthermore, these solutions provide programs that are usually too basic for genuine linguistic research, such as a dictionary-compiler into which the user must type every entry by hand. Because Weisser makes little reference to other work, readers leave this book with the discouraging feeling that no usable Perl code exists for linguists, and that they are still unable to write their own. The book’s greatest shortcomings are in the areas of subroutines, modules, and objects. Making only a single passing reference to CPAN in his chapter on modularity, Weisser refers to only three modules (pp. 110–112) but never provides a functioning program that uses them. Because one of Perl’s greatest strengths is in the reusability of its modules, it is disappointing to find so little on them in this book. Objects are also a great strength of Perl, but Weisser dismisses them in a dense fourpage chapter which ends, as usual, with the acknowledgment that the examples he has provided can only be made useful by writing a program “which, however, I shall leave to you as a further exercise” (p. 120). Linguists seeking a primer or classroom textbook on Perl are probably still best served by Michael Hammond’s Programming for Linguists (Blackwell, 2003), which, although it has slightly fewer examples of linguistically relevant code, brings readers to about the same skill level that Weisser’s book does, and with greater clarity.—Brian Ó Broin, William Paterson University",2010,CL,-0.5
Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai,"The past decade has seen a steady growth of interest in statistical language models for information retrieval, and much research work has been conducted on this subject. This book by ChengXiang Zhai summarizes most of this research. It opens with an introduction covering the basic concepts of information retrieval and statistical languagemodels, presenting the intuitions behind these concepts. This introduction is then followed by a chapter providing an overview of:",2010,CL,0.0
Squibs: On Paraphrase and Coreference,"Paraphrase extraction1 and coreference resolution have applications in Question Answering, Information Extraction, Machine Translation, and so forth. Paraphrase pairs might be coreferential, and coreference relations are sometimes paraphrases. The two overlap considerably (Hirst 1981), but their definitions make them significantly different in essence: Paraphrasing concerns meaning, whereas coreference is about discourse referents. Thus, they do not always coincide. In the following example, b and d are both coreferent and paraphrastic, whereas a, c, e, f, and h are coreferent but not paraphrastic, and g and i are paraphrastic but not coreferent.",2010,CL,0.0
"Last Words: Ancient Symbols, Computational Linguistics, and the Reviewing Practices of the General Science Journals","Few archaeological finds are as evocative as artifacts inscribed with symbols. Whenever an archaeologist finds a potsherd or a seal impression that seems to have symbols scratched or impressed on the surface, it is natural to want to “read” the symbols. And if the symbols come from an undeciphered or previously unknown symbol system it is common to ask what language the symbols supposedly represent and whether the system can be deciphered. Of course the first question that really should be asked is whether the symbols are in fact writing. A writing system, as linguists usually define it, is a symbol system that is used to represent language. Familiar examples are alphabets such as the Latin, Greek, Cyrillic, or Hangul alphabets, alphasyllabaries such as Devanagari or Tamil, syllabaries such as Cherokee or Kana, and morphosyllabic systems like Chinese characters. But symbol systems that do not encode language abound: European heraldry, mathematical notation, labanotation (used to represent dance), and Boy Scout merit badges are all examples of symbol systems that represent things, but do not function as part of a system that represents language. Whether an unknown system is writing or not is a difficult question to answer. It can only be answered definitively in the affirmative if one can develop a verifiable decipherment into some language or languages. Statistical techniques have been used in decipherment for years, but these have always been used under the assumption that the system one is dealing with is writing, and the techniques are used to uncover patterns or regularities that might aid in the decipherment. Patterns of symbol distribution might suggest that a symbol system is not linguistic: For example, odd repetition patterns might make it seem that a symbol system is unlikely to be writing. But until recently nobody had argued that statistical techniques could be used to determine that a system is linguistic.1 It was therefore quite a surprise when, in April 2009, there appeared in Science a short article by Rajesh Rao of the University of Washington and colleagues at two research institutes in India that purported to provide such a measure (Rao et al. 2009a). Rao et al.’s claim, which we will describe in more detail in the next section, was that",2010,CL,0.0
Inspecting the Structural Biases of Dependency Parsing Algorithms,"We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to underand overproduce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and firstand second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses.",2010,CoNLL,1.0
A Lucene and Maximum Entropy Model Based Hedge Detection System,"This paper describes the approach to hedge detection we developed, in order to participate in the shared task at CoNLL2010. A supervised learning approach is employed in our implementation. Hedge cue annotations in the training data are used as the seed to build a reliable hedge cue set. Maximum Entropy (MaxEnt) model is used as the learning technique to determine uncertainty. By making use of Apache Lucene, we are able to do fuzzy string match to extract hedge cues, and to incorporate part-of-speech (POS) tags in hedge cues. Not only can our system determine the certainty of the sentence, but is also able to find all the contained hedges. Our system was ranked third on the Wikipedia dataset. In later experiments with different parameters, we further improved our results, with a 0.612 F-score on the Wikipedia dataset, and a 0.802 F-score on the biological dataset.",2010,CoNLL,1.0
Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint,"Modern unsupervised POS taggers usually apply an optimization procedure to a nonconvex function, and tend to converge to local maxima that are sensitive to starting conditions. The quality of the tagging induced by such algorithms is thus highly variable, and researchers report average results over several random initializations. Consequently, applications are not guaranteed to use an induced tagging of the quality reported for the algorithm. In this paper we address this issue using an unsupervised test for intrinsic clustering quality. We run a base tagger with different random initializations, and select the best tagging using the quality test. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. We show that the correlation between our quality test and gold standard-based tagging quality measures is high. Our results are better in most evaluation measures than all results reported in the literature for this task, and are always better than the Clark average results.",2010,CoNLL,1.0
"(Invited Talk) Clueless: Explorations in Unsupervised, Knowledge-Lean Extraction of Lexical-Semantic Information","Why is it that if you know I’ll give a silly talk, it follows that you know I’ll give a talk, whereas if you doubt I’ll give a good talk, it doesn’t follow that you doubt I’ll give a talk? This pair of examples shows that the word “doubt” exhibits a special but prevalent kind of behavior known as downward entailingness — the licensing of reasoning from supersets to subsets, so to speak, but not vice versa. The first project I’ll describe is to identify words that are downward entailing, a task that promises to enhance the performance of systems that engage in textual inference, and one that is quite challenging since it is difficult to characterize these items as a class and no corpus with downward-entailingness annotations exists. We are able to surmount these challenges by utilizing some insights from the linguistics literature regarding the relationship between downward entailing operators and what are known as negative polarity items — words such as “ever” or the idiom “have a clue” that tend to occur only in negative contexts. A cross-linguistic analysis indicates some potentially interesting connections to findings in linguistic typology.",2010,CoNLL,0.30000000000000004
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text,"The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results. The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.",2010,CoNLL,0.0
Combining Manual Rules and Supervised Learning for Hedge Cue and Scope Detection,"Hedge cues were detected using a supervised Conditional Random Field (CRF) classifier exploiting features from the RASP parser. The CRF’s predictions were filtered using known cues and unseen instances were removed, increasing precision while retaining recall. Rules for scope detection, based on the grammatical relations of the sentence and the part-ofspeech tag of the cue, were manuallydeveloped. However, another supervised CRF classifier was used to refine these predictions. As a final step, scopes were constructed from the classifier output using a small set of post-processing rules. Development of the system revealed a number of issues with the annotation scheme adopted by the organisers.",2010,CoNLL,-0.2
Evaluating Models of Latent Document Semantics in the Presence of OCR Errors,"Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unprocessed OCR output in the case of LDA. To our knowledge, this study is the first of its kind.",2010,EMNLP,-0.2
It Depends on the Translation: Unsupervised Dependency Parsing via Word Alignment,"We reveal a previously unnoticed connection between dependency parsing and statistical machine translation (SMT), by formulating the dependency parsing task as a problem of word alignment. Furthermore, we show that two well known models for these respective tasks (DMV and the IBM models) share common modeling assumptions. This motivates us to develop an alignment-based framework for unsupervised dependency parsing. The framework (which will be made publicly available) is flexible, modular and easy to extend. Using this framework, we implement several algorithms based on the IBM alignment models, which prove surprisingly effective on the dependency parsing task, and demonstrate the potential of the alignment-based approach.",2010,EMNLP,1.0
Further Meta-Evaluation of Broad-Coverage Surface Realization,"We present the first evaluation of the utility of automatic evaluation metrics on surface realizations of Penn Treebank data. Using outputs of the OpenCCG and XLE realizers, along with ranked WordNet synonym substitutions, we collected a corpus of generated surface realizations. These outputs were then rated and post-edited by human annotators. We evaluated the realizations using seven automatic metrics, and analyzed correlations obtained between the human judgments and the automatic scores. In contrast to previous NLG meta-evaluations, we find that several of the metrics correlate moderately well with human judgments of both adequacy and fluency, with the TER family performing best overall. We also find that all of the metrics correctly predict more than half of the significant systemlevel differences, though none are correct in all cases. We conclude with a discussion of the implications for the utility of such metrics in evaluating generation in the presence of variation. A further result of our research is a corpus of post-edited realizations, which will be made available to the research community.",2010,EMNLP,0.8
Lessons Learned in Part-of-Speech Tagging of Conversational Speech,"This paper examines tagging models for spontaneous English speech transcripts. We analyze the performance of state-of-the-art tagging models, either generative or discriminative, left-to-right or bidirectional, with or without latent annotations, together with the use of ToBI break indexes and several methods for segmenting the speech transcripts (i.e., conversation side, speaker turn, or humanannotated sentence). Based on these studies, we observe that: (1) bidirectional models tend to achieve better accuracy levels than left-toright models, (2) generative models seem to perform somewhat better than discriminative models on this task, and (3) prosody improves tagging performance of models on conversation sides, but has much less impact on smaller segments. We conclude that, although the use of break indexes can indeed significantly improve performance over baseline models without them on conversation sides, tagging accuracy improves more by using smaller segments, for which the impact of the break indexes is marginal.",2010,EMNLP,0.0
Negative Training Data Can be Harmful to Text Classification,"This paper studies the effects of training data on binary text classification and postulates that negative training data is not needed and may even be harmful for the task. Traditional binary classification involves building a classifier using labeled positive and negative training examples. The classifier is then applied to classify test instances into positive and negative classes. A fundamental assumption is that the training and test data are identically distributed. However, this assumption may not hold in practice. In this paper, we study a particular problem where the positive data is identically distributed but the negative data may or may not be so. Many practical text classification and retrieval applications fit this model. We argue that in this setting negative training data should not be used, and that PU learning can be employed to solve the problem. Empirical evaluation has been conducted to support our claim. This result is important as it may fundamentally change the current binary classification paradigm.",2010,EMNLP,-0.1
A Fast Augmented Lagrangian Algorithm for Learning Low-Rank Matrices.,"We propose a general and efficient algorithm for learning low-rank matrices. The proposed algorithm converges super-linearly and can keep the matrix to be learned in a compact factorized representation without the need of specifying the rank beforehand. Moreover, we show that the framework can be easily generalized to the problem of learning multiple matrices and general spectral regularization. Empirically we show that we can recover a 10,000√ó10,000 matrix from 1.2 million observations in about 5 minutes. Furthermore, we show that in a brain-computer interface problem, the proposed method can speed-up the optimization by two orders of magnitude against the conventional projected gradient method and produces more reliable solutions.",2010,ICML,0.9
Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity.,"We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to recover the common set of relevant inputs for each output cluster. Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso. Our structured regularization is based on a grouplasso penalty, where groups are defined with respect to the tree structure. We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap. We present an efficient optimization method that can handle a largescale problem. Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.",2010,ICML,1.0
Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains.,"In the realm of multilabel classification (MLC), it has become an opinio communis that optimal predictive performance can only be achieved by learners that explicitly take label dependence into account. The goal of this paper is to elaborate on this postulate in a critical way. To this end, we formalize and analyze MLC within a probabilistic setting. Thus, it becomes possible to look at the problem from the point of view of risk minimization and Bayes optimal prediction. Moreover, inspired by our probabilistic setting, we propose a new method for MLC that generalizes and outperforms another approach, called classifier chains, that was recently introduced in the literature.",2010,ICML,1.0
Learning Sparse SVM for Feature Selection on Very High Dimensional Datasets.,"A sparse representation of Support Vector Machines (SVMs) with respect to input features is desirable for many applications. In this paper, by introducing a 0-1 control variable to each input feature, l0-norm Sparse SVM (SSVM) is converted to a mixed integer programming (MIP) problem. Rather than directly solving this MIP, we propose an efficient cutting plane algorithm combining with multiple kernel learning to solve its convex relaxation. A global convergence proof for our method is also presented. Comprehensive experimental results on one synthetic and 10 real world datasets show that our proposed method can obtain better or competitive performance compared with existing SVM-based feature selection methods in term of sparsity and generalization performance. Moreover, our proposed method can effectively handle large-scale and extremely high dimensional problems.",2010,ICML,1.0
SVM Classifier Estimation from Group Probabilities.,"A learning problem that has only recently gained attention in the machine learning community is that of learning a classifier from group probabilities. It is a learning task that lies somewhere between the well-known tasks of supervised and unsupervised learning, in the sense that for a set of observations we do not know the labels, but for some groups of observations, the frequency distribution of the label is known. This learning problem has important practical applications, for example in privacy-preserving data mining. This paper presents an approach to learn a classifier from group probabilities based on support vector regression and the idea of inverting a classifier calibration process. A detailed analysis will show that this new approach outperforms existing approaches.",2010,ICML,1.0
Learning the Linear Dynamical System with ASOS.,"We develop a new algorithm, based on EM, for learning the Linear Dynamical System model. Called the method of Approximated Second-Order Statistics (ASOS) our approach achieves dramatically superior computational performance over standard EM through its use of approximations, which we justify with both intuitive explanations and rigorous convergence results. In particular, after an inexpensive precomputation phase, the iterations of ASOS can be performed in time independent of the length of the training dataset.",2010,ICML,1.0
Learning Efficiently with Approximate Inference via Dual Losses.,"Many structured prediction tasks involve complex models where inference is computationally intractable, but where it can be well approximated using a linear programming relaxation. Previous approaches for learning for structured prediction (e.g., cuttingplane, subgradient methods, perceptron) repeatedly make predictions for some of the data points. These approaches are computationally demanding because each prediction involves solving a linear program to optimality. We present a scalable algorithm for learning for structured prediction. The main idea is to instead solve the dual of the structured prediction loss. We formulate the learning task as a convex minimization over both the weights and the dual variables corresponding to each data point. As a result, we can begin to optimize the weights even before completely solving any of the individual prediction problems. We show how the dual variables can be efficiently optimized using coordinate descent. Our algorithm is competitive with state-of-the-art methods such as stochastic subgradient and cutting-plane.",2010,ICML,1.0
Exploiting Data-Independence for Fast Belief-Propagation.,"Maximum a posteriori (MAP) inference in graphical models requires that we maximize the sum of two terms: a data-dependent term, encoding the conditional likelihood of a certain labeling given an observation, and a data-independent term, encoding some prior on labelings. Often, data-dependent factors contain fewer latent variables than dataindependent factors ‚Äì for instance, many grid and tree-structured models contain only firstorder conditionals despite having pairwise priors. In this paper, we note that MAPinference in such models can be made substantially faster by appropriately preprocessing their data-independent terms. Our main result is to show that message-passing in any such pairwise model has an expected-case exponent of only 1.5 on the number of states per node, leading to significant improvements over existing quadratic-time solutions.",2010,ICML,1.0
Active Learning for Networked Data.,"We introduce a novel active learning algorithm for classification of network data. In this setting, training instances are connected by a set of links to form a network, the labels of linked nodes are correlated, and the goal is to exploit these dependencies and accurately label the nodes. This problem arises in many domains, including social and biological network analysis and document classification, and there has been much recent interest in methods that collectively classify the nodes in the network. While in many cases labeled examples are expensive, often network information is available. We show how an active learning algorithm can take advantage of network structure. Our algorithm effectively exploits the links between instances and the interaction between the local and collective aspects of a classifier to improve the accuracy of learning from fewer labeled examples. We experiment with two real-world benchmark collective classification domains, and show that we are able to achieve extremely accurate results even when only a small fraction of the data is labeled.",2010,ICML,0.7000000000000001
Transfer Learning for Collective Link Prediction in Multiple Heterogenous Domains.,"Link prediction is a key technique in many applications such as recommender systems, where potential links between users and items need to be predicted. A challenge in link prediction is the data sparsity problem. In this paper, we address this problem by jointly considering multiple heterogeneous link prediction tasks such as predicting links between users and different types of items including books, movies and songs, which we refer to as the collective link prediction (CLP) problem. We propose a nonparametric Bayesian framework for solving the CLP problem, which allows knowledge to be adaptively transferred across heterogeneous tasks while taking into account the similarities between tasks. We learn the inter-task similarity automatically. We also introduce link functions for different tasks to correct their biases and skewness of distributions in their link data. We conduct experiments on several real world datasets and demonstrate significant improvements over several existing state-of-the-art methods.",2010,ICML,1.0
FAB-MAP: Appearance-Based Place Recognition and Mapping using a Learned Visual Vocabulary Model.,"We present an overview of FAB-MAP, an algorithm for place recognition and mapping developed for infrastructure-free mobile robot navigation in large environments. The system allows a robot to identify when it is revisiting a previously seen location, on the basis of imagery captured by the robot‚Äôs camera. We outline a complete probabilistic framework for the task, which is applicable even in visually repetitive environments where many locations may appear identical. Our work introduces a number of technical innovations notably we demonstrate that place recognition performance can be improved by learning an approximation to the joint distribution over visual elements. We also investigate several principled approaches to making the system robust in visually repetitive environments, and define an efficient bail-out strategy for multi-hypothesis testing to improve system speed. Our model has been shown to substantially outperform standard tf-idf ranking on our task of interest. We demonstrate the system performing reliable online appearance mapping and loop closure detection over a 1,000 km trajectory, with mean filter update times of 14ms.",2010,ICML,0.9
On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning.,"A learning problem might have several measures of complexity (e.g., norm and dimensionality) that affect the generalization error. What is the interaction between these complexities? Dimension-free learning theory bounds and parametric asymptotic analyses each provide a partial picture of the full learning curve. In this paper, we use high-dimensional asymptotics on two classical problems‚Äîmean estimation and linear regression‚Äîto explore the learning curve more completely. We show that these curves exhibit multiple regimes, where in each regime, the excess risk is controlled by a subset of the problem complexities.",2010,ICML,0.0
The Role of Machine Learning in Business Optimization.,"In a trend that reflects the increasing demand for intelligent applications driven by business data, IBM today is building out a significant number of applications that leverage machine learning technologies to optimize business process decisions. This talk highlights this trend; and describes the many different ways in which leading edge machine learning concepts are being utilized in business applications developed by IBM for its internal use and for clients.",2010,ICML,0.0
Learning to Link Entities with Knowledge Base,"This paper address the problem of entity linking. Specifically, given an entity mentioned in unstructured texts, the task is to link this entity with an entry stored in the existing knowledge base. This is an important task for information extraction. It can serve as a convenient gateway to encyclopedic information, and can greatly improve the web users’ experience. Previous learning based solutions mainly focus on classification framework. However, it’s more suitable to consider it as a ranking problem. In this paper, we propose a learning to rank algorithm for entity linking. It effectively utilizes the relationship information among the candidates when ranking. The experiment results on the TAC 20091 dataset demonstrate the effectiveness of our proposed framework. The proposed method achieves 18.5% improvement in terms of accuracy over the classification models for those entities which have corresponding entries in the Knowledge Base. The overall performance of the system is also better than that of the state-of-the-art methods.",2010,NAACL,1.0
Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems,The use of well-nested linear context-free rewriting systems has been empirically motivated for modeling of the syntax of languages with discontinuous constituents or relatively free word order. We present a chart-based parsing algorithm that asymptotically improves the known running time upper bound for this class of rewriting systems. Our result is obtained through a linear space construction of a binary normal form for the grammar at hand.,2010,NAACL,1.0
Generalizing Hierarchical Phrase-based Translation using Rules with Adjacent Nonterminals,"Hierarchical phrase-based translation (Hiero, (Chiang, 2005)) provides an attractive framework within which both shortand longdistance reorderings can be addressed consistently and ef ciently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero's generalization power and lead to signi cant performance gains in Chinese-English translation.",2010,NAACL,0.5
Products of Random Latent Variable Grammars,"We show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-ofthe-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.",2010,NAACL,1.0
"On Automated Evaluation of Readability of Summaries: Capturing Grammaticality, Focus, Structure and Coherence","Readability of a summary is usually graded manually on five aspects of readability: grammaticality, coherence and structure, focus, referential clarity and non-redundancy. In the context of automated metrics for evaluation of summary quality, content evaluations have been presented through the last decade and continue to evolve, however a careful examination of readability aspects of summary quality has not been as exhaustive. In this paper we explore alternative evaluation metrics for ‘grammaticality’ and ‘coherence and structure’ that are able to strongly correlate with manual ratings. Our results establish that our methods are able to perform pair-wise ranking of summaries based on grammaticality, as strongly as ROUGE is able to distinguish for content evaluations. We observed that none of the five aspects of readability are independent of each other, and hence by addressing the individual criterion of evaluation we aim to achieve automated appreciation of readability of summaries.",2010,NAACL,0.5
Can Recognising Multiword Expressions Improve Shallow Parsing?,"There is significant evidence in the literature that integrating knowledge about multiword expressions can improve shallow parsing accuracy. We present an experimental study to quantify this improvement, focusing on compound nominals, proper names and adjectivenoun constructions. The evaluation set of multiword expressions is derived from WordNet and the textual data are downloaded from the web. We use a classification method to aid human annotation of output parses. This method allows us to conduct experiments on a large dataset of unannotated data. Experiments show that knowledge about multiword expressions leads to an increase of between 7.5% and 9.5% in accuracy of shallow parsing in sentences containing these multiword expressions.",2010,NAACL,1.0
Testing a Grammar Customization System with Sahaptin,"I briefly describe a system for automatically creating an implemented grammar of a natural language based on answers to a web-based questionnaire, then present a grammar of Sahaptin, a language of the Pacific Northwest with complex argument-marking and agreement patterns, that was developed to test the system. The development of this grammar has proved useful in three ways: (1) verifying the correct functioning of the grammar customization system, (2) motivating the addition of a new pattern of agreement to the system, and (3) making detailed predictions that uncovered gaps in the linguistic descriptions of Sahaptin.",2010,NAACL,1.0
Towards Cross-Lingual Textual Entailment,"This paper investigates cross-lingual textual entailment as a semantic relation between two text portions in different languages, and proposes a prospective research direction. We argue that cross-lingual textual entailment (CLTE) can be a core technology for several cross-lingual NLP applications and tasks. Through preliminary experiments, we aim at proving the feasibility of the task, and providing a reliable baseline. We also introduce new applications for CLTE that will be explored in future work.",2010,NAACL,1.0
The Best Lexical Metric for Phrase-Based Statistical MT System Optimization,"Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we find that it’s important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model’s structure and features allow it to take advantage of the metric. Contrasting with TER’s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training.",2010,NAACL,0.0
Integer Linear Programming in NLP - Constrained Conditional Models,"Making decisions in natural language processing problems often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate, what assignments are possible. Structured learning problems such as semantic role labeling provide one such example, but the setting is broader and includes a range of problems such as name entity and relation recognition and co-reference resolution. The setting is also appropriate for cases that may require a solution to make use of multiple (possible pre-designed or pre-learned components) as in summarization, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints.",2010,NAACL,0.0
Ensemble Models for Dependency Parsing: Cheap and Good?,"Previous work on dependency parsing used various kinds of combination models but a systematic analysis and comparison of these approaches is lacking. In this paper we implemented such a study for English dependency parsing and find several non-obvious facts: (a) the diversity of base parsers is more important than complex models for learning (e.g., stacking, supervised meta-classification), (b) approximate, linear-time re-parsing algorithms guarantee well-formed dependency trees without significant performance loss, and (c) the simplest scoring model for re-parsing (unweighted voting) performs essentially as well as other more complex models. This study proves that fast and accurate ensemble parsers can be built with minimal effort.",2010,NAACL,0.5
Subword Variation in Text Message Classification,"For millions of people in less resourced regions of the world, text messages (SMS) provide the only regular contact with their doctor. Classifying messages by medical labels supports rapid responses to emergencies, the early identification of epidemics and everyday administration, but challenges include textbrevity, rich morphology, phonological variation, and limited training data. We present a novel system that addresses these, working with a clinic in rural Malawi and texts in the Chichewa language. We show that modeling morphological and phonological variation leads to a substantial average gain of F=0.206 and an error reduction of up to 63.8% for specific labels, relative to a baseline system optimized over word-sequences. By comparison, there is no significant gain when applying the same system to the English translations of the same texts/labels, emphasizing the need for subword modeling in many languages. Language independent morphological models perform as accurately as language specific models, indicating a broad deployment potential.",2010,NAACL,1.0
Investigations into the Crandem Approach to Word Recognition,"We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009). The previous authors’ work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition. As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions. We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors. We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task.",2010,NAACL,1.0
Movie Reviews and Revenues: An Experiment in Text Regression,"We consider the problem of predicting a movie’s opening weekend revenue. Previous work on this problem has used metadata about a movie—e.g., its genre, MPAA rating, and cast—with very limited work making use of text about the movie. In this paper, we use the text of film critics’ reviews from several sources to predict opening weekend revenue. We describe a new dataset pairing movie reviews with metadata and revenue data, and show that review text can substitute for metadata, and even improve over it, for prediction.",2010,NAACL,0.8
Evaluation Metrics for the Lexical Substitution Task,"We identify some problems of the evaluation metrics used for the English Lexical Substitution Task of SemEval-2007, and propose alternative metrics that avoid these problems, which we hope will better guide the future development of lexical substitution systems.",2010,NAACL,-0.4
The Effect of Ambiguity on the Automated Acquisition of WSD Examples,"Several methods for automatically generating labeled examples that can be used as training data for WSD systems have been proposed, including a semisupervised approach based on relevance feedback (Stevenson et al., 2008a). This approach was shown to generate examples that improved the performance of a WSD system for a set of ambiguous terms from the biomedical domain. However, we find that this approach does not perform as well on other data sets. The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result.",2010,NAACL,-1.0
Engaging learning groups using Social Interaction Strategies,"Conversational Agents have been shown to be effective tutors in a wide range of educational domains. However, these agents are often ignored and abused in collaborative learning scenarios involving multiple students. In our work presented here, we design and evaluate interaction strategies motivated from prior research in small group communication. We will discuss how such strategies can be implemented in agents. As a first step towards evaluating agents that can interact socially, we report results showing that human tutors employing these strategies are able to cover more concepts with the students besides being rated as better integrated, likeable and friendlier.",2010,NAACL,0.8
Improving Phrase-Based Translation with Prototypes of Short Phrases,"We investigate methods of generating additional bilingual phrase pairs for a phrasebased decoder by translating short sequences of source text. Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.",2010,NAACL,1.0
For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia,"We report on work in progress on extracting lexical simplifications (e.g., “collaborate” → “work together”), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.",2010,NAACL,1.0
Fast Query for Large Treebanks,"A variety of query systems have been developed for interrogating parsed corpora, or treebanks. With the arrival of efficient, widecoverage parsers, it is feasible to create very large databases of trees. However, existing approaches that use in-memory search, or relational or XML database technologies, do not scale up. We describe a method for storage, indexing, and query of treebanks that uses an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks.",2010,NAACL,1.0
"""cba to check the spelling"": Investigating Parser Performance on Discussion Forum Posts","We evaluate the Berkeley parser on text from an online discussion forum. We evaluate the parser output with and without gold tokens and spellings (using Sparseval and Parseval), and we compile a list of problematic phenomena for this domain. The Parseval f-score for a small development set is 77.56. This increases to 80.27 when we apply a set of simple transformations to the input sentences and to the Wall Street Journal (WSJ) training sections.",2010,NAACL,0.5
Universal Consistency of Multi-Class Support Vector Classification,"Steinwart was the first to prove universal consistency of support vector machine classification. His proof analyzed the ‚Äòstandard‚Äô support vector machine classifier, which is restricted to binary classification problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classification to more than two classes are inconsistent. Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart‚Äôs techniques to the multi-class case. Erratum, 20.01.2011 Unfortunately this paper contains a subtle flaw in the proof of Lemma 5. Furthermore it turns out the statement itself is wrong: The multi-class SVM by Crammer&Singer is not universally consistent.",2010,NIPS,-1.0
An analysis on negative curvature induced by singularity in multi-layer neural-network learning,"In the neural-network parameter space, an attractive field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a flat region). Therefore, it may be confused with ‚Äúattractive‚Äù local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efficient methods have been previously developed that avoided plateaus.",2010,NIPS,-1.0
"Online Learning: Random Averages, Combinatorial Parameters, and Learnability","We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.",2010,NIPS,0.2
PKU_HIT: An Event Detection System Based on Instances Expansion and Rich Syntactic Features,"This paper describes the PKU_HIT system on event detection in the SemEval-2010 Task. We construct three modules for the three sub-tasks of this evaluation. For target verb WSD, we build a Naïve Bayesian classifier which uses additional training instances expanded from an untagged Chinese corpus automatically. For sentence SRL and event detection, we use a feature-based machine learning method which makes combined use of both constituent-based and dependencybased features. Experimental results show that the Macro Accuracy of the WSD module reaches 83.81% and F-Score of the SRL module is 55.71%.",2010,SemEval,1.0
FBK_NK: A WordNet-Based System for Multi-Way Classification of Semantic Relations,"We describe a WordNet-based system for the extraction of semantic relations between pairs of nominals appearing in English texts. The system adopts a lightweight approach, based on training a Bayesian Network classifier using large sets of binary features. Our features consider: i) the context surrounding the annotated nominals, and ii) different types of knowledge extracted from WordNet, including direct and explicit relations between the annotated nominals, and more general and implicit evidence (e.g. semantic boundary collocations). The system achieved a Macro-averaged F1 of 68.02% on the “Multi-Way Classification of Semantic Relations Between Pairs of Nominals” task (Task #8) at SemEval-2010.",2010,SemEval,1.0
SemEval-2010 Task 17: All-Words Word Sense Disambiguation on a Specific Domain,"Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. This task presented all-words datasets on the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain.",2010,SemEval,0.7000000000000001
SUCRE: A Modular System for Coreference Resolution,"This paper presents SUCRE, a new software tool for coreference resolution and its feature engineering. It is able to separately do noun, pronoun and full coreference resolution. SUCRE introduces a new approach to the feature engineering of coreference resolution based on a relational database model and a regular feature definition language. SUCRE successfully participated in SemEval-2010 Task 1 on Coreference Resolution in Multiple Languages (Recasens et al., 2010) for gold and regular closed annotation tracks of six languages. It obtained the best results in several categories, including the regular closed annotation tracks of English and German.",2010,SemEval,1.0
SemEval-2010 Task 7: Argument Selection and Coercion,"We describe the Argument Selection and Coercion task for the SemEval-2010 evaluation exercise. This task involves characterizing the type of compositional operation that exists between a predicate and the arguments it selects. Specifically, the goal is to identify whether the type that a verb selects is satisfied directly by the argument, or whether the argument must change type to satisfy the verb typing. We discuss the problem in detail, describe the data preparation for the task, and analyze the results of the submissions.",2010,SemEval,0.0
Annotating ESL Errors: Challenges and Rewards,"In this paper, we present a corrected and error- tagged corpus of essays written by non-native speakers of English. The corpus contains 63000 words and includes data by learners of English of nine first language backgrounds. The annotation was performed at the sentence level and involved correcting all errors in the sentence. Error classification includes mis- takes in preposition and article usage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agree- ment on the task. We also describe a computer program that was developed to facilitate and standardize the an- notation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus.",2010,WS,1.0
Syntactic Constraints on Phrase Extraction for Phrase-Based Machine Translation,"A typical phrase-based machine transla- tion (PBMT) system uses phrase pairs extracted from word-aligned parallel corpora. All phrase pairs that are consis- tent with word alignments are collected. The resulting phrase table is very large and includes many non-syntactic phrases which may not be necessary. We propose to filter the phrase table based on source language syntactic constraints. Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words. Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when com- pared to a baseline PBMT system with full-size tables.",2010,WS,1.0
Towards Internet-Age Pharmacovigilance: Extracting Adverse Drug Reactions from User Posts to Health-Related Social Networks,"Adverse reactions to drugs are among the most common causes of death in industri- alized nations. Expensive clinical trials are not sufficient to uncover all of the adverse reactions a drug may cause, necessitating systems for post-marketing surveillance, or pharmacovigilance. These systems have typically relied on voluntary report- ing by health care professionals. However, self-reported patient data has become an increasingly important resource, with ef- forts such as MedWatch from the FDA al- lowing reports directly from the consumer. In this paper, we propose mining the re- lationships between drugs and adverse re- actions as reported by the patients them- selves in user comments to health-related websites. We evaluate our system on a manually annotated set of user comments, with promising performance. We also re- port encouraging correlations between the frequency of adverse drug reactions found by our system in unlabeled data and the frequency of documented adverse drug re- actions. We conclude that user comments pose a significant natural language pro- cessing challenge, but do contain useful extractable information which merits fur- ther exploration.",2010,WS,1.0
"What Is Word Meaning, Really? (And How Can Distributional Models Help Us Describe It?)","In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation. More specifically, we argue for a more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees. Or move away from dictionary senses completely, and only model similarities between individual word usages. We argue that distributional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.",2010,WS,0.8
Grammar-driven versus Data-driven: Which Parsing System is More Affected by Domain Shifts?,"In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely sta- tistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of parsing system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques.",2010,WS,-1.0
To Cache or not to Cache? Experiments with Adaptive Models in Statistical Machine Translation,We report results of our submissions to the WMT 2010 shared translation task in which we applied a system that includes adaptive language and translation models. Adaptation is implemented using exponentially decaying caches storing previous translations as the history for new predictions. Evidence from the cache is then mixed with the global background model. The main problem in this setup is error propagation and our submissions essentially failed to improve over the competitive baseline. There are slight improvements in lexical choice but the global performance decreases in terms of BLEU scores.,2010,WS,-1.0
Adverse-Effect Relations Extraction from Massive Clinical Records,"The rapid spread of electronic health records raised an interest to large-scale information extraction from clinical texts. Considering such a background, we are developing a method that can extract adverse drug event and effect (adverse–effect) relations from massive clinical records. Adverse–effect rela- tions share some features with relations proposed in previous relation extrac- tion studies, but they also have unique characteristics. Adverse–effect rela- tions are usually uncertain. Not even medical experts can usually determine whether a symptom that arises after a medication represents an adverse– effect relation or not. We propose a method to extract adverse–effect rela- tions using a machine-learning tech- nique with dependency features. We performed experiments to extract ad- verse–effect relations from 2,577 clini- cal texts, and obtained F1-score of 37.54 with an optimal parameters and F1-score of 34.90 with automatically tuned parameters. The results also show that dependency features increase the extraction F1-score by 3.59.",2010,WS,1.0
Close = Relevant? The Role of Context in Efficient Language Production,We formally derive a mathematical model for evaluating the effect of context relevance in language production. The model is based on the principle that distant contextual cues tend to gradually lose their relevance for predicting upcoming linguistic signals. We evaluate our model against a hypothesis of efficient communication (Genzel and Charniak’s Constant Entropy Rate hypothesis). We show that the development of entropy throughout discourses is described significantly better by a model with cue relevance decay than by previ- ous models that do not consider context effects.,2010,WS,1.0
Detecting Word Misuse in Chinese,"Social Network Service (SNS) and personal blogs have become the most popular platform for online communication and sharing infor- mation. However because most modern com- puter keyboards are Latin-based, Asian lan- guage speakers (such as Chinese) has to rely on a input system which accepts Romanisation of the characters and convert them into charac- ters or words in that language. In Chinese this form of Romanisation (usually called Pinyin) is highly ambiguous, word misuses often oc- cur because the user choose a wrong candi- date or deliverately substitute the word with another character string that has the identical Romanisation to convey certain semantics, or to achieve a sarcasm effect. In this paper we aim to develop a system that can automati- cally identify such word misuse, and suggest the correct word to be used.",2010,WS,1.0
HMM Word-to-Phrase Alignment with Dependency Constraints,"In this paper, we extend the HMM word- to-phrase alignment model with syntac- tic dependency constraints. The syn- tactic dependencies between multiple words in one language are introduced into the model in a bid to produce co- herent alignments. Our experimental re- sults on a variety of Chinese–English data show that our syntactically con- strained model can lead to as much as a 3.24% relative improvement in BLEU score over current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data. An intrin- sic alignment quality evaluation shows that our alignment model with depen- dency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information.",2010,WS,1.0
You talking to me? A predictive model for zero auxiliary constructions,"As a consequence of the established prac- tice to prefer training data obtained from written sources, NLP tools encounter problems in handling data from the spo- ken domain. However, accurate models of spoken data are increasingly in demand for naturalistic speech generation and ma- chine translations in speech-like contexts (such as chat windows and SMS). There is a widely held assumption in the lin- guistic field that spoken language is an impoverished form of written language. However, we show that spoken data is not unpredictably irregular and that lan- guage models can benefit from detailed consideration of spoken language features. This paper considers one specific con- struction which is largely restricted to the spoken domain - the ZERO AUXILIARY - and makes a predictive model of that con- struction for native speakers of British En- glish. The model can predict zero auxil- iary occurrence in the BNC with 96.9% accuracy. We will demonstrate how this model can be integrated into existing pars- ing tools, increasing the number of suc- cessful parses for this zero auxiliary con- struction by around 30%, and thus improv- ing the performance of NLP applications which rely on parsing.",2010,WS,1.0
Did Social Networks Shape Language Evolution? A Multi-Agent Cognitive Simulation,"Natural language as well as other communication forms are constrained by cogni- tive function and evolved through a social process. Here, we examine whether human memory may be uniquely adapted to the social structures prevalent in groups, specifically small-world networks. The emergence of domain languages is simulated using an empirically evaluated ACTR-based cognitive model of agents in a naming game played within communities. Several community structures are examined (grids, trees, random graphs and small-world networks). We present preliminary results from small-scale simula- tions, showing relative robustness of cognitive models to network structure.",2010,WS,0.30000000000000004
Preferred Explanations: Theory and Generation via Planning,In this paper we examine the general problem of generating preferred explanations for observed behavior with respect to a model of the behavior of a dynamical system. This problem arises in a diversity of applications including diagnosis of dynamical systems and activity recognition. We provide a logical characterization of the notion of an explanation. To generate explanations we identify and exploit a correspondence between explanation generation and planning. The determination of good explanations requires additional domainspecific knowledge which we represent as preferences over explanations. The nature of explanations requires us to formulate preferences in a somewhat retrodictive fashion by utilizing Past Linear Temporal Logic. We propose methods for exploiting these somewhat unique preferences effectively within state-of-the-art planners and illustrate the feasibility of generating (preferred) explanations via planning.,2011,AAAI,0.5
Size Adaptive Selection of Most Informative Features,"In this paper, we propose a novel method to select the most informative subset of features, which has little redundancy and very strong discriminating power. Our proposed approach automatically determines the optimal number of features and selects the best subset accordingly by maximizing the average pairwise informativeness, thus has obvious advantage over traditional filter methods. By relaxing the essential combinatorial optimization problem into the standard quadratic programming problem, the most informative feature subset can be obtained efficiently, and a strategy to dynamically compute the redundancy between feature pairs further greatly accelerates our method through avoiding unnecessary computations of mutual information. As shown by the extensive experiments, the proposed method can successfully select the most informative subset of features, and the obtained classification results significantly outperform the state-of-the-art results on most test datasets.",2011,AAAI,1.0
Transportability of Causal and Statistical Relations: A Formal Approach,"We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called ‚Äúselection diagrams‚Äù for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power. Introduction: Threats vs. Assumptions Science is about generalization; conclusions that are obtained in a laboratory setting are transported and applied elsewhere, in an environment that differs in many aspects from that of the laboratory. If the target environment is arbitrary, or drastically different from the study environment nothing can be learned from the latter. However, the fact that most experiments are conducted with the intention of applying the results elsewhere means that we usually deem the target environment sufficiently similar to the study environment to justify the transport of experimental results or their ramifications. Remarkably, the conditions that permit such transport have not received systematic formal treatment. The standard literature on this topic, falling under rubrics such as ‚Äúquasi-experiments,‚Äù ‚Äúmeta analysis,‚Äù and ‚Äúexternal validity,‚Äù consists primarily of ‚Äúthreats,‚Äù namely, verbal narratives of what can go wrong when we try to transport results from one study to another (e.g., [Shadish, Cook, and Campbell, 2002, chapter 3]). In contrast, we seek to establish ‚Äúlicensing assumptions,‚Äù namely, formal conditions under which the ‚àóThis work was supported in parts by National Institutes of Health #1R01 LM009961-01, National Science Foundation #IIS0914211 and #IIS-1018922, and Office of Naval Research #N00014-09-1-0665 and #N00014-10-1-0933. Copyright c ¬© 2011, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. transport of results across diverse environments is licensed from first principles. The machine learning literature, on the other hand, while seriously concerned about discrepancies between training and test environments [Daume III and Marcu, 2006; Storkey, 2009], has focused almost exclusively on predictive, or classification tasks as opposed to effect-learning tasks. Moreover, even in classification tasks, machine learning researchers have rarely allowed apriori causal knowledge to guide the learning process and, as a result, have not sought theoretical guarantees in the form of sufficient conditions under which discrepancies between the training and test environments can be circumvented, or necessary conditions without which bias will persist regardless of sample size. This paper establishes such conditions (see section on ‚ÄúTransportability across Observational domains‚Äù) and thus informs researchers on what can be gained by domain-specific knowledge when available, and what could explain why transfer-learning algorithms fail to converge or perform. Transportability analysis requires a formal language within which the notion of ‚Äúenvironment‚Äù is given precise characterization, and differences among environments can be encoded and analyzed. The advent of causal diagrams [Pearl, 1995; Spirtes, Glymour, and Scheines, 2000; Pearl, 2009; Koller and Friedman, 2009] provides such a language and renders the formalization of transportability possible. Using this language, this paper offers a precise definition for the notion of transportability and establishes formal conditions that, if held true, would permit us to transport results across domains, environments, or populations.",2011,AAAI,0.0
A Closer Look at the Probabilistic Description Logic Prob-EL,"We study probabilistic variants of the description logic EL. For the case where probabilities apply only to concepts, we provide a careful analysis of the borderline between tractability and EXPTIME-completeness. One outcome is that any probability value except zero and one leads to intractability in the presence of general TBoxes, while this is not the case for classical TBoxes. For the case where probabilities can also be applied to roles, we show PSPACE-completeness. This result is (positively) surprising as the best previously known upper bound was 2-EXPTIME and there were reasons to believe in completeness for this class.",2011,AAAI,0.9
Predicting Relative Prominence in Noun-Noun Compounds,"There are several theories regarding what influences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.",2011,ACL,1.0
Evaluating the Impact of Coder Errors on Active Learning,"Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when applied to noisy data.",2011,ACL,0.1
Translationese and Its Dialects,"While it is has often been observed that the product of translation is somehow different than non-translated text, scholars have emphasized two distinct bases for such differences. Some have noted interference from the source language spilling over into translation in a source-language-specific way, while others have noted general effects of the process of translation that are independent of source language. Using a series of text categorization experiments, we show that both these effects exist and that, moreover, there is a continuum between them. There are many effects of translation that are consistent among texts translated from a given source language, some of which are consistent even among texts translated from families of source languages. Significantly, we find that even for widely unrelated source languages and multiple genres, differences between translated texts and non-translated texts are sufficient for a learned classifier to accurately determine if a given text is translated or original.",2011,ACL,0.0
Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?,"Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach. In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefullydesigned experiments that led us to these conclusions.",2011,ACL,-0.7000000000000001
Learning Word Vectors for Sentiment Analysis,"Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",2011,ACL,0.4
Turn-Taking Cues in a Human Tutoring Corpus,"Most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. This research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. Results show that while there was variation between subjects, three features were significant turn-yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated.",2011,ACL,-0.8
Book Reviews: Syntax-Based Collocation Extraction by Violeta Seretan,"Collocation is a common language phenomenon which has attracted the interest of researchers in many subfields of both theoretical and computational linguistics. Although there is no commonly accepted and precise definition of this phenomenon, collocations are generally understood as complex lexical items, often characterized as unpredictable, idiosyncratic, holistic, mutually selective, and so forth. Together with other types of multiword expressions (or phraseological units, such as compound nouns, phrasal verbs, idioms, etc.), collocations form a borderline phenomenon positioned between lexis and grammar: On one hand, they are unpredictable and must be learned in the same way as single words are (as whole units); on the other hand, they often also have internal syntactic structure and their components must then adhere to grammatical rules. Collocations play an important role in applications involving text production (e.g., machine translation and language generation), text analysis (e.g., parsing and word sense disambiguation), and also in other related tasks (such as information extraction, text classification, etc.). The book Syntax-Based Collocation Extraction by Violeta Seretan is based on her doctoral dissertation defended in 2008 at the Department of Linguistics, University of Geneva, under the supervision of Eric Wehrli, and refers to a number of their previous publications. The main text is divided into six chapters (amounting to 128 pages) and six appendices (70 pages). The first chapter can be regarded as a motivation for the whole work. It introduces the notion of collocation, explains its relevance (and importance) for natural language processing, specifies the aims of the work, and most importantly, it presents arguments for syntax-based collocation extraction as a more appropriate alternative to the traditional syntax-free n-gram and window-based techniques.",2011,CL,0.0
Last Words: Improving Our Reviewing Processes,"Our reviewing practices today are failing. With the number of ACL submissions steadily growing over the last several years (for example, ACL 2009 had a 24% increase in submissions over ACL 2008), the need for more reviewers has become more pronounced. However, qualified reviewers are becoming hard to find, and when they are found, they are often hard-pressed for time. As a result, slipshod reviews are becoming commonplace. Allowing this situation to continue as before will result in the deterioration of our ability to recognize excellence in our research. An ‘intervention’ is therefore needed. As I see it, there are two distinct problems to tackle: first, a lack of qualified reviewers, and second, a lack of quality control in reviews. After discussing these, I will suggest some solutions that I believe are worth implementing.",2011,CL,0.0
Book Review: Cross-Language Information Retrieval by Jian-Yun Nie,"Cross-Language Information Retrieval is a compact book introducing a branch of information retrieval that has gained considerable research interest since the dawn of the WorldWideWeb in the mid 1990s. Information retrieval is generally concerned with the problem of finding documents within a large collection that are relevant to a given input query. Whereas the original formulation of IR assumes that queries and documents are written in the same language, cross-language IR (CLIR) presumes instead that they are written in two different languages. If the collection contains documents in more languages, then we refer to multi-lingual IR (MLIR), which is typically solved with multiple instances of CLIR. Recently, other variations on the theme have been proposed that address non-textual documents, such as image, music, and speech retrieval. An interesting application of CLIR is the retrieval of images that are provided with textual descriptions in any language. Computational linguistics could be interested in CLIR for several reasons. CLIR is mainly about the optimal integration ofmachine translation (MT) and IR, and it presents peculiar and difficult translation issues when short queries are involved, which is the most common case. For such problems, interesting approaches have been developed and refined over time, which mainly build on top of core statistical MT techniques (e.g., word alignment models, translation models) and various lexical resources (e.g., WordNet, dictionaries). In recent years, several books on IR have been published (e.g., Grossman and Frieder 2004; Manning, Raghavan, and Schütze 2008; Büttcher, Clarke, and Cormack 2010), which devoted at most a section or chapter to CLIR. As specific books on CLIR have been limited so far to edited collections of scientific papers (Grefenstette 1998), it was definitely time for the first monograph on the topic. Jian-Yun Nie’s volume is structured as five chapters, which are organized as follows:",2011,CL,0.0
Briefly Noted,"What are the typical genres on the Web? How can they be distinguished? What is the current application of genre theory to Web technologies? These are the sort of issues that would lead you to Genres on the Web, a collection of essays with a no-nonsense title, edited by Alexander Mehler, Serge Sharoff, and Marina Santini. If you understand categorization, but need to know something about the Web and text genres, this book is a pretty good and accessible starting point. The individual essay authors are generally well-respected experts in the field and the 16 individual essays are all solid pieces of work in their own right. The collection is well-structured with an introduction by the volume editors and four sections on, respectively, “Identifying the Sources of Web Genres,” “Automatic Web Genre Identification,” “Structure-Oriented Models of Web Genres,” and “Case Studies of Web Genres.” The bulk of the material is in the genre identification section, and could serve as the basis for a seminar in text classification more generally, as it illustrates many of the major issues and themes in text categorization. The target reader is assumed to be familiar with much of the underlying technology; for example, several authors (Santini, Chapter 5; Kim and Ross, Chapter 6; Sharoff, Chapter 7; Stein et al., Chapter 8; Braslavski, Chapter 9) talk about support vector machines, but none define or describe them. Of these authors, only Braslavski discusses alternatives, and then only in passing. This should not prove a major barrier. Other sections deal effectively with issues such as collecting and normalizing Web samples for genre research, the relationships between and among genres, and the evolution of “style” on the Web. Of particular note is the extensive collection of figures, diagrams, and tables illustrating the results of the various analyses, which help make this a particularly accessible book to the lay reader. There are two weaknesses in the collection. The first is the (lack of) discussion of the theory of genre itself; there is no clear definition of what exactly constitutes a “Web genre” aside from the simple enumeration of categories listed in benchmark corpora. Of course, this lack of clarity in the collection is a consequence of an equivalent lack of clarity among scholars of genre, as pointed out by the paper by Rosso and Haas, and this weakness should not be a stumbling block to most of the readership of Computational Linguistics. (If anything, it shows where more work on the theory of genres is needed.) The second weakness is the relative lack of discussion of genre in non-text documents. Aside from one essay (Paolillo, Warren, and Kunz) on Flash video, there is almost no attention paid to the multimedia aspects of Web publishing. This may be a more serious problem given that many of the papers describe their technology and applications in terms of improvements to Web search technologies, and text Web search is already so much more advanced than music or video search that there may be more and better applications (lower-hanging fruit) available in other areas. Despite these relatively minor weaknesses, the book remains a good reference for the current state of the art in the study of genre on the Web, and will be of interest to anyone trying to sort out a computational notion of “genre,” on or off the Web.—Patrick Juola, Duquesne University",2011,CL,-0.5
ETS: An Error Tolerable System for Coreference Resolution,"This paper presents our error tolerable system for coreference resolution in CoNLL2011(Pradhan et al., 2011) shared task (closed track). Different from most previous reported work, we detect mention candidates based on packed forest instead of single parse tree, and we use beam search algorithm based on the Bell Tree to create entities. Experimental results show that our methods achieve promising results on the development set.",2011,CoNLL,1.0
Gender Attribution: Tracing Stylometric Evidence Beyond Topic and Genre,"Sociolinguistic theories (e.g., Lakoff (1973)) postulate that women’s language styles differ from that of men. In this paper, we explore statistical techniques that can learn to identify the gender of authors in modern English text, such as web blogs and scientific papers. Although recent work has shown the efficacy of statistical approaches to gender attribution, we conjecture that the reported performance might be overly optimistic due to non-stylistic factors such as topic bias in gender that can make the gender detection task easier. Our work is the first that consciously avoids gender bias in topics, thereby providing stronger evidence to gender-specific styles in language beyond topic. In addition, our comparative study provides new insights into robustness of various stylometric techniques across topic and genre.",2011,CoNLL,1.0
Quasi-Synchronous Phrase Dependency Grammars for Machine Translation,"We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results.",2011,EMNLP,1.0
A Joint Model for Extended Semantic Role Labeling,"This paper presents a model that extends semantic role labeling. Existing approaches independently analyze relations expressed by verb predicates or those expressed as nominalizations. However, sentences express relations via other linguistic phenomena as well. Furthermore, these phenomena interact with each other, thus restricting the structures they articulate. In this paper, we use this intuition to define a joint inference model that captures the inter-dependencies between verb semantic role labeling and relations expressed using prepositions. The scarcity of jointly labeled data presents a crucial technical challenge for learning a joint model. The key strength of our model is that we use existing structure predictors as black boxes. By enforcing consistency constraints between their predictions, we show improvements in the performance of both tasks without retraining the individual models.",2011,EMNLP,1.0
Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization,"Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.",2011,EMNLP,0.8
A Probabilistic Forest-to-String Model for Language Generation from Typed Lambda Calculus Expressions,"This paper describes a novel probabilistic approach for generating natural language sentences from their underlying semantics in the form of typed lambda calculus. The approach is built on top of a novel reduction-based weighted synchronous context free grammar formalism, which facilitates the transformation process from typed lambda calculus into natural language sentences. Sentences can then be generated based on such grammar rules with a log-linear model. To acquire such grammar rules automatically in an unsupervised manner, we also propose a novel approach with a generative model, which maps from sub-expressions of logical forms to word sequences in natural language sentences. Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art machine translation models, in terms of both automatic metrics and human evaluation.",2011,EMNLP,1.0
Clusterpath: an Algorithm for Clustering using Convex Fusion Penalties.,"We present a new clustering algorithm by proposing a convex relaxation of hierarchical clustering, which results in a family of objective functions with a natural geometric interpretation. We give efficient algorithms for calculating the continuous regularization path of solutions, and discuss relative advantages of the parameters. Our method experimentally gives state-ofthe-art results similar to spectral clustering for non-convex clusters, and has the added benefit of learning a tree structure from the data.",2011,ICML,1.0
Efficient Sparse Modeling with Automatic Feature Grouping.,"The grouping of features is highly beneficial in learning with high-dimensional data. It reduces the variance in the estimation and improves the stability of feature selection, leading to improved generalization. Moreover, it can also help in data understanding and interpretation. OSCAR is a recent sparse modeling tool that achieves this by using a `1-regularizer and a pairwise `‚àû-regularizer. However, its optimization is computationally expensive. In this paper, we propose an efficient solver based on the accelerated gradient methods. We show that its key projection step can be solved by a simple iterative group merging algorithm. It is highly efficient and reduces the empirical time complexity from O(d ‚àº d) for the existing solvers to just O(d), where d is the number of features. Experimental results on toy and real-world data sets demonstrate that OSCAR is a competitive sparse modeling approach with the added ability of automatic feature grouping.",2011,ICML,1.0
Dynamic Tree Block Coordinate Ascent.,"This paper proposes a novel Linear Programming (LP) based algorithm, called Dynamic Tree-Block Coordinate Ascent (DTBCA), for performing maximum a posteriori (MAP) inference in probabilistic graphical models. Unlike traditional message passing algorithms, which operate uniformly on the whole factor graph, our method dynamically chooses regions of the factor graph on which to focus message-passing efforts. We propose two criteria for selecting regions, including an efficiently computable upperbound on the increase in the objective possible by passing messages in any particular region. This bound is derived from the theory of primal-dual methods from combinatorial optimization, and the forest that maximizes the bounds can be chosen efficiently using a maximum-spanning-tree-like algorithm. Experimental results show that our dynamic schedules significantly speed up state-of-theart LP-based message-passing algorithms on a wide variety of real-world problems.",2011,ICML,1.0
Large-Scale Learning of Embeddings with Reconstruction Sampling.,"In this paper, we present a novel method to speed up the learning of embeddings for large-scale learning tasks involving very sparse data, as is typically the case for Natural Language Processing tasks. Our speed-up method has been developed in the context of Denoising Auto-encoders, which are trained in a purely unsupervised way to capture the input distribution, and learn embeddings for words and text similar to earlier neural language models. The main contribution is a new method to approximate reconstruction error by a sampling procedure. We show how this approximation can be made to obtain an unbiased estimator of the training criterion, and we show how it can be leveraged to make learning much more computationally efficient. We demonstrate the effectiveness of this method on the Amazon and RCV1 NLP datasets. Instead of reducing vocabulary size to make learning practical, our method allows us to train using very large vocabularies. In particular, reconstruction sampling requires 22x less training time on the full Amazon dataset.",2011,ICML,1.0
A Graphbased Framework for Multi-Task Multi-View Learning.,"Many real-world problems exhibit dualheterogeneity. A single learning task might have features in multiple views (i.e., feature heterogeneity); multiple learning tasks might be related with each other through one or more shared views (i.e., task heterogeneity). Existing multi-task learning or multi-view learning algorithms only capture one type of heterogeneity. In this paper, we introduce Multi-Task MultiView (MTV ) learning for such complicated learning problems with both feature heterogeneity and task heterogeneity. We propose a graph-based framework (GraM) to take full advantage of the dual-heterogeneous nature. Our framework has a natural connection to Reproducing Kernel Hilbert Space (RKHS). Furthermore, we propose an iterative algorithm (IteM) for GraM framework, and analyze its optimality, convergence and time complexity. Experimental results on various real data sets demonstrate its effectiveness.",2011,ICML,1.0
An Augmented Lagrangian Approach to Constrained MAP Inference.,"We propose a new algorithm for approximate MAP inference on factor graphs, by combining augmented Lagrangian optimization with the dual decomposition method. Each slave subproblem is given a quadratic penalty, which pushes toward faster consensus than in previous subgradient approaches. Our algorithm is provably convergent, parallelizable, and suitable for fine decompositions of the graph. We show how it can efficiently handle problems with (possibly global) structural constraints via simple sort operations. Experiments on synthetic and real-world data show that our approach compares favorably with the state-of-the-art.",2011,ICML,1.0
PILCO: A Model-Based and Data-Efficient Approach to Policy Search.,"In this paper, we introduce pilco, a practical, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.",2011,ICML,1.0
Learning Output Kernels with Block Coordinate Descent.,"We propose a method to learn simultaneously a vector-valued function and a kernel between its components. The obtained kernel can be used both to improve learning performance and to reveal structures in the output space which may be important in their own right. Our method is based on the solution of a suitable regularization problem over a reproducing kernel Hilbert space of vector-valued functions. Although the regularized risk functional is non-convex, we show that it is invex, implying that all local minimizers are global minimizers. We derive a block-wise coordinate descent method that efficiently exploits the structure of the objective functional. Then, we empirically demonstrate that the proposed method can improve classification accuracy. Finally, we provide a visual interpretation of the learned kernel matrix for some well known datasets.",2011,ICML,1.0
Parsing Natural Scenes and Natural Language with Recursive Neural Networks.,"Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.",2011,ICML,1.0
Infinite SVM: a Dirichlet Process Mixture of Large-margin Kernel Machines.,"We present Infinite SVM (iSVM), a Dirichlet process mixture of large-margin kernel machines for multi-way classification. An iSVM enjoys the advantages of both Bayesian nonparametrics in handling the unknown number of mixing components, and large-margin kernel machines in robustly capturing local nonlinearity of complex data. We develop an efficient variational learning algorithm for posterior inference of iSVM, and we demonstrate the advantages of iSVM over Dirichlet process mixture of generalized linear models and other benchmarks on both synthetic and real Flickr image classification datasets.",2011,ICML,1.0
On the Necessity of Irrelevant Variables.,"This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.",2011,ICML,-0.30000000000000004
On optimization methods for deep learning.,"The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.",2011,ICML,1.0
A Unified Probabilistic Model for Global and Local Unsupervised Feature Selection.,"Existing algorithms for joint clustering and feature selection can be categorized as either global or local approaches. Global methods select a single cluster-independent subset of features, whereas local methods select cluster-specific subsets of features. In this paper, we present a unified probabilistic model that can perform both global and local feature selection for clustering. Our approach is based on a hierarchical beta-Bernoulli prior combined with a Dirichlet process mixture model. We obtain global or local feature selection by adjusting the variance of the beta prior. We provide a variational inference algorithm for our model. In addition to simultaneously learning the clusters and features, this Bayesian formulation allows us to learn both the number of clusters and the number of features to retain. Experiments on synthetic and real data show that our unified model can find global and local features and cluster data as well as competing methods of each type.",2011,ICML,0.5
Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,"Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called Hogwild which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild achieves a nearly optimal rate of convergence. We demonstrate experimentally that Hogwild outperforms alternative schemes that use locking by an order of magnitude.",2011,NIPS,0.8
Variance Penalizing AdaBoost,"This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results confirm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Significant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART).",2011,NIPS,1.0
High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity,"Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.",2011,NIPS,0.8
Solving Decision Problems with Limited Information,"We present a new algorithm for exactly solving decision-making problems represented as an influence diagram. We do not require the usual assumptions of no forgetting and regularity, which allows us to solve problems with limited information. The algorithm, which implements a sophisticated variable elimination procedure, is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and 1064 strategies.",2011,NIPS,1.0
The Fast Convergence of Boosting Authors Abstract,"This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/‚úè2)). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/‚úè)). Next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/‚úè)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/‚úè), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.",2011,NIPS,0.5
Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels Authors Abstract,"We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1-MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16].",2011,NIPS,0.8
Video Annotation and Tracking with Active Learning Authors Abstract,"We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost.",2011,NIPS,0.9
Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors Authors Abstract,"An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.",2011,NIPS,1.0
Environmental statistics and the trade-off between model-based and TD learning in humans Authors Abstract,"There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence ‚Äî especially in humans ‚Äî as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning.",2011,NIPS,-0.5
A Model for Temporal Dependencies in Event Streams Authors Abstract,"We introduce the Piecewise-Constant Conditional Intensity Model, a model for learning temporal dependencies in event streams. We describe a closed-form Bayesian approach to learning these models, and describe an importance sampling algorithm for forecasting future events using these models, using a proposal distribution based on Poisson superposition. We then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our importance sampling algorithm can effectively forecast future events.",2011,NIPS,1.0
Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning Authors Abstract,"We propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints. The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution. The proposed inference is based on a novel particle filter algorithm with state permeations. We apply the inference framework to a challenging problem of learning part-based, deformable object models. Two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints. Our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.",2011,NIPS,1.0
Learning to Learn with Compound HD Models Authors Abstract,"Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.",2011,NIPS,0.8
Joint 3D Estimation of Objects and Scene Layout Authors Abstract,"We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation.",2011,NIPS,1.0
Not all links are equal: Exploiting Dependency Types for the Extraction of Protein-Protein Interactions from Text,"The extraction of protein-protein interactions (PPIs) reported in scientific publications is one of the most studied topics in Text Mining in the Life Sciences, as such algorithms can sub- stantially decrease the effort for databases cu- rators. The currently best methods for this task are based on analyzing the dependency tree (DT) representation of sentences. Many approaches exploit only topological features and thus do not yet fully exploit the informa- tion contained in DTs. We show that incor- porating the grammatical information encoded in the types of the dependencies in DTs no- ticeably improves extraction performance by using a pattern matching approach. We au- tomatically infer a large set of linguistic pat- terns using only information about interact- ing proteins. Patterns are then refined based on shallow linguistic features and the seman- tics of dependency types. Together, these lead to a total improvement of 17.2 percent points in F1, as evaluated on five publicly available PPI corpora. More than half of that improve- ment is gained by properly handling depen- dency types. Our method provides a general framework for building task-specific relation- ship extraction methods that do not require an- notated training data. Furthermore, our obser- vations offer methods to improve upon rela- tion extraction approaches.",2011,WS,1.0
"What is new? News media, General Elections, Sentiment, and named entities","The repetition of names of persons, places, ideas and events, is used sometimes for emphasis. The same is true of the repe- tition of affect words - repeated preferen- tially to show negative/positive sentiment. During an election campaign, this repeti- tion may have a bearing on the electabil- ity of politicians and on the reputation of political parties. News media covering an election may be involved in endorsing political parties, attempting to set aspects of election agenda, and may have gender bias. Using Rocksteady, an affect anal- ysis system, we have analyzed samples of news published nationally and region- ally by Irish media between 21st Decem- ber 2010 and 20th Feb. 2011 - in the run up to the Irish General Election on 25th February 2011. Our results show that a diachronic study of the coverage, based on named-entity dictionary crafted from electoral lists and with key financial and economic terms added, supplemented by a General Inquirer type dictionary of affect, helped us to distinguish between the win- ners (two opposition parties that have sub- sequently formed a coalition government) from the loser (the incumbent party).",2011,WS,1.0
Error Correcting Romaji-kana Conversion for Japanese Language Education,We present an approach to help editors of Japanese on a language learning SNS cor- rect learners’ sentences written in Roman characters by converting them into kana. Our system detects foreign words and con- verts only Japanese words even if they contain spelling errors. Experimental re- sults show that our system achieves about 10 points higher conversion accuracy than traditional input method (IM). Error anal- ysis reveals some tendencies of the errors specific to language learners.,2011,WS,1.0
Error Detection for Treebank Validation,This paper describes an error detection mech- anism which helps in validation of dependency treebank annotation. Consistency in treebank annotation is a must for making data as error- free as possible and for assuring the usefulness of treebank. This work is aimed at ensuring this consistency and to make the task of vali- dation cost effective by detecting major errors induced during completely manual annotation. We evaluated our system on the Hindi de- pendency treebank which is currently under development. We could detect 76.63% of er- rors at dependency level. Results show that our system performs well even when the train- ing data is low.,2011,WS,1.0
"""The day after the day after tomorrow?"" A machine learning approach to adaptive temporal expression generation:training and evaluation with real users","Generating Temporal Expressions (TE) that are easy to understand, unambiguous, and rea- sonably short is a challenge for humans and Spoken Dialogue Systems. Rather than devel- oping hand-written decision rules, we adopt a data-driven approach by collecting user feed- back on a variety of possible TEs in terms of task success, ambiguity, and user prefer- ence. The data collected in this work is freely available to the research community. These data were then used to train a simulated user and a reinforcement learning policy that learns an adaptive Temporal Expression generation strategy for a variety of contexts. We evalu- ate our learned policy both in simulation and with real users and show that this data-driven adaptive policy is a significant improvement over a rule-based adaptive policy, leading to a 24% increase in perceived task completion, while showing a small increase in actual task completion, and a 16% decrease in call dura- tion. This means that dialogues are more ef- ficient and that users are also more confident about the appointment that they have agreed with the system.",2011,WS,1.0
"How Good is the Crowd at ""real"" WSD?","There has been a great deal of excitement re- cently about using the “wisdom of the crowd” to collect data of all kinds, quickly and cheaply (Howe, 2008; von Ahn and Dabbish, 2008). Snow et al. (Snow et al., 2008) were the first to give a convincing demonstration that at least some kinds of linguistic data can be gathered from workers on the web more cheaply than and as accurately as from local experts, and there has been a steady stream of papers and workshops since then with similar results. e.g. (Callison-Burch and Dredze, 2010).Many of the tasks which have been success- fully crowdsourced involve judgments which are similar to those performed in everyday life, such as recognizing unclear writing (von Ahn et al., 2008), or, for those tasks that require con- siderable judgment, the responses are usually binary or from a small set of responses, such as sentiment analysis (Mellebeek et al., 2010) or ratings (Heilman and Smith, 2010). Since the FrameNet process is known to be relatively expensive, we were interested in whether the FrameNet process of fine word sense discrimi- nation and marking of dependents with seman- tic roles could be performed more cheaply and equally accurately using Amazon’s Mechanical Turk (AMT) or similar resources. We report on a partial success in this respect and how it was achieved.",2011,WS,0.5
Extractive Multi-Document Summaries Should Explicitly Not Contain Document Specific Content,"Unsupervised approaches to multi-document summarization consist of two steps: find- ing a content model of the documents to be summarized, and then generating a summary that best represents the most salient informa- tion of the documents. In this paper, we present a sentence selection objective for ex- tractive summarization in which sentences are penalized for containing content that is spe- cific to the documents they were extracted from. We modify an existing system, HIER- SUM (Haghighi & Vanderwende, 2009), to use our objective, which significantly outperforms the original HIERSUM in pairwise user eval- uation. Additionally, our ROUGE scores ad- vance the current state-of-the-art for both su- pervised and unsupervised systems with sta- tistical significance.",2011,WS,1.0
Is it Worth Submitting this Run? Assess your RTE System with a Good Sparring Partner,"We address two issues related to the devel- opment of systems for Recognizing Textual Entailment. The first is the impossibility to capitalize on lessons learned over the different datasets available, due to the changing nature of traditional RTE evaluation settings. The second is the lack of simple ways to assess the results achieved by our system on a given training corpus, and figure out its real potential on unseen test data. Our contribution is the ex- tension of an open-source RTE package with an automatic way to explore the large search space of possible configurations, in order to select the most promising one over a given dataset. From the developers’ point of view, the efficiency and ease of use of the system, together with the good results achieved on all previous RTE datasets, represent a useful sup- port, providing an immediate term of compar- ison to position the results of their approach.",2011,WS,0.5
How can you say such things?!?: Recognizing Disagreement in Informal Political Argument,"The recent proliferation of political and so- cial forums has given rise to a wealth of freely accessible naturalistic arguments. People can “talk” to anyone they want, at any time, in any location, about any topic. Here we use a Mechanical Turk annotated corpus of forum discussions as a gold standard for the recog- nition of disagreement in online ideological forums. We analyze the utility of meta-post features, contextual features, dependency fea- tures and word-based features for signaling the disagreement relation. We show that us- ing contextual and dialogic features we can achieve accuracies up to 68% as compared to a unigram baseline of 63%.",2011,WS,1.0
A Data-Driven Approach to Question Subjectivity Identification in Community Question Answering,"Automatic Subjective Question Answering (ASQA), which aims at answering users‚Äô subjective questions using summaries of multiple opinions, becomes increasingly important. One challenge of ASQA is that expected answers for subjective questions may not readily exist in the Web. The rising and popularity of Community Question Answering (CQA) sites, which provide platforms for people to post and answer questions, provides an alternative to ASQA. One important task of ASQA is question subjectivity identification, which identifies whether a user is asking a subjective question. Unfortunately, there has been little labeled training data available for this task. In this paper, we propose an approach to collect training data automatically by utilizing social signals in CQA sites without involving any manual labeling. Experimental results show that our data-driven approach achieves 9.37% relative improvement over the supervised approach using manually labeled data, and achieves 5.15% relative gain over a stateof-the-art semi-supervised approach. In addition, we propose several heuristic features for question subjectivity identification. By adding these features, we achieve 11.23% relative improvement over word n-gram feature under the same experimental setting.",2012,AAAI,1.0
Basing Decisions on Sentences in Decision Diagrams,"The Sentential Decision Diagram (SDD) is a recently proposed representation of Boolean functions, containing Ordered Binary Decision Diagrams (OBDDs) as a distinguished subclass. While OBDDs are characterized by total variable orders, SDDs are characterized by dissections of variable orders, known as vtrees. Despite this generality, SDDs retain a number of properties, such as canonicity and a polytime Apply operator, that have been critical to the practical success of OBDDs. Moreover, upper bounds on the size of SDDs were also given, which are tighter than comparable upper bounds on the size of OBDDs. In this paper, we analyze more closely some of the theoretical properties of SDDs and their size. In particular, we consider the impact of basing decisions on sentences (using dissections as in SDDs), in comparison to basing decisions on variables (using total variable orders as in OBDDs). Here, we identify a class of Boolean functions where basing decisions on sentences using dissections of a variable order can lead to exponentially more compact SDDs, compared to OBDDs based on the same variable order. Moreover, we identify a fundamental property of the decompositions that underlie SDDs and use it to show how certain changes to a vtree can also lead to exponential differences in the size of an SDD.",2012,AAAI,0.2
Graph-based Semi-Supervised Learning Algorithms for NLP,"While labeled data is expensive to prepare, ever increasing amounts of unlabeled linguistic data are becoming widely available. In order to adapt to this phenomenon, several semi-supervised learning (SSL) algorithms, which learn from labeled as well as unlabeled data, have been developed. In a separate line of work, researchers have started to realize that graphs provide a natural way to represent data in a variety of domains. Graph-based SSL algorithms, which bring together these two lines of work, have been shown to outperform the state-ofthe-art in many applications in speech processing, computer vision and NLP. In particular, recent NLP research has successfully used graph-based SSL algorithms for PoS tagging (Subramanya et al., 2010), semantic parsing (Das and Smith, 2011), knowledge acquisition (Talukdar et al., 2008), sentiment analysis (Goldberg and Zhu, 2006) and text categorization (Subramanya and Bilmes, 2008). Recognizing this promising and emerging area of research, this tutorial focuses on graph-based SSL algorithms (e.g., label propagation methods). The tutorial is intended to be a sequel to the ACL 2008 SSL tutorial, focusing exclusively on graph-based SSL methods and recent advances in this area, which were beyond the scope of the previous tutorial. The tutorial is divided in two parts. In the first part, we will motivate the need for graph-based SSL methods, introduce some standard graph-based SSL algorithms, and discuss connections between these approaches. We will also discuss how linguistic data can be encoded as graphs and show how graph-based algorithms can be scaled to large amounts of data (e.g., web-scale data). Part 2 of the tutorial will focus on how graph-based methods can be used to solve several critical NLP tasks, including basic problems such as PoS tagging, semantic parsing, and more downstream tasks such as text categorization, information acquisition, and sentiment analysis. We will conclude the tutorial with some exciting avenues for future work. Familiarity with semi-supervised learning and graph-based methods will not be assumed, and the necessary background will be provided. Examples from NLP tasks will be used throughout the tutorial to convey the necessary concepts. At the end of this tutorial, the attendee will walk away with the following: • An in-depth knowledge of the current state-ofthe-art in graph-based SSL algorithms, and the ability to implement them. • The ability to decide on the suitability of graph-based SSL methods for a problem. • Familiarity with different NLP tasks where graph-based SSL methods have been successfully applied. In addition to the above goals, we hope that this tutorial will better prepare the attendee to conduct exciting research at the intersection of NLP and other emerging areas with natural graph-structured data (e.g., Computation Social Science). Please visit http://graph-ssl.wikidot.com/ for details.",2012,ACL,0.0
MIX Is Not a Tree-Adjoining Language,"The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal number of occurrences of each letter. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.",2012,ACL,0.0
Evaluating Unsupervised Ensembles when applied to Word Sense Induction,"Ensembles combine knowledge from distinct machine learning approaches into a general flexible system. While supervised ensembles frequently show great benefit, unsupervised ensembles prove to be more challenging. We propose evaluating various unsupervised ensembles when applied to the unsupervised task of Word Sense Induction with a framework for combining diverse feature spaces and clustering algorithms. We evaluate our system using standard shared tasks and also introduce new automated semantic evaluations and supervised baselines, both of which highlight the current limitations of existing Word Sense Induction evaluations.",2012,ACL,-0.9
Spectral Learning of Latent-Variable PCFGs,"Jeju, Republic of Korea, 8-14 July 2012. c ©2012 Association for Computational Linguistics Spectral Learning of Latent-Variable PCFGs Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar Dept. of Computer Science, Columbia University Dept. of Statistics/Dept. of Computer and Information Science, University of Pennsylvania {scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu Abstract",2012,ACL,0.0
"Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese","We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.",2012,ACL,1.0
Qualitative Modeling of Spatial Prepositions and Motion Expressions,"The ability to understand spatial prepositions and motion in natural language will enable a variety of new applications involving systems that can respond to verbal directions, map travel guides, display incident reports, etc., providing for enhanced information extraction, question-answering, information retrieval, and more principled text to scene rendering. Until now, however, the semantics of spatial relations and motion verbs has been highly problematic. This tutorial presents a new approach to the semantics of spatial descriptions and motion expressions based on linguistically interpreted qualitative reasoning. Our approach allows for formal inference from spatial descriptions in natural language, while leveraging annotation schemes for time, space, and motion, along with machine learning from annotated corpora. We introduce a compositional semantics for motion expressions that integrates spatial primitives drawn from qualitative calculi. No previous exposure to the semantics of spatial prepositions or motion verbs is assumed. The tutorial will sharpen cross-linguistic intuitions about the interpretation of spatial prepositions and motion constructions. The attendees will also learn about qualitative reasoning schemes for static and dynamic spatial information, as well as three annotation schemes: TimeML, SpatialML, and ISO-Space, for time, space, and motion, respectively. While both cognitive and formal linguistics have examined the meaning of motion verbs and spatial prepositions, these earlier approaches do not yield precise computable representations that are expressive enough for natural languages. However, the previous literature makes it clear that communication of motion relies on imprecise and highly abstract geometric descriptions, rather than Euclidean ones that specify the coordinates and shapes of every object. This property makes these expressions a fit target for the field of qualitative spatial reasoning in AI, which has developed a rich set of geometric primitives for representing time, space (including distance, orientation, and topological relations), and motion. The results of such research have yielded a wide variety of spatial and temporal reasoning logics and tools. By reviewing these calculi and resources, this tutorial aims to systematically connect qualitative reasoning to natural language. Tutorial Schedule:",2012,ACL,0.4
Assessing the Effect of Inconsistent Assessors on Summarization Evaluation,"We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.",2012,ACL,-0.6000000000000001
A Graphical Interface for MT Evaluation and Error Analysis,"Error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the MT systems under development and allow fair comparisons among them. This work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations. To do so, we have set up an online graphical interface for the ASIYA toolkit, a rich repository of evaluation measures working at different linguistic levels. The current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations, and word alignments. The intelligent visualization of the linguistic structures used by the metrics, as well as a set of navigational functionalities, may lead towards advanced methods for automatic error analysis.",2012,ACL,0.5
"A Scalable Distributed Syntactic, Semantic, and Lexical Language Model","This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and “readability” of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.",2012,CL,1.0
Did It Happen? The Pragmatic Complexity of Veridicality Assessment,"Natural language understanding depends heavily on assessing veridicality—whether events mentioned in a text are viewed as happening or not—but little consideration is given to this property in current relation and event extraction systems. Furthermore, the work that has been done has generally assumed that veridicality can be captured by lexical semantic properties whereas we show that context and world knowledge play a significant role in shaping veridicality. We extend the FactBank corpus, which contains semantically driven veridicality annotations, with pragmatically informed ones. Our annotations are more complex than the lexical assumption predicts but systematic enough to be included in computational work on textual understanding. They also indicate that veridicality judgments are not always categorical, and should therefore be modeled as distributions. We build a classifier to automatically assign event veridicality distributions based on our new annotations. The classifier relies not only on lexical features like hedges or negations, but also on structural features and approximations of world knowledge, thereby providing a nuanced picture of the diverse factors that shape veridicality.",2012,CL,0.6000000000000001
Modality and Negation: An Introduction to the Special Issue,"objects can be assertions, beliefs, facts, or eventualities. Discourse connectives and their arguments are assigned attribution-related features (Prasad et al. 2006) such as SOURCE (writer, other, arbitrary), TYPE (reflecting the nature of the relation between the agent and the abstract object), SCOPAL POLARITY of attribution, and DETERMINACY (indicating the presence of contexts canceling the entailment of attribution). The text spans signaling the attribution are also marked. Prasad et al. (2006) report that 34% of the discourse relations have some non-writer agent. SCOPAL POLARITY is annotated to identify cases when verbs of attribution (say, think, ...) are negated syntactically",2012,CL,0.0
Book Review: Graph-Based Natural Language Processing and Information Retrieval by Rada Mihalcea and Dragomir Radev,"Graphs are ubiquitous. There is hardly any domain in which objects and their relations cannot be intuitively represented as nodes and edges in a graph. Graph theory is a well-studied sub-discipline of mathematics, with a large body of results and a large number of efficient algorithms that operate on graphs. Like many other disciplines, the fields of natural language processing (NLP) and information retrieval (IR) also deal with data that can be represented as a graph. In this light, it is somewhat surprising that only in recent years the applicability of graph-theoretical frameworks to language technology became apparent and increasingly found its way into publications in the field of computational linguistics. Using algorithms that take the overall graph structure of a problem into account, rather than characteristics of single objects or (unstructured) sets of objects, graph-based methods have been shown to improve a wide range of NLP tasks. In a short but comprehensive overview of the field of graph-based methods for NLP and IR, Rada Mihalcea and Dragomir Radev list an extensive number of techniques and examples from a wide range of research papers by a large number of authors. This book provides an excellent review of this research area, and serves both as an introduction and as a survey of current graph-based techniques in NLP and IR. Because the few existing surveys in this field concentrate on particular aspects, such as graph clustering (Lancichinetti and Fortunato 2009) or IR (Liu 2006), a textbook on the topic was very much needed and this book surely fills this gap. The book is organized in four parts and contains a total of nine chapters. The first part gives an introduction to notions of graph theory, and the second part covers natural and random networks. The third part is devoted to graph-based IR, and part IV covers graph-based NLP. Chapter 1 lays the groundwork for the remainder of the book by introducing all necessary concepts in graph theory, including the notation, graph properties, and graph representations. In the second chapter, a glimpse is offered into the plethora of graph-based algorithms that have been developed independently of applications in NLP and IR. Sacrificing depth for breadth, this chapter does a great job in touching on a wide variety of methods, including minimum spanning trees, shortest-path algorithms, cuts and flows, subgraph matching, dimensionality reduction, random walks, spreading activation, and more. Algorithms are explained concisely, using examples, pseudo-code, and/or illustrations, some of which are very well suited for classroom examples. Network theory is presented in Chapter 3. The term network is here used to refer to naturally occurring relations, as opposed to graphs being generated by an automated process. After presenting the classical Erdős-Rényi random graph model and showing its inadequacy to model power-law degree distributions following Zipf’s law, scale-free small-world networks are introduced. Further,",2012,CL,0.0
Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization,"A lexicalized tree-adjoining grammar is a tree-adjoining grammar where each elementary tree contains some overt lexical item. Such grammars are being used to give lexical accounts of syntactic phenomena, where an elementary tree defines the domain of locality of the syntactic and semantic dependencies of its lexical items. It has been claimed in the literature that for every tree-adjoining grammar, one can construct a strongly equivalent lexicalized version. We show that such a procedure does not exist: Tree-adjoining grammars are not closed under strong lexicalization.",2012,CL,0.5
Modality and Negation in SIMT Use of Modality and Negation in Semantically-Informed Syntactic MT,"∗ U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755. E-mail: kathrynlb@gmail.com. ∗∗ Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD 20742. E-mail: meb@umd.edu. † Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153, College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu. ‡ Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street, Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu. § Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723. E-mail: christine.piatko@jhuapl.edu. || Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: lsl@cs.cmu.edu. # BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.",2012,CL,0.0
Book Review: Quantitative Syntax Analysis by Reinhard Köhler,"Quantitative linguistics (QL) is a discipline of linguistics, that, using real texts, studies languages with quantitative mathematical approaches, aiming to precisely describe and explain, with a system of mathematical laws, the operation and development of language systems. Later in this review, we will address the relationship between QL and computational linguistics. Quantitative Syntax Analysis is a recent work on QL by Reinhard Köhler that not only provides a comprehensive introduction to the work of QL on the syntactic level, but also sketches the theoretical grounds, the research paradigm, and the ultimate goals of quantitative linguistics in general.",2012,CL,0.0
Computational Generation of Referring Expressions: A Survey,"This article offers a survey of computational research on referring expression generation (REG). It introduces the REG problem and describes early work in this area, discussing what basic assumptions lie behind it, and showing how its remit has widened in recent years. We discuss computational frameworks underlying REG, and demonstrate a recent trend that seeks to link REG algorithms with well-established Knowledge Representation techniques. Considerable attention is given to recent efforts at evaluating REG algorithms and the lessons that they allow us to learn. The article concludes with a discussion of the way forward in REG, focusing on references in larger and more realistic settings.",2012,CL,0.0
"An AI readability"" Formula for French as a Foreign Language""","This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.",2012,CoNLL,1.0
Entity based Q&A Retrieval,"Bridging the lexical gap between the user’s question and the question-answer pairs in the Q&A archives has been a major challenge for Q&A retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. This results in degraded retrieval performance. In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities). We explore strategies to learn the translation probabilities between words and the concepts using the Q&A archives and a popular entity catalog. Experiments conducted on a large scale real data show that the proposed techniques are promising.",2012,CoNLL,0.2
Stability of matrix factorization for collaborative filtering.,"In this work, inspired by (B√ºhler & Hein, 2009), (Strang, 1983), and (Zhang et al., 2009), we give a continuous relaxation of the Cheeger cut problem on a weighted graph. We show that the relaxation is actually equivalent to the original problem. We then describe an algorithm for finding good cuts suggested by the similarities of the energy of the relaxed problem and various well studied energies in image processing. Finally we provide experimental validation of the proposed algorithm, demonstrating its efficiency in finding high quality cuts.",2012,ICML,0.9
Utilizing Static Analysis and Code Generation to Accelerate Neural Networks.,"In a trend that reflects the increasing demand for intelligent applications driven by business data, IBM today is building out a significant number of applications that leverage machine learning technologies to optimize business process decisions. This talk highlights this trend; and describes the many different ways in which leading edge machine learning concepts are being utilized in business applications developed by IBM for its internal use and for clients.",2012,ICML,0.0
Lognormal and Gamma Mixed Negative Binomial Regression.,"In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.",2012,ICML,1.0
Modeling Latent Variable Uncertainty for Loss-based Learning.,"We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent svm in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems‚Äîobject detection and action detection‚Äîusing publicly available datasets.",2012,ICML,1.0
Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes.,"We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.",2012,ICML,0.30000000000000004
Manifold Relevance Determination.,"In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a ‚Äúsoftly‚Äù shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.",2012,ICML,0.5
Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation.,"Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.",2012,ICML,-0.30000000000000004
Using CCA to improve CCA: A new spectral method for estimating vector models of words.,"Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank ‚Äúdictionary‚Äù by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.",2012,ICML,1.0
Predicting Consumer Behavior in Commerce Search.,"Traditional approaches to ranking in web search follow the paradigm of rank-by-score: a learned function gives each query-URL combination an absolute score and URLs are ranked according to this score. This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other. Scoring contradicts prior work in behavioral economics that preference between items depends not only on the items but also on the presented alternatives. Thus, for the same query, preference between items A and B may depend on the presence or absence of item C. We propose a new model of ranking, the Random Shopper Model, that allows and explains such behavior. In this model, each feature is viewed as a Markov chain over the items to be ranked, and the goal is to find a weighting of the features that best reflects their importance. We show that our model can be learned under the empirical risk minimization framework, and give an efficient learning algorithm. Experiments on commerce search logs demonstrate that our algorithm outperforms scoring-based approaches including regression and listwise ranking.",2012,ICML,1.0
Large-Scale Feature Learning With Spike-and-Slab Sparse Coding.,"We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models‚Äô Transfer Learning Challenge.",2012,ICML,1.0
A Unified Robust Classification Model.,"We present MI-CRF, a conditional random field (CRF) model for multiple instance learning (MIL). MI-CRF models bags as nodes in a CRF with instances as their states. It combines discriminative unary instance classifiers and pairwise dissimilarity measures. We show that both forces improve the classification performance. Unlike other approaches, MI-CRF considers all bags jointly during training as well as during testing. This makes it possible to classify test bags in an imputation setup. The parameters of MI-CRF are learned using constraint generation. Furthermore, we show that MI-CRF can incorporate previous MIL algorithms to improve on their results. MICRF obtains competitive results on five standard MIL datasets.",2012,ICML,0.5
On causal and anticausal learning.,"We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.",2012,ICML,1.0
Fast classification using sparse decision DAGs.,"In this paper we propose an algorithm that builds sparse decision DAGs (directed acyclic graphs) from a list of base classifiers provided by an external learning method such as AdaBoost. The basic idea is to cast the DAG design task as a Markov decision process. Each instance can decide to use or to skip each base classifier, based on the current state of the classifier being built. The result is a sparse decision DAG where the base classifiers are selected in a data-dependent way. The method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off. The algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks, and it clearly outperforms them when there is a small number of base classifiers. Unlike cascades, it is also readily applicable for multi-class classification. Using the multi-class setup, we show on a benchmark Web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker.",2012,ICML,1.0
Copula Mixture Model for Dependency-seeking Clustering.,"We introduce a copula mixture model to perform dependency-seeking clustering when cooccurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.",2012,ICML,0.7000000000000001
Learning Task Grouping and Overlap in Multi-task Learning.,"In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods.",2012,ICML,1.0
Discovering Support and Affiliated Features from Very High Dimensions.,"In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.",2012,ICML,1.0
Efficient Structured Prediction with Latent Variables for General Graphical Models.,"In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.",2012,ICML,1.0
Quasi-Newton Methods: A New Direction.,"Four decades after their invention, quasiNewton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.",2012,ICML,0.7000000000000001
Evaluating Bayesian and L1 Approaches for Sparse Unsupervised Learning .,"The use of L1 regularisation for sparse learning has generated immense research interest, with many successful applications in diverse areas such as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically under-performs in terms of predictive performance when compared to other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of ‚ÄúL1‚Äù, and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spikeand-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner, and avoid unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).",2012,ICML,0.9
Beauty Before Age? Applying Subjectivity to Automatic English Adjective Ordering,"The preferred order of pre-nominal adjectives in English is determined primarily by semantics. Nevertheless, Adjective Ordering (AO) systems do not generally exploit semantic features. This paper describes a system that orders adjectives with significantly abovechance accuracy (73.0%) solely on the basis of semantic features pertaining to the cognitive-semantic dimension of subjectivity. The results indicate that combining such semantic approaches with current methods could result in more accurate and robust AO systems.",2012,NAACL,1.0
Ranking-based readability assessment for early primary children's literature,"Determining the reading level of children’s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. Automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. However, guided reading levels developed by educators operate at a more fine-grained level, with multiple levels corresponding to each grade. We find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we find that we can predict the reading level up to 83% of the time on a small corpus of children’s books.",2012,NAACL,1.0
Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation Modeling,"It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).",2012,NAACL,1.0
Choosing an Evaluation Metric for Parser Design,This paper seeks to quantitatively evaluate the degree to which a number of popular metrics provide overlapping information to parser designers. Two routine tasks are considered: optimizing a machine learning regularization parameter and selecting an optimal machine learning feature set. The main result is that the choice of evaluation metric used to optimize these problems (with one exception among popular metrics) has little effect on the solution to the optimization.,2012,NAACL,0.0
One Year of Contender: What Have We Learned about Assessing and Tuning Industrial Spoken Dialog Systems?,"A lot. Since inception of Contender, a machine learning method tailored for computerassisted decision making in industrial spoken dialog systems, it was rolled out in over 200 instances throughout our applications processing nearly 40 million calls. The net effect of this data-driven method is a significantly increased system performance gaining about 100,000 additional automated calls every month. 1 From the unwieldiness of data to the Contender process Academic institutions involved in the research on spoken dialog systems often lack access to data for training, tuning, and testing their systems. This is simply because the majority of systems only live in laboratory environments and hardly get deployed to the live user. The lack of data can result in systems not sufficiently tested, models trained on nonrepresentative or artificial data, and systems of limited domains (usually restaurant or flight information). On the other hand, in industrial settings, spoken dialog systems are often deployed to take over tasks of call center agents associated with potentially very large amounts of traffic. Here, we are speaking of applications which may process more than one million calls per week. Having applications log every One of the few exceptions to this rule is the Let’s Go bus information system maintained at the Carnegie Mellon University in Pittsburgh (Raux et al., 2005). action they take during the course of a call can provide developers with valuable data to tune and test the systems they maintain. As opposed to the academic world, often, there appears to be too much data to capture, permanently store, mine, and retrieve. Harddisks on application servers run full, log processing scripts demand too much computing capacity, database queues get stuck, queries slow down, and so on and so forth. Even if these billions and billions of log entries are eventually available for random access from a highly indexed database cluster, it is not clear what one should search for in an attempt to improve a dialog system’s performance. About a year and a half ago, we proposed a method we called Contender playing the role of a live experiment in a deployed spoken dialog system (Suendermann et al., 2010a). Conceptually, a Contender is an activity in a call flow which has an input transition and multiple output transitions (alternatives). When a call hits a Contender’s input transition, a randomization is carried out to determine which alternative the call will continue with (see Figure 1). The Contender itself does not do anything else but performing the random decision during runtime. The different call flow activities and processes the individual alternatives get routed to make calls depend on the Contenders’ decisions. Say, one wants to find out which of ten possible time-out settings in an activity is optimal. This could be achieved by duplicating the activity in question ten times and setting each copy’s time-out to a different value. Now, a Contender is placed whose ten alternatives get connected to the ten competing ac-",2012,NAACL,1.0
Deep Unsupervised Feature Learning for Natural Language Processing,"Statistical natural language processing (NLP) builds models of language based on statistical features extracted from the input text. We investigate deep learning methods for unsupervised feature learning for NLP tasks. Recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results. In this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classifiers. We also hypothesise that modelling long-range dependencies in the input and (separately) in the output layers would further improve performance. We suggest methods for overcoming these limitations, which will form part of our final thesis work.",2012,NAACL,0.30000000000000004
"Directions for Research on Spoken Dialog Systems, Broadly Defined","To increase impact and accelerate progress, the spoken dialog systems research community should work on four shareable things that will also engage and support sister fields of science and engineering. 1 To Reach Out to the VoiceXML Community, a Commercial-Dialogs Corpus Although many people are frustrated with the commercial dialog systems they use every day, spoken dialog systems research has been only sporadically relevant to these issues. Although service interactions are pervasive in everyday life, and can be rich and interesting, the vast majority of attempts to model and engineer them have attempted to optimize efficiency and surface-goal completion. The results are all around us, from crudely scripted upselling attempts at fast food restaurants to stilted dialog systems that tediously elicit the pieces of information needed to complete a database query. One reason is that the research community has come to shun most practical dialog types, perhaps to avoid seeming old-fashioned or being tainted by low expectations, or perhaps due to a misperception that industry is addressing these issues. A resource that would help progress here would be a commercialdialogs corpora that is shareable by all. Personally, I would like this corpus to be one with a truly exemplary person in the service role, someone who puts customers at ease, develops rapport, ∗This work was supported by NSF Award IIS-0914868. brings humor and sparkle, and makes them want to call back. Having several thousand short dialogs where diverse customers call in to that person, and modeling how she handles them, would take us a long way to understanding responsive and adaptive behaviors. Even prototype systems built on such dialogs could help set the agenda for future generations of commercial dialog systems. 2 To Reach Out to the Applied Linguistics Communities, Dialog Analysis Tools Although many people are fascinated by language and dialog, spoken dialog systems research has only sporadically tapped this enthusiasm. For example, researchers in the conversation analysis tradition and teachers of foreign languages, not to mention many undergraduates, love to explore patterns of dialog. However spoken dialog research so far has produced scant findings about language behavior that are interesting to and graspable by non-engineers. Personally, I think the biggest opportunity here involves tools to support non-technical people in discovering things themselves. Even amateurs, such as high school science fair participants, should be able to satisfy curiosity or confirm hunches, and experience the joy of systematically examining dialog phenomena. Our community ought to be producing tools and toolsets that support the complete workflow in such inquiries, eclectically supporting tagging, searching, juxtaposing clips and so on, and supporting both perceptually-based analysis and quantitative analysis in an integrated way. In particular we need to go beyond in-lab solutions (Ward and Al Bayyari, 2006) to develop robust toolsets that",2012,NAACL,0.0
Multiple Narrative Disentanglement: Unraveling Infinite Jest,"Many works (of both fiction and non-fiction) span multiple, intersecting narratives, each of which constitutes a story in its own right. In this work I introduce the task of multiple narrative disentanglement (MND), in which the aim is to tease these narratives apart by assigning passages from a text to the sub-narratives to which they belong. The motivating example I use is David Foster Wallace’s fictional text Infinite Jest. I selected this book because it contains multiple, interweaving narratives within its sprawling 1,000-plus pages. I propose and evaluate a novel unsupervised approach to MND that is motivated by the theory of narratology. This method achieves strong empirical results, successfully disentangling the threads in Infinite Jest and significantly outperforming baseline strategies in doing so.",2012,NAACL,1.0
Trend Analysis in Harper's Bazaar,Topic modeling of fashion trends were analyzed using the MALLET toolkit. Harper’s Bazaar magazines from 1860-1899 were used (freely available online). This resulted in 20 topics with 4 characterizing words each. Trends over time were analyzed in several different ways using 100-topics and 20-topics.,2012,NAACL,0.5
"The Future of Spoken Dialogue Systems is in their Past: Long-Term Adaptive, Conversational Assistants","A sketch of dialogue systems as long-term adaptive, conversational agents.",2012,NAACL,0.0
How Text Segmentation Algorithms Gain from Topic Models,"This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.",2012,NAACL,1.0
A Comparative Investigation of Morphological Language Modeling for the Languages of the European Union,"We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages. Even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%. We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency.",2012,NAACL,1.0
Towards a computational approach to literary text analysis,We consider several types of literary-theoretic approaches to literary text analysis; we describe several concepts from Computational Linguistics and Artificial Intelligence that could be used to model,2012,NAACL,0.8
Optimized Online Rank Learning for Machine Translation,"We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.",2012,NAACL,1.0
Are You Sure? Confidence in Prediction of Dependency Tree Edges,We describe and evaluate several methods for estimating the confidence in the per-edge correctness of a predicted dependency parse. We show empirically that the confidence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efficiently. We evaluate our methods on parsing text in 14 languages.,2012,NAACL,1.0
100 Things You Always Wanted to Know about Linguistics But Were Afraid to Ask*,"Many NLP tasks have at their core a subtask of extracting the dependencies---who did what to whom---from natural language sentences. This task can be understood as the inverse of the problem solved in different ways by diverse human languages, namely, how to indicate the relationship between different parts of a sentence. Understanding how languages solve the problem can be extremely useful in both feature design and error analysis in the application of machine learning to NLP. Likewise, understanding cross-linguistic variation can be important for the design of MT systems and other multilingual applications. The purpose of this tutorial is to present in a succinct and accessible fashion information about the structure of human languages that can be useful in creating more linguistically sophisticated, more language independent, and thus more successful NLP systems. While many kinds of linguistic structure can be relevant to different NLP tasks, the focus of this tutorial will be on morphosyntax. The tutorial will take an explicitly typological perspective as an understanding of cross-linguistic variation can facilitate the design of more portable (language-independent) NLP systems. In order to help participants retain the information better, the tutorial will be structured interactively. I will ask participants for examples of tasks and data sets they work with, and then as a group we will brainstorm ways in which each of the linguistic properties discussed can related to feature design and/or error analysis for those tasks. OUTLINE",2012,NAACL,0.0
Topical Segmentation: a Study of Human Performance and a New Measure of Quality.,"In a large-scale study of how people find topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel. We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns. The results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks – places where most prominent topic shifts occur. We recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks. We propose to account for this in a simple modification of the windowDiff metric. We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis.",2012,NAACL,0.7000000000000001
Towards Quality-Adaptive Spoken Dialogue Management,"Information about the quality of a Spoken Dialogue System (SDS) is usually used only for comparing SDSs with each other or manually improving the dialogue strategy. This information, however, provides a means for inherently improving the dialogue performance by adapting the Dialogue Manager during the interaction accordingly. For a quality metric to be suitable, it must suffice certain conditions. Therefore, we address requirements for the quality metric and, additionally, present approaches for quality-adaptive dialogue management.",2012,NAACL,1.0
Position Paper: Towards Standardized Metrics and Tools for Spoken and Multimodal Dialog System Evaluation,We argue that standardized metrics and automatic evaluation tools are necessary for speeding up knowledge generation and development processes for dialog systems.,2012,NAACL,0.0
After Dialog Went Pervasive: Separating Dialog Behavior Modeling and Task Modeling,"Dialog Goes Pervasive Until recently, many dialog systems were information retrieval systems. For example, using a telephone-based interactive response system a US-based user can find flights from United (1-800-UNITED-1), get movie schedules (1-800777-FILM), or get bus information (Black et al., 2011). These systems save companies money and help users access information 24/7. However, the interaction between user and system is tightly constrained. For the most part, each system only deals with one domain, so the task models are typically flat slot-filling models (Allen et al., 2001b). Also, the dialogs are very structured, with system initiative and short user responses, giving limited scope to study important phenomena such as coreference. Smart phones and other mobile devices make possible pervasive human-computer spoken dialog. For example, the Vlingo system lets users do web searches (information retrieval), but also connects calls, opens other apps, and permits voice dictation of emails or social media updates1. Siri can also help users make reservations and schedule meetings2. These new dialog systems are different from traditional ones in several ways; they are multi-task, asynchronous, can involve rich context modeling, and have side effects in the “real world”: Multi-task – The system interacts with the user to accomplish a series of (possibly related) tasks. For example, a user might use the system to order a book and then say schedule it for book club a different task (e.g. requiring different backend DB lookups) but related to the previous one by the book informa-",2012,NAACL,-0.5
Tuning as Linear Regression,"We propose a tuning method for statistical machine translation, based on the pairwise ranking approach. Hopkins and May (2011) presented a method that uses a binary classifier. In this work, we use linear regression and show that our approach is as effective as using a binary classifier and converges faster.",2012,NAACL,1.0
Domain-Specific Semantic Relatedness From Wikipedia: Can A Course Be Transferred?,"Semantic relatedness, or its inverse, semantic distance, measures the degree of closeness between two pieces of text determined by their meaning. Related work typically measures semantics based on a sparse knowledge base such as WordNet1 or CYC that requires intensive manual efforts to build and maintain. Other work is based on the Brown corpus, or more recently, Wikipedia. Wikipediabased measures, however, typically do not take into account the rapid growth of that resource, which exponentially increases the time to prepare and query the knowledge base. Furthermore, the generalized knowledge domain may be difficult to adapt to a specific domain. To address these problems, this paper proposes a domain-specific semantic relatedness measure based on part of Wikipedia that analyzes course descriptions to suggest whether a course can be transferred from one institution to another. We show that our results perform well when compared to previous work.",2012,NAACL,-0.1
Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction,"Most unsupervised dependency systems rely on gold-standard Part-of-Speech (PoS) tags, either directly, using the PoS tags instead of words, or indirectly in the back-off mechanism of fully lexicalized models (Headden et al., 2009). It has been shown in supervised systems that using a hierarchical syntactic structure model can produce competitive sequence models; in other words that a parser can be a good tagger (Li et al., 2011; Auli and Lopez, 2011; Cohen et al., 2011). This is unsurprising, as the parser uses a rich set of hierarchical features that enable it to look at a less localized environment than a PoS tagger which in most cases relies solely on local contextual features. However this interaction has not been shown for the unsupervised setting. To our knowledge, this work is the first to show that using dependencies for unsupervised PoS induction is indeed useful.",2012,NAACL,0.4
Processing modality and negation,"1.1 Tutorial content Modality and negation are ubiquitous phenomena in language. Generally speaking, modality is a grammatical category that allows to express aspects related to the speaker's attitude towards her statements in terms of degree of certainty, reliability, and subjectivity. In this tutorial modality is understood in a broad sense, which involves related concepts like subjectivity, hedging, evidentiality, uncertainty, committed belief, and factuality. Negation is a grammatical category that allows to change the truth value of a proposition. Modality and negation are treated together because they are interrelated phenomena and are protypically expressed by linguistic devices that share some formal characteristics. For example, modality and negation cues function as operators that scope over certain parts of the sentence. From a natural language processing perspective, a very relevant aspect of modality and negation is that they encode extra-propositional aspects of meaning. While traditionally most research has focused on propositional aspects of meaning, the interest in processing extra-propositonal aspects has grown in recent years, as a natural consequence of the consolidation of areas that focus on the computational treatment of propositional aspects. Given a sentence, researchers aim at going beyond determining 'who/what does what to whom/what where and when', which would be the goal of a typical semantic role labeling or event extraction task, and are interested in finding also features such as the source, certainty level, epistemological type, truth value, and subjective aspects of the statements contained in a text. Additionally, researchers are also interested in analysing discourse level phenomena such as finding contradictions and textual entailments or modelling how the status of events changes throughout a text. Modality and negation play a main role in these phenomena. That there is growing interest in these topics among the NLP community is reflected by a number of recent publications, the edition of the workshop 'Negation and Speculation in Natural Language Processing (NeSp-NLP 2010)', as well as the popularity of the CoNLL 2010 shared task on 'Learning to detect hedges and their scope in natural language tex't and the future publication of a special issue of the journal Computational Linguistics. Research on modality and negation has also been stimulated by the release of a number of data sets annotated with various types of information related to these phenomena. This tutorial is divided in five modules. In Module 1, I will introduce modality and negation as devices that express extra-propositional aspects of meaning, I will define related concepts and I will show why it is interesting and complex to process them. In Module 2, I will present different categorisation schemes and annotation efforts, as well as an overview of existing resources. In Module 3, I will describe how several related tasks have been modelled and solved. I will present in detail the rule-based and machine learning approaches that have been used to solve the tasks. In Module 4, I will focus on applications that have incorporated the treatment of modality and negation, and on research that analyses the impact of processing these phenomena. The applications range from sentiment analysis to biomedical text mining. Finally, in Module 5, I will summarize achievements and point out open problems. 1.2 Relevance for the ACL community Processing modality and negation is relevant for the ACL community because of several reasons. First, the treatment of modality and negation is very relevant for all NLP applications that involve text understanding. This includes applications that need to discriminate between factual and non-factual information (uncertain facts, opinions, attitudes, emotions, and beliefs), like information extraction, opinion mining, sentiment analysis, (biomedical) text mining, or question answering, as well as other applications that process the meaning of texts, like recognizing textual entailment, paraphrasing, or summarization. Incorporating information about modality and negation has been shown to be useful for a number of applications, such as biomedical text processing (Friedman et al., 1994; Di Marco and Mercer, 2005; Mutalik et al., 2001; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005a), recognizing textual entailment (Marneffe et al., 2006; Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). Hence this topic is of general importance to the NLP community as a whole, as evidenced by the fact that a number of researchers and groups are currently working on this phenomena. Second, this topic has received a noticeable boost from several recent events: the workshop Negation and Speculation in Natural Language Processing (NeSp-NLP 2010), which I co-organized in Uppsala just before ACL 2010; the CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text, which attracted 51 submissions from 23 teams; and the publication of the Special Issue on Modality and Negation by the journal Computational Linguistics, that will appear at the end of 2011. This SI has received a considerable number of submissions, which shows that the community is active in treating these phenomena. Research on modality and negation is also supported by the fact that a number of data sets annotated with various aspects of modality and negation information have been made available, such as the MPQA Opinion Corpus (Wiebe et al., 2005), Rubinâ€TMs (2006; 2007) certainty corpus, the ACE 2008 corpus (Linguistic Data Consortium, 2008), and the FactBank corpus (2009), the BioScope corpus (Vincze et al., 2008). Given that there is clearly substantial interest in this topic from the ACL community and given that the research on this area is evolving quickly, I believe that the proposed tutorial will help attendees to keep up to date with recent advances in the field and discover new directions for future research.",2012,NAACL,0.0
Combining the Sparsity and Unambiguity Biases for Grammar Induction,"In this paper we describe our participating system for the dependency induction track of the PASCAL Challenge on Grammar Induction. Our system incorporates two types of inductive biases: the sparsity bias and the unambiguity bias. The sparsity bias favors a grammar with fewer grammar rules. The unambiguity bias favors a grammar that leads to unambiguous parses, which is motivated by the observation that natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is very small. We introduce our approach to combining these two types of biases and discuss the system implementation. Our experiments show that both types of inductive biases are beneficial to grammar induction.",2012,NAACL,1.0
Up from Limited Dialog Systems!,"In the last two decades, information-seeking spoken dialog systems (SDS) have moved from research prototypes to real-life commercial applications. Still, dialog systems are limited by the scale, complexity of the task and coverage of knowledge required by problemsolving machines or mobile personal assistants. Future spoken interaction are required to be multilingual, understand and act on large scale knowledge bases in all its forms (from structured to unstructured). The Web research community have striven to build large scale and open multilingual resources (e.g. Wikipedia) and knowledge bases (e.g. Yago). We argue that a) it is crucial to leverage this massive amount of Web lightly structured knowledge and b) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community.",2012,NAACL,-0.4
Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems,"We examine evaluation methods for systems that automatically annotate images using cooccurring text. We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. Some of our baselines match or exceed the best published scores for those datasets. These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. We conclude with a list of recommended practices for future research combining language and vision processing techniques.",2012,NAACL,-1.0
The Challenges of Parsing Chinese with Combinatory Categorial Grammar,"We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese with the new Chinese CCGbank, bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent. We train two state-of-the-art English  parsers: the parser of Petrov and Klein (P&K), and the Clark and Curran (C&C) parser, uncovering a surprising performance gap between them not observed in English — 72.73 (P&K) and 67.09 (C&C) F -score on  6. We explore the challenges of Chinese  parsing through three novel ideas: developing corpus variants rather than treating the corpus as fixed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop.",2012,NAACL,1.0
Computational Analysis of Referring Expressions in Narratives of Picture Books,"This paper discusses successes and failures of computational linguistics techniques in the study of how inter-event time intervals in a story affect the narrator’s use of different types of referring expressions. The success story shows that a conditional frequency distribution analysis of proper nouns and pronouns yields results that are consistent with our previous results – based on manual coding – that the narrator’s choice of referring expression depends on the amount of time that elapsed between events in a story. Unfortunately, the less successful story indicates that state-of-the-art coreference resolution systems fail to achieve high accuracy for this genre of discourse. Fine-grained analyses of these failures provide insight into the limitations of current coreference resolution systems, and ways of improving them.",2012,NAACL,-0.8
Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization,"Probabilistic graphical models are powerful tools for analyzing constrained, continuous domains. However, finding most-probable explanations (MPEs) in these models can be computationally expensive. In this paper, we improve the scalability of MPE inference in a class of graphical models with piecewise-linear and piecewise-quadratic dependencies and linear constraints over continuous domains. We derive algorithms based on a consensus-optimization framework and demonstrate their superior performance over state of the art. We show empirically that in a large-scale voter-preference modeling problem our algorithms scale linearly in the number of dependencies and constraints.",2012,NIPS,1.0
Probabilistic Event Cascades for Alzheimer's disease,"Accurate and detailed models of the progression of neurodegenerative diseases such as Alzheimer's (AD) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments. In this paper, we introduce the ALPACA (Alzheimer's disease Probabilistic Cascades) model, a generative model linking latent Alzheimer's progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models. We describe efficient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer's patients from the Alzheimer's Disease Neuroimaging Initiative",2012,NIPS,0.9
Phoneme Classification using Constrained Variational Gaussian Process Dynamical System,"This paper describes a new acoustic model based on variational Gaussian process dynamical system (VGPDS) for phoneme classification. The proposed model overcomes the limitations of the classical HMM in modeling the real speech data, by adopting a nonlinear and nonparametric model. In our model, the GP prior on the dynamics function enables representing the complex dynamic structure of speech, while the GP prior on the emission function successfully models the global dependency over the observations. Additionally, we introduce variance constraint to the original VGPDS for mitigating sparse approximation error of the kernel matrix. The effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets.",2012,NIPS,1.0
Selective Labeling via Error Bound Minimization,"In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the generalization error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic generalization error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.",2012,NIPS,1.0
Efficient and direct estimation of a neural subunit model for sensory coding Authors Abstract,"Many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of spatially shifted linear filters. These filters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple filters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the filters reside rather than the filters themselves. Here, we assume a linear-nonlinear‚Äìlinear-nonlinear (LN-LN) cascade model in which the first linear stage is a set of shifted (‚Äòconvolutional‚Äô) copies of a common filter, and the first nonlinear stage consists of rectifying scalar nonlinearities that are identical for all filter outputs. We refer to these initial LN elements as the ‚Äòsubunits‚Äô of the receptive field. The second linear stage then computes a weighted sum of the responses of the rectified subunits. We present a method for directly fitting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model significantly outperforms STA and STC in terms of cross-validated accuracy and efficiency.",2012,NIPS,1.0
"On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking Authors Abstract","We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk.",2012,NIPS,-0.5
Collaborative Ranking With 17 Parameters Authors Abstract,"Solomonoff‚Äôs general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Gr√ºnwald, 2007; Rissanen, 2007) formalize Occam‚Äôs razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks (Hinton and Van Camp, 1993; Schmidhuber, 1997). Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff‚Äôs approach.",2012,NIPS,0.8
Non-parametric Approximate Dynamic Programming via the Kernel Method Authors Abstract,"This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our procedure is competitive with parametric ADP approaches.",2012,NIPS,0.8
Scalable imputation of genetic data with a discrete fragmentation-coagulation process Authors Abstract,"Let f : {‚àí1, 1} ‚Üí R be an n-variate polynomial consisting of 2 monomials, in which only s 2 coefficients are non-zero. The goal is to learn the polynomial by querying the values of f . We introduce an active learning framework that is associated with a low query cost and computational runtime. The significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse-graph codes, such as Low-Density-Parity-Check (LDPC) codes, which represent the state-of-the-art of modern packet communications. More significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding. The key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size n and sparsity s). Our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = O(2) for any Œ¥ ‚àà (0, 1), where f is exactly learned using O(ns) queries in time O(ns log s), even if the queries are perturbed by Gaussian noise. We further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts. By writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub-linearly in the graph size n. Experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.",2012,NIPS,0.4
On the connections between saliency and tracking Authors Abstract,"Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(Œª)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.",2012,NIPS,-1.0
ETS: Discriminative Edit Models for Paraphrase Scoring,"Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts. However, many systems that address these tasks focus on a single task and may or may not generalize well. In this work, we extend an existing machine translation metric, TERp (Snover et al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.",2012,SemEval,0.8
Multilingual Sentiment Analysis using Machine Translation?,"The past years have shown a steady growth in interest in the Natural Language Process- ing task of sentiment analysis. The research community in this field has actively proposed and improved methods to detect and classify the opinions and sentiments expressed in dif- ferent types of text - from traditional press ar- ticles, to blogs, reviews, fora or tweets. A less explored aspect has remained, however, the issue of dealing with sentiment expressed in texts in languages other than English. To this aim, the present article deals with the prob- lem of sentiment detection in three different languages - French, German and Spanish - us- ing three distinct Machine Translation (MT) systems - Bing, Google and Moses. Our ex- tensive evaluation scenarios show that SMT systems are mature enough to be reliably em- ployed to obtain training data for languages other than English and that sentiment analysis systems can obtain comparable performances to the one obtained for English.",2012,WS,1.0
Sweeping through the Topic Space: Bad luck? Roll again!,"Topic Models (TM) such as Latent Dirich- let Allocation (LDA) are increasingly used in Natural Language Processing applica- tions. At this, the model parameters and the influence of randomized sampling and inference are rarely examined — usually, the recommendations from the original pa- pers are adopted. In this paper, we ex- amine the parameter space of LDA topic models with respect to the application of Text Segmentation (TS), specifically target- ing error rates and their variance across dif- ferent runs. We find that the recommended settings result in error rates far from opti- mal for our application. We show substan- tial variance in the results for different runs of model estimation and inference, and give recommendations for increasing the robust- ness and stability of topic models. Run- ning the inference step several times and se- lecting the last topic ID assigned per token, shows considerable improvements. Similar improvements are achieved with the mode method: We store all assigned topic IDs during each inference iteration step and se- lect the most frequent topic ID assigned to each word. These recommendations do not only apply to TS, but are generic enough to transfer to other applications.",2012,WS,-0.7000000000000001
"""I Don't Know Where He is Not"": Does Deception Research yet offer a basis for Deception Detectives?","Suppose we wanted to create an intelligent machine that somehow drew its intelligence from large collections of text, possibly involving the processing of collections available on the Web such as Wikipedia. Does past research in deception offer a sufficiently robust basis upon which we might develop a means to filter out texts that are deceptive, either partially or entirely? Could we identify, for example, any deliberately deceptive edits to Wikipedia without consulting the edit history? In this paper, we offer a critical review of deception research. We suggest that there are a range of inconsistencies, contradictions, and other difficulties in recent deception research, and identify how we might begin to address deception research in a more systematic manner.",2012,WS,-1.0
Do NLP and machine learning improve traditional readability formulas?,"Readability formulas are methods used to match texts with the readers’ reading level. Several methodological paradigms have pre- viously been investigated in the field. The most popular paradigm dates several decades back and gave rise to well known readability formulas such as the Flesch formula (among several others). This paper compares this ap- proach (henceforth ”classic”) with an emerg- ing paradigm which uses sophisticated NLP- enabled features and machine learning tech- niques. Our experiments, carried on a corpus of texts for French as a foreign language, yield four main results: (1) the new readability for- mula performed better than the “classic” for- mula; (2) “non-classic” features were slightly more informative than “classic” features; (3) modern machine learning algorithms did not improve the explanatory power of our read- ability model, but allowed to better classify new observations; and (4) combining “classic” and “non-classic” features resulted in a signif- icant gain in performance.",2012,WS,1.0
Bridging the Gap Between Scope-based and Event-based Negation/Speculation Annotations: A Bridge Not Too Far,"We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope rep- resentation considered in the CoNLL-2010 Shared Task, and the tagged-event representa- tion applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems rep- resenting the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of ad- dressing them. We demonstrate the feasibility of our approach by constructing a method that uses cue-and-scope analyses together with a small set of features motivated by data anal- ysis to predict event negation and speculation. Evaluation on BioNLP Shared Task 2011 data indicates the method to outperform the nega- tion/speculation components of state-of-the- art event extraction systems.The system and resources introduced in this work are publicly available for research pur- poses at: https://github.com/ninjin/eepura",2012,WS,0.30000000000000004
Is Syntactic Binding Rational?,"Recent results show that both TAG and Minimalist grammars can be enriched with rational constraints without increasing their strong generative capacity, where a con- straint is rational iff it can be computed by a bottom-up tree automaton. This raises the question which aspects of syntax can be ad- equately formalized using only such con- straints. One of hardest phenomena com- monly studied by syntacticians is binding theory. In this paper, we give a high-level implementation of (the syntactic parts of) binding theory in terms of rational con- straints, and we argue that this implemen- tation is sufficiently powerful for natural language. This conclusion is backed up by data drawn from English, German, and American Sign Language.",2012,WS,1.0
Can Machine Learning Algorithms Improve Phrase Selection in Hybrid Machine Translation?,"We describe a substitution-based, hybrid machine translation (MT) system that has been extended with a machine learning component controlling its phrase selection. Our approach is based on a rule-based MT (RBMT) system which creates template translations. Based on the generation parse tree of the RBMT system and standard word alignment computation, we identify potential “translation snippets” from one or more translation engines which could be substituted into our translation templates. The substitution process is controlled by a binary classifier trained on feature vectors from the different MT engines. Using a set of manually annotated training data, we are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system.",2012,WS,1.0
A Topic-Based Coherence Model for Statistical Machine Translation,"Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose a topic-based coherence model to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. The proposed topic-based coherence model then uses the predicted target coherence chain to help decoder select coherent word/phrase translations. Our experiments show that incorporating the topic-based coherence model into machine translation achieves substantial improvement over both the baseline and previous methods that integrate document topics rather than coherence chains into machine translation.",2013,AAAI,1.0
Learning to Rank Effective Paraphrases from Query Logs for Community Question Answering,"We present a novel method for ranking query paraphrases for effective search in community question answering (cQA). The method uses query logs from Yahoo! Search and Yahoo! Answers for automatically extracting a corpus of paraphrases of queries and questions using the query-question click history. Elements of this corpus are automatically ranked according to recall and mean reciprocal rank, and then used for learning two independent learning to rank models (SVMRank), whereby a set of new query paraphrases can be scored according to recall and MRR. We perform several automatic evaluation procedures using cross-validation for analyzing the behavior of various aspects of our learned ranking functions, which show that our method is useful and effective for search in cQA.",2013,AAAI,0.9
"Reasoning about Saturated Conditional Independence Under Uncertainty: Axioms, Algorithms, and Levesque's Situations to the Rescue","The implication problem of probabilistic conditional independencies is investigated in the presence of missing data. Here, graph separation axioms fail to hold for saturated conditional independencies, unlike the known idealized case with no missing data. Several axiomatic, algorithmic, and logical characterizations of the implication problem for saturated conditional independencies are established. In particular, equivalences are shown to the implication problem of a propositional fragment under Levesque‚Äôs situations, and that of Lien‚Äôs class of multivalued database dependencies under null values.",2013,AAAI,-0.5
Invited Talks,"Most approaches to semantics in computational linguistics represent meaning in terms of words or abstract symbols. Grounded-language research bases the meaning of natural language on perception and/or action in the (real or virtual) world. Machine learning has become the most effective approach to constructing natural-language systems; however, current methods require a great deal of laboriously annotated training data. Ideally, a computer would be able to acquire language like a child, by being exposed to language in the context of a relevant but ambiguous environment, thereby grounding its learning in perception and action. We will review recent research in grounded language learning and discuss future directions. Raymond J. Mooney is a professor in the Department of Computer Science at the University of Texas at Austin. He received his Ph.D. in 1988 from the University of Illinois at Urbana/Champaign. He is an author of over 150 published research papers, primarily in the areas of machine learning and natural language processing. He was the president of the International Machine Learning Society from 2008-2011, program cochair for AAAI 2006, and is a AAAI and ACM Fellow. His recent research has focused on learning for natural-language processing, statistical relational learning, active transfer learning, and connecting language, perception and action.",2013,AAAI,0.0
An Extended GHKM Algorithm for Inducing Lambda-SCFG,"Semantic parsing, which aims at mapping a natural language (NL) sentence into its formal meaning representation (e.g., logical form), has received increasing attention in recent years. While synchronous context-free grammar (SCFG) augmented with lambda calculus (ŒªSCFG) provides an effective mechanism for semantic parsing, how to learn such Œª-SCFG rules still remains a challenge because of the difficulty in determining the correspondence between NL sentences and logical forms. To alleviate this structural divergence problem, we extend the GHKM algorithm, which is a state-ofthe-art algorithm for learning synchronous grammars in statistical machine translation, to induce Œª-SCFG from pairs of NL sentences and logical forms. By treating logical forms as trees, we reformulate the theory behind GHKM that gives formal semantics to the alignment between NL words and logical form tokens. Experiments on the GEOQUERY dataset show that our semantic parser achieves an F-measure of 90.2%, the best result published to date.",2013,AAAI,1.0
Heterogeneous Metric Learning with Joint Graph Regularization for Cross-Media Retrieval,"As the major component of big data, unstructured heterogeneous multimedia content such as text, image, audio, video and 3D increasing rapidly on the Internet. User demand a new type of cross-media retrieval where user can search results across various media by submitting query of any media. Since the query and the retrieved results can be of different media, how to learn a heterogeneous metric is the key challenge. Most existing metric learning algorithms only focus on a single media where all of the media objects share the same data representation. In this paper, we propose a joint graph regularized heterogeneous metric learning (JGRHML) algorithm, which integrates the structure of different media into a joint graph regularization. In JGRHML, different media are complementary to each other and optimizing them simultaneously can make the solution smoother for both media and further improve the accuracy of the final metric. Based on the heterogeneous metric, we further learn a high-level semantic metric through label propagation. JGRHML is effective to explore the semantic relationship hidden across different modalities. The experimental results on two datasets with up to five media types show the effectiveness of our proposed approach.",2013,AAAI,0.7000000000000001
Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain,"In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.",2013,ACL,-0.4
DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German,"Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books.",2013,ACL,0.5
Models of Translation Competitions,"What do we want to learn from a translation competition and how do we learn it with confidence? We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust. In response, we provide the first framework that allows an empirical comparison of different analyses of competition results. We then use this framework to compare several analytical models on data from the Workshop on Machine Translation (WMT). 1 The WMT Translation Competition Every year, the Workshop on Machine Translation (WMT) conducts a competition between machine translation systems. The WMT organizers invite research groups to submit translation systems in eight different tracks: Czech to/from English, French to/from English, German to/from English, and Spanish to/from English. For each track, the organizers also assemble a panel of judges, typically machine translation specialists.1 The role of a judge is to repeatedly rank five different translations of the same source text. Ties are permitted. In Table 1, we show an example2 where a judge (we’ll call him “jdoe”) has ranked five translations of the French sentence “Il ne va pas.” Each such elicitation encodes ten pairwise comparisons, as shown in Table 2. For each competition track, WMT typically elicits between 5000 and 20000 comparisons. Once the elicitation process is complete, WMT faces a large database of comparisons and a question that must be answered: whose system is the best? Although in recent competitions, some of the judging has also been crowdsourced (Callison-Burch et al., 2010). The example does not use actual system output. rank system translation 1 bbn “He does not go.” 2 (tie) uedin “He goes not.” 2 (tie) jhu “He did not go.” 4 cmu “He go not.” 5 kit “He not go.” Table 1: WMT elicits preferences by asking judges to simultaneously rank five translations, with ties permitted. In this (fictional) example, the source sentence is the French “Il ne va pas.” source text sys1 sys2 judge preference “Il ne va pas.” bbn cmu jdoe 1 “Il ne va pas.” bbn jhu jdoe 1 “Il ne va pas.” bbn kit jdoe 1 “Il ne va pas.” bbn uedin jdoe 1 “Il ne va pas.” cmu jhu jdoe 2 “Il ne va pas.” cmu kit jdoe 1 “Il ne va pas.” cmu uedin jdoe 2 “Il ne va pas.” jhu kit jdoe 1 “Il ne va pas.” jhu uedin jdoe 0 “Il ne va pas.” kit uedin jdoe 2 Table 2: Pairwise comparisons encoded by Table 1. A preference of 0 means neither translation was preferred. Otherwise the preference specifies the preferred system.",2013,ACL,0.5
Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation,"Query segmentation, like text chunking, is the first step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.",2013,ACL,-0.8
The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia,"With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.",2013,ACL,-0.30000000000000004
Name-aware Machine Translation,"We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1.",2013,ACL,1.0
Robust Automated Natural Language Processing with Multiword Expressions and Collocations,"This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of multiword expressions (MWEs), their relevance for robust automated natural language processing and language technology, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication.",2013,ACL,0.0
Scalable Decipherment for Machine Translation via Hash Sampling,"In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).",2013,ACL,1.0
Evaluating Text Segmentation using Boundary Edit Distance,"This work proposes a new segmentation evaluation metric, named boundary similarity (B), an inter-coder agreement coefficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing segmentation metrics such as Pk, WindowDiff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement & performance, leading this work to propose a solution.",2013,ACL,-0.2
Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation,"In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets.",2013,ACL,1.0
The mathematics of language learning,"Over the past decade, attention has gradually shifted from the estimation of parameters to the learning of linguistic structure (for a survey see Smith 2011). The Mathematics of Language (MOL) SIG put together this tutorial, composed of three lectures, to highlight some alternative learning paradigms in speech, syntax, and semantics in the hopes of accelerating this trend. Compounding the enormous variety of formal models one may consider is the bewildering range of ML techniques one may bring to bear. In addition to the surprisingly useful classical techniques inherited from multivariate statistics such as Principal Component Analysis (PCA, Pearson 1901) and Linear Discriminant Analysis (LDA, Fisher 1936), computational linguists have experimented with a broad range of neural net, nearest neighbor, maxent, genetic/evolutionary, decision tree, max margin, boost, simulated annealing, and graphical model learners. While many of these learners became standard in various domains of ML, within CL the basic HMM approach proved surprisingly resilient, and it is only very recently that deep learning techniques from neural computing are becoming competitive not just in speech, but also in OCR, paraphrase, sentiment analysis, parsing and vector-based semantic representations. The first lecture will provide a mathematical introduction to some of the fundamental techniques that lie beneath these linguistic applications of neural networks, such as: BFGS optimization, finite difference approximations of Hessians and Hessianfree optimization, contrastive divergence and variational inference.",2013,ACL,0.0
Offspring from Reproduction Problems: What Replication Failure Teaches Us,"Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.",2013,ACL,-1.0
Does Korean defeat phonotactic word segmentation?,"Computational models of infant word segmentation have not been tested on a wide range of languages. This paper applies a phonotactic segmentation model to Korean. In contrast to the undersegmentation pattern previously found in English and Russian, the model exhibited more oversegmentation errors and more errors overall. Despite the high error rate, analysis suggested that lexical acquisition might not be problematic, provided that infants attend only to frequently segmented items.",2013,ACL,-0.4
A Tree Transducer Model for Grammatical Error Correction,"We present an approach to grammatical error correction for the CoNLL 2013 shared task based on a weighted tree-to-string transducer. Rules for the transducer are extracted from the NUCLE training data. An n-gram language model is used to rerank k-best sentence lists generated by the transducer. Our system obtains a precision, recall and F1 score of 0.27, 0.1333 and 0.1785, respectively, on the official test set. On the revised annotations, the F1 score increases to 0.2505. Our system ranked 6th out of the participating teams on both the original and revised test set annotations.",2013,CoNLL,1.0
Hidden Markov tree models for semantic class induction,"In this paper, we propose a new method for semantic class induction. First, we introduce a generative model of sentences, based on dependency trees and which takes into account homonymy. Our model can thus be seen as a generalization of Brown clustering. Second, we describe an efficient algorithm to perform inference and learning in this model. Third, we apply our proposed method on two large datasets (108 tokens, 105 words types), and demonstrate that classes induced by our algorithm improve performance over Brown clustering on the task of semisupervised supersense tagging and named entity recognition.",2013,CoNLL,1.0
Analysis of Stopping Active Learning based on Stabilizing Predictions,"Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen’s Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where T > 0), then the difference in F-measure performance between those models is bounded above by 4(1−T ) T in all cases. If precision of the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4(1−T ) (p+1)T .",2013,CoNLL,0.30000000000000004
Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!,"The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE, as well as academia’s perception that rulebased IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice.",2013,EMNLP,0.0
MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text,"We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone’s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today’s computers and algorithms.",2013,EMNLP,1.0
Exploiting Language Models for Visual Recognition,"The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.",2013,EMNLP,0.0
"Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts","Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.",2013,EMNLP,0.30000000000000004
Title:Knowledge Matters: Importance of Prior Information for Optimization,"We explore the effect of introducing prior information into the intermediate level of deep supervised neural networks for a learning task on which all the black-box state-of-the-art machine learning algorithms tested have failed to learn. We motivate our work from the hypothesis that there is an optimization obstacle involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for which each image input contains three sprites, and the binary target class is 1 if all three have the same shape. Black-box machine learning algorithms only got chance on this task. Standard deep supervised neural networks also failed. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allows to nail the task. Much better than chance but imperfect results are also obtained by exploring architecture and optimization variants, pointing towards a difficult optimization task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of effective local minima (possibly due to ill-conditioning and the training procedure not being able to escape what appears like a local minimum).",2013,ICLR,0.7000000000000001
Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations,"We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach can be used for improving semisupervised sparse coding.",2013,ICML,1.0
Better Mixing via Deep Representations,"It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the highdensity manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.",2013,ICML,1.0
Online Latent Dirichlet Allocation with Infinite Vocabulary,"Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary. This is reasonable in batch settings but not reasonable for streaming and online settings. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and‚Äîto only consider a finite number of words for each topic‚Äîpropose heuristics to dynamically order, expand, and contract the set of words we consider in our vocabulary. We show our model can successfully incorporate new words and that it performs better than topic models with finite vocabularies in evaluations of topic quality and classification performance.",2013,ICML,1.0
Online Feature Selection for Model-based Reinforcement Learning,"We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.",2013,ICML,1.0
Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing,"Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing ‚Äúmessages‚Äù over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a ‚Äúunified‚Äù message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).",2013,ICML,0.4
On the importance of initialization and momentum in deep learning,"Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",2013,ICML,0.30000000000000004
Algorithms for Direct 0‚Äì1 Loss Optimization in Binary Classification,"While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers. On the other hand, while the nonconvex 0‚Äì1 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice. In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0‚Äì1 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0‚Äì1 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0‚Äì1 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers. To this end, we believe this work reiterates the importance of 0‚Äì1 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.",2013,ICML,0.5
Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation,"We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.",2013,ICML,1.0
Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data,"We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.",2013,ICML,0.2
To Link or Not to Link? A Study on End-to-End Tweet Entity Linking,"Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% F1.",2013,NAACL,1.0
Improving reordering performance using higher order and structural features,"Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and finds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efficiently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points.",2013,NAACL,1.0
What's in a Domain? Multi-Domain Learning for Multi-Attribute Data,"Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute.",2013,NAACL,1.0
Systematic Comparison of Professional and Crowdsourced Reference Translations for Machine Translation,"We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation.",2013,NAACL,0.1
Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters,"We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data.",2013,NAACL,1.0
Robust Systems for Preposition Error Correction Using Wikipedia Revisions,"We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. We present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections.",2013,NAACL,0.7000000000000001
Topic Segmentation with a Structured Topic Model,We present a new hierarchical Bayesian model for unsupervised topic segmentation. This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents. We develop an MCMC inference algorithm to split/merge segment(s). Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi’s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets.,2013,NAACL,1.0
Text Alignment for Real-Time Crowd Captioning,"The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based on weighted A search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.",2013,NAACL,1.0
Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion for Automated Narrative Retelling Assessment,"Automatically assessing the fidelity of a retelling to the original narrative – a task of growing clinical importance – is challenging, given extensive paraphrasing during retelling along with cascading automatic speech recognition (ASR) errors. We present a word tagging approach using conditional random fields (CRFs) that allows a diversity of features to be considered during inference, including some capturing acoustic confusions encoded in word confusion networks. We evaluate the approach under several scenarios, including both supervised and unsupervised training, the latter achieved by training on the output of a baseline automatic word-alignment model. We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance. We find strong robustness to ASR errors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone.",2013,NAACL,1.0
Improving speech synthesis quality by reducing pitch peaks in the source recordings,"We present a method for improving the perceived naturalness of corpus-based speech synthesizers. It consists in removing pronounced pitch peaks in the original recordings, which typically lead to noticeable discontinuities in the synthesized speech. We perceptually evaluated this method using two concatenative and two HMM-based synthesis systems, and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility.",2013,NAACL,1.0
Segmentation Strategies for Streaming Speech Translation,"The study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics. We address the goal of achieving a system that balances translation accuracy and latency. In order to improve ASR performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. In order to improve machine translation (MT) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments. Among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. It was also found to be important to synchronize various pipeline components in order to minimize latency.",2013,NAACL,1.0
Negative Deceptive Opinion Spam,"The rising influence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM—fictitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship.",2013,NAACL,0.7000000000000001
Towards Topic Labeling with Phrase Entailment and Aggregation,"We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.",2013,NAACL,1.0
What do row and column marginals reveal about your dataset?,"Numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices. While this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis. Here, we investigate how these data can be exploited to make inferences about the underlying matrix H. Instead of assuming a generative model for H, we view the input marginals as constraints on the dataspace of possible realizations of H and compute the probability density function of particular entries H(i,j) of interest. We do this, for all the cells of H simultaneously, without generating realizations but rather via implicitly sampling the datasets that satisfy the input marginals. The end result is an efficient algorithm with running time equal to the time required by standard sampling techniques to generate a single dataset from the same dataspace. Our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.",2013,NIPS,0.8
Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints,"We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9, 23] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and, an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.",2013,NIPS,0.9
"Reconciling ""priors"" & ""priors"" without prejudice?","There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efficiency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization.",2013,NIPS,1.0
Which Space Partitioning Tree to Use for Search?,"We consider the task of nearest-neighbor search with the class of binary-space-partitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question which tree to use for nearest-neighbor search?'' To this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance -- margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve the search performance of a space-partitioning tree.",2013,NIPS,1.0
Bellman Error Based Feature Generation using Random Projections on Sparse Spaces,"This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging.",2013,NIPS,1.0
Universal models for binary spike patterns using centered Dirichlet processes Authors Abstract,"Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.",2013,NIPS,1.0
One-shot learning and big data with n=2 Authors Abstract,"We model a ‚Äúone-shot learning‚Äù situation, where very few observations y1, ..., yn ‚àà R are available. Associated with each observation yi is a very highdimensional vector xi ‚àà R, which provides context for yi and enables us to predict subsequent observations, given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of xi is large; in other words, prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance, we show that classical PCR estimators may be inconsistent in the specified setting, unless they are multiplied by a scalar c > 1; that is, unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods (c < 1), which are far more common in big data analyses.",2013,NIPS,0.8
Spike train entropy-rate estimation using hierarchical Dirichlet process priors Authors Abstract,"We present a joint image segmentation and labeling model (JSL) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.",2013,NIPS,1.0
Projecting Ising Model Parameters for Fast Mixing Authors Abstract,"We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al.",2013,NIPS,1.0
Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel Authors Abstract,"In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.",2013,NIPS,1.0
Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result Authors Abstract,"Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(Œª)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.",2013,NIPS,-0.7000000000000001
Non-Linear Domain Adaptation with Boosting Authors Abstract,"A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state of the art.",2013,NIPS,1.0
Multi-Prediction Deep Boltzmann Machines Authors Abstract,"We introduce the multi-prediction deep Boltzmann machine (MP-DBM). The MPDBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.1",2013,NIPS,0.5
Perfect Associative Learning with Spike-Timing-Dependent Plasticity Authors Abstract,"We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a unified framework. When learning the topic model, we leverage a distilled underlying distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. Such a strategy provides the updating of word embeddings with robust guidance, improving the algorithmic convergence. As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.",2013,NIPS,0.8
A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks Authors Abstract,"Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, which includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing poolbased methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness by comparing with existing heuristics on common benchmark datasets.",2013,NIPS,0.7000000000000001
More Words and Bigger Pictures,"Object recognition is a little like translation: a picture (text in a source language) goes in, and a description (text in a target language) comes out. I will use this analogy, which has proven fertile, to describe recent progress in object recognition. We have very good methods to spot some objects in images, but extending these methods to produce descriptions of images remains very difficult. The description might come in the form of a set of words, indicating objects, and boxes or regions spanned by the object. This representation is difficult to work with, because some objects seem to be much more important than others, and because objects interact. An alternative is a sentence or a paragraph describing the picture, and recent work indicates how one might generate rich structures like this. Furthermore, recent work suggests that it is easier and more effective to generate descriptions of images in terms of chunks of meaning (”person on a horse”) rather than just objects (”person”; ”horse”). Finally, if the picture contains objects that are unfamiliar, then we need to generate useful descriptions that will make it possible to interact with them, even though we don’t know what they are. About the Speaker David Forsyth is currently a full professor at U. Illinois at Urbana-Champaign, where he moved from U.C Berkeley, where he was also full professor. He has published over 130 papers on computer vision, computer graphics and machine learning. He has served as program chair and as general chair for various international conferences on computer vision. He received an IEEE technical achievement award for 2005 for his research and became an IEEE fellow in 2009. His textbook, ”Computer Vision: A Modern Approach” (joint with J. Ponce and published by Prentice Hall) is widely adopted as a course text. A second edition appeared in 2011. He was named editor in chief of IEEE TPAMI for a term starting in Jan 2013.",2013,SemEval,0.0
UTurku: Drug Named Entity Recognition and Drug-Drug Interaction Extraction Using SVM Classification and Domain Knowledge,"The DDIExtraction 2013 task in the SemEval conference concerns the detection of drug names and statements of drug-drug interactions (DDI) from text. Extraction of DDIs is important for providing up-to-date knowledge on adverse interactions between coadministered drugs. We apply the machine learning based Turku Event Extraction System to both tasks. We evaluate three feature sets, syntactic features derived from deep parsing, enhanced optionally with features derived from DrugBank or from both DrugBank and MetaMap. TEES achieves F-scores of 60% for the drug name recognition task and 59% for the DDI extraction task.",2013,SemEval,0.2
IIRG: A Naive Approach to Evaluating Phrasal Semantics,"This paper describes the IIRG 1 system entered in SemEval-2013, the 7th International Workshop on Semantic Evaluation. We participated in Task 5 Evaluating Phrasal Semantics. We have adopted a token-based approach to solve this task using 1) Naı̈ve Bayes methods and 2) Word Overlap methods, both of which rely on the extraction of syntactic features. We found that the word overlap method significantly out-performs the Naı̈ve Bayes methods, achieving our highest overall score with an accuracy of approximately 78%.",2013,SemEval,1.0
Does Size Matter? Text and Grammar Revision for Parsing Social Media Data,"We explore improving parsing social media and other web data by altering the input data, namely by normalizing web text, and by revis- ing output parses. We find that text normal- ization improves performance, though spell checking has more of a mixed impact. We also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model. The results also demonstrate that, more than the size of the training data, the goodness of fit of the data has a great impact on the parser.",2013,WS,1.0
Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue,"More and more of the information on the web is dialogic, from Facebook newsfeeds, to fo- rum conversations, to comment threads on news articles. In contrast to traditional, mono- logic Natural Language Processing resources such as news, highly social dialogue is fre- quent in social media, making it a challenging context for NLP. This paper tests a bootstrap- ping method, originally proposed in a mono- logic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indica- tors to be used in a first level classifier aimed at maximizing precision at the expense of re- call. The best performing classifier for the first phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to cre- ate more general sarcasm indicators, improv- ing precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we boot- strap over the first level with generalized syn- tactic patterns.",2013,WS,1.0
Equilibria in Epidemic Containment Games,"The spread of epidemics and malware is commonly modeled by diffusion processes on networks. Protective interventions such as vaccinations or installing anti-virus software are used to contain their spread. Typically, each node in the network has to decide its own strategy of securing itself, and its benefit depends on which other nodes are secure, making this a natural game-theoretic setting. There has been a lot of work on network security game models, but most of the focus has been either on simplified epidemic models or homogeneous",2014,AAAI,0.0
Fraudulent Support Telephone Number Identification Based on Co-Occurrence Information on the Web,"‚ÄúFraudulent support phones‚Äù refers to the misleading telephone numbers placed on Web pages or other media that claim to provide services with which they are not associated. Most fraudulent support phone information is found on search engine result pages (SERPs), and such information substantially degrades the search engine user experience. In this paper, we propose an approach to identify fraudulent support telephone numbers on the Web based on the co-occurrence relations between telephone numbers that appear on SERPs. We start from a small set of seed official support phone numbers and seed fraudulent numbers. Then, we construct a co-occurrence graph according to the co-occurrence relationships of the telephone numbers that appear on Web pages. Additionally, we take the page layout information into consideration on the assumption that telephone numbers that appear in nearby page blocks should be regarded as more closely related. Finally, we develop a propagation algorithm to diffuse the trust scores of seed official support phone numbers and the distrust scores of the seed fraudulent numbers on the co-occurrence graph to detect additional fraudulent numbers. Experimental results based on over 1.5 million SERPs produced by a popular Chinese commercial search engine indicate that our approach outperforms TrustRank, Anti-TrustRank and Good-Bad Rank algorithms by achieving an AUC value of over 0.90.",2014,AAAI,1.0
A Joint Optimization Model for Image Summarization Based on Image Content and Tags,"As an effective technology for navigating a large number of images, image summarization is becoming a promising task with the rapid development of image sharing sites and social networks. Most existing summarization approaches use the visual-based features for image representation without considering tag information. In this paper, we propose a novel framework, named JOINT, which employs both image content and tag information to summarize images. Our model generates the summary images which can best reconstruct the original collection. Based on the assumption that an image with representative content should also have typical tags, we introduce a similarity-inducing regularizer to our model. Furthermore, we impose the lasso penalty on the objective function to yield a concise summary set. Extensive experiments demonstrate our model outperforms the state-of-the-art approaches.",2014,AAAI,1.0
Learning Unknown Event Models,"Agents with incomplete environment models are likely to be surprised, and this represents an opportunity to learn. We investigate approaches for situated agents to detect surprises, discriminate among different forms of surprise, and hypothesize new models for the unknown events that surprised them. We instantiate these approaches in a new goal reasoning agent (named FOOLMETWICE), investigate its performance in simulation studies, and report that it produces plans with significantly reduced execution cost in comparison to not learning models for surprising events.",2014,AAAI,0.0
Lazy Defenders Are Almost Optimal against Diligent Attackers,"Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender‚Äôs randomized assignment of resources to targets. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. We analytically demonstrate that in zero-sum security games, lazy defenders, who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations. This result implies that, in some realistic situations, limited surveillance may not need to be explicitly addressed.",2014,AAAI,-1.0
Synthesis of Geometry Proof Problems,"This paper presents a semi-automated methodology for generating geometric proof problems of the kind found in a highschool curriculum. We formalize the notion of a geometry proof problem and describe an algorithm for generating such problems over a user-provided figure. Our experimental results indicate that our problem generation algorithm can effectively generate proof problems in elementary geometry. On a corpus of 110 figures taken from popular geometry textbooks, our system generated an average of about 443 problems per figure in an average time of 4.7 seconds per figure.",2014,AAAI,1.0
That's Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text,"We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for selfmonitoring, we find that with a stateof-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.",2014,ACL,0.2
Semantics for Large-Scale Multimedia: New Challenges for NLP,"Thousands of videos are constantly being uploaded to the web, creating a vast resource, and an ever-growing demand for methods to make them easier to retrieve, search, and index. As it becomes feasible to extract both low-level as well as highlevel (symbolic) audio, speech, and video features from this data, these need to be processed further, in order to learn and extract meaningful relations between these. The language processing community has made huge process in analyzing the vast amounts of very noisy text data that is available on the Internet. While it is very difficult to create semantic units of low-level image descriptors or non-speech sounds by themselves, it is comparatively easy to ground semantics in the word output of a speech recognizer, or text data that is loosely associated with a video. This creates an opportunity for NLP researchers to use their unique skills, and make significant contributions to solve tasks on data that is even noisier than web text, but (we argue) even more interesting and challenging.",2014,ACL,0.0
"Semi-Automatic Development of KurdNet, The Kurdish WordNet","Recently, we reported on our efforts to build the first prototype of KurdNet. In this proposal, we highlight the shortcomings of the current prototype and put forward a detailed plan to transform this prototype to a full-fledged lexical database for the Kurdish language.",2014,ACL,0.0
The VerbCorner Project: Findings from Phase 1 of crowd-sourcing a semantic decomposition of verbs,"Any given verb can appear in some syntactic frames (Sally broke the vase, The vase broke) but not others (*Sally broke at the vase, *Sally broke the vase to John). There is now considerable evidence that the syntactic behaviors of some verbs can be predicted by their meanings, and many current theories posit that this is true for most if not all verbs. If true, this fact would have striking implications for theories and models of language acquisition, as well as numerous applications in natural language processing. However, empirical investigations to date have focused on a small number of verbs. We report on early results from VerbCorner, a crowd-sourced project extending this work to a large, representative sample of English verbs.",2014,ACL,0.8
Logical Inference on Dependency-based Compositional Semantics,"Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics. In this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS. An inference engine is built to achieve inference on abstract denotations. Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation. Experiments on FraCaS and PASCAL RTE datasets show promising results.",2014,ACL,0.8
Does the Phonology of L1 Show Up in L2 Texts?,"The relative frequencies of character bigrams appear to contain much information for predicting the first language (L1) of the writer of a text in another language (L2). Tsur and Rappoport (2007) interpret this fact as evidence that word choice is dictated by the phonology of L1. In order to test their hypothesis, we design an algorithm to identify the most discriminative words and the corresponding character bigrams, and perform two experiments to quantify their impact on the L1 identification task. The results strongly suggest an alternative explanation of the effectiveness of character bigrams in identifying the native language of a writer.",2014,ACL,-0.8
Modeling Factuality Judgments in Social Media Text,"How do journalists mark quoted content as certain or uncertain, and how do readers interpret these signals? Predicates such as thinks, claims, and admits offer a range of options for framing quoted content according to the author’s own perceptions of its credibility. We gather a new dataset of direct and indirect quotes from Twitter, and obtain annotations of the perceived certainty of the quoted statements. We then compare the ability of linguistic and extra-linguistic features to predict readers’ assessment of the certainty of quoted content. We see that readers are indeed influenced by such framing devices — and we find no evidence that they consider other factors, such as the source, journalist, or the content itself. In addition, we examine the impact of specific framing devices on perceptions of credibility.",2014,ACL,0.6000000000000001
Improving the Recognizability of Syntactic Relations Using Contextualized Examples,"A common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words. Previous interfaces for searching over syntactic structures require programming-style queries. User interface research suggests that it is easier to recognize a pattern than to compose it from scratch; therefore, interfaces for non-experts should show previews of syntactic relations. What these previews should look like is an open question that we explored with a 400-participant Mechanical Turk experiment. We found that syntactic relations are recognized with 34% higher accuracy when contextual examples are shown than a baseline of naming the relations alone. This suggests that user interfaces should display contextual examples of syntactic relations to help users choose between different relations.",2014,ACL,0.2
Parser Evaluation Using Derivation Trees: A Complement to evalb,"This paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees. We analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank. This provides some insight into the evalb scores, and the problem of domain adaptation with the web data. We also analyze a “test-ontrain” dataset, showing a wide variance in how the parser is generalizing from different structures in the training material.",2014,ACL,-0.2
How much do word embeddings encode about syntax?,"Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a stateof-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art baseline. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.",2014,ACL,-0.30000000000000004
Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies,"We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Qlearning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.",2014,ACL,1.0
Automatic Keyphrase Extraction: A Survey of the State of the Art,"While automatic keyphrase extraction has been examined extensively, state-of-theart performance on this task is still much lower than that on many core natural language processing tasks. We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.",2014,ACL,-0.6000000000000001
"Semantics, Discourse and Statistical Machine Translation","In the past decade, statistical machine translation (SMT) has been advanced from word-based SMT to phraseand syntax-based SMT. Although this advancement produces significant improvements in BLEU scores, crucial meaning errors and lack of cross-sentence connections at discourse level still hurt the quality of SMT-generated translations. More recently, we have witnessed two active movements in SMT research: one towards combining semantics and SMT in attempt to generate not only grammatical but also meaningpreserved translations, and the other towards exploring discourse knowledge for document-level machine translation in order to capture intersentence dependencies. The emergence of semantic SMT are due to the combination of two factors: the necessity of semantic modeling in SMT and the renewed interest of designing models tailored to relevant NLP/SMT applications in the semantics community. The former is represented by recent numerous studies on exploring word sense disambiguation, semantic role labeling, bilingual semantic representations as well as semantic evaluation for SMT. The latter is reflected in CoNLL shared tasks, SemEval and SenEval exercises in recent years. The need of capturing cross-sentence dependencies for document-level SMT triggers the resurgent interest of modeling translation from the perspective of discourse. Discourse phenomena, such as coherent relations, discourse topics, lexical cohesion that are beyond the scope of conventional sentence-level n-grams, have been recently considered and explored in the context of SMT. This tutorial aims at providing a timely and combined introduction of such recent work along these two trends as discourse is inherently connected with semantics. The tutorial has three parts. The first part critically reviews the phraseand syntax-based SMT. The second part is devoted to the lines of research oriented to semantic SMT, including a brief introduction of semantics, lexical and shallow semantics tailored to SMT, semantic representations in SMT, semantically motivated evaluation as well as advanced topics on deep semantic learning for SMT. The third part is dedicated to recent work on SMT with discourse, including a brief review on discourse studies from linguistics and computational viewpoints, discourse research from monolingual to multilingual, discourse-based SMT and a few advanced topics. The tutorial is targeted for researchers in the SMT, semantics and discourse communities. In particular, the expected audience comes from two groups: 1) Researchers and students in the SMT community who want to design cutting-edge models and algorithms for semantic SMT with various semantic knowledge and representations, and who would like to advance SMT from sentence-bysentence translation to document-level translation with discourse information; 2) Researchers and students from the semantics and discourse community who are interested in developing models and methods and adapting them to SMT.",2014,ACL,-0.1
A Recursive Recurrent Neural Network for Statistical Machine Translation,"In this paper, we propose a novel recursive recurrent neural network (R2NN) to model the end-to-end decoding process for statistical machine translation. R2NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the stateof-the-art baseline by about 1.5 points in BLEU.",2014,ACL,1.0
Negation Focus Identification with Contextual Discourse Information,"Negative expressions are common in natural language text and play a critical role in information extraction. However, the performances of current systems are far from satisfaction, largely due to its focus on intrasentence information and its failure to consider inter-sentence information. In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information. *",2014,ACL,1.0
Linguistically debatable or just plain wrong?,"In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them. We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated.",2014,ACL,-0.6000000000000001
"Book Reviews: Semantic Relations Between Nominals by Vivi Nastase, Preslav Nakov, Diarmuid Ó Séaghdha, and Stan Szpakowicz","Understanding noun compounds is the challenge that drew me to study computational linguistics. Think about how just two words, side by side, evoke a whole story: cacao seeds evokes the tree on which the cacao seeds grow, and to understand cacao powder we need to also imagine the seeds of the cacao tree that are crushed to powder. What conjures up these concepts of tree and grow, and seeds and crush, which are not explicitly present in the written word but are essential for our complete understanding of the compounds? The mechanisms by which we make sense of noun compounds can illuminate how we understand language more generally. And because the human mind is so wily as to provide interpretations even when we do not ask it to, I have always found it useful to study these phenomena of language on the computer, because the computer surely does not (yet) have the type of knowledge that must be brought to bear on the problem. If you find these phenomena equally intriguing and puzzling, then you will find this book by Nastase, Nakov, Ó Séaghdga, and Szpakowicz a wonderful summary of past research efforts and a good introduction to the current methods for analyzing semantic relations. To be clear, this book is not only about noun compounds, but explores all types of relations that can hold between what is expressed linguistically as nominal. Such nominals include entities (e.g., Godiva, Belgium) as well as nominals that refer to events (cultivation, roasting) and nominals with complex structure (delicious milk chocolate). In doing so, describing the different semantic relations between chocolate in the 20th century and chocolate in Belgium is within the scope of this book. This is a wise choice as there are then some linguistic cues that will help define and narrow the types of semantic relations (e.g., the prepositions above). Noun compounds are degenerate in the sense that there are few if any overt linguistic cues as to the semantic relations between the nominals.",2014,CL,0.0
Improved Estimation of Entropy for Evaluation of Word Sense Induction,"Information-theoretic measures are among the most standard techniques for evaluation of clustering methods including word sense induction (WSI) systems. Such measures rely on sample-based estimates of the entropy. However, the standard maximum likelihood estimates of the entropy are heavily biased with the bias dependent on, among other things, the number of clusters and the sample size. This makes the measures unreliable and unfair when the number of clusters produced by different systems vary and the sample size is not exceedingly large. This corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number arguably does not exist and the standard evaluation scenarios use a small number of instances of each word to compute the score. We describe more accurate entropy estimators and analyze their performance both in simulations and on evaluation of WSI systems.",2014,CL,0.1
Squibs: On the Universal Generation Problem for Unification Grammars,"The universal generation problem for unification grammars is the problem of determining whether a given grammar derives any terminal string with a given feature structure. It is known that the problem is decidable for LFG and PATR grammars if only acyclic feature structures are taken into consideration. In this brief note, we show that the problem is undecidable for cyclic structures. This holds even for grammars that are off-line parsable.",2014,CL,0.0
Erratum,"The authors of the article ”Frame-Semantic Parsing” and a graduate student discovered that in rows 7 and 8 of Table 8, at inference time for argument identification with gold frames, the described model included gold spans along with the candidate set of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially bloating the precision, recall, and F1 metrics. The revised metrics are:",2014,CL,-1.0
Book Reviews: Sentiment Analysis and Opinion Mining by Bing Liu,"This 2012 book is written as a comprehensive introductory and survey text for sentiment analysis and opinion mining, a field of study that investigates computational techniques for analyzing text to uncover the opinions, sentiment, emotions, and evaluations expressed therein. As such, it aims to be accessible to a broad audience that includes students, researchers, and practitioners, as well as to cover all important topics in the field. With regard to the first aim, the book is very much a success: The writing is clear and concise, informative examples motivate each new topic, terminology is clearly defined, and descriptions of key algorithms are provided in the running text along with short (usually one-line) descriptions of each piece of relevant related work. The latter, in particular, makes the book an excellent platform from which to dive into the quickly expanding body of literature on sentiment and opinion analysis. In addition, I believe that the book should be easily accessible to anyone with a computer science background.",2014,CL,0.0
What's in a p-value in NLP?,"In NLP, we need to document that our proposed methods perform significantly better with respect to standard metrics than previous approaches, typically by reporting p-values obtained by rankor randomization-based tests. We show that significance results following current research standards are unreliable and, in addition, very sensitive to sample size, covariates such as sentence length, as well as to the existence of multiple metrics. We estimate that under the assumption of perfect metrics and unbiased data, we need a significance cut-off at ⇠0.0025 to reduce the risk of false positive results to <5%. Since in practice we often have considerable selection bias and poor metrics, this, however, will not do alone.",2014,CoNLL,-1.0
Testing for Significance of Increased Correlation with Human Judgment,"Automatic metrics are widely used in machine translation as a substitute for human assessment. With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality. This is often measured by correlation with human judgment. Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance, however. In this paper, we introduce a significance test for comparing correlations of two metrics, along with an open-source implementation of the test. When applied to a range of metrics across seven language pairs, tests show that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU.",2014,EMNLP,-0.4
Invited Talk: Learning from Rational Behavior,"The ability to learn from user interactions can give systems access to unprecedented amounts of knowledge. This is evident in search engines, recommender systems, and electronic commerce, and it can be the key to solving other knowledge intensive tasks. However, extracting the knowledge conveyed by user interactions is less straightforward than standard machine learning, since it requires learning systems that explicitly account for human decision making, human motivation, and human abilities. In this talk, I argue that the design space of such interactive learning systems encompasses not only the machine learning algorithm itself, but also the design of the interaction under an appropriate model of user behavior. To this effect, the talk explores how integrating microeconomic models of human behavior into the learning process leads to new interaction models and their associated learning algorithms, leading to systems that have provable guarantees and that perform robustly in practice.",2014,EMNLP,0.0
Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics,We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images.,2014,EMNLP,0.6000000000000001
Brighter than Gold: Figurative Language in User Generated Comparisons,"Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense—for example, “I slept like a log” does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful.",2014,EMNLP,0.6000000000000001
Title:Multi-View Priors for Learning Detectors from Sparse Viewpoint Data,"While the majority of today‚Äôs object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way ‚Äì the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.",2014,ICLR,0.9
Title:Group-sparse Embeddings in Collective Matrix Factorization,"CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of useritem, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.",2014,ICLR,0.8
Title:An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks,"Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models ‚Äúforget‚Äù how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm‚Äì the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.",2014,ICLR,0.2
Title:Intriguing properties of neural networks,"Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network‚Äôs prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",2014,ICLR,-1.0
"Title:OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks","We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",2014,ICLR,1.0
Bias in Natural Actor-Critic Algorithms,"We show that several popular discounted reward natural actor-critics, including the popular NACLSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(Œª) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics has only global optima, so policy gradient methods are guaranteed to converge to globally optimal policies as well.",2014,ICML,0.1
Discrete Chebyshev Classifiers,"In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals. Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input.",2014,ICML,0.9
Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts,"We present a Bayesian nonparametric framework for multilevel clustering which utilizes grouplevel context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polyaurn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.",2014,ICML,1.0
Fast Stochastic Alternating Direction Method of Multipliers,"We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, it improves the convergence rate on convex problems fromO(1/ ‚àö T ) toO(1/T ), where T is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.",2014,ICML,1.0
Approximate Policy Iteration Schemes: A Comparison,"We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration (API) (Bertsekas & Tsitsiklis, 1996), Conservative Policy Iteration (CPI) (Kakade & Langford, 2002), a natural adaptation of the Policy Search by Dynamic Programming algorithm (Bagnell et al., 2003) to the infinite-horizon case (PSDP‚àû), and the recently proposed Non-Stationary Policy Iteration (NSPI(m)) (Scherrer & Lesner, 2012). For all algorithms, we describe performance bounds with respect the per-iteration error , and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API, but this comes at the cost of a relative‚Äîexponential in 1 ‚Äîincrease of the number of iterations. 2) PSDP‚àû enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP‚àû is proportional to their number of iterations, which may be problematic when the discount factor Œ≥ is close to 1 or the approximation error is close to 0; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).",2014,ICML,0.0
Pitfalls in the use of Parallel Inference for the Dirichlet Process,"Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it ‚Äì work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.",2014,ICML,-1.0
Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget,"Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.",2014,ICML,0.4
Robust Classification Under Sample Selection Bias,"In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample-reweighted loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for robustly learning a probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on synthetic and UCI binary classification tasks.",2014,NIPS,0.8
"Signal Aggregate Constraints in Additive Factorial HMMs, with Application to Energy Disaggregation","Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the art approach.",2014,NIPS,1.0
Graph Clustering With Missing Data: Convex Algorithms and Analysis,"We consider the problem of finding clusters in an unweighted graph, when the graph is partially observed. We analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model, we obtain explicit bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk, and observe significant performance improvement over traditional methods such as k-means.",2014,NIPS,1.0
Do Convnets Learn Correspondence?,"Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass alignment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.",2014,NIPS,1.0
Online combinatorial optimization with stochastic decision sets and adversarial losses,"Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.",2014,NIPS,1.0
Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation,"We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2×, while keeping the accuracy within 1% of the original model.",2014,NIPS,1.0
Feature Cross-Substitution in Adversarial Classification,"The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.",2014,NIPS,0.7000000000000001
Learning Generative Models with Visual Attention Authors Abstract,"In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be outputs from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, such as image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of deep CNNs. We validate the framework on the task of refinement of image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.",2014,NIPS,0.9
Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning Authors Abstract,"Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.",2014,NIPS,1.0
Sparse PCA with Oracle Property Authors Abstract,"This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient programs. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target input distribution. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.",2014,NIPS,1.0
A Latent Source Model for Online Collaborative Filtering Authors Abstract,"We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.",2014,NIPS,0.9
More or less supervised supersense tagging of Twitter,"We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet.",2014,SemEval,1.0
How well can a corpus-derived co-occurrence network simulate human associative behavior?,"Free word associations are the words people spontaneously come up with in re- sponse to a stimulus word. Such informa- tion has been collected from test persons and stored in databases. A well known example is the Edinburgh Associative Thesaurus (EAT). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, en- abling the computer to produce similar associative responses as people do. While in the past test sets typically consisted of approximately 100 words, we will use here a large part of the EAT which, in to- tal, comprises 8400 words. Apart from extending the test set, we consider differ- ent properties of words: saliency, fre- quency and part-of-speech. For each fea- ture categorize our test set, and we com- pare the simulation results to those based on the EAT. It turns out that there are surprising similarities which supports our claim that a corpus-derived co-occur- rence network can simulate human asso- ciative behavior, i.e. an important part of language acquisition and verbal behavior.",2014,WS,1.0
Word Order Does NOT Differ Significantly Between Chinese and Japanese,"We propose a pre-reordering approach for Japanese-to-Chinese statistical machine translation (SMT). The approach uses de- pendency structure and manually designed reordering rules to arrange morphemes of Japanese sentences into Chinese-like word order, before a baseline phrase-based (PB) SMT system applied. Experimental results on the ASPEC-JC data show that the im- provement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evalu- ation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language.",2014,WS,0.8
"The Fewer, the Better? A Contrastive Study about Ways to Simplify","Simplified texts play an important role in providing accessible and easy-to-understand informa- tion for a whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in understanding materials which are not adapted and/or simplified. However, the production of simplified texts can be a time-consuming and labour-intensive task. In this paper we show that the employment of a short list of simple simplification rules could result in texts of comparable readability to those written as a result of applying a long list of more fine-grained rules. We also prove that the simplification process based on the short list of simple rules is more time efficient and consistent.",2014,WS,1.0
Aligning Chinese-English Parallel Parse Trees: Is it Feasible?,We investigate the feasibility of aligning Chinese and English parse trees by examining cases of incompatibility between Chinese-English parallel parse trees. This work is done in the context of an annotation project where we construct a parallel treebank by doing word and phrase alignments simultaneously. We discuss the most common incompatibility patterns identified within VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with some modification of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models.,2014,WS,1.0
STTS 2.0? Improving the Tagset for the Part-of-Speech-Tagging of German Spoken Data,"Part-of-speech tagging (POS-tagging) of spoken data requires different means of annotation than POS-tagging of written and edited texts. In order to capture the features of German spo- ken language, a distinct tagset is needed to respond to the kinds of elements which only occur in speech. In order to create such a coherent tagset the most prominent phenomena of spoken language need to be analyzed, especially with respect to how they differ from written lan- guage. First evaluations have shown that the most prominent cause (over 50%) of errors in the existing automatized POS-tagging of transcripts of spoken German with the Stuttgart Tübingen Tagset (STTS) and the treetagger was the inaccurate interpretation of speech particles. One reason for this is that this class of words is virtually absent from the current STTS. This paper proposes a recategorization of the STTS in the field of speech particles based on distributional factors rather than semantics. The ultimate aim is to create a comprehensive reference corpus of spoken German data for the global research community. It is imperative that allcphenomena are reliably recorded in future part-of-speech tag labels.",2014,WS,1.0
"""I am borrowing ya mixing?"" An Analysis of English-Hindi Code Mixing in Facebook","Code-Mixing is a frequently observed phenomenon in social media content gen- erated by multi-lingual users. The pro- cessing of such data for linguistic analysis as well as computational modelling is challenging due to the linguistic complex- ity resulting from the nature of the mixing as well as the presence of non-standard variations in spellings and grammar, and transliteration. Our analysis shows the ex- tent of Code-Mixing in English-Hindi data. The classification of Code-Mixed words based on frequency and linguistic typology underline the fact that while there are easily identifiable cases of bor- rowing and mixing at the two ends, a large majority of the words form a continuum in the middle, emphasizing the need to han- dle these at different levels for automatic processing of the data.",2014,WS,-0.5
"""ye word kis lang ka hai bhai?"" Testing the Limits of Word level Language Identification","Language identification is a necessary pre- requisite for processing any user gener- ated text, where the language is unknown. It becomes even more challenging when the text is code-mixed, i.e., two or more languages are used within the same text. Such data is commonly seen in social me- dia, where further challenges might arise due to contractions and transliterations. The existing language identification sys- tems are not designed to deal with code- mixed text, and as our experiments show, perform poorly on a synthetically created code-mixed dataset for 28 languages.We propose extensions to an existing approach for word level language identification. Our technique not only outperforms the exist- ing methods, but also makes no assump- tion about the language pairs mixed in the text - a common requirement of the ex- isting word level language identification systems.This study shows that word level language identification is most likely to confuse between languages which are lin- guistically related (e.g., Hindi and Gu- jarati, Czech and Slovak), for which spe- cial disambiguation techniques might be required.",2014,WS,0.8
Lexical Access Preference and Constraint Strategies for Improving Multiword Expression Association within Semantic MT Evaluation,"We examine lexical access preferences and constraints in computing multiword expression asso- ciations from the standpoint of a high-impact extrinsic task-based performance measure, namely semantic machine translation evaluation. In automated MT evaluation metrics, machine transla- tions are compared against human reference translations, which are almost never worded exactly the same way except in the most trivial of cases. Because of this, one of the most important factors in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative lexical realizations of the same multiword expressions in semantic role fillers. Our results com- paring bag-of-words, maximum alignment, and inversion transduction grammars indicate that cognitively motivated ITGs provide superior lexical access characteristics for multiword expres- sion associations, leading to state-of-the-art improvements in correlation with human adequacy judgments.",2014,WS,1.0
Application-Driven Relation Extraction with Limited Distant Supervision,"Recent approaches to relation extraction following the distant supervision paradigm have focused on exploiting large knowledge bases, from which they extract substantial amount of supervision. However, for many relations in real-world applications, there are few instances available to seed the relation extraction process, and appropriate named entity recognizers which are necessary for pre-processing do not exist. To overcome this issue, we learn entity filters jointly with relation extraction using imitation learning. We evaluate our approach on architect names and building completion years, using only around 30 seed instances for each relation and show that the jointly learned entity filters improved the performance by 30 and 7 points in average precision.",2014,WS,1.0
Aligning an Italian WordNet with a Lexicographic Dictionary: Coping with limited data,"This work describes the evaluations of two ap- proaches, Lexical Matching and Sense Sim- ilarity, for word sense alignment between MultiWordNet and a lexicographic dictio- nary, Senso Comune De Mauro, when having few sense descriptions (MultiWordNet) and no structure over senses (Senso Comune De Mauro). The results obtained from the merg- ing of the two approaches are satisfying, with F1 values of 0.47 for verbs and 0.64 for nouns.",2014,WS,1.0
Assessing the Readability of Sentences: Which Corpora and Features?,"The paper investigates the problem of sentence readability assessment, which is modelled as a classification task, with a specific view to text simplification. In par- ticular, it addresses two open issues con- nected with it, i.e. the corpora to be used for training, and the identification of the most effective features to determine sen- tence readability. An existing readabil- ity assessment tool developed for Italian was specialized at the level of training cor- pus and learning algorithm. A maximum entropy–based feature selection and rank- ing algorithm (grafting) was used to iden- tify to the most relevant features: it turned out that assessing the readability of sen- tences is a complex task, requiring a high number of features, mainly syntactic ones.",2014,WS,-1.0
A Poodle or a Dog? Evaluating Automatic Image Annotation Using Human Descriptions at Different Levels of Granularity,"Different people may describe the same object in different ways, and at varied levels of granular- ity (“poodle”, “dog”, “pet” or “animal”?) In this paper, we propose the idea of ‘granularity- aware’ groupings where semantically related concepts are grouped across different levels of granularity to capture the variation in how different people describe the same image content. The idea is demonstrated in the task of automatic image annotation, where these semantic group- ings are used to alter the results of image annotation in a manner that affords different insights from its initial, category-independent rankings. The semantic groupings are also incorporated during evaluation against image descriptions written by humans. Our experiments show that se- mantic groupings result in image annotations that are more informative and flexible than without groupings, although being too flexible may result in image annotations that are less informative.",2014,WS,0.5
German Compounds and Statistical Machine Translation. Can they get along?,"This paper reports different experiments created to study the impact of using linguistics to preprocess German com- pounds prior to translation in Statistical Machine Translation (SMT). Compounds are a known challenge both in Machine Translation (MT) and Translation in gen- eral as well as in other Natural Language Processing (NLP) applications. In the case of SMT, German compounds are split into their constituents to decrease the number of unknown words and improve the re- sults of evaluation measures like the Bleu score. To assess to which extent it is neces- sary to deal with German compounds as a part of preprocessing in SMT systems, we have tested different compound splitters and strategies, such as adding lists of com- pounds and their translations to the train- ing set. This paper summarizes the re- sults of our experiments and attempts to yield better translations of German nom- inal compounds into Spanish and shows how our approach improves by up to 1.4 Bleu points with respect to the baseline.",2014,WS,1.0
Function Words in Authorship Attribution From Black Magic to Theory?,"This position paper focuses on the use of function words in computational au- thorship attribution. Although recently there have been multiple successful appli- cations of authorship attribution, the field is not particularly good at the explication of methods and theoretical issues, which might eventually compromise the accep- tance of new research results in the tra- ditional humanities community. I wish to partially help remedy this lack of explica- tion and theory, by contributing a theoreti- cal discussion on the use of function words in stylometry. I will concisely survey the attractiveness of function words in stylom- etry and relate them to the use of charac- ter n-grams. At the end of this paper, I will propose to replace the term ‘function word’ by the term ‘functor’ in stylometry, due to multiple theoretical considerations.",2014,WS,0.4
Who evoked that frame? Some thoughts on context effects and event types,"Lexical substitution is an annotation task in which annotators provide one-word paraphrases (lexical substitutes) for indi- vidual target words in a sentence context. Lexical substitution yields a fine-grained characterization of word meaning that can be done by non-expert annotators. We dis- cuss results of a recent lexical substitution annotation effort, where we found strong contextual modulation effects: Many sub- stitutes were not synonyms, hyponyms or hypernyms of the targets, but were highly specific to the situation at hand. This data provides some food for thought for frame- semantic analysis.",2014,WS,0.6000000000000001
Preference Grammars and Soft Syntactic Constraints for GHKM Syntax-based Statistical Machine Translation,"In this work, we investigate the effec- tiveness of two techniques for a feature- based integration of syntactic information into GHKM string-to-tree statistical ma- chine translation (Galley et al., 2004): (1.) Preference grammars on the tar- get language side promote syntactic well- formedness during decoding while also al- lowing for derivations that are not linguis- tically motivated (as in hierarchical trans- lation). (2.) Soft syntactic constraints aug- ment the system with additional source- side syntax features while not modifying the set of string-to-tree translation rules or the baseline feature scores. We conduct experiments with a state- of-the-art setup on an English→German translation task. Our results suggest that preference grammars for GHKM trans- lation are inferior to the plain target- syntactified model, whereas the enhance- ment with soft source syntactic constraints provides consistent gains. By employ- ing soft source syntactic constraints with sparse features, we are able to achieve im- provements of up to 0.7 points BLEU and 1.0 points TER.",2014,WS,0.7000000000000001
Quo Vadis UIMA?,"In this position paper, we will examine the current state of UIMA from the perspective of a text analytics practitioner, and propose an evolution of the architecture that overcomes some of the current shortcomings.",2014,WS,1.0
Extracting Bounded-Level Modules from Deductive RDF Triplestores,"We present a novel semantics for extracting bounded-level modules from RDF ontologies and databases augmented with safe inference rules, √† la Datalog. Dealing with a recursive rule language poses challenging issues for defining the module semantics, and also makes module extraction algorithmically unsolvable in some cases. Our results include a set of module extraction algorithms compliant with the novel semantics. Experimental results show that the resulting framework is effective in extracting expressive modules from RDF datasets with formal guarantees, whilst controlling their suc-",2015,AAAI,0.8
Clustering-Based Collaborative Filtering for Link Prediction,"In this paper, we propose a novel collaborative filtering approach for predicting the unobserved links in a network (or graph) with both topological and node features. Our approach improves the well-known compressed sensing based matrix completion method by introducing a new multipleindependent-Bernoulli-distribution model as the data sampling mask. It makes better link predictions since the model is more general and better matches the data distributions in many real-world networks, such as social networks like Facebook. As a result, a satisfying stability of the prediction can be guaranteed. To obtain an accurate multiple-independentBernoulli-distribution model of the topological feature space, our approach adjusts the sampling of the adjacency matrix of the network (or graph) using the clustering information in the node feature space. This yields a better performance than those methods which simply combine the two types of features. Experimental results on several benchmark datasets suggest that our approach outperforms the best existing link prediction methods.",2015,AAAI,1.0
Assessing the Robustness of Cremer-McLean with Automated Mechanism Design,"In a classic result in the mechanism design literature, Cremer and McLean (1985) show that if buyers‚Äô valuations are sufficiently correlated, a mechanism exists that allows the seller to extract the full surplus from efficient allocation as revenue. This result is commonly seen as ‚Äútoo good to be true‚Äù (in practice), casting doubt on its modeling assumptions. In this paper, we use an automated mechanism design approach to assess how sensitive the Cremer-McLean result is to relaxing its main technical assumption. That assumption implies that each valuation that a bidder can have results in a unique conditional distribution over the external signal(s). We relax this, allowing multiple valuations to be consistent with the same distribution over the external signal(s). Using similar insights to Cremer-McLean, we provide a highly efficient algorithm for computing the optimal revenue in this more general case. Using this algorithm, we observe that indeed, as the number of valuations consistent with a distribution grows, the optimal revenue quickly drops to that of a reserve-price mechanism. Thus, automated mechanism design allows us to gain insight into the precise sense in which Cremer-McLean is ‚Äútoo good to be true.‚Äù",2015,AAAI,1.0
Hedonic Coalition Formation in Networks,"Coalition formation is a fundamental problem in the organization of many multi-agent systems. In large populations, the formation of coalitions is often restricted by structural visibility and locality constraints under which agents can reorganize. We capture and study this aspect using a novel network-based model for dynamic locality within the popular framework of hedonic coalition formation games. We analyze the effects of network-based visibility and structure on the convergence of coalition formation processes to stable states. Our main result is a tight characterization of the structures based on which dynamic coalition formation can stabilize quickly. Maybe surprisingly, polynomial-time convergence can be achieved if and only if coalition formation is based on complete or star graphs.",2015,AAAI,0.0
Mining Query Subtopics from Questions in Community Question Answering,"This paper proposes mining query subtopics from questions in community question answering (CQA). The subtopics are represented as a number of clusters of questions with keywords summarizing the clusters. The task is unique in that the subtopics from questions can not only facilitate user browsing in CQA search, but also describe aspects of queries from a question-answering perspective. The challenges of the task include how to group semantically similar questions and how to find keywords capable of summarizing the clusters. We formulate the subtopic mining task as a non-negative matrix factorization (NMF) problem and further extend the model of NMF to incorporate question similarity estimated from metadata of CQA into learning. Compared with existing methods, our method can jointly optimize question clustering and keyword extraction and encourage the former task to enhance the latter. Experimental results on large scale real world CQA datasets show that the proposed method significantly outperforms the existing methods in terms of keyword extraction, while achieving a comparable performance to the state-ofthe-art methods for question clustering.",2015,AAAI,0.9
Vector-space calculation of semantic surprisal for predicting word pronunciation duration,"In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data.",2015,ACL,-0.1
How Well Do Distributional Models Capture Different Types of Semantic Knowledge?,"In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information.",2015,ACL,-0.8
Joint Dependency Parsing and Multiword Expression Tokenization,"Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We propose a model for joint dependency parsing and multiword expressions identification, in which complex function words are represented as individual tokens linked with morphological dependencies. Our graphbased parser includes standard secondorder features and verbal subcategorization features derived from a syntactic lexicon.We train it on a modified version of the French Treebank enriched with morphological dependencies. It recognizes 81.79% of ADV+que conjunctions with 91.57% precision, and 82.74% of de+DET determiners with 86.70% precision.",2015,ACL,1.0
A Web-based Collaborative Evaluation Tool for Automatically Learned Relation Extraction Patterns,"Patterns extracted from dependency parses of sentences are a major source of knowledge for most state-of-the-art relation extraction systems, but can be of low quality in distantly supervised settings. We present a linguistic annotation tool that allows human experts to analyze and categorize automatically learned patterns, and to identify common error classes. The annotations can be used to create datasets that enable machine learning approaches to pattern quality estimation. We also present an experimental pattern error analysis for three semantic relations, where we find that between 24% and 61% of the learned dependency patterns are defective due to preprocessing or parsing errors, or due to violations of the distant supervision assumption.",2015,ACL,-0.6000000000000001
Learning Topic Hierarchies for Wikipedia Categories,"Existing studies have utilized Wikipedia for various knowledge acquisition tasks. However, no attempts have been made to explore multi-level topic knowledge contained in Wikipedia articles’",2015,ACL,0.2
Revisiting Word Embedding for Contrasting Meaning,"Contrasting meaning is a basic aspect of semantics. Recent word-embedding models based on distributional semantics hypothesis are known to be weak for modeling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the first glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget’s and WordNet)—corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem.",2015,ACL,1.0
Ground Truth for Grammatical Error Correction Metrics,"How do we know which grammatical error correction (GEC) system is best? A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded in human judgments. We conducted the first human evaluation of GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth. As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modified to account for both the source and the reference, and show that it hews much more closely to human judgments.",2015,ACL,0.8
Successful Data Mining Methods for NLP,"Historically Natural Language Processing (NLP) focuses on unstructured data (speech and text) understanding while Data Mining (DM) mainly focuses on massive, structured or semi-structured datasets. The general research directions of these two fields also have followed different philosophies and principles. For example, NLP aims at deep understanding of individual words, phrases and sentences (“micro-level”), whereas DM aims to conduct a high-level understanding, discovery and synthesis of the most salient information from a large set of documents when working on text data (“macro-level”). But they share the same goal of distilling knowledge from data. In the past five years, these two areas have had intensive interactions and thus mutually enhanced each other through many successful text mining tasks. This positive progress mainly benefits from some innovative intermediate representations such as “heterogeneous information networks” [Han et al., 2010, Sun et al., 2012b]. However, successful collaborations between any two fields require substantial mutual understanding, patience and passion among researchers. Similar to the applications of machine learning techniques in NLP, there is usually a gap of at least several years between the creation of a new DM approach and its first successful application in NLP. More importantly, many DM approaches such as gSpan [Yan and Han, 2002] and RankClus [Sun et al., 2009a] have demonstrated their power on structured data. But they remain relatively unknown in the NLP community, even though there are many obvious potential applications. On the other hand, compared to DM, the NLP community has paid more attention to developing large-scale data annotations, resources, shared tasks which cover a wide range of multiple genres and multiple domains. NLP can also provide the basic building blocks for many DM tasks such as text cube construction [Tao et al., 2014]. Therefore in many scenarios, for the same approach the NLP experiment setting is often much closer to real-world applications than its DM counterpart. We would like to share the experiences and lessons from our extensive inter-disciplinary collaborations in the past five years. The primary goal of this tutorial is to bridge the knowledge gap between these two fields and speed up the transition process. We will introduce two types of DM methods: (1). those state-of-the-art DM methods that have already been proven effective for NLP; and (2). some newly developed DM methods that we believe will fit into some specific NLP problems. In addition, we aim to suggest some new research directions in order to better marry these two areas and lead to more fruitful outcomes. The tutorial will thus be useful for researchers from both communities. We will try to provide a concise roadmap of recent perspectives and results, as well as point to the related DM software and resources, and NLP data sets that are available to both research communities.",2015,ACL,0.0
What's in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation,"Domain adaptation is an active field of research in statistical machine translation (SMT), but so far most work has ignored the distinction between the topic and genre of documents. In this paper we quantify and disentangle the impact of genre and topic differences on translation quality by introducing a new data set that has controlled topic and genre distributions. In addition, we perform a detailed analysis showing that differences across topics only explain to a limited degree translation performance differences across genres, and that genre-specific errors are more attributable to model coverage than to suboptimal scoring of translation candidates.",2015,ACL,1.0
UNRAVEL-A Decipherment Toolkit,"In this paper we present the UNRAVEL toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers like e.g. the ZODIAC-408 cipher and Part two of the BEALE ciphers, as well as decipherment of probabilistic ciphers and unsupervised training for machine translation. It also includes data and example configuration files so that the previously published experiments are easy to reproduce.",2015,ACL,0.5
Evaluating Machine Translation Systems with Second Language Proficiency Tests,"A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed. It extrinsically evaluates MT systems using human subjects’ scores on second language ability test problems that are machine-translated to the subjects’ native language. A largescale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation.",2015,ACL,0.4
Discontinuous Incremental Shift-reduce Parsing,"We present an extension to incremental shift-reduce parsing that handles discontinuous constituents, using a linear classifier and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F1 on TiGer).",2015,ACL,1.0
Tagging Performance Correlates with Author Age,"Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region.",2015,ACL,-0.2
A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets,"Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness. Based on our procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi.",2015,ACL,1.0
Learning Word Representations from Scarce and Noisy Data with Embedding Subspaces,"We investigate a technique to adapt unsupervised word embeddings to specific applications, when only small and noisy labeled datasets are available. Current methods use pre-trained embeddings to initialize model parameters, and then use the labeled data to tailor them for the intended task. However, this approach is prone to overfitting when the training is performed with scarce and noisy data. To overcome this issue, we use the supervised data to find an embedding subspace that fits the task complexity. All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task.",2015,ACL,0.9
SACRY: Syntax-based Automatic Crossword puzzle Resolution sYstem,"In this paper, we present our Crossword Puzzle Resolution System (SACRY), which exploits syntactic structures for clue reranking and answer extraction. SACRY uses a database (DB) containing previously solved CPs in order to generate the list of candidate answers. Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores. Our system is based on WebCrow, one of the most advanced systems for automatic crossword puzzle resolution. Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%.",2015,ACL,1.0
Exploring the Planet of the APEs: a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing,"Downstream processing of machine translation (MT) output promises to be a solution to improve translation quality, especially when the MT system’s internal decoding process is not accessible. Both rule-based and statistical automatic postediting (APE) methods have been proposed over the years, but with contrasting results. A missing aspect in previous evaluations is the assessment of different methods: i) under comparable conditions, and ii) on different language pairs featuring variable levels of MT quality. Focusing on statistical APE methods (more portable across languages), we propose the first systematic analysis of two approaches. To understand their potential, we compare them in the same conditions over six language pairs having English as source. Our results evidence consistent improvements on all language pairs, a relation between the extent of the gain and MT output quality, slight but statistically significant performance differences between the two methods, and their possible complementarity.",2015,ACL,0.2
Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases,"It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efficiency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies.",2015,ACL,0.0
Automatic Keyword Extraction on Twitter,"In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.",2015,ACL,1.0
Identifying Cascading Errors using Constraints in Dependency Parsing,"Dependency parsers are usually evaluated on attachment accuracy. Whilst easily interpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse. We apply arc-level constraints to MSTparser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Erroneous NP and punctuation attachments cause the most cascading errors, while incorrect PP and coordination attachments are frequent but less influential. Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles.",2015,ACL,-0.6000000000000001
Learning Answer-Entailing Structures for Machine Comprehension,"Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension evaluates the system’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correctness of the answer is evident. Since the structure is latent, it must be inferred. We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different subtasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our framework outperforms various IR and neuralnetwork baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.)",2015,ACL,1.0
Squibs: Evaluation Methods for Statistically Dependent Text,"In recent years, many studies have been published on data collected from social media, especially microblogs such as Twitter. However, rather few of these studies have considered evaluation methodologies that take into account the statistically dependent nature of such data, which breaks the theoretical conditions for using cross-validation. Despite concerns raised in the past about using cross-validation for data of similar characteristics, such as time series, some of these studies evaluate their work using standard k-fold cross-validation. Through experiments on Twitter data collected during a two-year period that includes disastrous events, we show that by ignoring the statistical dependence of the text messages published in social media, standard cross-validation can result in misleading conclusions in a machine learning task. We explore alternative evaluation methods that explicitly deal with statistical dependence in text. Our work also raises concerns for any other data for which similar conditions might hold.",2015,CL,-0.8
Design and Evaluation of Metaphor Processing Systems,"ness–concreteness algorithm. However, the evaluation was done on only five",2015,CL,0.9
"The DCU Discourse Parser for Connective, Argument Identification and Explicit Sense Classification","This paper describes our submission to the CoNLL-2015 shared task on discourse parsing. We factor the pipeline into subcomponents which are then used to form the final sequential architecture. Focusing on achieving good performance when inferring explicit discourse relations, we apply maximum entropy and recurrent neural networks to different sub-tasks such as connective identification, argument extraction, and sense classification. The our final system achieves 16.51%, 12.73% and 11.15% overall F1 scores on the dev, WSJ and blind test sets, respectively.",2015,CoNLL,0.8
One Million Sense-Tagged Instances for Word Sense Disambiguation and Induction,"Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source WSD systems, very few large annotated datasets are available to the research community. The two main goals of this paper are to extract and annotate a large number of samples and release them for public use, and also to evaluate this dataset against some word sense disambiguation and induction tasks. We show that the open source IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines.",2015,CoNLL,0.6000000000000001
Do dependency parsing metrics correlate with human judgments?,"Using automatic measures such as labeled and unlabeled attachment scores is common practice in dependency parser evaluation. In this paper, we examine whether these measures correlate with human judgments of overall parse quality. We ask linguists with experience in dependency annotation to judge system outputs. We measure the correlation between their judgments and a range of parse evaluation metrics across five languages. The humanmetric correlation is lower for dependency parsing than for other NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech.",2015,CoNLL,-1.0
Detecting Content-Heavy Sentences: A Cross-Language Case Study,"The information conveyed by some sentences would be more easily understood by a reader if it were expressed in multiple sentences. We call such sentences content heavy: these are possibly grammatical but difficult to comprehend, cumbersome sentences. In this paper we introduce the task of detecting content-heavy sentences in cross-lingual context. Specifically we develop methods to identify sentences in Chinese for which English speakers would prefer translations consisting of more than one sentence. We base our analysis and definitions on evidence from multiple human translations and reader preferences on flow and understandability. We show that machine translation quality when translating content heavy sentences is markedly worse than overall quality and that this type of sentence are fairly common in Chinese news. We demonstrate that sentence length and punctuation usage in Chinese are not sufficient clues for accurately detecting heavy sentences and present a richer classification model that accurately identifies these sentences.",2015,EMNLP,1.0
Detecting Risks in the Banking System by Sentiment Analysis,"In November 2014, the European Central Bank (ECB) started to directly supervise the largest banks in the Eurozone via the Single Supervisory Mechanism (SSM). While supervisory risk assessments are usually based on quantitative data and surveys, this work explores whether sentiment analysis is capable of measuring a bank’s attitude and opinions towards risk by analyzing text data. For realizing this study, a collection consisting of more than 500 CEO letters and outlook sections extracted from bank annual reports is built up. Based on these data, two distinct experiments are conducted. The evaluations find promising opportunities, but also limitations for risk sentiment analysis in banking supervision. At the level of individual banks, predictions are relatively inaccurate. In contrast, the analysis of aggregated figures revealed strong and significant correlations between uncertainty or negativity in textual disclosures and the quantitative risk indicator’s future evolution. Risk sentiment analysis should therefore rather be used for macroprudential analyses than for assessments of individual banks.",2015,EMNLP,0.9
Traversing Knowledge Graphs in Vector Space,"Path queries on a knowledge graph can be used to answer compositional questions such as “What languages are spoken by people living in Lisbon?”. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new “compositional” training objective, which dramatically improves all models’ ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.",2015,EMNLP,1.0
Human Evaluation of Grammatical Error Correction Systems,The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems. Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment.,2015,EMNLP,0.0
Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE,"We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics. Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.",2015,EMNLP,0.8
Evaluation of Word Vector Representations by Subspace Alignment,"Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks. We present QVEC—a computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1",2015,EMNLP,0.9
Topic Identification and Discovery on Text and Speech,"We compare the multinomial i-vector framework from the speech community with LDA, SAGE, and LSA as feature learners for topic ID on multinomial speech and text data. We also compare the learned representations in their ability to discover topics, quantified by distributional similarity to gold-standard topics and by human interpretability. We find that topic ID and topic discovery are competing objectives. We argue that LSA and i-vectors should be more widely considered by the text processing community as pre-processing steps for downstream tasks, and also speculate about speech processing tasks that could benefit from more interpretable representations like SAGE.",2015,EMNLP,0.0
Title:Object Detectors Emerge in Deep Scene CNNs,"With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.",2015,ICLR,0.2
An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization,"We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any «´ > 0, an «´-optimal and «´-feasible solution can be computed within O(log(«´‚àí1)) DFAL iterations, which require O( 1.5 max dmin «´‚àí1) proximal gradient computations and communications per node in total, where œàmax denotes the largest eigenvalue of the graph Laplacian, and dmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems.",2015,ICML,0.9
The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling,"Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and highdimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo. With the preponderance of applications featuring enormous data sets, methods of inference requiring only subsamples of data are becoming more and more appealing. Subsampled Markov Chain Monte Carlo algorithms, (Neiswanger et al., 2013; Welling & Teh, 2011), are particularly desired for their potential applicability to most statistical models. Unfortunately, careful analysis of these algorithms reveals unavoidable biases unless the data are tall, or highly redundant (Bardenet et al., 2014; Teh et al., 2014; Vollmer et al., 2015). Because redundancy can be defined only relative to a given model, the utility of these subsampled algorithms is then a consequence of not only the desired accuracy and also the particular model and data under consideration, severely restricting practicality. Recently (Chen et al., 2014) considered subsampling within Hamiltonian Monte Carlo (Duane et al., 1987; Neal, 2011; Betancourt et al., 2014b) and demonstrated that the biases induced by naive subsampling lead to unacceptably large biases. Ultimately the authors rectified this bias by sacrificing the coherent exploration of Hamiltonian flow for a diffusive correction, fundamentally compromising the scalability of the algorithm with respect to the complexity Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). of the target distribution. An algorithm scalable with respect to both the size of the data and the complexity of the target distribution would have to maintain the coherent exploration of Hamiltonian flow while subsampling and, unfortunately, these objectives are mutually exclusive in general. In this paper I review the elements of Hamiltonian Monte Carlo critical to its robust and scalable performance in practice and demonstrate how different subsampling strategies all compromise those properties and consequently induce poor performance. 1. Hamiltonian Monte Carlo in Theory Hamiltonian Monte Carlo utilizes deterministic, measurepreserving maps to generate efficient Markov transitions (Betancourt et al., 2014b). Formally, we begin by complementing a target distribution, œÄ ‚àù exp[‚àíV (q)] dq, with a conditional distribution over auxiliary momenta parameters, œÄq ‚àù exp[‚àíT (p, q)] dp. Together these define a joint distribution, $H ‚àù exp[‚àí (T (q, p) + V (q))] dq dp ‚àù exp[‚àíH(q, p)] dq dp, and a Hamiltonian system corresponding to the Hamiltonian, H(q, p). We refer to T (q, p) and V (q) as the kinetic energy and potential energy, respectively. The Hamiltonian immediately defines a Hamiltonian vector field, ~ H = ‚àÇH ‚àÇp ‚àÇ ‚àÇq ‚àí ‚àÇH ‚àÇq ‚àÇ ‚àÇp , and an application of the exponential map yields a Hamiltonian flow on the joint space, œÜœÑ = e œÑ ~ H (Lee, 2013), which exactly preserves the joint distribution under a pullback, ( œÜt ) ‚àó œÄH = œÄH . The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling Consequently, we can compose a Markov chain by sampling the auxiliary momenta, q ‚Üí (q, p), p ‚àº œÄq, applying the Hamiltonian flow, (q, p)‚Üí œÜt (q, p) and then projecting back down to the target space, (q, p)‚Üí q. By construction, the trajectories generated by the Hamiltonian flow explore the level sets of the Hamiltonian function. Because these level sets can also span large volumes of the joint space, sufficiently-long trajectories can yield transitions far away from the initial state of the Markov chain, drastically reducing autocorrelations and producing computationally efficient Monte Carlo estimators. When the kinetic energy does not depend on position we say that the Hamiltonian is separable, H(q, p) = T (p) + V (q), and the Hamiltonian vector field decouples into a kinetic vector field, ~ T and potential vector field, ~ V , ~ H = ‚àÇH ‚àÇp ‚àÇ ‚àÇq ‚àí ‚àÇH ‚àÇq ‚àÇ ‚àÇp = ‚àÇT ‚àÇp ‚àÇ ‚àÇq ‚àí ‚àÇV ‚àÇq ‚àÇ ‚àÇp ‚â° ~ T + ~ V . In this paper I consider only separable Hamiltonians, although the conclusions also carry over to the non-seperable Hamiltonians, for example those arising in Riemannian Hamiltonian Monte Carlo (Girolami & Calderhead, 2011). 2. Hamiltonian Monte Carlo in Practice The biggest challenge of implementing Hamiltonian Monte Carlo is that the exact Hamiltonian flow is rarely calculable in practice and we must instead resort to approximate integration. Symplectic integrators, which yield numerical trajectories that closely track the true trajectories, are of particular importance to any high-performance implementation. An especially transparent strategy for constructing symplectic integrators is to split the Hamiltonian into terms with soluble flows which can then be composed together (Leimkuhler & Reich, 2004; Hairer et al., 2006). For example, consider the symmetric Strang splitting, œÜ 2 ‚ó¶ œÜ ‚ó¶ œÜ 2 = e 2 ~ V ‚ó¶ e ~ T ‚ó¶ e 2 ~ V , where is a small interval of time known as the step size. Appealing to the Baker-Campbell-Hausdorff formula, this symmetric composition yields œÜ 2 ‚ó¶ œÜ ‚ó¶ œÜ 2 = e 2 ~ V ‚ó¶ e ~ T ‚ó¶ e 2 ~ V = e 2 ~ V ‚ó¶ exp ( ~ T + 2 ~ V + 2 4 [ ~ T , ~ V ]) +O ( 3 )",2015,ICML,0.4
Learning Deep Structured Models,"Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",2015,ICML,1.0
"\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods","In recent years, the `1,p-regularizer has been widely used to induce structured sparsity in the solutions to various optimization problems. Currently, such `1,p-regularized problems are typically solved by first-order methods. Motivated by the desire to analyze the convergence rates of these methods, we show that for a large class of `1,p-regularized problems, an error bound condition is satisfied when p ‚àà [1, 2] or p = ‚àû but fails to hold for any p ‚àà (2,‚àû). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to `1,p-regularized linear or logistic regression with p ‚àà [1, 2] or p = ‚àû. By contrast, numerical experiments suggest that for the same class of problems with p ‚àà (2,‚àû), the aforementioned methods may not converge linearly.",2015,ICML,-0.4
Learning Word Representations with Hierarchical Sparse Coding,"We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks‚Äîword similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis‚Äîdemonstrate that the method outperforms or is competitive with state-of-the-art methods.",2015,ICML,1.0
An Aligned Subtree Kernel for Weighted Graphs,"In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depthbased representations. We demonstrate that this kernel can be seen as an aligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy.",2015,ICML,1.0
Safe Exploration for Optimization with Gaussian Processes,"We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multiarmed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified ‚Äúsafety‚Äù threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SAFEOPT, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SAFEOPT on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",2015,ICML,0.5
Community Detection Using Time-Dependent Personalized PageRank,"Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions.",2015,ICML,1.0
Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets,"The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections the computational bottleneck in many applications replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization.",2015,ICML,-0.2
BilBOWA: Fast Bilingual Distributed Representations without Word Alignments,"We introduce BilBOWA (Bilingual Bag-ofWords without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.",2015,ICML,1.0
A Lower Bound for the Optimization of Finite Sums,"This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is Œº-strongly convex. We show that no algorithm can reach an error Œµ in minimizing all functions from this class in fewer than Œ©(n+ ‚àö n(Œ∫‚àí1) log(1/Œµ)) iterations, where Œ∫ = L/Œº is a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.",2015,ICML,0.30000000000000004
Phrase-based Image Captioning,"Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.",2015,ICML,0.7000000000000001
Controversy in mechanistic modelling with Gaussian processes,"Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.",2015,ICML,-1.0
Asymmetric Transfer Learning with Deep Gaussian Processes,"We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.",2015,ICML,1.0
Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes,"Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",2015,ICML,1.0
Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods,"Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method.",2015,NAACL,0.8
Why Read if You Can Scan? Trigger Scoping Strategy for Biographical Fact Extraction,"The rapid growth of information sources brings a unique challenge to biographical information extraction: how to find specific facts without having to read all the words. An effective solution is to follow the human scanning strategy which keeps a specific keyword in mind and searches within a specific scope. In this paper, we mimic a scanning process to extract biographical facts. We use event and relation triggers as keywords, identify their scopes and apply type constraints to extract answers within the scope of a trigger. Experiments demonstrate that our approach outperforms state-of-the-art methods up to 26% absolute gain in F-score without using any syntactic analysis or external knowledge bases.",2015,NAACL,1.0
Disfluency Detection with a Semi-Markov Model and Prosodic Features,"We present a discriminative model for detecting disfluencies in spoken language transcripts. Structurally, our model is a semiMarkov conditional random field with features targeting characteristics unique to speech repairs. This gives a significant performance improvement over standard chain-structured CRFs that have been employed in past work. We then incorporate prosodic features over silences and relative word duration into our semi-CRF model, resulting in further performance gains; moreover, these features are not easily replaced by discrete prosodic indicators such as ToBI breaks. Our final system, the semi-CRF with prosodic information, achieves an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset.",2015,NAACL,1.0
Do We Really Need Lexical Information? Towards a Top-down Approach to Sentiment Analysis of Product Reviews,"Most of the current approaches to sentiment analysis of product reviews are dependent on lexical sentiment information and proceed in a bottom-up way, adding new layers of features to lexical data. In this paper, we maintain that a typical product review is not a bag of sentiments, but a narrative with an underlying structure and reoccurring patterns, which allows us to predict its sentiments knowing only its general polarity and discourse cues that occur in it. We hypothesize that knowing only the review’s score and its discourse patterns would allow us to accurately predict the sentiments of its individual sentences. The experiments we conducted prove this hypothesis and show a substantial improvement over the lexical baseline.",2015,NAACL,0.8
Unediting: Detecting Disfluencies Without Careful Transcripts,"Speech transcripts often only capture semantic content, omitting disfluencies that can be useful for analyzing social dynamics of a discussion. This work describes steps in building a model that can recover a large fraction of locations where disfluencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disfluencies, and applying semi-supervised learning. Experiments show improvement in disfluency detection on Supreme Court oral arguments, nearly 23% improvement in F1.",2015,NAACL,1.0
Not All Character N-grams Are Created Equal: A Study in Authorship Attribution,"Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present. We demonstrate that character ngrams that capture information about affixes and punctuation account for almost all of the power of character n-grams as features. Our study contributes new insights into the use of n-grams for future AA work and other classification tasks.",2015,NAACL,1.0
When and why are log-linear models self-normalizing?,"Several techniques have recently been proposed for training “self-normalized” discriminative models. These attempt to find parameter settings for which unnormalized model scores approximate the true label probability. However, the theoretical properties of such techniques (and of self-normalization generally) have not been investigated. This paper examines the conditions under which we can expect self-normalization to work. We characterize a general class of distributions that admit self-normalization, and prove generalization bounds for procedures that minimize empirical normalizer variance. Motivated by these results, we describe a novel variant of an established procedure for training self-normalized models. The new procedure avoids computing normalizers for most training examples, and decreases training time by as much as factor of ten while preserving model quality.",2015,NAACL,1.0
Removing the Training Wheels: A Coreference Dataset that Entertains Humans and Challenges Computers,"Coreference is a core nlp problem. However, newswire data, the primary source of existing coreference data, lack the richness necessary to truly solve coreference. We present a new domain with denser references—quiz bowl questions—that is challenging and enjoyable to humans, and we use the quiz bowl community to develop a new coreference dataset, together with an annotation framework that can tag any text data with coreferences and named entities. We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models. State-of-the-art coreference systems underperform a simple classifier on our new dataset, motivating non-newswire data for future coreference research.",2015,NAACL,-0.30000000000000004
"What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision","We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then refine this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest.",2015,NAACL,1.0
Accurate Evaluation of Segment-level Machine Translation Metrics,"Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels in human assessments; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of significance testing improvements over a baseline. In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed specifically at assessment of segment-level metrics. We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed. Three segment-level metrics — METEOR, NLEPOR and SENTBLEUMOSES — are found to correlate with human assessment at a level not significantly outperformed by any other metric in both the individual language pair assessment for Spanish-toEnglish and the aggregated set of 9 language pairs.",2015,NAACL,0.6000000000000001
Semantics-based Graph Approach to Complex Question-Answering,"This paper suggests an architectural approach of representing knowledge graph for complex question-answering. There are four kinds of entity relations added to our knowledge graph: syntactic dependencies, semantic role labels, named entities, and coreference links, which can be effectively applied to answer complex questions. As a proof of concept, we demonstrate how our knowledge graph can be used to solve complex questions such as arithmetics. Our experiment shows a promising result on solving arithmetic questions, achieving the 3folds cross-validation score of 71.75%.",2015,NAACL,1.0
Personalized Page Rank for Named Entity Disambiguation,The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in microand 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions.,2015,NAACL,1.0
Large-Scale Native Language Identification with Cross-Corpus Evaluation,"We present a large-scale Native Language Identification (NLI) experiment on new data, with a focus on cross-corpus evaluation to identify corpusand genre-independent language transfer features. We test a new corpus and show it is comparable to other NLI corpora and suitable for this task. Cross-corpus evaluation on two large corpora achieves good accuracy and evidences the existence of reliable language transfer features, but lower performance also suggests that NLI models are not completely portable across corpora. Finally, we present a brief case study of features distinguishing Japanese learners’ English writing, demonstrating the presence of cross-corpus and cross-genre language transfer features that are highly applicable to SLA and ESL research.",2015,NAACL,0.5
Improving the Translation of Discourse Markers for Chinese into English,"Discourse markers (DMs) are ubiquitous cohesive devices used to connect what is said or written. However, across languages there is divergence in their usage, placement, and frequency, which is considered to be a major problem for machine translation (MT). This paper presents an overview of a proposed thesis, exploring the difficulties around DMs in MT, with a focus on Chinese and English. The thesis will examine two main areas: modelling cohesive devices within sentences and modelling discourse relations (DRs) across sentences. Initial experiments have shown promising results for building a prediction model that uses linguistically inspired features to help improve word alignments with respect to the implicit use of cohesive devices, which in turn leads to improved hierarchical phrasebased MT.",2015,NAACL,1.0
Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains,"One of the weaknesses of current supervised word sense disambiguation (WSD) systems is that they only treat a word as a discrete entity. However, a continuous-space representation of words (word embeddings) can provide valuable information and thus improve generalization accuracy. Since word embeddings are typically obtained from unlabeled data using unsupervised methods, this method can be seen as a semi-supervised word sense disambiguation approach. This paper investigates two ways of incorporating word embeddings in a word sense disambiguation setting and evaluates these two methods on some SensEval/SemEval lexical sample and all-words tasks and also a domain-specific lexical sample task. The obtained results show that such representations consistently improve the accuracy of the selected supervised WSD system. Moreover, our experiments on a domainspecific dataset show that our supervised baseline system beats the best knowledge-based systems by a large margin.",2015,NAACL,1.0
Constraint-Based Models of Lexical Borrowing,"Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a “donor” language to a “recipient” language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and—in contrast to cognate relationships—borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili’s vocabulary is borrowed from Arabic). In this paper, we develop a model of morpho-phonological transformations across languages with features based on universal constraints from Optimality Theory (OT). Compared to several standard— but linguistically naïve—baselines, our OTinspired model obtains good performance with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages.",2015,NAACL,1.0
Sign constraints on feature weights improve a joint model of word segmentation and phonology,"This paper describes a joint model of word segmentation and phonological alternations, which takes unsegmented utterances as input and infers word segmentations and underlying phonological representations. The model is a Maximum Entropy or log-linear model, which can express a probabilistic version of Optimality Theory (OT; Prince and Smolensky (2004)), a standard phonological framework. The features in our model are inspired by OT’s Markedness and Faithfulness constraints. Following the OT principle that such features indicate “violations”, we require their weights to be non-positive. We apply our model to a modified version of the Buckeye corpus (Pitt et al., 2007) in which the only phonological alternations are deletions of word-final /d/ and /t/ segments. The model sets a new state-ofthe-art for this corpus for word segmentation, identification of underlying forms, and identification of /d/ and /t/ deletions. We also show that the OT-inspired sign constraints on feature weights are crucial for accurate identification of deleted /d/s; without them our model posits approximately 10 times more deleted underlying /d/s than appear in the manually annotated data.",2015,NAACL,1.0
Unsupervised Most Frequent Sense Detection using Word Embeddings,"An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The field of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent.",2015,NAACL,1.0
Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks,"In this paper we introduce a generative model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks (convnets) within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach. Samples drawn from our model are of significantly higher quality than existing models. In a quantitive assessment by human evaluators our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for GAN samples. We also show samples from more diverse datasets such as STL10 and LSUN.",2015,NIPS,1.0
Learning visual biases from human imagination,"Although the human visual system can recognize many concepts under challengingconditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system.We introduce a novel method that, inspired by well-known tools in humanpsychophysics, estimates the biases that the human visual system might use forrecognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree withthe bias from human visual system. Our results suggest that transferring thishuman bias into machines may help object recognition systems generalize acrossdatasets and perform better when very little training data is available",2015,NIPS,0.8
"A fast, universal algorithm to learn parametric nonlinear embeddings","Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N-body methods developed for nonlinear embeddings, yielding linear-time iterations.",2015,NIPS,1.0
Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models,"Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.",2015,NIPS,1.0
Where are they looking?,"Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural network-based approach for gaze-following and a new benchmark dataset for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. After training, the network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance at this task. Overall, we believe that this is a challenging and important task that deserves more attention from the community.",2015,NIPS,0.1
Top-k Multiclass SVM,"Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines",2015,NIPS,0.9
Adversarial Prediction Games for Multivariate Losses,"Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain",2015,NIPS,1.0
Monotone k-Submodular Function Maximization with Size Constraints,"A k-submodular function is a generalization of a submodular function, where the input consists of k disjoint subsets, instead of a single subset, of the domain.Many machine learning problems, including influence maximization with k kinds of topics and sensor placement with k kinds of sensors, can be naturally modeled as the problem of maximizing monotone k-submodular functions.In this paper, we give constant-factor approximation algorithms for maximizing monotone k-submodular functions subject to several size constraints.The running time of our algorithms are almost linear in the domain size.We experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.",2015,NIPS,1.0
On the Limitation of Spectral Methods: From the Gaussian Hidden Clique Problem to Rank-One Perturbations of Gaussian Tensors Authors Abstract,"There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence ‚Äî especially in humans ‚Äî as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning.",2015,NIPS,-0.5
Beyond Convexity: Stochastic Quasi-Convex Optimization Authors Abstract,"We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension ‚Äî the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.",2015,NIPS,-0.30000000000000004
Adaptive Stochastic Optimization: From Sets to Paths Authors Abstract,"Adaptive stochastic optimization (ASO) optimizes an objective function adaptively under uncertainty. It plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general. This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which, together with pointwise submodularity, enable efficient approximate solution of ASO. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning. We describe Recursive Adaptive Coverage, a new ASO algorithm that exploits these conditions, and apply the algorithm to two robot planning tasks under uncertainty. In contrast to the earlier submodular optimization approach, our algorithm applies to ASO over both sets and paths.",2015,NIPS,0.5
Learning Bayesian Networks with Thousands of Variables Authors Abstract,We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.,2015,NIPS,1.0
The Pareto Regret Frontier for Bandits Authors Abstract,"Given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least Œ©(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.",2015,NIPS,-0.4
Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images Authors Abstract,"We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.",2015,NIPS,0.6000000000000001
End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture Authors Abstract,"We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.",2015,NIPS,1.0
Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso Authors Abstract,"In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frameper-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs endto-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https://github.com/facebookresearch/ELF.",2015,NIPS,1.0
Fast Second Order Stochastic Backpropagation for Variational Inference Authors Abstract,"We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation.",2015,NIPS,1.0
Expressing an Image Stream with a Sequence of Natural Sentences Authors Abstract,"We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.",2015,NIPS,1.0
Lisbon: Evaluating TurboSemanticParser on Multiple Languages and Out-of-Domain Data,"As part of the SemEval-2015 shared task on Broad-Coverage Semantic Dependency Parsing, we evaluate the performace of our last year’s system (TurboSemanticParser) on multiple languages and out-of-domain data. Our system is characterized by a feature-rich linear model, that includes scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). For decoding this second-order model, we solve a linear relaxation of that problem using alternating directions dual decomposition (AD). The experiments have shown that, even though the parser’s performance in Chinese and Czech attains around 80% (not too far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research.",2015,SemEval,1.0
SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing,"Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, but tree-oriented parsers are ill-suited for producing meaning representations, i.e. moving from the analysis of grammatical structure to sentence semantics. Even if syntactic parsing arguably can be limited to tree structures, this is not the case in semantic analysis, where a node will often be the argument of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 18 at SemEval 2015, Broad-Coverage Semantic Dependency Parsing (SDP 2015),1 seeks to stimulate the parsing community to move towards See http://alt.qcri.org/semeval2015/ task18/ for further technical details, information on how to obtain the data, and official results. more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacuous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘fullIn much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation",2015,SemEval,1.0
TECHLIMED@QALB-Shared Task 2015: a hybrid Arabic Error Correction System,"This paper reports on the participation of Techlimed in the Second Shared Task on Automatic Arabic Error Correction orga- nized by the Arabic Natural Language Processing Workshop. This year's compe- tition includes two tracks, and, in addition to errors produced by native speakers (L1), also includes correction of texts written by learners of Arabic as a foreign language (L2). Techlimed participated in the L1 track. For our participation in the L1 evaluation task, we developed two sys- tems. The first one is based on the spell- checker Hunspell with specific dictionar- ies. The second one is a hybrid system based on rules, morphology analysis and statistical machine translation. Our results on the test set show that the hybrid system outperforms the lexicon driven approach with a precision of 71.2%, a recall of 64.94% and an F-measure of 67.93%.",2015,WS,1.0
Embarrassed or Awkward? Ranking Emotion Synonyms for ESL Learners' Appropriate Wording,"We introduce a novel framework based on the probabilistic model for emotion wording as- sistance. The example sentences from the on- line dictionary, Vocabulary.com are utilized as the training data; and the writings in a de- signed ESL’s writing task are the testing cor- pus. The emotion events are captured by extracting patterns of the example sentences. Our approach learns the joint probability of contextual emotion events and the emotion words from the training corpus. After extract- ing patterns in the testing corpus, we then ag- gregate their probabilities to suggest the emotion word that describes the ESL’s con- text most appropriately. We evaluate the pro- posed approach by the NDCG@5 of the suggested words for the writings in the testing corpus. The experiment result shows our ap- proach can more appropriately suggest the emotion words compared to SVM, PMI and two representative on-line reference tools, PIGAI and Thesaurus.com.",2015,WS,1.0
Judge a Book by its Cover: Conservative Focused Crawling under Resource Constraints,"In this paper, we propose a domain spe- cific crawler that decides the domain rele- vance of a URL without downloading the page. In contrast, a focused crawler re- lies on the content of the page to make the same decision. To achieve this, we use a classifier model which harnesses features such as the page’s URL and its parents’ information to score a page. The classi- fier model is incrementally trained at each depth in order to learn the facets of the domain. Our approach modifies the fo- cused crawler by circumventing the need for extra resource usage in terms of band- width. We test the performance of our ap- proach on Wikipedia data. Our Conserva- tive Focused Crawler (CFC) shows a per- formance equivalent to that of a focused crawler (skyline system) with an average resource usage reduction of ≈30% across two domains viz., tourism and sports.",2015,WS,1.0
CKY Parsing With Independence Constraints,"The CKY algorithm is an important com- ponent in many natural language parsers. We propose a novel type of constraint for context-free parsing called independence constraints. Based on the concept of in- dependence between words, we show how these constraints can be used to reduce the work done in the CKY algorithm. We demonstrate a classifier which can be used to identify boundaries between indepen- dent words in a sentence using only sur- face features, and show that it can be used to speed up a CKY parser. We investigate the trade-off between speed and accuracy, and indicate directions for improvement.",2015,WS,1.0
Condition Random Fields-based Grammatical Error Detection for Chinese as Second Language,"The foreign learners are not easy to learn Chinese as a second language. Because there are many special rules different from other languages in Chinese. When the people learn Chinese as a foreign language usually make some grammatical errors, such as missing, re- dundant, selection and disorder. In this paper, we proposed the conditional random fields (CRFs) to detect the grammatical errors. The features based on statistical word and part-of- speech (POS) pattern were adopted here. The relationships between words by part-of-speech are helpful for Chinese grammatical error de- tection. Finally, we according to CRF deter- mined which error types in sentences. Accord- ing to the observation of experimental results, the performance of the proposed model is ac- ceptable in precision and recall rates.",2015,WS,1.0
When is Lying the Right Choice?,"Restricting the spread of sensitive information is important in domains ranging from commerce to military operations. In this position paper, we propose research aimed at exploring techniques for privacy enforcement when humans are the recipient of — possibly obfuscated — information. Such obfuscation could be considered to be a white lie, and we argue that determining what information to share and whether it should be obfuscated must be done through controlled query evaluation, which depends on each agent’s risk/benefit evaluation. We concentrate on machine-human interactions, and note that appropriate specific natural language interfaces need to be developed to handle obfusca- tion. We propose a solution for creating controlled query evaluation mechanisms based on robust approaches for data representation under uncertainty, viz. SDL-Lite, and propose using ITA Con- trolled English for natural language generation so as to handle subjectivity. We present the general architecture for our approach, with a focus on the relationship between formal ontology, controlled query evaluation, and natural language.",2015,WS,1.0
Shallow Training is cheap but is it good enough? Experiments with Medical Fact Coding,"A typical NLP system for medical fact coding uses multiple layers of supervision involving fact- attributes, relations and coding. Training such a system involves expensive and laborious annotation process involving all layers of the pipeline. In this work, we investigate the feasibility of a shal- low medical coding model that trains only on fact annotations, while disregarding fact-attributes and relations, potentially saving considerable annota- tion time and costs. Our results show that the shal- low system, despite using less supervision, is only 1.4% F1 points behind the multi-layered system on Disorders, and contrary to expectation, is able to improve over the latter by about 2.4% F1 points on Procedure facts. Further, our experiments also show that training the shallow system using only sentence-level fact labels with no span information has no negative effect on performance, indicating further cost savings through weak supervision.",2015,WS,1.0
Semi-automated typical error annotation for learner English essays: integrating frameworks,"This paper proposes integration of three open source utilities: brat web annotation tool, Freeling suite of linguistic analyzers and Aspell spellchecker. We demonstrate how their combination can be used to pre- annotate texts in a learner corpus of En- glish essays with potential errors and ease human annotators’ work. Spellchecker alerts and morphological an- alyzer tagging probabilities are used to de- tect students’ possible errors of most typ- ical sorts. F-measure for the developed pre-annotation framework with regard to human annotation is 0.57, which already makes the system a substantial help to human annotators, but at the same time leaves room for further improvement.",2015,WS,1.0
Non-projectivity and processing constraints: Insights from Hindi,"Non-projectivity is an important theoret- ical and computational concept that has been investigated extensively in the depen- dency grammar/parsing paradigms. How- ever, from a human sentence processing perspective, non-projectivity has received very little attention. In this paper, we look at existing work and propose new factors related to processing non-projective con- figuration. We argue that (a) counter to the claims in the psycholinguistic litera- ture (Levy et al, 2012), different aspects of prediction maintenance can lead to higher processing cost for a non-projective de- pendency, (b) parsing strategies can in- teract with the expectation for a non- projective dependency, and (c) memory (re)activation can explain processing cost in certain non-projective configurations.",2015,WS,0.30000000000000004
Grammatical Error Correction Considering Multi-word Expressions,"Multi-word expressions (MWEs) have been recognized as important linguistic information and much research has been conducted especially on their extraction and interpretation. On the other hand, they have hardly been used in real application areas. While those who are learning English as a second language (ESL) use MWEs in their writings just like native speakers, MWEs haven’t been taken into consideration in grammatical error correction tasks. In this paper, we investigate the grammatical er- ror correction method using MWEs. Our method proposes a straightforward appli- cation of MWEs to grammatical error cor- rection, but experimental results show that MWEs have a beneficial effect on gram- matical error correction.",2015,WS,1.0
Does Universal Dependencies need a parsing representation? An investigation of English,"This paper investigates the potential of defining a parsing representation for En- glish data in Universal Dependencies, a crosslingual dependency scheme. We in- vestigate structural transformations that change the choices of headedness in the dependency tree. The transformations make auxiliaries, copulas, subordinat- ing conjunctions and prepositions heads, while in UD they are dependents of a lexical head. We show experimental re- sults for the performance of MaltParser, a data-driven transition-based parser, on the product of each transformation. While some transformed representations favor performance, inverting the transforma- tions to obtain UD for the final product propagates errors, in part due to the nature of lexical-head representations. This pre- vents the transformations from being prof- itably used to improve parser performance in that representation.",2015,WS,-1.0
Using Learner Data to Improve Error Correction in Adjective–Noun Combinations,"This paper presents a novel approach to error correction in content words in learner writing focussing on adjective–noun (AN) combina- tions. We show how error patterns can be used to improve the performance of the error cor- rection system, and demonstrate that our ap- proach is capable of suggesting an appropri- ate correction within the top two alternatives in half of the cases and within top 10 alterna- tives in 71% of the cases, performing with an MRR of 0.5061. We then integrate our error correction system with a state-of-the-art con- tent word error detection system and discuss the results.",2015,WS,1.0
QCMUQ@QALB-2015 Shared Task: Combining Character level MT and Error-tolerant Finite-State Recognition for Arabic Spelling Correction,"We describe the CMU-Q and QCRI’s joint efforts in building a spelling correction system for Arabic in the QALB 2015 Shared Task. Our system is based on a hybrid pipeline that combines rule-based linguistic techniques with statistical meth- ods using language modeling and machine translation, as well as an error-tolerant finite-state automata method. We trained and tested our spelling corrector using the dataset provided by the shared task orga- nizers. Our system outperforms the base- line system and yeilds better correction quality with an F-score of 68.12 on L1- test-2015 testset and 38.90 on the L2-test- 2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask.",2015,WS,1.0
Multi-source Cross-lingual Delexicalized Parser Transfer: Prague or Stanford?,"We compare two annotation styles, Prague dependencies and Universal Stanford De- pendencies, in their adequacy for pars- ing. We specifically focus on comparing the adposition attachment style, used in these two formalisms, applied in multi- source cross-lingual delexicalized depen- dency parser transfer performed by parse tree combination. We show that in our set- ting, converting the adposition annotation to Stanford style in the Prague style train- ing treebanks leads to promising results. We find that best results can be obtained by parsing the target sentences with parsers trained on treebanks using both of the ad- position annotation styles in parallel, and combining all the resulting parse trees to- gether after having converted them to the Stanford adposition style (+0.39% UAS over Prague style baseline). The score im- provements are considerably more signif- icant when using a smaller set of diverse source treebanks (up to +2.24% UAS over the baseline).",2015,WS,1.0
Analysing Inconsistencies and Errors in PoS Tagging in two Icelandic Gold Standards,"This paper describes work in progress. We experiment with training a state-of-the-art tagger, Stagger, on a new gold standard,MIM-GOLD, for the PoS tagging of Ice- landic. We compare the results to results obtained using a previous gold standard, IFD. Using MIM-GOLD, tagging accu- racy is considerably lower, 92.76% com- pared to 93.67% accuracy for IFD. We an- alyze and classify the errors made by the tagger in order to explain this difference. We find that inconsistencies and incorrect tags in MIM-GOLD may account for this difference.",2015,WS,-0.30000000000000004
"""So, which one is it?"" The effect of alternative incremental architectures in a high-performance game-playing agent","This paper introduces Eve, a high- performance agent that plays a fast-paced image matching game in a spoken dia- logue with a human partner. The agent can be optimized and operated in three differ- ent modes of incremental speech process- ing that optionally include incremental speech recognition, language understand- ing, and dialogue policies. We present our framework for training and evaluating the agent’s dialogue policies. In a user study involving 125 human participants, we evaluate three incremental architec- tures against each other and also compare their performance to human-human game- play. Our study reveals that the most fully incremental agent achieves game scores that are comparable to those achieved in human-human gameplay, are higher than those achieved by partially and non- incremental versions, and are accompa- nied by improved user perceptions of effi- ciency, understanding of speech, and natu- ralness of interaction.",2015,WS,1.0
WikiTrans: Swedish-Danish Machine Translation in a Constraint Grammar Framework (invited talk),"This talk presents an MT system for the au- tomatic generation of Danish Wikipedia articles from Swedish originals. The translated Wikipedia (WikiTrans) is indexed for both title and content, and integrated with original Danish articles where they exist. Newly added or modified articles in the Swedish Wikipedia are monitored and handled on a daily basis. The translation approach (Gram- Trans) uses a grammar-based machine translation system with a deep, structural source-language analysis. Morphosyntactic disambiguation and lexical transfer rules exploit Constraint Grammar tags and dependency links to access contextual in- formation, such as syntactic argument function, semantic type and quantifiers. Out-of-vocabulary words are handled by derivational and compound analysis with a combined coverage of 99.3%, as well as systematic morpho-phonemic translitera- tions for the remaining cases. Reflecting the sim- ilarities between Swedish and Danish, the system achieved high BLEU scores (0.65-0.8 depending on references), and outperformed standard STMT and RBMT competitors by a large margin.",2015,WS,1.0
Why Predicting Post-Edition is so Hard? Failure Analysis of LIMSI Submission to the APE Shared Task,"This paper describes the two systems sub- mitted by LIMSI to the WMT’15 Shared Task on Automatic Post-Editing. The first one relies on a reformulation of the APE task as a Machine Translation task; the second implements a simple rule-based approach. Neither of these two systems manage to improve the automatic transla- tion. We show, by carefully analyzing the failure of our systems that this counter- performance mainly results from the in- consistency in the annotations.",2015,WS,-0.9
Towards Taxonomy of Errors in Chat-oriented Dialogue Systems,"This paper presents a taxonomy of errors in chat-oriented dialogue systems. Com- pared to human-human conversations and task-oriented dialogues, little is known about the errors made in chat-oriented di- alogue systems. Through a data collection of chat dialogues and analyses of dialogue breakdowns, we classified errors and cre- ated a taxonomy. Although the proposed taxonomy may not be complete, this pa- per is the first to present a taxonomy of er- rors in chat-oriented dialogue systems. We also highlight the difficulty in pinpointing errors in such systems.",2015,WS,0.2
Chinese Grammatical Error Diagnosis System Based on Hybrid Model,"This paper describes our system in the Chinese Grammatical Error Diagnosis (CGED) task for learning Chinese as a Foreign Language (CFL). Our work adopts a hybrid model by integrating rule- based method and n-gram statistical method to detect Chinese grammatical errors, identify the error type and point out the position of error in the input sentences. Tri-gram is applied to disorder mistake. And the rest of mistakes are solved by the conservation rules sets. Empirical evaluation results demonstrate the utility of our CGED system.",2015,WS,1.0
"Translating Negation: Induction, Search And Model Errors","Statistical Machine Translation systems show considerably worse performance in translating negative sentences than positive ones (Fan- cellu and Webber, 2014; Wetzel and Bond, 2012). Various techniques have addressed the problem of translating negation, but their un- derlying assumptions have never been vali- dated by a proper error analysis. A related paper (Fancellu and Webber, 2015) reports on a manual error analysis of the kinds of errors involved in translating negation. The present paper presents ongoing work to discover their causes by considering which, if any, are in- duction, search or model errors. We show that standard oracle decoding techniques provide little help due to the locality of negation scope and their reliance on a single reference. We are working to address these weaknesses using a chart analysis based on oracle hypotheses, guided by the negation elements contained in a source span and by how these elements are ex- pected to be translated at each decoding step. Preliminary results show chart analysis is able to give a more in-depth analysis of the above errors and better explains the results of the manual analysis.",2015,WS,0.6000000000000001
Inferring a Personalized Next Point-of-Interest Recommendation Model with Latent Behavior Patterns,"In this paper, we address the problem of personalized next Point-of-interest (POI) recommendation which has become an important and very challenging task in location-based social networks (LBSNs), but not well studied yet. With the conjecture that, under different contextual scenario, human exhibits distinct mobility patterns, we attempt here to jointly model the next POI recommendation under the influence of user‚Äôs latent behavior pattern. We propose to adopt a third-rank tensor to model the successive check-in behaviors. By incorporating softmax function to fuse the personalized Markov chain with latent pattern, we furnish a Bayesian Personalized Ranking (BPR) approach and derive the optimization criterion accordingly. Expectation Maximization (EM) is then used to estimate the model parameters. Extensive experiments on two large-scale LBSNs datasets demonstrate the significant improvements of our model over several state-of-the-art methods.",2016,AAAI,1.0
Refining Subgames in Large Imperfect Information Games,"The leading approach to solving large imperfect information games is to pre-calculate an approximate solution using a simplified abstraction of the full game; that solution is then used to play the original, full-scale game. The abstraction step is necessitated by the size of the game tree. However, as the original game progresses, the remaining portion of the tree (the subgame) becomes smaller. An appealing idea is to use the simplified abstraction to play the early parts of the game and then, once the subgame becomes tractable, to calculate a solution using a finer-grained abstraction in real time, creating a combined final strategy. While this approach is straightforward for perfect information games, it is a much more complex problem for imperfect information games. If the subgame is solved locally, the opponent can alter his play in prior to this subgame to exploit our combined strategy. To prevent this, we introduce the notion of subgame margin, a simple value with appealing properties. If any best response reaches the subgame, the improvement of exploitability of the combined strategy is (at least) proportional to the subgame margin. This motivates subgame refinements resulting in large positive margins. Unfortunately, current techniques either neglect subgame margin (potentially leading to a large negative subgame margin and drastically more exploitable strategies), or guarantee only non-negative subgame margin (possibly producing the original, unrefined strategy, even if much stronger strategies are possible). Our technique remedies this problem by maximizing the subgame margin and is guaranteed to find the optimal solution. We evaluate our technique using one of the top participants of the AAAI-14 Computer Poker Competition, the leading playground for agents in imperfect information settings.",2016,AAAI,1.0
Commonsense in Parts: Mining Part-Whole Relations from the Web and Image Tags,"Commonsense knowledge about part-whole relations (e.g., screen partOf notebook) is important for interpreting user input in web search and question answering, or for object detection in images. Prior work on knowledge base construction has compiled part-whole assertions, but with substantial limitations: i) semantically different kinds of part-whole relations are conflated into a single generic relation, ii) the arguments of a part-whole assertion are merely words with ambiguous meaning, iii) the assertions lack additional attributes like visibility (e.g., a nose is visible but a kidney is not) and cardinality information (e.g., a bird has two legs while a spider eight), iv) limited coverage of only tens of thousands of assertions. This paper presents a new method for automatically acquiring part-whole commonsense from Web contents and image tags at an unprecedented scale, yielding many millions of assertions, while specifically addressing the four shortcomings of prior work. Our method combines pattern-based information extraction methods with logical reasoning. We carefully distinguish different relations: physicalPartOf, memberOf, substanceOf. We consistently map the arguments of all assertions onto WordNet senses, eliminating the ambiguity of wordlevel assertions. We identify whether the parts can be visually perceived, and infer cardinalities for the assertions. The resulting commonsense knowledge base has very high quality and high coverage, with an accuracy of 89% determined by extensive sampling, and is publicly available.",2016,AAAI,0.8
Analyzing Biases in Human Perception of User Age and Gender from Text,"User traits disclosed through written text, such as age and gender, can be used to personalize applications such as recommender systems or conversational agents. However, human perception of these traits is not perfectly aligned with reality. In this paper, we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets. We systematically analyze the quality and possible biases of these predictions. We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice. Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception.",2016,ACL,-0.6000000000000001
Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric,"Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.",2016,ACL,-0.4
Reference Bias in Monolingual Machine Translation Evaluation,"In the translation industry, human translations are assessed by comparison with the source texts. In the Machine Translation (MT) research community, however, it is a common practice to perform quality assessment using a reference translation instead of the source text. In this paper we show that this practice has a serious issue – annotators are strongly biased by the reference translation provided, and this can have a negative impact on the assessment of MT quality.",2016,ACL,-1.0
"Book Reviews: Semantic Similarity from Natural Language and Ontology Analysis by Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain","Learning semantic similarity for units of language or concepts is crucial not only for numerous tasks in computational linguistics, but also for language understanding and reasoning in the broad context of artificial intelligence. With growing interests and efforts in modeling and computing semantic measures in recent years, we have witnessed much progress in the following two strands of research that approach semantic similarity: corpus-based statistical methods and knowledge-enhanced methods with human knowledge defined in ontologies. This book by Harispe, Ranwez, Janaqi, and Montmain provides a detailed introduction to state-of-the-art research in these two lines of work.",2016,CL,0.0
Surveys: A Survey of Word Reordering in Statistical Machine Translation: Computational Models and Language Phenomena,"linguistic representations. The main advantages of SMT are versatility and costeffectiveness: In principle, the same modeling framework can be applied to any pair of languages with minimal engineering effort, given sufficient amounts of translation data. However, experience in a diverse range of language pairs has revealed that this form of modeling is highly sensitive to structural differences between source and target language, particularly at the level of word order. Indeed, natural languages vary greatly in how they arrange sentence components, and translating words in the correct order is essential to preserving meaning across languages. In English, for instance, the role of different predicate arguments is determined precisely by their relative position within the sentence. Consider the translation example in Figure 1: Looking at the English glosses of the Arabic sentence, one can see that corresponding words in the two languages are placed in overall similar orders with the notable exception of the verb (jdd/renewed), which occurs at the beginning of the Arabic sentence but in the middle of the English one—more specifically, between the subject and the object. To reach the correct English order, three other reorderings are required between pairs of adjacent Arabic words: (AlEAhl/the-monarch, Almgrby/the-Moroccan), (dEm/support, -h/his), and (Alr}ys/the-president, Alfrnsy/the-French). This example suggests a simple division of reordering patterns into long range, or global, and short range, or local. However, other language pairs display more complex, hierarchical patterns. Word reordering phenomena are naturally handled by human translators1 but are a major source of complexity for SMT. In very general terms, the task of SMT consists of breaking the input sentence into smaller units, selecting an optimal translation for each unit, and placing them in the correct order. Searching for the overall best translation throughout the space of all possible reorderings is, however, computationally intractable (Knight 1999). This crucial fact has motivated an impressive amount of research around two inter-related questions: namely, how to effectively restrict the set of allowed word permutations and how to detect the best permutation among them. Existing solutions to these problems range from heuristic constraints, based on word-to-word distances and completely agnostic about the sentence content, to linguistically motivated SMT frameworks where the entire translation process is guided by syntactic structure. The research in word reordering has advanced together with core SMT research and has sometimes directed it, being one of the main motivations for the development of tree-based SMT. At the same time, the variety of word orders existing in world languages has pressed the SMT community to admit the importance of 1 Nevertheless, learning and understanding a new language has been shown to be more difficult when the new language is structurally distant from one’s native language (Corder 1979).",2016,CL,0.0
Reviewers for Volume 42,"This journal has a knowledgeable and hard-working editorial board, listed on the journal’s Web site, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express her gratitude to the external reviewers listed here, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 42). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.",2016,CL,0.0
A Hybrid Approach to Generation of Missing Abstracts in Biomedical Literature,"Readers usually rely on abstracts to identify relevant medical information from scientific articles. Abstracts are also essential to advanced information retrieval methods. More than 50 thousand scientific publications in PubMed lack author-generated abstracts, and the relevancy judgements for these papers have to be based on their titles alone. In this paper, we propose a hybrid summarization technique that aims to select the most pertinent sentences from articles to generate an extractive summary in lieu of a missing abstract. We combine i) health outcome detection, ii) keyphrase extraction, and iii) textual entailment recognition between sentences. We evaluate our hybrid approach and analyze the improvements of multi-factor summarization over techniques that rely on a single method, using a collection of 295 manually generated reference summaries. The obtained results show that the hybrid approach outperforms the baseline techniques with an improvement of 13% in recall and 4% in F1 score.",2016,COLING,0.9
A Distribution-based Model to Learn Bilingual Word Embeddings,"We introduce a distribution based model to learn bilingual word embeddings from monolingual data. It is simple, effective and does not require any parallel data or any seed lexicon. We take advantage of the fact that word embeddings are usually in form of dense real-valued low-dimensional vector and therefore the distribution of them can be accurately estimated. A novel cross-lingual learning objective is proposed which directly matches the distributions of word embeddings in one language with that in the other language. During the joint learning process, we dynamically estimate the distributions of word embeddings in two languages respectively and minimize the dissimilarity between them through standard back propagation algorithm. Our learned bilingual word embeddings allow to group each word and its translations together in the shared vector space. We demonstrate the utility of the learned embeddings on the task of finding word-to-word translations from monolingual corpora. Our model achieved encouraging performance on data in both related languages and substantially different languages.",2016,COLING,1.0
Product Classification in E-Commerce using Distributional Semantics,"Product classification is the task of automatically predicting a taxonomy path for a product in a predefined taxonomy hierarchy given a textual product description or title. For efficient product classification we require a suitable representation for a document (the textual description of a product) feature vector and efficient and fast algorithms for prediction.To address the above challenges, we propose a new distributional semantics representation for document vector formation. We also develop a new two-level ensemble approach utilising (with respect to the taxonomy tree) path-wise, node-wise and depth-wise classifiers to reduce error in the final product classification task. Our experiments show the effectiveness of the distributional representation and the ensemble approach on data sets from a leading e-commerce platform and achieve improved results on various evaluation metrics compared to earlier approaches.",2016,COLING,1.0
"Zara: A Virtual Interactive Dialogue System Incorporating Emotion, Sentiment and Personality Recognition","Zara, or ‘Zara the Supergirl’ is a virtual robot, that can exhibit empathy while interacting with an user, with the aid of its built in facial and emotion recognition, sentiment analysis, and speech module. At the end of the 5-10 minute conversation, Zara can give a personality analysis of the user based on all the user utterances. We have also implemented a real-time emotion recognition, using a CNN model that detects emotion from raw audio without feature extraction, and have achieved an average of 65.7% accuracy on six different emotion classes, which is an impressive 4.5% improvement from the conventional feature based SVM classification. Also, we have described a CNN based sentiment analysis module trained using out-of-domain data, that recognizes sentiment from the speech recognition transcript, which has a 74.8 F-measure when tested on human-machine dialogues.",2016,COLING,1.0
Representation and Learning of Temporal Relations,"Determining the relative order of events and times described in text is an important problem in natural language processing. It is also a difficult one: general state-of-the-art performance has been stuck at a relatively low ceiling for years. We investigate the representation of temporal relations, and empirically evaluate the effect that various temporal relation representations have on machine learning performance. While machine learning performance decreases with increased representational expressiveness, not all representation simplifications have equal impact.",2016,COLING,-0.6000000000000001
Keyphrase Annotation with Graph Co-Ranking,"Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods.",2016,COLING,1.0
SoNLP-DP System for ConLL-2016 Chinese Shallow Discourse Parsing,"This paper describes our submission to the CoNLL-2016 shared task (Xue et al., 2016) on end-to-end Chinese shallow discourse parsing. We decompose the endto-end process into four steps. Firstly, we define a syntactically heuristic algorithm to identify elementary discourse units (EDUs) and further to recognize valid EDU pairs. Secondly, we recognize explicit discourse connectives. Thirdly, we link each explicit connective to valid EDU pairs to obtain explicit discourse relations. For those valid EDU pairs not linked to any explicit connective, they become non-explicit discourse relations.1 Finally, we assign each discourse relation, either explicit or non-explicit with a discourse sense. Our system is evaluated on the closed track of the CoNLL2016 shared task and achieves 35.54% and 23.46% in F1-measure on the official test set and blind test set, respectively.",2016,CoNLL,1.0
A Constituent Syntactic Parse Tree Based Discourse Parser,"This paper describes our system in the CoNLL-2016 shared task. Our system takes a piece of newswire text as input and returns the discourse relations. In our system we use a pipeline to conduct each subtask. Our system is evaluated on the CoNLL-2016 Shared Task closed track and obtains 0.1515 in F1 measurement, especially the part of detecting connectives, which achieves 0.9838 on blind test set.",2016,CoNLL,1.0
Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,"Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-theart accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset.",2016,CoNLL,1.0
Does 'well-being' translate on Twitter?,"We investigate whether psychological wellbeing translates across English and Spanish Twitter, by building and comparing source language and automatically translated weighted lexica in English and Spanish. We find that the source language models perform substantially better than the machine translated versions. Moreover, manually correcting translation errors does not improve model performance, suggesting that meaningful cultural information is being lost in translation. Further work is needed to clarify when automatic translation of well-being lexica is effective and how it can be improved for crosscultural analysis.",2016,EMNLP,0.0
Neural Shift-Reduce CCG Semantic Parsing,"We present a shift-reduce CCG semantic parser. Our parser uses a neural network architecture that balances model capacity and computational cost. We train by transferring a model from a computationally expensive loglinear CKY parser. Our learner addresses two challenges: selecting the best parse for learning when the CKY parser generates multiple correct trees, and learning from partial derivations when the CKY parser fails to parse. We evaluate on AMR parsing. Our parser performs comparably to the CKY parser, while doing significantly fewer operations. We also present results for greedy semantic parsing with a relatively small drop in performance.",2016,EMNLP,1.0
Beyond Canonical Texts: A Computational Analysis of Fanfiction,"While much computational work on fiction has focused on works in the literary canon, user-created fanfiction presents a unique opportunity to study an ecosystem of literary production and consumption, embodying qualities both of large-scale literary data (55 billion tokens) and also a social network (with over 2 million users). We present several empirical analyses of this data in order to illustrate the range of affordances it presents to research in NLP, computational social science and the digital humanities. We find that fanfiction deprioritizes main protagonists in comparison to canonical texts, has a statistically significant difference in attention allocated to female characters, and offers a framework for developing models of reader reactions to stories.",2016,EMNLP,0.0
Improving Semantic Parsing via Answer Type Inference,"In this work, we show the possibility of inferring the answer type before solving a factoid question and leveraging the type information to improve semantic parsing. By replacing the topic entity in a question with its type, we are able to generate an abstract form of the question, whose answer corresponds to the answer type of the original question. A bidirectional LSTM model is built to train over the abstract form of questions and infer their answer types. It is also observed that if we convert a question into a statement form, our LSTM model achieves better accuracy. Using the predicted type information to rerank the logical forms returned by AgendaIL, one of the leading semantic parsers, we are able to improve the F1-score from 49.7% to 52.6% on the WEBQUESTIONS data.",2016,EMNLP,1.0
Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task,"Natural language acquisition relies on appropriate generalization: the ability to produce novel sentences, while learning to restrict productions to acceptable forms in the language. Psycholinguists have proposed various properties that might play a role in guiding appropriate generalizations, looking at learning of verb alternations as a testbed. Several computational cognitive models have explored aspects of this phenomenon, but their results are hard to compare given the high variability in the linguistic properties represented in their input. In this paper, we directly compare two recent approaches, a Bayesian model and a connectionist model, in their ability to replicate human judgments of appropriate generalizations. We find that the Bayesian model more accurately mimics the judgments due to its richer learning mechanism that can exploit distributional properties of the input in a manner consistent with human behaviour.",2016,EMNLP,0.0
Title:Visual Representations: Defining Properties and Deep Approximations,"Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.",2016,ICLR,0.30000000000000004
Title:A note on the evaluation of generative models,"Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria‚Äîaverage log-likelihood, Parzen window estimates, and visual fidelity of samples‚Äîare largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.",2016,ICLR,0.0
Title:The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations,"We introduce a new test of how well language models capture meaning in children‚Äôs books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lowerfrequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.",2016,ICLR,1.0
On the Power and Limits of Distance-Based Learning,"We initiate the study of low-distortion finite metric embeddings in multi-class (and multi-label) classification where (i) both the space of input instances and the space of output classes have combinatorial metric structure, and (ii) the concepts we wish to learn are low-distortion embeddings. We develop new geometric techniques and prove strong learning lower bounds. These provable limits hold even when we allow learners and classifiers to get advice by one or more experts. Our study overwhelmingly indicates that post-geometry assumptions are necessary in multi-class classification, as in natural language processing (NLP). Technically, the mathematical tools we developed in this work could be of independent interest to NLP. To the best of our knowledge, this is the first work which formally studies classification problems in combinatorial spaces and where the concepts are low-distortion embeddings.",2016,ICML,0.6000000000000001
"Why Most Decisions Are Easy in Tetris‚ÄîAnd Perhaps in Other Sequential Decision Problems, As Well","We examined the sequence of decision problems that are encountered in the game of Tetris and found that most of the problems are easy in the following sense: One can choose well among the available actions without knowing an evaluation function that scores well in the game. This is a consequence of three conditions that are prevalent in the game: simple dominance, cumulative dominance, and noncompensation. These conditions can be exploited to develop faster and more effective learning algorithms. In addition, they allow certain types of domain knowledge to be incorporated with ease into a learning algorithm. Among the sequential decision problems we encounter, it is unlikely that Tetris is unique or rare in having these properties.",2016,ICML,0.0
Meta‚ÄìGradient Boosted Decision Tree Model for Weight and Target Learning,"Labeled training data is an essential part of any supervised machine learning framework. In practice, there is a trade-off between the quality of a label and its cost. In this paper, we consider a problem of learning to rank on a large-scale dataset with low-quality relevance labels aiming at maximizing the quality of a trained ranker on a small validation dataset with high-quality ground truth relevance labels. Motivated by the classical Gauss-Markov theorem for the linear regression problem, we formulate the problems of (1) reweighting training instances and (2) remapping learning targets. We propose meta‚Äì gradient decision tree learning framework for optimizing weight and target functions by applying gradient-based hyperparameter optimization. Experiments on a large-scale real-world dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by incorporating our framework.",2016,ICML,1.0
Rich Component Analysis,"In many settings, we have multiple data sets (also called views) that capture different and overlapping aspects of the same phenomenon. We are often interested in finding patterns that are unique to one or to a subset of the views. For example, we might have one set of molecular observations and one set of physiological observations on the same group of individuals, and we want to quantify molecular patterns that are uncorrelated with physiology. Despite being a common problem, this is highly challenging when the correlations come from complex distributions. In this paper, we develop the general framework of Rich Component Analysis (RCA) to model settings where the observations from different views are driven by different sets of latent components, and each component can be a complex, high-dimensional distribution. We introduce algorithms based on cumulant extraction that provably learn each of the components without having to model the other components. We show how to integrate RCA with stochastic gradient descent into a meta-algorithm for learning general models, and demonstrate substantial improvement in accuracy on several synthetic and real datasets in both supervised and unsupervised tasks. Our method makes it possible to learn latent variable models when we don‚Äôt have samples from the true model but only samples after complex perturbations.",2016,ICML,0.8
Learning End-to-end Video Classification with Rank-Pooling,"We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks.",2016,ICML,0.6000000000000001
Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing,"Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-toimplement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.",2016,ICML,1.0
Synset Ranking of Hindi WordNet,"Word Sense Disambiguation (WSD) is one of the open problems in the area of natural language processing. Various supervised, unsupervised and knowledge based approaches have been proposed for automatically determining the sense of a word in a particular context. It has been observed that such approaches often find it difficult to beat the WordNet First Sense (WFS) baseline which assigns the sense irrespective of context. In this paper, we present our work on creating the WFS baseline for Hindi language by manually ranking the synsets of Hindi WordNet. A ranking tool is developed where human experts can see the frequency of the word senses in the sense-tagged corpora and have been asked to rank the senses of a word by using this information and also his/her intuition. The accuracy of WFS baseline is tested on several standard datasets. F-score is found to be 60%, 65% and 55% on Health, Tourism and News datasets respectively. The created rankings can also be used in other NLP applications viz., Machine Translation, Information Retrieval, Text Summarization, etc.",2016,LREC,1.0
Transfer-Based Learning-to-Rank Assessment of Medical Term Technicality,"While measuring the readability of texts has been a long-standing research topic, assessing the technicality of terms has only been addressed more recently and mostly for the English language. In this paper, we train a learning-to-rank model to determine a specialization degree for each term found in a given list. Since no training data for this task exist for French, we train our system with non-lexical features on English data, namely, the Consumer Health Vocabulary, then apply it to French. The features include the likelihood ratio of the term based on specialized and lay language models, and tests for containing morphologically complex words. The evaluation of this approach is conducted on 134 terms from the UMLS Metathesaurus and 868 terms from the Eugloss thesaurus. The Normalized Discounted Cumulative Gain obtained by our system is over 0.8 on both test sets. Besides, thanks to the learning-to-rank approach, adding morphological features to the language model features improves the results on the Eugloss thesaurus.",2016,LREC,1.0
A Neural Lemmatizer for Bengali,"We propose a novel neural lemmatization model which is language independent and supervised in nature. To handle the words in a neural framework, word embedding technique is used to represent words as vectors. The proposed lemmatizer makes use of contextual information of the surface word to be lemmatized. Given a word along with its contextual neighbours as input, the model is designed to produce the lemma of the concerned word as output. We introduce a new network architecture that permits only dimension specific connections between the input and the output layer of the model. For the present work, Bengali is taken as the reference language. Two datasets are prepared for training and testing purpose consisting of 19,159 and 2,126 instances respectively. As Bengali is a resource scarce language, these datasets would be beneficial for the respective research community. Evaluation method shows that the neural lemmatizer achieves 69.57% accuracy on the test dataset and outperforms the simple cosine similarity based baseline strategy by a margin of 1.37%.",2016,LREC,1.0
MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP,"We present MultiVec, a new toolkit for computing continuous representations for text at different granularity levels (word-level or sequences of words). MultiVec includes word2vec’s features, paragraph vector (batch and online) and bivec for bilingual distributed representations. MultiVec also includes different distance measures between words and sequences of words. The toolkit is written in C++ and is aimed at being fast (in the same order of magnitude as word2vec), easy to use, and easy to extend. It has been evaluated on several NLP tasks: the analogical reasoning task, sentiment analysis, and crosslingual document classification.",2016,LREC,0.4
What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment,"We propose an end-to-end, domainindependent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WEATHERGOV dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the ROBOCUP dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.",2016,NAACL,1.0
Counter-fitting Word Vectors to Linguistic Constraints,"In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors’ capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.",2016,NAACL,1.0
English Resource Semantics,"Recent years have seen a dramatic increase in interest in semantically­informed natural language processing, including parsing into semantic representations, grounded language processing that connects linguistic structures to world representations, proposals to integrate compositional and distributional approaches to semantics, and approaches to semantically­sensitive tasks including sentiment analysis, summarization, generation, machine translation, and information extraction which take into account linguistic structure beyond n­grams. The semantic inputs to this work include a wide range of representations, from word embeddings, to syntactic dependencies used as a proxy for semantic dependencies, to sentence­level semantic representations either partial (e.g. semantic role labels) or fully articulated.",2016,NAACL,0.0
Controlling Politeness in Neural Machine Translation via Side Constraints,"Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honorifics in neural machine translation (NMT) via side constraints, focusing on English→German. We show that by marking up the (English) source side of the training data with a feature that encodes the use of honorifics on the (German) target side, we can control the honorifics produced at test time. Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness.",2016,NAACL,1.0
Questioning Arbitrariness in Language: a Data-Driven Study of Conventional Iconicity,"This paper presents a data-driven investigation of phonesthemes, phonetic units said to carry meaning associations, thus challenging the traditionally assumed arbitrariness of language. Phonesthemes have received a substantial amount of attention within the cognitive science literature on sound iconicity, but nevertheless remain a controversial and understudied phenomenon. Here we employ NLP techniques to address two main questions: How can the existence of phonesthemes be tested at a large scale with quantitative methods? And how can the meaning arguably carried by a phonestheme be induced automatically from word embeddings? We develop novel methods to make progress on these fronts and compare our results to previous work, obtaining substantial improvements.",2016,NAACL,1.0
A Long Short-Term Memory Framework for Predicting Humor in Dialogues,"We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes.",2016,NAACL,1.0
A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories,"Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the ‘Story Cloze Test’. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.",2016,NAACL,-0.7000000000000001
Error Analysis of Generalized Nyström Kernel Regression,"Nyström method has been used successfully to improve the computational efficiency of kernel ridge regression (KRR). Recently, theoretical analysis of Nyström KRR, including generalization bound and convergence rate, has been established based on reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However, in real world applications, RKHS is not always optimal and kernel function is not necessary to be symmetric or positive semi-definite. In this paper, we consider the generalized Nyström kernel regression (GNKR) with ℓ2 coefficient regularization, where the kernel just requires the continuity and boundedness. Error analysis is provided to characterize its generalization performance and the column norm sampling is introduced to construct the refined hypothesis space. In particular, the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling.",2016,NIPS,0.5
Maximal Sparsity with Deep Networks?,"The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal ℓ0-norm representations in regimes where existing methods fail. The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.",2016,NIPS,0.5
Learning the Number of Neurons in Deep Networks Authors Abstract,"Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.",2016,NIPS,1.0
Launch and Iterate: Reducing Prediction Churn,"Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. [[In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.]]",2016,NIPS,1.0
Strategic Attentive Writer for Learning Macro-Actions Authors Abstract,"We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to ‚Äì i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.",2016,NIPS,1.0
Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs Authors Abstract,"This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.",2016,NIPS,0.8
Launch and Iterate: Reducing Prediction Churn Authors Abstract,"Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn. 1 The Curse of Version 2.0 In most practical settings, training and launching an initial machine-learned model is only the first step: as new and improved features are created, additional training data is gathered, and the model and learning algorithm are improved, it is natural to launch a series of ever-improving models. Each new candidate may bring wins, but also unnecessary changes. In practice, it is desirable to minimize any unnecessary changes for two key reasons. First, unnecessary changes can hinder usability and debugability as they can be disorienting to users and follow-on system components. Second, unnecessary changes make it more difficult to measure with statistical confidence whether the change is truly an improvement. For both these reasons, there is great interest in making only those changes that are wins, and minimizing any unnecessary changes, while making sure such process does not hinder the overall accuracy objective. There is already a large body of work in machine learning that treats the stability of learning algorithms. These range from the early works of Devroye and Wagner [1] and Vapnik [2, 3] to more recent studies of learning stability in more general hypothesis spaces [4, 5, 6]. Most of the literature on this topic focus on stability of the learning algorithm in terms of the risk or loss function and how such properties translate into uniform generalization with specific convergence rates. We build on these notions, but the problem treated here is substantively different. We address the problem of training consecutive classifiers to reduce unnecessary changes in the presence of realistic evolution of the problem domain and the training sets over time. The main contributions of this paper include: (I) discussion and formulation of the ‚Äúchurn‚Äù metric between trained models, (II) design of stabilization operators for regularization towards a previous model, (III) proposing a Markov chain Monte Carlo (MCMC) stabilization technique, (VI) theoretical analysis of the proposed stabilization in terms of churn, and (V) empirical analysis of the proposed methods on benchmark datasets with different classification algorithms. 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Table 1: Win-loss ratio (WLR) needed to establish a change is statistically significant at the p = 0.05 level for k wins out of n diffs from a binomial distribution. The empirical WLR column shows the WLR one must actually see in the diffs. The true WLR column is the WLR the change must have so that any random draw of diffs has at least a 95% chance of producing the needed empirical WLR. # Diffs Min # Wins Max # Losses Empirical WLR True WLR Needed Allowed Needed Needed 10 9 1 9.000 26.195 100 59 41 1.439 1.972 1,000 527 473 1.114 1.234 10,000 5,083 4,917 1.034 1.068 1.1 Testing for Improvements In the machine learning literature, it is common to compare classifiers on a fixed pre-labeled test set. However, a fixed test set has a few practical downsides. First, if many potential changes to the model are evaluated on the same dataset, it becomes difficult to avoid observing spurious positive effects that are actually due to chance. Second, the true test distribution may be evolving over time, meaning that a fixed test set will eventually diverge from the true distribution of interest. Third, and most important to our discussion, any particular change may affect only a small subset of the test examples, leaving too small a sample of differences (diffs) to determine whether a change is statistically significant. For example, suppose one has a fixed test set of 10,000 samples with which to evaluate a classifier. Consider a change to one of the features, say a Boolean string-similarity feature that causes the feature to match more synonyms, and suppose that re-training a classifier with this small change to this one feature impacts only 0.1% of random examples. Then only 10 of the 10,000 test examples would be affected. As shown in the first row of Table 1, given only 10 diffs, there must be 9 or more wins to declare the change statistically significantly positive for p = 0.05. Note that cross-validation (CV), even in leave-one-out form, does not solve this issue. First, we are still bound by the size of the training set which might not include enough diffs between the two models. Second, and more importantly, the model in the previous iteration has likely seen the entire dataset, which breaks the independence assumption needed for the statistical test. To address these problems and ensure a fresh, sufficiently large test set for each comparison, practitioners often instead measure changes on a set of diffs for the proposed change. For example, to compare classifier A and B, each classifier is evaluated on a billion unlabeled examples, and then the set of diffs is defined as those examples for which classifiers A and B predict a different class.",2016,NIPS,0.0
Satisfying Real-world Goals with Dataset Constraints Authors Abstract,"The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.",2016,NIPS,0.8
"An Architecture for Deep, Hierarchical Generative Models Authors Abstract","We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.",2016,NIPS,1.0
Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling Authors Abstract,"We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",2016,NIPS,0.6000000000000001
Tagger: Deep Unsupervised Perceptual Grouping Authors Abstract,"We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism. We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.",2016,NIPS,0.9
Learning under uncertainty: a comparison between R-W and Bayesian approach Authors Abstract,"Accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition. To examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an R-W model and a Bayesian approach in a visual search task with different volatility levels. Both R-W model and the Bayesian approach reflected an individual‚Äôs estimation of the environmental volatility, and there is a strong correlation between the learning rate in R-W model and the belief of stationarity in the Bayesian approach in different volatility conditions. In a low volatility condition, R-W model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted U shape). The Bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted U shape). In addition, we showed that comparing to Expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from R-W model and lower belief of stationarity from the Bayesian model.",2016,NIPS,0.0
SURGE: Surface Regularized Geometry Estimation from a Single Image Authors Abstract,"This paper introduces an approach to regularize 2.5D surface normal and depth predictions at each pixel given a single input image. The approach infers and reasons about the underlying 3D planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects. Our approach comprises two components: (i) a fourstream convolutional neural network (CNN) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (DCRF) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information. The DCRF is formulated such that gradients can be passed to the surface normal and depth CNNs via backpropagation. In addition, we propose new planar-wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3D editing applications. We show that our regularization yields a 30% relative improvement in planar consistency on the NYU v2 dataset [24].",2016,NIPS,0.7000000000000001
Dual Space Gradient Descent for Online Learning Authors Abstract,"One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.",2016,NIPS,1.0
Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables Authors Abstract,"We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.",2016,NIPS,1.0
Large Margin Discriminant Dimensionality Reduction in Prediction Space Authors Abstract,"In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.",2016,NIPS,0.6000000000000001
WHUNlp at SemEval-2016 Task DiMSUM: A Pilot Study in Detecting Minimal Semantic Units and their Meanings using Supervised Models,"This paper describes our approach towards the SemEval-2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiMSUM). We consider that the two problems are similar to multiword expression detection and supersense tagging, respectively. The former problem is formalized as a sequence labeling problem solved by first-order CRFs, and the latter one is formalized as a classification problem solved by Maximum Entropy Algorithm. To carry out our pilot study quickly, we extract some simple features such as words or part-of-speech tags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored.",2016,SemEval,0.8
When Hyperparameters Help: Beneficial Parameter Combinations in Distributional Semantic Models,"Distributional semantic models can predict many linguistic phenomena, including word similarity, lexical ambiguity, and semantic priming, or even to pass TOEFL synonymy and analogy tests (Landauer and Dumais, 1997; Griffiths et al., 2007; Turney and Pantel, 2010). But what does it take to create a competitive distributional model? Levy et al. (2015) argue that the key to success lies in hyperparameter tuning rather than in the model’s architecture. More hyperparameters trivially lead to potential performance gains, but what do they actually do to improve the models? Are individual hyperparameters’ contributions independent of each other? Or are only specific parameter combinations beneficial? To answer these questions, we perform a quantitative and qualitative evaluation of major hyperparameters as identified in previous research.",2016,SemEval,0.0
CUFE at SemEval-2016 Task 4: A Gated Recurrent Model for Sentiment Classification,In this paper we describe a deep learning system that has been built for SemEval 2016 Task4 (Subtask A and B). In this work we trained a Gated Recurrent Unit (GRU) neural network model on top of two sets of word embeddings: (a) general word embeddings generated from unsupervised neural language model; and (b) task specific word embeddings generated from supervised neural language model that was trained to classify tweets into positive and negative categories. We also added a method for analyzing and splitting multi-words hashtags and appending them to the tweet body before feeding it to our model. Our models achieved 0.58 F1-measure for Subtask A (ranked 12/34) and 0.679 Recall for Subtask B (ranked 12/19).,2016,SemEval,1.0
Generating and Scoring Correction Candidates in Chinese Grammatical Error Diagnosis,"Grammatical error diagnosis is an essential part in a language-learning tutoring system. Based on the data sets of Chinese grammar error detection tasks, we proposed a system which measures the likelihood of correction candidates generated by deleting or inserting characters or words, moving substrings to different positions, substituting prepositions with other prepositions, or substituting words with their synonyms or similar strings. Sentence likelihood is measured based on the frequencies of substrings from the space-removed version of Google n-grams. The evaluation on the training set shows that Missing-related and Selection-related candidate generation methods have promising performance. Our final system achieved a precision of 30.28% and a recall of 62.85% in the identification level evaluated on the test set.",2016,WS,1.0
Character-Aware Neural Networks for Arabic Named Entity Recognition for Social Media,"Named Entity Recognition (NER) is the task of classifying or labelling atomic elements in the text into categories such as Person, Location or Organisation. For Arabic language, recognizing named entities is a challenging task because of the complexity and the unique characteristics of this language. In addition, most of the previous work focuses on Modern Standard Arabic (MSA), however, recognizing named entities in social media is becoming more interesting these days. Dialectal Arabic (DA) and MSA are both used in social media, which is deemed as another challenging task. Most state-of-the-art Arabic NER systems count heavily on handcrafted engineering features and lexicons which is time consuming. In this paper, we introduce a novel neural network architecture which benefits both from character- and word-level representations automatically, by using combination of bidirectional LSTM and Conditional Random Field (CRF), eliminating the need for most feature engineering. Moreover, our model relies on unsupervised word representations learned from unannotated corpora. Experimental results demonstrate that our model achieves state-of-the-art performance on publicly available benchmark for Arabic NER for social media and surpassing the previous system by a large margin.",2016,WS,0.9
Problems With Evaluation of Word Embeddings Using Word Similarity Tasks,"Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy for intrinsic evaluation of word vectors. Word similarity evaluation, which correlates the distance between vectors and human judgments of “semantic similarity” is attractive, because it is computationally inexpensive and fast. In this paper we present several problems associated with the evaluation of word vectors on word similarity datasets, and summarize existing solutions. Our study suggests that the use of word similarity tasks for evaluation of word vectors is not sustainable and calls for further research on evaluation methods.",2016,WS,-0.9
Error analysis for anaphora resolution in Russian: new challenging issues for anaphora resolution task in a morphologically rich language,"This paper presents a quantitative and qualitative error analysis of Russian anaphora resolvers which participated in the RU-EVAL event. Its aim is to identify and characterize a set of challenging errors common to state-of-the-art systems dealing with Russian. We examined three types of pronouns: 3rd person pronouns, reflexive and relative pronouns. The investigation has shown that a high level of grammatical ambiguity, specific features of reflexive pronouns, free word order and special cases of non-referential pronouns in Russian impact the quality of anaphora resolution systems. Error analysis reveals some specific features of anaphora resolution for morphologically rich and free word order languages with a lack of gold standard resources.",2016,WS,-0.6000000000000001
Private or Corporate? Predicting User Types on Twitter,"In this paper we present a series of experiments on discriminating between private and corporate accounts on Twitter. We define features based on Twitter metadata, morphosyntactic tags and surface forms, showing that the simple bag-of-words model achieves single best results that can, however, be improved by building a weighted soft ensemble of classifiers based on each feature type. Investigating the time and language dependence of each feature type delivers quite unexpecting results showing that features based on metadata are neither time- nor language-insensitive as the way the two user groups use the social network varies heavily through time and space.",2016,WS,-0.1
Can We Make Computers Laugh at Talks?,"Considering the importance of public speech skills, a system which makes a prediction on where audiences laugh in a talk can be helpful to a person who prepares for a talk. We investigated a possibility that a state-of-the-art humor recognition system can be used in detecting sentences inducing laughters in talks. In this study, we used TED talks and laughters in the talks as data. Our results showed that the state-of-the-art system needs to be improved in order to be used in a practical application. In addition, our analysis showed that classifying humorous sentences in talks is very challenging due to close distance between humorous and non-humorous sentences.",2016,WS,-0.6000000000000001
Intrinsic Evaluations of Word Embeddings: What Can We Do Better?,"This paper presents an analysis of existing methods for the intrinsic evaluation of word embeddings. We show that the main methodological premise of such evaluations is “interpretability” of word embeddings: a “good” embedding produces results that make sense in terms of traditional linguistic categories. This approach is not only of limited practical use, but also fails to do justice to the strengths of distributional meaning representations. We argue for a shift from abstract ratings of word embedding “quality” to exploration of their strengths and weaknesses.",2016,WS,-1.0
Find the word that does not belong:A Framework for an Intrinsic Evaluation of Word Vector Representations,"We present a new framework for an intrinsic evaluation of word vector representations based on the outlier detection task. This task is intended to test the capability of vector space models to create se- mantic clusters in the space. We carried out a pilot study building a gold standard dataset and the results revealed two important features: human performance on the task is extremely high compared to the standard word similarity task, and state-of-the-art word embedding models, whose current shortcomings were highlighted as part of the evaluation, still have considerable room for improvement.",2016,WS,-0.4
Global Pre-ordering for Improving Sublanguage Translation,"When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations. This paper proposes a novel global reordering method with particular focus on long-distance reordering for capturing the global sentence structure of a sublanguage. The proposed method learns global reordering models from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations.",2016,WS,1.0
Improve Sentiment Analysis of Citations with Author Modelling,"In this paper, we introduce a novel approach to sentiment polarity classification of citations, which integrates data about the authors’ reputation. More specifically, our method extends the h-index with citation polarities and utilizes it in sentiment classification of citation sen- tences. Our computational results show that our method yields significant improvement in terms of classification performance.",2016,WS,1.0
Is Sentiment in Movies the Same as Sentiment in Psychotherapy? Comparisons Using a New Psychotherapy Sentiment Database,"The sharing of emotional material is central to the process of psychotherapy and emotional problems are a primary reason for seeking treatment. Surprisingly, very little systematic research has been done on patterns of emo- tional exchange during psychotherapy. It is likely that a major reason for this void in the research is the enormous cost of annotating sessions for affective content. In the field of NLP, there have been major strides in the cre- ation of algorithms for sentiment analysis, but most of this work has focused on written reviews of movies and twitter feeds with lit- tle work on spoken dialogue. We have cre- ated a new database of 97,497 utterances from psychotherapy transcripts labeled by humans for sentiment. We describe this dataset and present initial results for models identifying sentiment. We also show that one of the best models from the literature, trained on movie reviews, performed below many of our base- line models that trained on the psychotherapy corpus.",2016,WS,-0.2
Vectors or Graphs? On Differences of Representations for Distributional Semantic Models,"Distributional Semantic Models (DSMs) have recently received increased attention, together with the rise of neural architectures for scalable training of dense vector embeddings. While some of the literature even includes terms like ’vectors’ and ’dimensionality’ in the definition of DSMs, there are some good reasons why we should consider alternative formulations of distributional models. As an instance, I present a scalable graph-based solution to distributional semantics. The model belongs to the family of ’count-based’ DSMs, keeps its representation sparse and explicit, and thus fully interpretable. I will highlight some important differences between sparse graph-based and dense vector approaches to DSMs: while dense vector-based models are computationally easier to handle and provide a nice uniform representation that can be compared and combined in many ways, they lack interpretability, provenance and robustness. On the other hand, graph-based sparse models have a more straightforward interpretation, handle sense distinctions more naturally and can straightforwardly be linked to knowledge bases, while lacking the ability to compare arbitrary lexical units and a compositionality operation. Since both representations have their merits, I opt for exploring their combination in the outlook.",2016,WS,-0.4
Obvious Strategyproofness Needs Monitoring for Good Approximations,"Obvious strategyproofness (OSP) is an appealing concept as it allows to maintain incentive compatibility even in the presence of agents that are not fully rational, e.g., those who struggle with contingent reasoning (Li 2015). However, it has been shown to impose some limitations, e.g., no OSP mechanism can return a stable matching (Ashlagi and Gonczarowski",2017,AAAI,-1.0
On Pareto Optimality in Social Distance Games,"We investigate Pareto stability in Social Distance Games, that are coalition forming games in which agents utilities are proportional to their harmonic centralities in the respective coalitions, i.e., to the average inverse distance from the other agents. Pareto optimal solutions have been already considered in the literature as outcomes arising from the strategic interaction of the agents. In particular, they are stable under the deviation of the grand coalition, as they do not permit a simultaneous deviation by all the agents making all of them weakly better off and some strictly better off. We first show that, while computing a Pareto stable solution maximizing the social welfare is NP-hard in bounded degree graphs, a 2min{Œî,‚àön}-approximating one can be determined in polynomial time, where n is the number of agents and Œî the maximum node degree. We then determine asymptotically tight bounds on the Price of Pareto Optimality for several classes of social graphs arising from the following combinations: unbounded and bounded node degree, undirected and directed edges, unweighted and weighted edges.",2017,AAAI,0.0
The Benefit in Free Information Disclosure When Selling Information to People,"This paper studies the benefit for information providers in free public information disclosure in settings where the prospective information buyers are people. The underlying model, which applies to numerous real-life situations, considers a standard decision making setting where the decision maker is uncertain about the outcomes of her decision. The information provider can fully disambiguate this uncertainty and wish to maximize her profit from selling such information. We use a series of AMT-based experiments with people to test the benefit for the information provider from reducing some of the uncertainty associated with the decision maker‚Äôs problem, for free. Free information disclosure of this kind can be proved to be ineffective when the buyer is a fullyrational agent. Yet, when it comes to people we manage to demonstrate that a substantial improvement in the information provider‚Äôs profit can be achieved with such an approach. The analysis of the results reveals that the primary reason for this phenomena is people‚Äôs failure to consider the strategic nature of the interaction with the information provider. Peoples‚Äô inability to properly calculate the value of information is found to be secondary in its influence.",2017,AAAI,0.0
A Dependency-Based Neural Reordering Model for Statistical Machine Translation,"In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-theart statistical MT system that uses sparse dependency-based reordering features.",2017,AAAI,1.0
Web-Based Semantic Fragment Discovery for On-Line Lingual-Visual Similarity,"In this paper, we present an automatic approach for on-line discovery of visual-lingual semantic fragments from weakly labeled Internet images. Instead of learning region-entity correspondences from well-labeled image-sentence pairs, our approach directly collects and enhances the weakly labeled visual contents from the Web and constructs an adaptive visual representation which automatically links generic lingual phrases to their related visual contents. To ensure reliable and efficient semantic discovery, we adopt non-parametric density estimation to re-rank the related visual instances and proposed a fast self-similarity-based quality assessment method to identify the high-quality semantic fragments. The discovered semantic fragments provide an adaptive joint representation for texts and images, based on which lingual-visual similarity can be defined for further co-analysis of heterogeneous multimedia data. Experimental results on semantic fragment quality assessment, sentence-based image retrieval, automatic multimedia insertion and ordering demonstrated the effectiveness of the proposed framework.The experiments show that the proposed methods can make effective use of the Web knowledge, and are able to generate competitive results compared to state-of-the-art approaches in various tasks.",2017,AAAI,1.0
CLARE: A Joint Approach to Label Classification and Tag Recommendation,"Data classification and tag recommendation are both important and challenging tasks in social media. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, labels in data classification and tags in tag recommendation are inherently related. For example, a Youtube video annotated with NCAA, stadium, pac12 is likely to be labeled as football, while a video/image with the class label of coast is likely to be tagged with beach, sea, water and sand. The existence of relations between labels and tags motivates us to jointly perform classification and tag recommendation for social media data in this paper. In particular, we provide a principled way to capture the relations between labels and tags, and propose a novel framework CLARE, which fuses data CLAssification and tag REcommendation into a coherent model. With experiments on three social media datasets, we demonstrate that the proposed framework CLARE achieves superior performance on both tasks compared to the state-of-the-art methods.",2017,AAAI,1.0
A Generic Bet-and-Run Strategy for Speeding Up Stochastic Local Search,"A common strategy for improving optimization algorithms is to restart the algorithm when it is believed to be trapped in an inferior part of the search space. However, while specific restart strategies have been developed for specific problems (and specific algorithms), restarts are typically not regarded as a general tool to speed up an optimization algorithm. In fact, many optimization algorithms do not employ restarts at",2017,AAAI,0.0
SnapNETS: Automatic Segmentation of Network Sequences with Node Labels,"Given a sequence of snapshots of flu propagating over a population network, can we find a segmentation when the patterns of the disease spread change, possibly due to interventions? In this paper, we study the problem of segmenting graph sequences with labeled nodes. Memes on the Twitter network, diseases over a contact network, movie-cascades over a social network, etc. are all graph sequences with labeled nodes. Most related work is on plain graphs (and hence ignore the label dynamics) or fix parameters or require much feature engineering. Instead, we propose SNAPNETS, to automatically find segmentations of such graph sequences, with different characteristics of nodes of each label in adjacent segments. It satisfies all the desired properties (being parameter-free, comprehensive and scalable) by leveraging a principled, multilevel, flexible framework which maps the problem to a path optimization problem over a weighted DAG. Extensive experiments on several diverse real datasets show that it finds cut points matching ground-truth or meaningful external signals outperforming non-trivial baselines. We also show that SNAPNETS scales near-linearly with the size of the input.",2017,AAAI,0.8
Resource Graph Games: A Compact Representation for Games with Structured Strategy Spaces,"In many real-world systems, strategic agents‚Äô decisions can be understood as complex‚Äîi.e., consisting of multiple subdecisions‚Äîand hence can give rise to an exponential number of pure strategies. Examples include network congestion games, simultaneous auctions, and security games. However, agents‚Äô sets of strategies are often structured, allowing them to be represented compactly. There currently exists no general modeling language that captures a wide range of commonly seen strategy structure and utility structure. We propose Resource Graph Games (RGGs), the first general compact representation for games with structured strategy spaces, which is able to represent a wide range of games studied in literature. We leverage recent results about multilinearity, a key property of games that allows us to represent the mixed strategies compactly, and, as a result, to compute various equilibrium concepts efficiently. While not all RGGs are multilinear, we provide a general method of converting RGGs to those that are multilinear, and identify subclasses of RGGs whose converted version allow efficient computation.",2017,AAAI,0.8
Bandit Structured Prediction for Neural Sequence-to-Sequence Learning,"Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.",2017,ACL,0.2
Neural Belief Tracker: Data-Driven Dialogue State Tracking,"One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",2017,ACL,1.0
Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,"Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.",2017,ACL,1.0
Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,"Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.",2017,ACL,1.0
FOIL it! Find One mismatch between Image and Language caption,"In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and 'foil' captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake ('foil word'). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image.",2017,ACL,-1.0
Learning to Parse and Translate Improves Neural Machine Translation,"There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.",2017,ACL,1.0
From Characters to Words to in Between: Do We Capture Morphology?,"Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data.",2017,ACL,0.0
A Corpus of Natural Language for Visual Reasoning,"We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",2017,ACL,1.0
The State of the Art in Semantic Representation,"Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",2017,ACL,-0.7000000000000001
A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes,"We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.",2017,ACL,1.0
An Analysis of Action Recognition Datasets for Language and Vision Tasks,"A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods. One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval. In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages. We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.",2017,ACL,0.0
A Fast and Lightweight System for Multilingual Dependency Parsing,"We present a multilingual dependency parser with a bidirectional-LSTM (BiLSTM) feature extractor and a multi-layer perceptron (MLP) classifier. We trained our transition-based projective parser in UD version 2.0 datasets without any additional data. The parser is fast, lightweight and effective on big treebanks. In the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, the official results show that the macro-averaged LAS F1 score of our system Mengest is 61.33%.",2017,CoNLL,1.0
The HIT-SCIR System for End-to-End Parsing of Universal Dependencies,"This paper describes our system (HIT-SCIR) for the CoNLL 2017 shared task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system includes three pipelined components: tokenization, Part-of-Speech (POS) tagging and dependency parsing. We use character-based bidirectional long short-term memory (LSTM) networks for both tokenization and POS tagging. Afterwards, we employ a list-based transition-based algorithm for general non-projective parsing and present an improved Stack-LSTM-based architecture for representing each transition state and making predictions. Furthermore, to parse low/zero-resource languages and cross-domain data, we use a model transfer approach to make effective use of existing resources. We demonstrate substantial gains against the UDPipe baseline, with an average improvement of 3.76% in LAS of all languages. And finally, we rank the 4th place on the official test sets.",2017,CoNLL,1.0
A System for Multilingual Dependency Parsing based on Bidirectional LSTM Feature Representations,"In this paper, we present our multilingual dependency parser developed for the CoNLL 2017 UD Shared Task dealing with ""Multilingual Parsing from Raw Text to Universal Dependencies"". Our parser extends the monolingual BIST-parser as a multi-source multilingual trainable parser. Thanks to multilingual word embeddings and one hot encodings for languages, our system can use both monolingual and multi-source training. We trained 69 monolingual language models and 13 multilingual models for the shared task. Our multilingual approach making use of different resources yield better results than the monolingual approach for 11 languages. Our system ranked 5 th and achieved 70.93 overall LAS score over the 81 test corpora (macro-averaged LAS F1 score).",2017,CoNLL,1.0
Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts,"This paper presents a novel approach to character identification, that is an entity linking task that maps mentions to characters in dialogues from TV show transcripts. We first augment and correct several cases of annotation errors in an existing corpus so the corpus is clearer and cleaner for statistical learning. We also introduce the agglomerative convolutional neural network that takes groups of features and learns mention and mention-pair embeddings for coreference resolution. We then propose another neural model that employs the embeddings learned and creates cluster embeddings for entity linking. Our coreference resolution model shows comparable results to other state-of-the-art systems. Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.",2017,CoNLL,1.0
Instances and concepts in distributional space,"Instances (“Mozart”) are ontologically distinct from concepts or classes (“composer”). Natural language encompasses both, but instances have received comparatively little attention in distributional semantics. Our results show that instances and concepts differ in their distributional properties. We also establish that instantiation detection (“Mozart – composer”) is generally easier than hypernymy detection (“chemist – scientist”), and that results on the influence of input representation do not transfer from hyponymy to instantiation.",2017,EACL,-0.30000000000000004
Identifying beneficial task relations for multi-task learning in deep neural networks,"Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.",2017,EACL,-0.30000000000000004
A Question Answering Approach for Emotion Cause Extraction,"Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.",2017,EMNLP,1.0
"A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances","A taxonomy is a semantic hierarchy, consisting of concepts linked by is-a relations. While a large number of taxonomies have been constructed from human-compiled resources (e.g., Wikipedia), learning taxonomies from text corpora has received a growing interest and is essential for long-tailed and domain-specific knowledge acquisition. In this paper, we overview recent advances on taxonomy construction from free texts, reorganizing relevant subtasks into a complete framework. We also overview resources for evaluation and discuss challenges for future research.",2017,EMNLP,0.0
Adversarial Examples for Evaluating Reading Comprehension Systems,"Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.",2017,EMNLP,-1.0
Challenges in Data-to-Document Generation,"Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",2017,EMNLP,-0.9
Semi-Supervised Classification with Graph Convolutional Networks,We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.,2017,ICLR,1.0
LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,"We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",2017,ICLR,0.5
Do Deep Convolutional Nets Really Need to be Deep and Convolutional?,"Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.",2017,ICLR,-0.5
Temporal Ensembling for Semi-Supervised Learning,"In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.",2017,ICLR,1.0
Learning to Navigate in Complex Environments,"Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.",2017,ICLR,0.30000000000000004
A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",2017,ICLR,1.0
Tree-structured decoding with doubly-recurrent neural networks,"We propose a neural network architecture for generating tree-structured objects from encoded representations. The core of the method is a doubly recurrent neural network model comprised of separate width and depth recurrences that are combined inside each cell (node) to generate an output. The topology of the tree is modeled explicitly together with the content. That is, in response to an encoded vector representation, co-evolving recurrences are used to realize the associated tree and the labels for the nodes in the tree. We test this architecture in an encoderdecoder framework, where we train a network to encode a sentence as a vector, and then generate a tree structure from it. The experimental results show the effectiveness of this architecture at recovering latent tree structure in sequences and at mapping sentences to simple functional programs.",2017,ICLR,0.4
Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,"In this paper, we propose a new framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom. Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model [Mnih et al. (2016)]) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents‚Äô information [Lample & Chaplot (2016)]. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35% higher score than the second place.",2017,ICLR,0.7000000000000001
Understanding deep learning requires rethinking generalization,"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.",2017,ICLR,0.0
PixelVAE: A Latent Variable Model for Natural Images,"Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64 √ó 64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.",2017,ICLR,1.0
On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,"The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32‚Äì512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions‚Äîand as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.",2017,ICLR,-0.5
Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data,"We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",2017,ICLR,1.0
Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks,"Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity. The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fullyconnected networks can be reduced by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN). We then propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks. The proposed architecture can save up to 90% of memory compared to the conventional implementations of fully-connected neural networks. Moreover, implementation results show up to 84% reduction in the energy consumption of a single neuron of the proposed sparsely-connected networks compared to a single neuron of fully-connected neural networks.",2017,ICLR,0.9
Mode Regularized Generative Adversarial Networks,"Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",2017,ICLR,1.0
Failures of Gradient-Based Deep Learning,"In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradientbased algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.",2017,ICML,-1.0
Efficient softmax approximation for GPUs,"We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/ facebookresearch/adaptive-softmax.",2017,ICML,0.8
Sharp Minima Can Generalize For Deep Nets,"Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.",2017,ICML,-0.8
A Closer Look at Memorization in Deep Networks,"We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",2017,ICML,0.2
Learning in POMDPs with Monte Carlo Tree Search,"The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",2017,ICML,0.9
Automated Curriculum Learning for Neural Networks,"We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",2017,ICML,0.8
ChoiceRank: Identifying Preferences from Node Traffic in Networks,"Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce‚Äôs axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n) transition probabilities. We show how to make the inference problem well-posed regardless of the network‚Äôs structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (nodelevel) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City‚Äôs bicycle-sharing system.",2017,ICML,1.0
Lost Relatives of the Gumbel Trick,"The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.",2017,ICML,1.0
Modular Multitask Reinforcement Learning with Policy Sketches,"We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them‚Äîspecifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor‚Äìcritic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",2017,ICML,1.0
Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning,"Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algorithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding BRP will asymptotically prune any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that BRP results in a factor of 7 reduction in space, and the reduction factor increases with game size.",2017,ICML,1.0
On Calibration of Modern Neural Networks,"Confidence calibration ‚Äì the problem of predicting probability estimates representative of the true correctness likelihood ‚Äì is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling ‚Äì a singleparameter variant of Platt Scaling ‚Äì is surprisingly effective at calibrating predictions.",2017,ICML,0.30000000000000004
Variational Inference for Sparse and Undirected Models,"Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.",2017,ICML,0.0
Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.",2017,ICML,1.0
The Price of Differential Privacy for Online Learning,"In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user‚Äôs data is present as opposed to when it is absent in a dataset.",2017,ICML,0.0
Prediction and Control with Temporal Segment Models,"We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.",2017,ICML,1.0
Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter,"Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue ‚àíœÉ of the Hessian. This parameter œÉ captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization. At least in theory, our methods outperform known results for a range of parameter œÉ, and can also be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold œÉ0 so that the (currently) fastest methods for œÉ > œÉ0 and for œÉ < œÉ0 have different behaviors: the former scales with n and the latter scales with n.",2017,ICML,0.5
meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting,"We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1‚Äì4% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.",2017,ICML,1.0
Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",2017,ICML,1.0
Oracle Complexity of Second-Order Methods for Finite-Sum Problems,"Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer ‚Äì perhaps surprisingly ‚Äì is negative, at least in terms of worst-case guarantees. We also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.",2017,ICML,-0.5
Dynamic Word Embeddings,"We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms‚Äîskipgram smoothing and skip-gram filtering‚Äîthat allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.",2017,ICML,1.0
Consistency Analysis for Binary Classification Revisited,"Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics. The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics.",2017,ICML,0.0
Learning to Align the Source Code to the Compiled Object Code,"We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code. Our architecture learns the alignment between the two sequences ‚Äì one being the translation of the other ‚Äì by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains. Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.",2017,ICML,0.9
Understanding Synthetic Gradients and Decoupled Neural Interfaces,"When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking ‚Äì without waiting for a true error gradient to be backpropagated ‚Äì resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.",2017,ICML,0.2
A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization,"Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization.",2017,ICML,0.4
Substring Frequency Features for Segmentation of Japanese Katakana Words with Unlabeled Corpora,"Word segmentation is crucial in natural language processing tasks for unsegmented languages. In Japanese, many out-of-vocabulary words appear in the phonetic syllabary katakana, making segmentation more difficult due to the lack of clues found in mixed script settings. In this paper, we propose a straightforward approach based on a variant of tf-idf and apply it to the problem of word segmentation in Japanese. Even though our method uses only an unlabeled corpus, experimental results show that it achieves performance comparable to existing methods that use manually labeled corpora. Furthermore, it improves performance of simple word segmentation models trained on a manually labeled corpus.",2017,IJCNLP,0.8
What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?,"There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.",2017,NIPS,1.0
Structured Bayesian Pruning via Log-Normal Multiplicative Noise Authors Abstract,"Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.",2017,NIPS,0.8
SchNet: A continuous-filter convolutional neural network for modeling quantum interactions Authors Abstract,"Modern classification tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classifiers, which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time. We present an active classification process at the test time, where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost. The expected classification gain is computed using a probabilistic model that uses the outcome from previous observations. This active classification process is applied at test time for each individual test instance, resulting in an efficient instance-specific decision path. We demonstrate the benefit of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.",2017,NIPS,1.0
Associative Embedding: End-to-End Learning for Joint Detection and Grouping Authors Abstract,"We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets.",2017,NIPS,1.0
Nonbacktracking Bounds on the Influence in Independent Cascade Models Authors Abstract,"This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/‚úè2)). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/‚úè)). Next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/‚úè)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/‚úè), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.",2017,NIPS,-0.30000000000000004
Learning Affinity via Spatial Propagation Networks Authors Abstract,"In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be outputs from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, such as image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of deep CNNs. We validate the framework on the task of refinement of image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.",2017,NIPS,1.0
InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations Authors Abstract,"The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",2017,NIPS,1.0
Conservative Contextual Linear Bandits Authors Abstract,"Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.",2017,NIPS,1.0
Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations Authors Abstract,"A powerful approach for understanding neural population dynamics is to extract low-dimensional trajectories from population recordings using dimensionality reduction methods. Current approaches for dimensionality reduction on neural data are limited to single population recordings, and can not identify dynamics embedded across multiple measurements. We propose an approach for extracting low-dimensional dynamics from multiple, sequential recordings. Our algorithm scales to data comprising millions of observed dimensions, making it possible to access dynamics distributed across large populations or multiple brain areas. Building on subspace-identification approaches for dynamical systems, we perform parameter estimation by minimizing a moment-matching objective using a scalable stochastic gradient descent algorithm: The model is optimized to predict temporal covariations across neurons and across time. We show how this approach naturally handles missing data and multiple partial recordings, and can identify dynamics and predict correlations even in the presence of severe subsampling and small overlap between recordings. We demonstrate the effectiveness of the approach both on simulated data and a whole-brain larval zebrafish imaging dataset.",2017,NIPS,1.0
The Marginal Value of Adaptive Gradient Methods in Machine Learning Authors Abstract,"Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several stateof-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.",2017,NIPS,-1.0
Acceleration and Averaging in Stochastic Descent Dynamics Authors Abstract,"We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.",2017,NIPS,0.5
Deep Voice 2: Multi-Speaker Neural Text-to-Speech Authors Abstract,"We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.",2017,NIPS,1.0
Learning Efficient Object Detection Models with Knowledge Distillation Authors Abstract,"We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known ‚Äúexperts‚Äù setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds.",2017,NIPS,0.30000000000000004
Learned D-AMP: Principled Neural Network based Compressive Image Recovery Authors Abstract,"Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix. It was recently demonstrated that iterative sparse-signal-recovery algorithms can be ‚Äúunrolled‚Äù to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network Learned D-AMP (LDAMP). The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over 50√ó faster than BM3D-AMP and hundreds of times faster than NLR-CS.",2017,NIPS,1.0
Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets Authors Abstract,"In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.",2017,NIPS,1.0
Reconstructing perceived faces from brain activations with deep adversarial neural decoding Authors Abstract,"Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations. ConvNet (pretrained) + PCA ConvNet (adversarial training) la te nt fe at . prior (Gaussian) maximum a posteriori likelihood (Gaussian) posterior (Gaussian) pe rc ei ve d st im . br ai n re sp . *reconstruction *from brain resp. Figure 1: An illustration of our approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning.",2017,NIPS,1.0
Fast amortized inference of neural activity from calcium imaging data with variational autoencoders Authors Abstract,"We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Numerical evidence supports the effectiveness of our method in real-world problems.",2017,NIPS,1.0
Label Efficient Learning of Transferable Representations acrosss Domains and Tasks Authors Abstract,"We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.",2017,NIPS,1.0
How regularization affects the critical points in linear networks Authors Abstract,"A fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans. Here we adopt the hypothesis that when we are in an interactive social setting, our brains perform Bayesian inference of the intentions and cooperativeness of others using probabilistic representations. We employ the framework of partially observable Markov decision processes (POMDPs) to model human decision making in a social context, focusing specifically on the volunteer‚Äôs dilemma in a version of the classic Public Goods Game. We show that the POMDP model explains both the behavior of subjects as well as neural activity recorded using fMRI during the game. The decisions of subjects can be modeled across all trials using two interpretable parameters. Furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions. Our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization.",2017,NIPS,0.0
Overcoming Catastrophic Forgetting by Incremental Moment Matching Authors Abstract,"Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSDBirds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.",2017,NIPS,1.0
Collecting Telemetry Data Privately Authors Abstract,"The collection and analysis of telemetry data from user‚Äôs devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users‚Äô privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.",2017,NIPS,0.5
Deep Sets Authors Abstract,"We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.",2017,NIPS,0.0
Diving into the shallows: a computational perspective on large-scale shallow learning Authors Abstract,"Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow architecture. In this paper we identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data. To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a significant performance boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.",2017,NIPS,1.0
Deliberation Networks: Sequence Generation Beyond One-Pass Decoding Authors Abstract,"The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human‚Äôs daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.",2017,NIPS,1.0
Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation Authors Abstract,"The sparse matrix estimation problem consists of estimating the distribution of an n√ó n matrix Y , from a sparsely observed single instance of this matrix where the entries of Y are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filteringstyle algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to 0 at the rate of O(d2(pn)‚àí2/5) as long as œâ(dn) random entries from a total of n entries of Y are observed (uniformly sampled), E[Y ] has rank d, and the entries of Y have bounded support. The maximum squared error across all entries converges to 0 with high probability as long as we observe a little more, Œ©(dn ln(n)) entries. Our results are the best known sample complexity results in this generality.",2017,NIPS,1.0
Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes Authors Abstract,"Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.",2017,NIPS,1.0
YNU-HPCC at SemEval 2017 Task 4: Using A Multi-Channel CNN-LSTM Model for Sentiment Classification,"In this paper, we propose a multi-channel convolutional neural network-long short-term memory (CNN-LSTM) model that consists of two parts: multi-channel CNN and LSTM to analyze the sentiments of short English messages from Twitter. Un-like a conventional CNN, the proposed model applies a multi-channel strategy that uses several filters of different length to extract active local n-gram features in different scales. This information is then sequentially composed using LSTM. By combining both CNN and LSTM, we can consider both local information within tweets and long-distance dependency across tweets in the classification process. Officially released results show that our system outperforms the baseline algo-rithm.",2017,SemEval,1.0
FA3L at SemEval-2017 Task 3: A ThRee Embeddings Recurrent Neural Network for Question Answering,"In this paper we present ThReeNN, a model for Community Question Answering, Task 3, of SemEval-2017. The proposed model exploits both syntactic and semantic information to build a single and meaningful embedding space. Using a dependency parser in combination with word embeddings, the model creates sequences of inputs for a Recurrent Neural Network, which are then used for the ranking purposes of the Task. The score obtained on the official test data shows promising results.",2017,SemEval,1.0
YNUDLG at SemEval-2017 Task 4: A GRU-SVM Model for Sentiment Classification and Quantification in Twitter,"Sentiment analysis is one of the central issues in Natural Language Processing and has become more and more important in many fields. Typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.,positive or negative). In this paper we describe our deep learning system(combining GRU and SVM) to solve both two-, three- and five-tweet polarity classifications. We first trained a gated recurrent neural network using pre-trained word embeddings, then we extracted features from GRU layer and input these features into support vector machine to fulfill both the classification and quantification subtasks. The proposed approach achieved 37th, 19th, and 14rd places in subtasks A, B and C, respectively.",2017,SemEval,1.0
Ways of Asking and Replying in Duplicate Question Detection,"This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios.",2017,SemEval,0.0
SemEval-2017 Task 11: End-User Development using Natural Language,"This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. The task aims for supporting the evaluation and evolution of the discussions surrounding the natural language processing approaches within the context of end-user natural language programming, under scenarios of high semantic heterogeneity/gap.",2017,SemEval,1.0
DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison,"In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ''#HashtagWars: Learning a Sense of Humor"". We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on #HashtagWars dataset.",2017,SemEval,1.0
SwissAlps at SemEval-2017 Task 3: Attention-based Convolutional Neural Network for Community Question Answering,"In this paper we propose a system for reranking answers for a given question. Our method builds on a siamese CNN architecture which is extended by two attention mechanisms. The approach was evaluated on the datasets of the SemEval-2017 competition for Community Question Answering (cQA), where it achieved 7th place obtaining a MAP score of 86:24 points on the Question-Comment Similarity subtask.",2017,SemEval,1.0
Comparing Machine Translation and Human Translation: A Case Study,"As machine translation technology improves comparisons to human performance are often made in quite general and exaggerated terms. Thus, it is important to be able to account for differences accurately. This paper reports a simple, descriptive scheme for comparing translations and applies it to two translations of a British opinion article published in March, 2017. One is a human translation (HT) into Swedish, and the other a machine translation (MT). While the comparison is limited to one text, the results are indicative of current limitations in MT.",2017,WS,-0.5
Identification of Risk Factors in Clinical Texts through Association Rules,"We describe a method which extracts Association Rules from texts in order to recognise verbalisations of risk factors. Usually some basic vocabulary about risk factors is known but medical conditions are expressed in clinical narratives with much higher variety. We propose an approach for data-driven learning of specialised medical vocabulary which, once collected, enables early alerting of potentially affected patients. The method is illustrated by experimens with clinical records of patients with Chronic Obstructive Pulmonary Disease (COPD) and comorbidity of CORD, Diabetes Melitus and Schizophrenia. Our input data come from the Bulgarian Diabetic Register, which is built using a pseudonymised collection of outpatient records for about 500,000 diabetic patients. The generated Association Rules for CORD are analysed in the context of demographic, gender, and age information. Valuable anounts of meaningful words, signalling risk factors, are discovered with high precision and confidence.",2017,WS,1.0
Telling Apart Tweets Associated with Controversial versus Non-Controversial Topics,"In this paper, we evaluate the predictability of tweets associated with controversial versus non-controversial topics. As a first step, we crowd-sourced the scoring of a predefined set of topics on a Likert scale from non-controversial to controversial. Our feature set entails and goes beyond sentiment features, e.g., by leveraging empathic language and other features that have been previously used but are new for this particular study. We find focusing on the structural characteristics of tweets to be beneficial for this task. Using a combination of emphatic, language-specific, and Twitter-specific features for supervised learning resulted in 87% accuracy (F1) for cross-validation of the training set and 63.4% accuracy when using the test set. Our analysis shows that features specific to Twitter or social media, in general, are more prevalent in tweets on controversial topics than in non-controversial ones. To test the premise of the paper, we conducted two additional sets of experiments, which led to mixed results. This finding will inform our future investigations into the relationship between language use on social media and the perceived controversiality of topics.",2017,WS,0.1
Training and Evaluating Improved Dependency-Based Word Embeddings,"Word embedding has been widely used in many natural language processing tasks. In this paper, we focus on learning word embeddings through selective higher-order relationships in sentences to improve the embeddings to be less sensitive to local context and more accurate in capturing semantic compositionality. We present a novel multi-order dependency-based strategy to composite and represent the context under several essential constraints. In order to realize selective learning from the word contexts, we automatically assign the strengths of different dependencies between co-occurred words in the stochastic gradient descent process. We evaluate and analyze our proposed approach using several direct and indirect tasks for word embeddings. Experimental results demonstrate that our embeddings are competitive to or better than state-of-the-art methods and significantly outperform other methods in terms of context stability. The output weights and representations of dependencies obtained in our embedding model conform to most of the linguistic characteristics and are valuable for many downstream tasks.",2018,AAAI,1.0
A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction,"We improve automatic correction of grammatical, orthographic, and collocation errors in text using a multilayer convolutional encoder-decoder neural network. The network is initialized with embeddings that make use of character N-gram information to better suit this task. When evaluated on common benchmark test data sets (CoNLL-2014 and JFLEG), our model substantially outperforms all prior neural approaches on this task as well as strong statistical machine translation-based systems with neural and task-specific features trained on the same data. Our analysis shows the superiority of convolutional neural networks over recurrent neural networks such as long short-term memory (LSTM) networks in capturing the local context via attention, and thereby improving the coverage in correcting grammatical errors. By ensembling multiple models, and incorporating an N-gram language model and edit features via rescoring, our novel method becomes the first neural approach to outperform the current state-of-the-art statistical machine translation-based approach, both in terms of grammaticality and fluency.",2018,AAAI,1.0
Improving Neural Fine-Grained Entity Typing With Knowledge Attention,"Fine-grained entity typing aims to identify the semantic type of an entity in a particular plain text. It is an important task which can be helpful for a lot of natural language processing (NLP) applications. Most existing methods typically extract features separately from the entity mention and context words for type classification. These methods inevitably fail to model complex correlations between entity mentions and context words. They also neglect rich background information about these entities in knowledge bases (KBs). To address these issues, we take information from KBs into consideration to bridge entity mentions and their context together, and thereby propose Knowledge-Attention Neural Fine-Grained Entity Typing. Experimental results and case studies on real-world datasets demonstrate that our model significantly outperforms other state-of-the-art methods, revealing the effectiveness of incorporating KB information for entity typing. Code and data for this paper can be found at https://github.com/thunlp/KNET.",2018,AAAI,0.5
Representation Learning for Scale-Free Networks,"Network embedding aims to learn the low-dimensional representations of vertexes in a network, while structure and inherent properties of the network is preserved. Existing network embedding works primarily focus on preserving the microscopic structure, such as the first- and second-order proximity of vertexes, while the macroscopic scale-free property is largely ignored. Scale-free property depicts the fact that vertex degrees follow a heavy-tailed distribution (i.e., only a few vertexes have high degrees) and is a critical property of real-world networks, such as social networks. In this paper, we study the problem of learning representations for scale-free networks. We first theoretically analyze the difficulty of embedding and reconstructing a scale-free network in the Euclidean space, by converting our problem to the sphere packing problem. Then, we propose the ""degree penalty"" principle for designing scale-free property preserving network embedding algorithm: punishing the proximity between high-degree vertexes. We introduce two implementations of our principle by utilizing the spectral techniques and a skip-gram model respectively. Extensive experiments on six datasets show that our algorithms are able to not only reconstruct heavy-tailed distributed degree distribution, but also outperform state-of-the-art embedding models in various network mining tasks, such as vertex classification and link prediction.",2018,AAAI,1.0
Overlap-Robust Decision Boundary Learning for Within-Network Classification,"We study the problem of within network classification, where given a partially labeled network, we infer the labels of the remaining nodes based on the link structure. Conventional loss functions penalize a node based on a function of its predicted label and target label. Such loss functions under-perform while learning on a network having overlapping classes. In relational setting, even though the ground truth is not known for the unlabeled nodes, some evidence is present in the form of labeling acquired by the nodes in their neighborhood. We propose a structural loss function for learning in networks based on the hypothesis that loss is induced when a node fails to acquire a label that is consistent with the labels of the majority of the nodes in its neighborhood. We further combine this with a novel semantic regularizer, which we call homophily regularizer, to capture the smooth transition of discriminatory power and behavior of semantically similar nodes. The proposed structural loss along with the regularizer permits relaxation labeling. Through extensive comparative study on different real-world datasets, we found that our method improves over the state-of-the-art approaches.",2018,AAAI,1.0
SPINE: SParse Interpretable Neural Embeddings,"Prediction without justification has limited utility. Much of the success of neural models can be attributed to their ability to learn rich, dense and expressive representations. While these representations capture the underlying complexity and latent trends in the data, they are far from being interpretable. We propose a novel variant of denoising k-sparse autoencoders that generates highly efficient and interpretable distributed word representations (word embeddings), beginning with existing word representations from state-of-the-art methods like GloVe and word2vec. Through large scale human evaluation, we report that our resulting word embedddings are much more interpretable than the original GloVe and word2vec embeddings. Moreover, our embeddings outperform existing popular word embeddings on a diverse suite of benchmark downstream tasks.",2018,AAAI,1.0
A Voting-Based System for Ethical Decision Making,"We present a general approach to automating ethical decisions, drawing on machine learning and computational social choice. In a nutshell, we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. We provide a concrete algorithm that instantiates our approach; some of its crucial steps are informed by a new theory of swap-dominance efficient voting rules. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million people through the Moral",2018,AAAI,0.8
Fair Inference on Outcomes,"In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are ‚Äúsensitive,‚Äù in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.",2018,AAAI,-0.5
Discovering and Distinguishing Multiple Visual Senses for Polysemous Words,"To reduce the dependence on labeled data, there have been increasing research efforts on learning visual classifiers by exploiting web images. One issue that limits their performance is the problem of polysemy. To solve this problem, in this work, we present a novel framework that solves the problem of polysemy by allowing sense-specific diversity in search results. Specifically, we first discover a list of possible semantic senses to retrieve sense-specific images. Then we merge visual similar semantic senses and prune noises by using the retrieved images. Finally, we train a visual classifier for each selected semantic sense and use the learned sense-specific classifiers to distinguish multiple visual senses. Extensive experiments on classifying images into sense-specific categories and re-ranking search results demonstrate the superiority of our proposed approach.",2018,AAAI,1.0
Generating an Event Timeline About Daily Activities From a Semantic Concept Stream,"Recognizing activities of daily living (ADLs) in the real world is an important task for understanding everyday human life. However, even though our life events consist of chronological ADLs with the corresponding places and objects (e.g., drinking coffee in the living room after making coffee in the kitchen and walking to the living room), most existing works focus on predicting individual activity labels from sensor data. In this paper, we introduce a novel framework that produces an event timeline of ADLs in a home environment. The proposed method combines semantic concepts such as action, object, and place detected by sensors for generating stereotypical event sequences with the following three realworld properties. First, we use temporal interactions among concepts to remove objects and places unrelated to each action. Second, we use commonsense knowledge mined from a language resource to find a possible combination of concepts in the real world. Third, we use temporal variations of events to filter repetitive events, since our daily life changes over time. We use cross-place validation to evaluate our proposed method on a daily-activities dataset with manually labeled event descriptions. The empirical evaluation demonstrates that our method using real-world properties improves the performance of generating an event timeline over diverse",2018,AAAI,1.0
"Learning Nonlinear Dynamics in Efficient, Balanced Spiking Networks Using Local Plasticity Rules","The brain uses spikes in neural circuits to perform many dynamical computations. The computations are performed with properties such as spiking efficiency, i.e. minimal number of spikes, and robustness to noise. A major obstacle for learning computations in artificial spiking neural networks with such desired biological properties is due to lack of our understanding of how biological spiking neural networks learn",2018,AAAI,1.0
Understanding Over Participation in Simple Contests,"One key motivation for using contests in real-life is the substantial evidence reported in empirical contest-design literature for people‚Äôs tendency to act more competitively in contests than predicted by the Nash Equilibrium. This phenomenon has been traditionally explained by people‚Äôs eagerness to win and maximize their relative (rather than absolute) payoffs. In this paper we make use of ‚Äúsimple contests‚Äù, where contestants only need to strategize on whether to participate in the contest or not, as an infrastructure for studying whether indeed more effort is exerted in contests due to competitiveness, or perhaps this can be attributed to other factors that hold also in non-competitive settings. The experimental methodology we use compares contestants‚Äô participation decisions in eight contest settings differing in the nature of the contest used, the number of contestants used and the theoretical participation predictions to those obtained (whenever applicable) by subjects facing equivalent non-competitive decision situations in the form of a lottery. We show that indeed people tend to over-participate in contests compared to the theoretical predictions, yet the same phenomenon holds (to a similar extent) also in the equivalent non-competitive settings. Meaning that many of the contests used nowadays as a means for inducing extra human effort, that are often complex to organize and manage, can be replaced by a simpler non-competitive mechanism that uses probabilistic prizes.",2018,AAAI,0.0
Deep Region Hashing for Generic Instance Search from Images,"Instance Search (INS) is a fundamental problem for many applications, while it is more challenging comparing to traditional image search since the relevancy is defined at the instance level. Existing works have demonstrated the success of many complex ensemble systems that are typically conducted by firstly generating object proposals, and then extracting handcrafted and/or CNN features of each proposal for matching. However, object bounding box proposals and feature extraction are often conducted in two separated steps, thus the effectiveness of these methods collapses. Also, due to the large amount of generated proposals, matching speed becomes the bottleneck that limits its application to largescale datasets. To tackle these issues, in this paper we propose an effective and efficient Deep Region Hashing (DRH) approach for large-scale INS using an image patch as the query. Specifically, DRH is an end-to-end deep neural network which consists of object proposal, feature extraction, and hash code generation. DRH shares full-image convolutional feature map with the region proposal network, thus enabling nearly cost-free region proposals. Also, each highdimensional, real-valued region features are mapped onto a low-dimensional, compact binary codes for the efficient object region level matching on large-scale dataset. Experimental results on four datasets show that our DRH can achieve even better performance than the state-of-the-arts in terms of mAP, while the efficiency is improved by nearly 100 times.",2018,AAAI,1.0
Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction,"The availability of a large amount of electronic health records (EHR) provides huge opportunities to improve health care service by mining these data. One important application is clinical endpoint prediction, which aims to predict whether a disease, a symptom or an abnormal lab test will happen in the future according to patients‚Äô history records. This paper develops deep learning techniques for clinical endpoint prediction, which are effective in many practical applications. However, the problem is very challenging since patients‚Äô history records contain multiple heterogeneous temporal events such as lab tests, diagnosis, and drug administrations. The visiting patterns of different types of events vary significantly, and there exist complex nonlinear relationships between different events. In this paper, we propose a novel model for learning the joint representation of heterogeneous temporal events. The model adds a new gate to control the visiting rates of different events which effectively models the irregular patterns of different events and their nonlinear correlations. Experiment results with real-world clinical data on the tasks of predicting death and abnormal lab tests prove the effectiveness of our proposed approach over competitive baselines.",2018,AAAI,0.5
Stream Reasoning in Temporal Datalog,"In recent years, there has been an increasing interest in extending traditional stream processing engines with logical, rule-based, reasoning capabilities. This poses significant theoretical and practical challenges since rules can derive new information and propagate it both towards past and future time points; as a result, streamed query answers can depend on data that has not yet been received, as well as on data that arrived far in the past. Stream reasoning algorithms, however, must be able to stream out query answers as soon as possible, and can only keep a limited number of previous input facts in memory. In this paper, we propose novel reasoning problems to deal with these challenges, and study their computational properties on Datalog extended with a temporal sort and the successor function‚Äîa core rule-based language for stream reasoning applications.",2018,AAAI,0.30000000000000004
Neural Semantic Parsing,"Semantic parsing, the study of translating natural language utterances into machine-executable programs, is a well-established research area and has applications in question answering, instruction following, voice assistants, and code generation. In the last two years, the models used for semantic parsing have changed dramatically with the introduction of neural encoder-decoder methods that allow us to rethink many of the previous assumptions underlying semantic parsing. We aim to inform those already interested in semantic parsing research of these new developments in the field, as well as introduce the topic as an exciting research area to those who are unfamiliar with it. Current approaches for neural semantic parsing share several similarities with neural machine translation, but the key difference between the two fields is that semantic parsing translates natural language into a formal language, while machine translation translates it into a different natural language. The formal language used in semantic parsing allows for constrained decoding, where the model is constrained to only produce outputs that are valid formal statements. We will describe the various approaches researchers have taken to do this. We will also discuss the choice of formal languages used by semantic parsers, and describe why much recent work has chosen to use standard programming languages instead of more linguistically-motivated representations. We will then describe a particularly challenging setting for semantic parsing, where there is additional context or interaction that the parser must take into account when translating natural language to formal language, and give an overview of recent work in this direction. Finally, we will introduce some tools available in AllenNLP for doing semantic parsing research.",2018,ACL,0.0
A Hybrid System for Chinese Grammatical Error Diagnosis and Correction,"This paper introduces the DM_NLP team's system for NLPTEA 2018 shared task of Chinese Grammatical Error Diagnosis (CGED), which can be used to detect and correct grammatical errors in texts written by Chinese as a Foreign Language (CFL) learners. This task aims at not only detecting four types of grammatical errors including redundant words (R), missing words (M), bad word selection (S) and disordered words (W), but also recommending corrections for errors of M and S types. We proposed a hybrid system including four models for this task with two stages: the detection stage and the correction stage. In the detection stage, we first used a BiLSTM-CRF model to tag potential errors by sequence labeling, along with some handcraft features. Then we designed three Grammatical Error Correction (GEC) models to generate corrections, which could help to tune the detection result. In the correction stage, candidates were generated by the three GEC models and then merged to output the final corrections for M and S types. Our system reached the highest precision in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors.",2018,ACL,1.0
Assessment of an Index for Measuring Pronunciation Difficulty,"This study assesses an index for measur-ing the pronunciation difficulty of sen-tences (henceforth, pronounceability) based on the normalized edit distance from a reference sentence to a transcrip-tion of learners' pronunciation. Pro-nounceability should be examined when language teachers use a computer-assisted language learning system for pronunciation learning to maintain the motivation of learners. However, unlike the evaluation of learners' pronunciation performance, previous research did not focus on pronounceability not only for English but also for Asian languages. This study found that the normalized edit distance was reliable but not valid. The lack of validity appeared to be because of an English test used for determining the proficiency of learners.",2018,ACL,-1.0
Stop Word Lists in Free Open-source Software Packages,"Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. ""hasn't"" but not ""hadn't"") and inclusions (""computer""), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.",2018,ACL,-0.2
On the Limitations of Unsupervised Bilingual Dictionary Induction,"Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",2018,ACL,-0.6000000000000001
CRF-LSTM Text Mining Method Unveiling the Pharmacological Mechanism of Off-target Side Effect of Anti-Multiple Myeloma Drugs,"Sequence labeling of biomedical entities, e.g., side effects or phenotypes, was a long-term task in BioNLP and MedNLP communities. Thanks to effects made among these communities, adverse reaction NER has developed dramatically in recent years. As an illuminative application, to achieve knowledge discovery via the combination of the text mining result and bioinformatics idea shed lights on the pharmacological mechanism research.",2018,ACL,0.1
Findings of the Second Workshop on Neural Machine Translation and Generation,"This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop's shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.",2018,ACL,0.0
Characterizing Departures from Linearity in Word Translation,"We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",2018,ACL,0.0
Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information,"We propose a novel neural method to extract drug-drug interactions (DDIs) from texts using external drug molecular structure information. We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks (GCNs), and then we concatenate the outputs of these two networks. In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.",2018,ACL,1.0
Comparison of Representations of Named Entities for Document Classification,"We explore representations for multi-word names in text classification tasks, on Reuters (RCV1) topic and sector classification. We find that: the best way to treat names is to split them into tokens and use each token as a separate feature; NEs have more impact on sector classification than topic classification; replacing NEs with entity types is not an effective strategy; representing tokens by different embeddings for proper names vs. common nouns does not improve results. We highlight the improvements over state-of-the-art results that our CNN models yield.",2018,ACL,1.0
Recognizing Complex Entity Mentions: A Review and Future Directions,"Standard named entity recognizers can effectively recognize entity mentions that consist of contiguous tokens and do not overlap with each other. However, in practice, there are many domains, such as the biomedical domain, in which there are nested, overlapping, and discontinuous entity mentions. These complex mentions cannot be directly recognized by conventional sequence tagging models because they may break the assumptions based on which sequence tagging techniques are built. We review the existing methods which are revised to tackle complex entity mentions and categorize them as tokenlevel and sentence-level approaches. We then identify the research gap, and discuss some directions that we are exploring.",2018,ACL,0.0
DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension,"We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.",2018,ACL,-1.0
Did the Model Understand the Question?,"We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of ""attribution"" (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.",2018,ACL,-1.0
Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures,"Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.",2018,ACL,1.0
Character-Level Models versus Morphology in Semantic Role Labeling,"Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.",2018,ACL,-1.0
Graph-to-Sequence Learning using Gated Graph Neural Networks,"Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.",2018,ACL,1.0
No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",2018,ACL,1.0
The risk of sub-optimal use of Open Source NLP Software: UKB is inadvertently state-of-the-art in knowledge-based WSD,"UKB is an open source collection of programs for performing, among other tasks, Knowledge-Based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility.",2018,ACL,-0.7000000000000001
Do Neural Network Cross-Modal Mappings Really Bridge Modalities?,"Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.",2018,ACL,-0.8
A Neural Architecture for Automated ICD Coding,"The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding - which assigns a subset of ICD codes to a patient visit - is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.",2018,ACL,1.0
Book Review: Automatic Text Simplification by Horacio Saggion,"Automatic text simplification is a special task of text-to-text generation, and it converts a text into another text that is easier to read and understand, while the underlying meaning and information remains the same. A text simplification system usually replaces difficult or unknown phrases with simpler equivalents and transforms long and syntactically complex sentences into shorter and less complex ones. Here is an example from Siddharthan (2006). The first sentence contains two relative clauses and one conjoined verb phrase, and the text below is the simplified version.",2018,CL,0.0
A Structured Review of the Validity of BLEU,"The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique-in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.",2018,CL,-0.2
Deep Neural Networks at the Service of Multilingual Parallel Sentence Extraction,"Wikipedia provides an invaluable source of parallel multilingual data, which are in high demand for various sorts of linguistic inquiry, including both theoretical and practical studies. We introduce a novel end-to-end neural model for large-scale parallel data harvesting from Wikipedia. Our model is language-independent, robust, and highly scalable. We use our system for collecting parallel German-English, French-English and Persian-English sentences. Human evaluations at the end show the strong performance of this model in collecting high-quality parallel data. We also propose a statistical framework which extends the results of our human evaluation to other language pairs. Our model also obtained a state-of-the-art result on the German-English dataset of BUCC 2017 shared task on parallel sentence extraction from comparable corpora.",2018,COLING,1.0
Relation Induction in Word Embeddings Revisited,"Given a set of instances of some relation, the relation induction task is to predict which other word pairs are likely to be related in the same way. While it is natural to use word embeddings for this task, standard approaches based on vector translations turn out to perform poorly. To address this issue, we propose two probabilistic relation induction models. The first model is based on translations, but uses Gaussians to explicitly model the variability of these translations and to encode soft constraints on the source and target words that may be chosen. In the second model, we use Bayesian linear regression to encode the assumption that there is a linear relationship between the vector representations of related words, which is considerably weaker than the assumption underlying translation based models.",2018,COLING,0.2
Exploratory Neural Relation Classification for Domain Knowledge Acquisition,"The state-of-the-art methods for relation classification are primarily based on deep neural net- works. This kind of supervised learning method suffers from not only limited training data, but also the large number of low-frequency relations in specific domains. In this paper, we propose the task of exploratory relation classification for domain knowledge harvesting. The goal is to learn a classifier on pre-defined relations and discover new relations expressed in texts. A dynamically structured neural network is introduced to classify entity pairs to a continuously expanded relation set. We further propose the similarity sensitive Chinese restaurant process to discover new relations. Experiments conducted on a large corpus show the effectiveness of our neural network, while new relations are discovered with high precision and recall.",2018,COLING,0.8
Urdu Word Segmentation using Conditional Random Fields (CRFs),"State-of-the-art Natural Language Processing algorithms rely heavily on efficient word segmentation. Urdu is amongst languages for which word segmentation is a complex task as it exhibits space omission as well as space insertion issues. This is partly due to the Arabic script which although cursive in nature, consists of characters that have inherent joining and non-joining attributes regardless of word boundary. This paper presents a word segmentation system for Urdu which uses a Conditional Random Field sequence modeler with orthographic, linguistic and morphological features. Our proposed model automatically learns to predict white space as word boundary as well as Zero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated corpus, our model achieves F1 score of 0.97 for word boundary identification and 0.85 for sub-word boundary identification tasks. We have made our code and corpus publicly available to make our results reproducible.",2018,COLING,1.0
A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators,"Visual Question Generation (VQG) aims to ask natural questions about an image automatically. Existing research focus on training model to fit the annotated data set that makes it indifferent from other language generation tasks. We argue that natural questions need to have two specific attributes from the perspectives of content and linguistic respectively, namely, natural and human-written. Inspired by the setting of discriminator in adversarial learning, we propose two discriminators, one for each attribute, to enhance the training. We then use the reinforcement learning framework to incorporate scores from the two discriminators as the reward to guide the training of the question generator. Experimental results on a benchmark VQG dataset show the effectiveness and robustness of our model compared to some state-of-the-art models in terms of both automatic and human evaluation metrics.",2018,COLING,0.9
Exploring word embeddings and phonological similarity for the unsupervised correction of language learner errors,"The presence of misspellings and other errors or non-standard word forms poses a consider- able challenge for NLP systems. Although several supervised approaches have been proposed previously to normalize these, annotated training data is scarce for many languages. We in- vestigate, therefore, an unsupervised method where correction candidates for Swedish language learners’ errors are retrieved from word embeddings. Furthermore, we compare the usefulness of combining cosine similarity with orthographic and phonological similarity based on a neural grapheme-to-phoneme conversion system we train for this purpose. Although combinations of similarity measures have been explored for finding correction candidates, it remains unclear how these measures relate to each other and how much they contribute individually to identifying the correct alternative. We experiment with different combinations of these and find that integrating phonological information is especially useful when the majority of learner errors are related to misspellings, but less so when errors are of a variety of types including, e.g. grammatical errors.",2018,COLING,-0.30000000000000004
Recognizing Humour using Word Associations and Humour Anchor Extraction,"This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.",2018,COLING,-0.30000000000000004
Experiments on Morphological Reinflection: CoNLL-2018 Shared Task,"We present a system for the task of morphological inflection, i.e., finding a target morphological form, given a lemma and a set of target tags. System is trained on datasets of three sizes: low, medium and high. The system uses a simple Long Short-Term Memory (LSTM) based encoder-decoder based model. The performance for low size dataset is poor in general while it improves significantly for medium and high sized training dataset. The average performance over all languages is poor as compared to baseline for low dataset, it is comparable for medium dataset, and significantly more for high dataset.",2018,CoNLL,1.0
AntNLP at CoNLL 2018 Shared Task: A Graph-Based Parser for Universal Dependency Parsing,"We describe the graph-based dependency parser in our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task. We use bidirectional lstm to get the word representation, then a bi-affine pointer networks to compute scores of candidate dependency edges and the MST algorithm to get the final dependency tree. From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX (8/26).",2018,CoNLL,1.0
SEx BiST: A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations,"We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies). The main characteristic of our work is the encoding of three different modes of contextual information for parsing: (i) Treebank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via unsupervised learning from external resources. Our parser performed well in the official end-to-end evaluation (73.02 LAS - 4th/26 teams, and 78.72 UAS - 2nd/26); remarkably, we achieved the best UAS scores on all the English corpora by applying the three suggested feature representations. Finally, we were also ranked 1st at the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign.",2018,CoNLL,1.0
The 2018 Shared Task on Extrinsic Parser Evaluation: On the Downstream Utility of English Universal Dependency Parsers,"We summarize empirical results and tentative conclusions from the Second Extrinsic Parser Evaluation Initiative (EPE 2018). We review the basic task setup, downstream applications involved, and end-to-end results for seventeen participating teams. Based on in-depth quantitative and qualitative analysis, we correlate intrinsic evaluation results at different layers of morph-syntactic analysis with observed downstream behavior.",2018,CoNLL,0.0
Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention Similarity,"Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. In a vein similar to recent pieces of work investigating the linguistic abilities of these representations, we study another core, defining property of language: the property of long-distance dependencies. Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent. This ability is blocked if a similar, but extraneous, element intervenes between the discontinuous components. We present results that show, under exhaustive and precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the properties of intervention similarity in long-distance dependencies, and that therefore they fail to represent this core linguistic notion.",2018,CoNLL,-0.8
"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",Training and testing many possible parameters or model architectures of state-of-the-art machine translation or automatic speech recognition system is a cumbersome task. They usually require a long pipeline of commands reaching from pre-processing the training data to post-processing and evaluating the output.,2018,EMNLP,1.0
On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.",2018,EMNLP,0.0
"Er ... well, it matters, right? On the role of data representations in spoken language dependency parsing","Despite the significant improvement of data-driven dependency parsing systems in recent years, they still achieve a considerably lower performance in parsing spoken language data in comparison to written data. On the example of Spoken Slovenian Treebank, the first spoken data treebank using the UD annotation scheme, we investigate which speech-specific phenomena undermine parsing performance, through a series of training data and treebank modification experiments using two distinct state-of-the-art parsing systems. Our results show that utterance segmentation is the most prominent cause of low parsing performance, both in parsing raw and pre-segmented transcriptions. In addition to shorter utterances, both parsers perform better on normalized transcriptions including basic markers of prosody and excluding disfluencies, discourse markers and fillers. On the other hand, the effects of written training data addition and speech-specific dependency representations largely depend on the parsing system selected.",2018,EMNLP,0.0
Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,"Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese-English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",2018,EMNLP,-1.0
Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point,"We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semi-automatic metrics and test suites in place of fully automatic metrics.",2018,EMNLP,0.0
Tilde's Parallel Corpus Filtering Methods for WMT 2018,"The paper describes parallel corpus filtering methods that allow reducing noise of noisy ""parallel"" corpora from a level where the corpora are not usable for neural machine translation training (i.e., the resulting systems fail to achieve reasonable translation quality; well below 10 BLEU points) up to a level where the trained systems show decent (over 20 BLEU points on a 10 million word dataset and up to 30 BLEU points on a 100 million word dataset). The paper also documents Tilde's submissions to the WMT 2018 shared task on parallel corpus filtering.",2018,EMNLP,1.0
Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets,"Extracting typed entity mentions from text is a fundamental component to language understanding and reasoning. While there exist substantial labeled text datasets for multiple subsets of biomedical entity types-such as genes and proteins, or chemicals and diseases-it is rare to find large labeled datasets containing labels for all desired entity types together. This paper presents a method for training a single CRF extractor from multiple datasets with disjoint or partially overlapping sets of entity types. Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in ""missing labels"". This allows us to leverage all the available data within a single model. In experimental results on the Biocreative V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.",2018,EMNLP,1.0
Retrieve and Re-rank: A Simple and Effective IR Approach to Simple Question Answering over Knowledge Graphs,"SimpleQuestions is a commonly used benchmark for single-factoid question answering (QA) over Knowledge Graphs (KG). Existing QA systems rely on various components to solve different sub-tasks of the problem (such as entity detection, entity linking, relation prediction and evidence integration). In this work, we propose a different approach to the problem and present an information retrieval style solution for it. We adopt a two-phase approach: candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. Our approach achieves an accuracy of 80% which sets a new state-of-the-art on the SimpleQuestions dataset.",2018,EMNLP,1.0
"Making ""fetch"" happen: The influence of social and linguistic context on nonstandard word growth and decline","In an online community, new words come and go: today's ""haha"" may be replaced by tomorrow's ""lol."" Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the grammatical system in which it takes part. To investigate the role of social and structural factors in language change, we undertake a large-scale analysis of the frequencies of non-standard words in Reddit. Dissemination across many linguistic contexts is a predictor of success: words that appear in more linguistic contexts grow faster and survive longer. Furthermore, social dissemination plays a less important role in explaining word growth and decline than previously hypothesized.",2018,EMNLP,0.0
Implicational Universals in Stochastic Constraint-Based Phonology,"This paper focuses on the most basic implicational universals in phonological theory, called T-orders after Anttila and Andrus (2006). It shows that the T-orders predicted by stochastic (and partial order) Optimality Theory coincide with those predicted by categorical OT. Analogously, the T-orders predicted by stochastic Harmonic Grammar coincide with those predicted by categorical HG. In other words, these stochastic constraint-based frameworks do not tamper with the typological structure induced by the original categorical frameworks.",2018,EMNLP,0.0
HUMIR at IEST-2018: Lexicon-Sensitive and Left-Right Context-Sensitive BiLSTM for Implicit Emotion Recognition,"This paper describes the approaches used in HUMIR system for the WASSA-2018 shared task on the implicit emotion recognition. The objective of this task is to predict the emotion expressed by the target word that has been excluded from the given tweet. We suppose this task as a word sense disambiguation in which the target word is considered as a synthetic word that can express 6 emotions depending on the context. To predict the correct emotion, we propose a deep neural network model that uses two BiLSTM networks to represent the contexts in the left and right sides of the target word. The BiLSTM outputs achieved from the left and right contexts are considered as context-sensitive features. These features are used in a feed-forward neural network to predict the target word emotion. Besides this approach, we also combine the BiLSTM model with lexicon-based and emotion-based features. Finally, we employ all models in the final system using Bagging ensemble method. We achieved macro F-measure value of 68.8 on the official test set and ranked sixth out of 30 participants.",2018,EMNLP,0.8
Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment,"Network embeddings, which learns low-dimensional representations for each vertex in a large-scale network, have received considerable attention in recent years. For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. In this paper, we propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices. We introduce an word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. The experimental results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.",2018,EMNLP,1.0
Object Hallucination in Image Captioning,"Despite continuously improving performance, contemporary image captioning models are prone to ""hallucinating"" objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.",2018,EMNLP,0.8
Do explanations make VQA models more predictable to a human?,"A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model - its responses as well as failures - more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.",2018,EMNLP,-0.2
Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study,"Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. In this work we examine the impact of a test set question's difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question's difficulty. In addition, as DNNs are trained on larger datasets easy questions start to have a higher probability of being answered correctly than harder questions.",2018,EMNLP,0.0
The First Komi-Zyrian Universal Dependencies Treebanks,"Two Komi-Zyrian treebanks were included in the Universal Dependencies 2.2 release. This article contextualizes the treebanks, discusses the process through which they were created, and outlines the future plans and timeline for the next improvements. Special attention is paid to the possibilities of using UD in the documentation and description of endangered languages.",2018,EMNLP,0.0
In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition,"Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult for a timely evaluation of the latest literature. That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods. In this study we target the problem of named entity recognition, where texts are processed to annotate terms that are relevant for biomedical studies. Terms of interest in the domain include gene and protein names, and cell lines and types. Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (AllenNLP). We trained context-aware token embeddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for named entity recognition. We show these representations improve named entity recognition for different types of biomedical named entities. We also achieve a new state of the art in gene mention detection on the BioCreative II gene mention shared task.",2018,EMNLP,1.0
Linking News Sentiment to Microblogs: A Distributional Semantics Approach to Enhance Microblog Sentiment Classification,"Social media's popularity in society and research is gaining momentum and simultaneously increasing the importance of short textual content such as microblogs. Microblogs are affected by many factors including the news media, therefore, we exploit sentiments conveyed from news to detect and classify sentiment in microblogs. Given that texts can deal with the same entity but might not be vastly related when it comes to sentiment, it becomes necessary to introduce further measures ensuring the relatedness of texts while leveraging the contained sentiments. This paper describes ongoing research introducing distributional semantics to improve the exploitation of news-contained sentiment to enhance microblog sentiment classification.",2018,EMNLP,1.0
Analysing the potential of seq-to-seq models for incremental interpretation in task-oriented dialogue,"We investigate how encoder-decoder models trained on a synthetic dataset of task-oriented dialogues process disfluencies, such as hesitations and self-corrections. We find that, contrary to earlier results, disfluencies have very little impact on the task success of seq-to-seq models with attention. Using visualisations and diagnostic classifiers, we analyse the representations that are incrementally built by the model, and discover that models develop little to no awareness of the structure of disfluencies. However, adding disfluencies to the data appears to help the model create clearer representations overall, as evidenced by the attention patterns the different models exhibit.",2018,EMNLP,-0.2
Combining Human and Machine Transcriptions on the Zooniverse Platform,"Transcribing handwritten documents to create fully searchable texts is an essential part of the archival process. Traditional text recognition methods, such as optical character recognition (OCR), do not work on handwritten documents due to their frequent noisiness and OCR's need for individually segmented letters. Crowdsourcing and improved machine models are two modern methods for transcribing handwritten documents.",2018,EMNLP,-1.0
Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation,"Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",2018,EMNLP,0.8
Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis,"Recently, researchers have found that deep LSTMs trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech. These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives-language modeling, translation, skip-thought, and autoencoding-on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture.",2018,EMNLP,-0.2
Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation,"To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the encoder, decoder, and each embedding space) and consider each component's contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain.",2018,EMNLP,1.0
The Importance of Generation Order in Language Modeling,"Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence ""templates"" and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of the generation order for neural language models.",2018,EMNLP,0.0
Picking Apart Story Salads,"During natural disasters and conflicts, information about what happened is often confusing and messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and coherence.",2018,EMNLP,1.0
Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task,"We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider.",2018,EMNLP,1.0
Disambiguated skip-gram model,"We present disambiguated skip-gram: a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a word sense disambiguation model. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neural network. We also introduce an effective pruning strategy for the embeddings learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our model. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.",2018,EMNLP,1.0
Jump to better conclusions: SCAN both left and right,"Lake and Baroni (2018) recently introduced the SCAN data set, which consists of simple commands paired with action sequences and is intended to test the strong generalization abilities of recurrent sequence-to-sequence models. Their initial experiments suggested that such models may fail because they lack the ability to extract systematic rules. Here, we take a closer look at SCAN and show that it does not always capture the kind of generalization that it was designed for. To mitigate this we propose a complementary dataset, which requires mapping actions back to the original commands, called NACS. We show that models that do well on SCAN do not necessarily do well on NACS, and that NACS exhibits properties more closely aligned with realistic use-cases for sequence-to-sequence models.",2018,EMNLP,-1.0
The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation,"Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.",2018,EMNLP,-0.5
On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling,"A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world's languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.",2018,EMNLP,-0.4
Iterative Document Representation Learning Towards Summarization with Polishing,"In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.",2018,EMNLP,1.0
"Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence: Argument Mining in the Context of Legal Rules Governing Evidence Assessment","This paper reports on the results of an empirical study of adjudicatory decisions about veterans' claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or ""soft rules"" that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter-operable with legal rules governing the evidence assessment process in the U.S. This approach should be transferable to other legal and non-legal domains.",2018,EMNLP,0.30000000000000004
An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation,"Recent work has shown that the encoder-decoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention mechanisms pay more attention to context tokens when translating ambiguous words. We explore the attention distribution patterns when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that attention is not the main mechanism used by NMT models to incorporate contextual information for WSD. The experimental results suggest that NMT models learn to encode contextual information necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to ""align"" source and target tokens and the last few layers learn to extract features from the related but unaligned context tokens.",2018,EMNLP,0.0
Auto-Encoding Sequential Monte Carlo,"We build on auto-encoding sequential Monte Carlo (AESMC):1 a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and experiment with a new training procedure which can improve both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.",2018,ICLR,0.9
On the importance of single directions for generalization,"Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network‚Äôs reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.",2018,ICLR,-0.5
Lifelong Learning with Dynamically Expandable Networks,"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained siginficantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.",2018,ICLR,1.0
Adaptive Dropout with Rademacher Complexity Regularization,"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns. Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of-the-art dropout algorithms.",2018,ICLR,1.0
Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling,"Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",2018,ICLR,-0.5
Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step,"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players‚Äô parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.",2018,ICLR,-1.0
Attacking Binarized Neural Networks,"Neural networks with low-precision weights and activations offer compelling efficiency advantages over their full-precision equivalents. The two most frequently discussed benefits of quantization are reduced memory consumption, and a faster forward pass when implemented with efficient bitwise operations. We propose a third benefit of very low-precision neural networks: improved robustness against some adversarial attacks, and in the worst case, performance that is on par with full-precision models. We focus on the very low-precision case where weights and activations are both quantized to ¬±1, and note that stochastically quantizing weights in just one layer can sharply reduce the impact of iterative attacks. We observe that non-scaled binary neural networks exhibit a similar effect to the original defensive distillation procedure that led to gradient masking, and a false notion of security. We address this by conducting both black-box and white-box experiments with binary models that do not artificially mask gradients.1",2018,ICLR,1.0
Adversarial Dropout Regularization,"We present a domain adaptation method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by ‚Äúfooling‚Äù a special domain classifier network. However, a drawback of this approach is that the domain classifier simply labels the generated features as in-domain or not, without considering the boundaries between classes. This means that ambiguous target features can be generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), which encourages the generator to output more discriminative features for the target domain. Our key idea is to replace the traditional domain critic with a critic that detects non-discriminative features by using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvements over the state of the art.",2018,ICLR,1.0
On the Convergence of Adam and Beyond,"Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ‚Äúlong-term memory‚Äù of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.",2018,ICLR,0.8
Do GANs learn the distribution? Some Theory and Empirics,"Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if they were given ‚Äúsufficiently large‚Äù deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support ‚Äîin other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size using the birthday paradox of discrete probability. Using this evidence is presented that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g., BiGAN/ALI), which were proposed to learn more meaningful features via GANs and (consequently) to also solve the mode-collapse issue. Our result shows that such encoder-decoder training objectives also cannot guarantee learning of the full distribution because they cannot prevent serious mode collapse. More seriously, they cannot prevent learning meaningless codes for data, contrary to usual intuition.",2018,ICLR,-0.30000000000000004
An efficient framework for learning sentence representations,"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform stateof-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.",2018,ICLR,1.0
Twin Networks: Matching the Future for Sequence Generation,"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ‚Äúbackward‚Äù recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.",2018,ICLR,1.0
Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input,"The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.",2018,ICLR,0.4
A Hierarchical Model for Device Placement,"We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. Our method learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed method is trained with policy gradient and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.",2018,ICLR,1.0
Understanding Short-Horizon Bias in Stochastic Meta-Optimization,"Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the metaobjective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if metaoptimization is to scale to practical neural net training regimes.",2018,ICLR,-1.0
AmbientGAN: Generative models from lossy measurements,"Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fullyobserved samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain 2-4x higher inception scores than the baselines.",2018,ICLR,0.4
Towards Reverse-Engineering Black-Box Neural Networks,"Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks ‚Äì we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models. The code is available at goo.gl/MbYfsv.",2018,ICLR,-0.2
META LEARNING SHARED HIERARCHIES,"We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives‚Äîpolicies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover1 meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.",2018,ICLR,0.7000000000000001
On the State of the Art of Evaluation in Neural Language Models,"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",2018,ICLR,1.0
Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.",2018,ICLR,0.6000000000000001
Distributed Prioritized Experience Replay,"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",2018,ICLR,1.0
Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm‚Äôs ability to generate convincing, identity-matched photographs.",2018,ICLR,0.6000000000000001
A Deep Reinforced Model for Abstractive Summarization,"Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intraattention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit ‚Äúexposure bias‚Äù ‚Äì they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",2018,ICLR,1.0
The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,"In this work, we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2017) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms, designed for expected value evaluation, into distributional algorithms. Next, we introduce the Œ≤-leave-one-out policy gradient algorithm, which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.",2018,ICLR,1.0
Wavelet Pooling for Convolutional Neural Networks,"Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classification. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level decomposition, and discards the first-level subbands to reduce feature dimensions. This method addresses the overfitting problem encountered by max pooling, while reducing features in a more structurally compact manner than pooling via neighborhood regions. Experimental results on four benchmark classification datasets demonstrate our proposed method outperforms or performs comparatively with methods like max, mean, mixed, and stochastic pooling.",2018,ICLR,0.7000000000000001
Neural Language Modeling by Jointly Learning Syntax and Lexicon,"We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.",2018,ICLR,1.0
Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions,"In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.",2018,ICML,0.0
Learning Adversarially Fair and Transferable Representations,"In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",2018,ICML,1.0
Understanding the Loss Surface of Neural Networks for Binary Classification,"It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcutlike connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.",2018,ICML,-0.5
Weightless: Lossy weight encoding for deep neural network compression,"The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496√ó without loss of model accuracy. This results in up to a 1.51√ó improvement over the state-of-the-art.",2018,ICML,1.0
On the Limitations of First-Order Approximation in GAN Dynamics,"While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.",2018,ICML,0.0
Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the longterm well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",2018,ICML,0.0
Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks,"Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb ‚Äúdax,‚Äù he or she can immediately understand the meaning of ‚Äúdax twice‚Äù or ‚Äúsing and dax.‚Äù In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply ‚Äúmix-and-match‚Äù strategies to solve the task. However, when generalization requires systematic compositional skills (as in the ‚Äúdax‚Äù example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks‚Äô notorious training data thirst.",2018,ICML,-0.5
Spectrally Approximating Large Graphs with Smaller Graphs,"How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual graph sizes. Our results carry implications for learning methods that utilize coarsening. For the particular case of spectral clustering, they imply that coarse eigenvectors can be used to derive good quality assignments even without refinement‚Äîthis phenomenon was previously observed, but lacked formal justification.",2018,ICML,0.30000000000000004
Semi-Supervised Learning via Compact Latent Space Clustering,"We present a novel cost function for semisupervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.",2018,ICML,0.8
Detecting context-dependent sentences in parallel corpora,"In this article, we provide several approaches to the automatic identification of parallel sentences that require sentence-external linguistic context to be correctly translated. Our long-term goal is to automatically construct a test set of context-dependent sentences in order to evaluate machine translation models designed to improve the translation of contextual, discursive phenomena. We provide a discussion and critique that show that current approaches do not allow us to achieve our goal, and suggest that for now evaluating individual phenomena is likely the best solution.",2018,JEP/TALN/RECITAL,-0.6000000000000001
Multi-lingual neural title generation for e-Commerce browse pages,"To provide better access of the inventory to buyers and better search engine optimization, e-Commerce websites are automatically generating millions of browse pages. A browse page consists of a set of slot name/value pairs within a given category, grouping multiple items which share some characteristics. These browse pages require a title describing the content of the page. Since the number of browse pages are huge, manual creation of these titles is infeasible. Previous statistical and neural approaches depend heavily on the availability of large amounts of data in a language. In this research, we apply sequence-to-sequence models to generate titles for high-resource as well as low-resource languages by leveraging transfer learning. We train these models on multi-lingual data, thereby creating one joint model which can generate titles in various different languages. Performance of the title generation system is evaluated on three different languages; English, German, and French, with a particular focus on low-resourced French language.",2018,NAACL,1.0
A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain Classification in Natural Language Understanding,"Intelligent personal digital assistants (IPDAs), a popular real-life application with spoken language understanding capabilities, can cover potentially thousands of overlapping domains for natural language understanding, and the task of finding the best domain to handle an utterance becomes a challenging problem on a large scale. In this paper, we propose a set of efficient and scalable shortlisting-reranking neural models for effective large-scale domain classification for IPDAs. The shortlisting stage focuses on efficiently trimming all domains down to a list of k-best candidate domains, and the reranking stage performs a list-wise reranking of the initial k-best domains with additional contextual information. We show the effectiveness of our approach with extensive experiments on 1,500 IPDA domains.",2018,NAACL,1.0
Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis,We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for sentiment analysis. The novel two-layered attention network takes advantage of the external knowledge bases to improve the sentiment prediction. It uses the Knowledge Graph Embedding generated using the WordNet. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for sentiment analysis. We evaluate our model on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed model surpasses the top system of SemEval 2017 Task 5. The model performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.,2018,NAACL,1.0
A Novel Approach to Part Name Discovery in Noisy Text,"As a specialized example of information extraction, part name extraction is an area that presents unique challenges. Part names are typically multi-word terms longer than two words. There is little consistency in how terms are described in noisy free text, with variations spawned by typos, ad hoc abbreviations, acronyms, and incomplete names. This makes search and analyses of parts in these data extremely challenging. In this paper, we present our algorithm, PANDA (Part Name Discovery Analytics), based on a unique method that exploits statistical, linguistic and machine learning techniques to discover part names in noisy text such as that in manufacturing quality documentation, supply chain management records, service communication logs, and maintenance reports. Experiments show that PANDA is scalable and outperforms existing techniques significantly.",2018,NAACL,1.0
Document-based Recommender System for Job Postings using Dense Representations,"Job boards and professional social networks heavily use recommender systems in order to better support users in exploring job advertisements. Detecting the similarity between job advertisements is important for job recommendation systems as it allows, for example, the application of item-to-item based recommendations. In this work, we research the usage of dense vector representations to enhance a large-scale job recommendation system and to rank German job advertisements regarding their similarity. We follow a two-folded evaluation scheme: (1) we exploit historic user interactions to automatically create a dataset of similar jobs that enables an offline evaluation. (2) In addition, we conduct an online A/B test and evaluate the best performing method on our platform reaching more than 1 million users. We achieve the best results by combining job titles with full-text job descriptions. In particular, this method builds dense document representation using words of the titles to weigh the importance of words of the full-text description. In the online evaluation, this approach allows us to increase the click-through rate on job recommendations for active users by 8.0%.",2018,NAACL,1.0
Visual Referring Expression Recognition: What Do Systems Actually Learn?,"We present an empirical analysis of state-of-the-art systems for referring expression recognition - the task of identifying the object in an image referred to by a natural language expression - with the goal of gaining insight into how these systems reason about language and vision. Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore linguistic structure, instead relying on shallow correlations introduced by unintended biases in the data selection and annotation process. For example, we show that a system trained and tested on the input image without the input referring expression can achieve a precision of 71.2% in top-2 predictions. Furthermore, a system that predicts only the object category given the input can achieve a precision of 84.2% in top-2 predictions. These surprisingly positive results for what should be deficient prediction scenarios suggest that careful analysis of what our models are learning - and further, how our data is constructed - is critical as we seek to make substantive progress on grounded language tasks.",2018,NAACL,-1.0
Automatic Focus Annotation: Bringing Formal Pragmatics Alive in Analyzing the Information Structure of Authentic Data,"Analyzing language in context, both from a theoretical and from a computational perspective, is receiving increased interest. Complementing the research in linguistics on discourse and information structure, in computational linguistics identifying discourse concepts was also shown to improve the performance of certain applications, for example, Short Answer Assessment systems (Ziai and Meurers, 2014). Building on the research that established detailed annotation guidelines for manual annotation of information structural concepts for written (Dipper et al., 2007; Ziai and Meurers, 2014) and spoken language data (Calhoun et al., 2010), this paper presents the first approach automating the analysis of focus in authentic written data. Our classification approach combines a range of lexical, syntactic, and semantic features to achieve an accuracy of 78.1% for identifying focus.",2018,NAACL,1.0
Detecting Sarcasm is Extremely Easy ;-),"Detecting sarcasm in text is a particularly challenging problem in computational semantics, and its solution may vary across different types of text. We analyze the performance of a domain-general sarcasm detection system on datasets from two very different domains: Twitter, and Amazon product reviews. We categorize the errors that we identify with each, and make recommendations for addressing these issues in NLP systems in the future.",2018,NAACL,0.0
"Si O No, Que Penses? Catalonian Independence and Linguistic Identity on Social Media","Political identity is often manifested in language variation, but the relationship between the two is still relatively unexplored from a quantitative perspective. This study examines the use of Catalan, a language local to the semi-autonomous region of Catalonia in Spain, on Twitter in discourse related to the 2017 independence referendum. We corroborate prior findings that pro-independence tweets are more likely to include the local language than anti-independence tweets. We also find that Catalan is used more often in referendum-related discourse than in other contexts, contrary to prior findings on language variation. This suggests a strong role for the Catalan language in the expression of Catalonian political identity.",2018,NAACL,0.0
Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences,"We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.",2018,NAACL,1.0
Neural Network based Extreme Classification and Similarity Models for Product Matching,"Matching a seller listed item to an appropriate product has become a fundamental and one of the most significant step for e-commerce platforms for product based experience. It has a huge impact on making the search effective, search engine optimization, providing product reviews and product price estimation etc. along with many other advantages for a better user experience. As significant and vital it has become, the challenge to tackle the complexity has become huge with the exponential growth of individual and business sellers trading millions of products everyday. We explored two approaches; classification based on shallow neural network and similarity based on deep siamese network. These models outperform the baseline by more than 5% in term of accuracy and are capable of extremely efficient training and inference.",2018,NAACL,1.0
The Social and the Neural Network: How to Make Natural Language Processing about People again,"Over the years, natural language processing has increasingly focused on tasks that can be solved by statistical models, but ignored the social aspects of language. These limitations are in large part due to historically available data and the limitations of the models, but have narrowed our focus and biased the tools demographically. However, with the increased availability of data sets including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues. I argue that this combination can broaden the focus of NLP to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone.",2018,NAACL,-1.0
Tied Multitask Learning for Neural Speech Translation,"We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions. First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input.",2018,NAACL,1.0
QuickEdit: Editing Text & Translations by Crossing Words Out,We propose a framework for computer-assisted text editing. It applies to translation post-editing and to paraphrasing. Our proposal relies on very simple interactions: a human editor modifies a sentence by marking tokens they would like the system to change. Our model then generates a new sentence which reformulates the initial sentence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation post-editing through simulated post-edits. We also evaluate our model for paraphrasing through a user study.,2018,NAACL,0.8
Parsing Speech: a Neural Approach to Integrating Lexical and Acoustic-Prosodic Information,"In conversational speech, the acoustic signal provides cues that help listeners disambiguate difficult parses. For automatically parsing spoken utterances, we introduce a model that integrates transcribed text and acoustic-prosodic features using a convolutional neural network over energy and pitch trajectories coupled with an attention-based recurrent neural network that accepts text and prosodic features. We find that different types of acoustic-prosodic features are individually helpful, and together give statistically significant improvements in parse and disfluency detection F1 scores over a strong text-only baseline. For this study with known sentence boundaries, error analyses show that the main benefit of acoustic-prosodic features is in sentences with disfluencies, attachment decisions are most improved, and transcription errors obscure gains from prosody.",2018,NAACL,1.0
Sensing and Learning Human Annotators Engaged in Narrative Sensemaking,"While labor issues and quality assurance in crowdwork are increasingly studied, how annotators make sense of texts and how they are personally impacted by doing so are not. We study these questions via a narrative-sorting annotation task, where carefully selected (by sequentiality, topic, emotional content, and length) collections of tweets serve as examples of everyday storytelling. As readers process these narratives, we measure their facial expressions, galvanic skin response, and self-reported reactions. From the perspective of annotator well-being, a reassuring outcome was that the sorting task did not cause a measurable stress response, however readers reacted to humor. In terms of sensemaking, readers were more confident when sorting sequential, target-topical, and highly emotional tweets. As crowdsourcing becomes more common, this research sheds light onto the perceptive capabilities and emotional impact of human readers.",2018,NAACL,0.30000000000000004
"On the Utility of Lay Summaries and AI Safety Disclosures: Toward Robust, Open Research Oversight","In this position paper, we propose that the community consider encouraging researchers to include two riders, a ""Lay Summary"" and an ""AI Safety Disclosure"", as part of future NLP papers published in ACL forums that present user-facing systems. The goal is to encourage researchers-via a relatively non-intrusive mechanism-to consider the societal implications of technologies carrying (un)known and/or (un)knowable long-term risks, to highlight failure cases, and to provide a mechanism by which the general public (and scientists in other disciplines) can more readily engage in the discussion in an informed manner. This simple proposal requires minimal additional up-front costs for researchers; the lay summary, at least, has significant precedence in the medical literature and other areas of science; and the proposal is aimed to supplement, rather than replace, existing approaches for encouraging researchers to consider the ethical implications of their work, such as those of the Collaborative Institutional Training Initiative (CITI) Program and institutional review boards (IRBs).",2018,NAACL,0.0
Fast and Scalable Expansion of Natural Language Understanding Functionality for Intelligent Agents,"Fast expansion of natural language functionality of intelligent virtual agents is critical for achieving engaging and informative interactions. However, developing accurate models for new natural language domains is a time and data intensive process. We propose efficient deep neural network architectures that maximally re-use available resources through transfer learning. Our methods are applied for expanding the understanding capabilities of a popular commercial agent and are evaluated on hundreds of new domains, designed by internal or external developers. We demonstrate that our proposed methods significantly increase accuracy in low resource settings and enable rapid development of accurate models with less data.",2018,NAACL,1.0
Scalable Construction and Reasoning of Massive Knowledge Bases,"In today's information-based society, there is abundant knowledge out there carried in the form of natural language texts (e.g., news articles, social media posts, scientific publications), which spans across various domains (e.g., corporate documents, advertisements, legal acts, medical reports), which grows at an astonishing rate. Yet this knowledge is mostly inaccessible to computers and overwhelming for human experts to absorb. How to turn such massive and unstructured text data into structured, actionable knowledge, and furthermore, how to teach machines learn to reason and complete the extracted knowledge is a grand challenge to the research community. Traditional IE systems assume abundant human annotations for training high quality machine learning models, which is impractical when trying to deploy IE systems to a broad range of domains, settings and languages. In the first part of the tutorial, we introduce how to extract structured facts (i.e., entities and their relations for types of interest) from text corpora to construct knowledge bases, with a focus on methods that are weakly-supervised and domain-independent for timely knowledge base construction across various application domains. In the second part, we introduce how to leverage other knowledge, such as the distributional statistics of characters and words, the annotations for other tasks and other domains, and the linguistics and problem structures, to combat the problem of inadequate supervision, and conduct low-resource information extraction. In the third part, we describe recent advances in knowledge base reasoning. We start with the gentle introduction to the literature, focusing on path-based and embedding based methods. We then describe DeepPath, a recent attempt of using deep reinforcement learning to combine the best of both worlds for knowledge base reasoning.",2018,NAACL,0.4
Is Something Better than Nothing? Automatically Predicting Stance-based Arguments Using Deep Learning and Small Labelled Dataset,"Online reviews have become a popular portal among customers making decisions about purchasing products. A number of corpora of reviews have been widely investigated in NLP in general, and, in particular, in argument mining. This is a subset of NLP that deals with extracting arguments and the relations among them from user-based content. A major problem faced by argument mining research is the lack of human-annotated data. In this paper, we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets. We do this by building on previous work that explores the classification of opinions present in reviews based whether the stance is expressed explicitly or implicitly. In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit/explicit opinion classification.",2018,NAACL,0.7000000000000001
Are All Languages Equally Hard to Language-Model?,"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.",2018,NAACL,1.0
"Bootstrapping a Neural Conversational Agent with Dialogue Self-Play, Crowdsourcing and On-Line Reinforcement Learning","End-to-end neural models show great promise towards building conversational agents that are trained from data and on-line experience using supervised and reinforcement learning. However, these models require a large corpus of dialogues to learn effectively. For goal-oriented dialogues, such datasets are expensive to collect and annotate, since each task involves a separate schema and database of entities. Further, the Wizard-of-Oz approach commonly used for dialogue collection does not provide sufficient coverage of salient dialogue flows, which is critical for guaranteeing an acceptable task completion rate in consumer-facing conversational agents. In this paper, we study a recently proposed approach for building an agent for arbitrary tasks by combining dialogue self-play and crowd-sourcing to generate fully-annotated dialogues with diverse and natural utterances. We discuss the advantages of this approach for industry applications of conversational agents, wherein an agent can be rapidly bootstrapped to deploy in front of users and further optimized via interactive learning from actual users of the system.",2018,NAACL,1.0
The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. As arguments are highly contextualized, warrants are usually presupposed and left implicit. Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense. In this paper we develop a methodology for reconstructing warrants systematically. We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims. A solution to this task will define a substantial step towards automatic warrant reconstruction. However, experiments with several neural attention and language models reveal that current approaches do not suffice.",2018,NAACL,-0.2
Predicting Human Metaphor Paraphrase Judgments with Deep Neural Networks,"We propose a new annotated corpus for metaphor interpretation by paraphrase, and a novel DNN model for performing this task. Our corpus consists of 200 sets of 5 sentences, with each set containing one reference metaphorical sentence, and four ranked candidate paraphrases. Our model is trained for a binary classification of paraphrase candidates, and then used to predict graded paraphrase acceptability. It reaches an encouraging 75% accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task.",2018,NAACL,1.0
Dynamics of an idiostyle of a Russian suicidal blogger,"Over 800000 people die of suicide each year. It is es-timated that by the year 2020, this figure will have in-creased to 1.5 million. It is considered to be one of the major causes of mortality during adolescence. Thus there is a growing need for methods of identifying su-icidal individuals. Language analysis is known to be a valuable psychodiagnostic tool, however the material for such an analysis is not easy to obtain. Currently as the Internet communications are developing, there is an opportunity to study texts of suicidal individuals. Such an analysis can provide a useful insight into the peculiarities of suicidal thinking, which can be used to further develop methods for diagnosing the risk of suicidal behavior. The paper analyzes the dynamics of a number of linguistic parameters of an idiostyle of a Russian-language blogger who died by suicide. For the first time such an analysis has been conducted using the material of Russian online texts. For text processing, the LIWC program is used. A correlation analysis was performed to identify the relationship between LIWC variables and number of days prior to suicide. Data visualization, as well as comparison with the results of related studies was performed.",2018,NAACL,0.2
Deep Learning Approaches to Text Production,"Text production is a key component of many NLP applications. In data-driven approaches, it is used for instance, to generate dialogue turns from dialogue moves, to verbalise the content of Knowledge bases or to generate natural English sentences from rich linguistic representations, such as dependency trees or Abstract Meaning Representations. In text-driven methods on the other hand, text production is at work in sentence compression, sentence fusion, paraphrasing, sentence (or text) simplification, text summarisation and end-to-end dialogue systems. Following the success of encoder-decoder models in modeling sequence-rewriting tasks such as machine translation, deep learning models have successfully been applied to the various text production tasks. In this tutorial, we will cover the fundamentals and the state-of-the-art research on neural models for text production. Each text production task raises a slightly different communication goal (e.g, how to take the dialogue context into account when producing a dialogue turn; how to detect and merge relevant information when summarising a text; or how to produce a well-formed text that correctly capture the information contained in some input data in the case of data-to-text generation). We will outline the constraints specific to each subtasks and examine how the existing neural models account for them.",2018,NAACL,0.0
Training Structured Prediction Energy Networks with Indirect Supervision,"This paper introduces rank-based training of structured prediction energy networks (SPENs). Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined with domain knowledge. We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function. Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.",2018,NAACL,1.0
Bag of Experts Architectures for Model Reuse in Conversational Language Understanding,"Slot tagging, the task of detecting entities in input user utterances, is a key component of natural language understanding systems for personal digital assistants. Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow. To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models. Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06% in absolute F1-score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16% when only 25% of the training data is used.",2018,NAACL,1.0
Slot-Gated Modeling for Joint Slot Filling and Intent Prediction,"Attention-based recurrent neural network models for joint intent detection and slot filling have achieved the state-of-the-art performance, while they have independent attention weights. Considering that slot and intent have the strong relationship, this paper proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. The experiments show that our proposed model significantly improves sentence-level semantic frame accuracy with 4.2% and 1.9% relative improvement compared to the attentional model on benchmark ATIS and Snips datasets respectively",2018,NAACL,1.0
RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses,"Self-reported diagnosis statements have been widely employed in studying language related to mental health in social media. However, existing research has largely ignored the temporality of mental health diagnoses. In this work, we introduce RSDD-Time: a new dataset of 598 manually annotated self-reported depression diagnosis posts from Reddit that include temporal information about the diagnosis. Annotations include whether a mental health condition is present and how recently the diagnosis happened. Furthermore, we include exact temporal spans that relate to the date of diagnosis. This information is valuable for various computational methods to examine mental health through social media because one's mental health state is not static. We also test several baseline classification and extraction approaches, which suggest that extracting temporal information from self-reported diagnosis statements is challenging.",2018,NAACL,0.9
Multi-hop Inference for Sentence-level TextGraphs: How Challenging is Meaningfully Combining Information for Science Question Answering?,"Question Answering for complex questions is often modelled as a graph construction or traversal task, where a solver must build or traverse a graph of facts that answer and explain a given question. This ""multi-hop"" inference has been shown to be extremely challenging, with few models able to aggregate more than two facts before being overwhelmed by ""semantic drift"", or the tendency for long chains of facts to quickly drift off topic. This is a major barrier to current inference models, as even elementary science questions require an average of 4 to 6 facts to answer and explain. In this work we empirically characterize the difficulty of building or traversing a graph of sentences connected by lexical overlap, by evaluating chance sentence aggregation quality through 9,784 manually-annotated judgements across knowledge graphs built from three free-text corpora (including study guides and Simple Wikipedia). We demonstrate semantic drift tends to be high and aggregation quality low, at between 0.04 and 3, and highlight scenarios that maximize the likelihood of meaningfully combining information.",2018,NAACL,0.4
Neural Syntactic Generative Models with Exact Marginalization,"We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shift-reduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a state-of-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in language modeling. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models.",2018,NAACL,0.5
Speaker Naming in Movies,"We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",2018,NAACL,1.0
Towards Understanding Text Factors in Oral Reading,"Using a case study, we show that variation in oral reading rate across passages for professional narrators is consistent across readers and much of it can be explained using features of the texts being read. While text complexity is a poor predictor of the reading rate, a substantial share of variability can be explained by timing and story-based factors with performance reaching r=0.75 for unseen passages and narrator.",2018,NAACL,0.0
What we need to learn if we want to do and not just talk,"In task-oriented dialog, agents need to generate both fluent natural language responses and correct external actions like database queries and updates. Our paper makes the first attempt at evaluating state of the art models on a large real world task with human users. We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks. We propose a hybrid model, where nearest neighbor is used to generate fluent responses and Seq2Seq type models ensure dialogue coherency and generate accurate external actions. The hybrid model on the customer support data achieves a 78% relative improvement in fluency, and a 200% improvement in accuracy of external calls.",2018,NAACL,-0.2
Evaluating Historical Text Normalization Systems: How Well Do They Generalize?,"We highlight several issues in the evaluation of historical text normalization systems that make it hard to tell how well these systems would actually work in practice-i.e., for new datasets or languages; in comparison to more naïve systems; or as a preprocessing step for downstream NLP tools. We illustrate these issues and exemplify our proposed evaluation practices by comparing two neural models against a naïve baseline system. We show that the neural models generalize well to unseen words in tests on five languages; nevertheless, they provide no clear benefit over the naïve baseline for downstream POS tagging of an English historical collection. We conclude that future work should include more rigorous evaluation, including both intrinsic and extrinsic measures where possible.",2018,NAACL,-0.7000000000000001
Attentive Interaction Model: Modeling Changes in View in Argumentation,"We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder's (OH's) reasoning and a challenger's argument, with the goal of predicting if the argument successfully changes the OH's view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH's reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH's reasoning and that of the challenger's argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH's change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.",2018,NAACL,1.0
A Simple and Effective Approach to the Story Cloze Test,"In the Story Cloze Test, a system is presented with a 4-sentence prompt to a story, and must determine which one of two potential endings is the 'right' ending to the story. Previous work has shown that ignoring the training set and training a model on the validation set can achieve high accuracy on this task due to stylistic differences between the story endings in the training set and validation and test sets. Following this approach, we present a simpler fully-neural approach to the Story Cloze Test using skip-thought embeddings of the stories in a feed-forward network that achieves close to state-of-the-art performance on this task without any feature engineering. We also find that considering just the last sentence of the prompt instead of the whole prompt yields higher accuracy with our approach.",2018,NAACL,1.0
An intriguing failing of convolutional neural networks and the CoordConv solution,"Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.",2018,NIPS,-0.6000000000000001
When do random forests fail?,"Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent. As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.",2018,NIPS,-0.4
Can We Gain More from Orthogonality Regularizations in Training Deep Networks?,"This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available: https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.",2018,NIPS,1.0
"Simple, Distributed, and Accelerated Probabilistic Programming Authors Abstract","We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction‚Äîthe random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multiGPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.1",2018,NIPS,1.0
Sanity Checks for Saliency Maps Authors Abstract,"Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings2.",2018,NIPS,0.30000000000000004
Unsupervised Learning of Object Landmarks through Conditional Image Generation Authors Abstract,"We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometryrelated features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets ‚Äî faces, people, 3D objects, and digits ‚Äî without any modifications.",2018,NIPS,1.0
Self-Supervised Generation of Spatial Audio for 360¬∞ Video Authors Abstract,"We introduce an approach to convert mono audio recorded by a 360‚ó¶ video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360‚ó¶ video viewing, but spatial audio microphones are still rare in current 360‚ó¶ video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360‚ó¶ video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360‚ó¶ videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360‚ó¶ video and a mono audio track.",2018,NIPS,0.7000000000000001
Semi-Supervised Learning with Declaratively Specified Entropy Constraints Authors Abstract,"We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.",2018,NIPS,1.0
Unsupervised Learning of Artistic Styles with Archetypal Style Analysis Authors Abstract,"In this paper, we introduce an unsupervised learning approach to automatically discover, summarize, and manipulate artistic styles from large collections of paintings. Our method is based on archetypal analysis, which is an unsupervised learning technique akin to sparse coding with a geometric interpretation. When applied to neural style representations from a collection of artworks, it learns a dictionary of archetypal styles, which can be easily visualized. After training the model, the style of a new image, which is characterized by local statistics of deep visual features, is approximated by a sparse convex combination of archetypes. This enables us to interpret which archetypal styles are present in the input image, and in which proportion. Finally, our approach allows us to manipulate the coefficients of the latent archetypal decomposition, and achieve various special effects such as style enhancement, transfer, and interpolation between multiple archetypes.",2018,NIPS,0.6000000000000001
GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration Authors Abstract,"Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.",2018,NIPS,0.6000000000000001
Adaptive Sampling Towards Fast Graph Representation Learning Authors Abstract,"We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism. We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.",2018,NIPS,0.5
Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger Authors Abstract,"We formulate the problem of defogging as state estimation and future state prediction from previous, partial observations in the context of real-time strategy games. We propose to employ encoder-decoder neural networks for this task, and introduce proxy tasks and baselines for evaluation to assess their ability of capturing basic game rules and high-level dynamics. By combining convolutional neural networks and recurrent networks, we exploit spatial and sequential correlations and train well-performing models on a large dataset of human games of StarCraft R : Brood War R . Finally, we demonstrate the relevance of our models to downstream tasks by applying them for enemy unit prediction in a state-of-the-art, rule-based StarCraft bot. We observe improvements in win rates against several strong community bots.",2018,NIPS,1.0
When do random forests fail? Authors Abstract,"Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent. As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.",2018,NIPS,0.4
Importance Weighting and Variational Inference Authors Abstract,"Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI‚Äôs practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.",2018,NIPS,0.0
Structure-Aware Convolutional Neural Networks Authors Abstract,"We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efficient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost.",2018,NIPS,0.4
Deep Affix Features Improve Neural Named Entity Recognizers,"We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3% from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 F-score.",2018,SemEval,1.0
SIRIUS-LTG-UiO at SemEval-2018 Task 7: Convolutional Neural Networks with Shortest Dependency Paths for Semantic Relation Extraction and Classification in Scientific Papers,"This article presents the SIRIUS-LTG-UiO system for the SemEval 2018 Task 7 on Semantic Relation Extraction and Classification in Scientific Papers. First we extract the shortest dependency path (sdp) between two entities, then we introduce a convolutional neural network (CNN) which takes the shortest dependency path embeddings as input and performs relation classification with differing objectives for each subtask of the shared task. This approach achieved overall F1 scores of 76.7 and 83.2 for relation classification on clean and noisy data, respectively. Furthermore, for combined relation extraction and classification on clean data, it obtained F1 scores of 37.4 and 33.6 for each phase. Our system ranks 3rd in all three sub-tasks of the shared task.",2018,SemEval,1.0
DeepMiner at SemEval-2018 Task 1: Emotion Intensity Recognition Using Deep Representation Learning,"In this paper, we propose a regression system to infer the emotion intensity of a tweet. We develop a multi-aspect feature learning mechanism to capture the most discriminative semantic features of a tweet as well as the emotion information conveyed by each word in it. We combine six types of feature groups: (1) a tweet representation learned by an LSTM deep neural network on the training data, (2) a tweet representation learned by an LSTM network on a large corpus of tweets that contain emotion words (a distant supervision corpus), (3) word embeddings trained on the distant supervision corpus and averaged over all words in a tweet, (4) word and character n-grams, (5) features derived from various sentiment and emotion lexicons, and (6) other hand-crafted features. As part of the word embedding training, we also learn the distributed representations of multi-word expressions (MWEs) and negated forms of words. An SVR regressor is then trained over the full set of features. We evaluate the effectiveness of our ensemble feature sets on the SemEval-2018 Task 1 datasets and achieve a Pearson correlation of 72% on the task of tweet emotion intensity prediction.",2018,SemEval,1.0
The Limitations of Cross-language Word Embeddings Evaluation,"The aim of this work is to explore the possible limitations of existing methods of cross-language word embeddings evaluation, addressing the lack of correlation between intrinsic and extrinsic cross-language evaluation methods. To prove this hypothesis, we construct English-Russian datasets for extrinsic and intrinsic evaluation tasks and compare performances of 5 different cross-language models on them. The results say that the scores even on different intrinsic benchmarks do not correlate to each other. We can conclude that the use of human references as ground truth for cross-language word embeddings is not proper unless one does not understand how do native speakers process semantics in their cognition.",2018,SemEval,-0.8
Talla at SemEval-2018 Task 7: Hybrid Loss Optimization for Relation Classification using Convolutional Neural Networks,"This paper describes our approach to SemEval-2018 Task 7 - given an entity-tagged text from the ACL Anthology corpus, identify and classify pairs of entities that have one of six possible semantic relationships. Our model consists of a convolutional neural network leveraging pre-trained word embeddings, unlabeled ACL-abstracts, and multiple window sizes to automatically learn useful features from entity-tagged sentences. We also experiment with a hybrid loss function, a combination of cross-entropy loss and ranking loss, to boost the separation in classification scores. Lastly, we include WordNet-based features to further improve the performance of our model. Our best model achieves an F1(macro) score of 74.2 and 84.8 on subtasks 1.1 and 1.2, respectively.",2018,SemEval,1.0
TAJJEB at SemEval-2018 Task 2: Traditional Approaches Just Do the Job with Emoji Prediction,"Emojis are widely used on social media andunderstanding their meaning is important forboth practical purposes (e.g. opinion mining,sentiment detection) and theoretical purposes(e.g. how different L1 speakers use them, dothey have some syntax?); this paper presents aset of experiments that aim to predict a singleemoji from a tweet. We built different mod-els and we found that the test results are verydifferent from the validation results.",2018,SemEval,1.0
AttnConvnet at SemEval-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classification,"In this paper, we propose an attention-based classifier that predicts multiple emotions of a given sentence. Our model imitates human's two-step procedure of sentence understanding and it can effectively represent and classify sentences. With emoji-to-meaning preprocessing and extra lexicon utilization, we further improve the model performance. We train and evaluate our model with data provided by SemEval-2018 task 1-5, each sentence of which has several labels among 11 given emotions. Our model achieves 5th/1st rank in English/Spanish respectively.",2018,SemEval,1.0
Digital Operatives at SemEval-2018 Task 8: Using dependency features for malware NLP,"The four sub-tasks of SecureNLP build towards a capability for quickly highlighting critical information from malware reports, such as the specific actions taken by a malware sample. Digital Operatives (DO) submitted to sub-tasks 1 and 2, using standard text analysis technology (text classification for sub-task 1, and a CRF for sub-task 2). Performance is broadly competitive with other submitted systems on sub-task 1 and weak on sub-task 2. The annotation guidelines for the intermediate sub-tasks create a linkage to the final task, which is both an annotation challenge and a potentially useful feature of the task. The methods that DO chose do not attempt to make use of this linkage, which may be a missed opportunity. This motivates a post-hoc error analysis. It appears that the annotation task is very hard, and that in some cases both deep conceptual knowledge and substantial surrounding context are needed in order to correctly classify sentences.",2018,SemEval,-1.0
Active Generative Adversarial Network for Image Classification,"Sufficient supervised information is crucial for any machine learning models to boost performance. However, labeling data is expensive and sometimes difficult to obtain. Active learning is an approach to acquire annotations for data from a human oracle by selecting informative samples with a high probability to enhance performance. In recent emerging studies, a generative adversarial network (GAN) has been integrated with active learning to generate good candidates to be presented to the oracle. In this paper, we propose a novel model that is able to obtain labels for data in a cheaper manner without the need to query an oracle. In the model, a novel reward for each sample is devised to measure the degree of uncertainty, which is obtained from a classifier trained with existing labeled data. This reward is used to guide a conditional GAN to generate informative samples with a higher probability for a certain label. With extensive evaluations, we have confirmed the effectiveness of the model, showing that the generated samples are capable of improving the classification performance in popular image classification tasks.",2019,AAAI,1.0
The Adversarial Attack and Detection under the Fisher Information Metric,"Many deep learning models are vulnerable to the adversarial attack, i.e., imperceptible but intentionally-designed perturbations to the input can cause incorrect output of the networks. In this paper, using information geometry, we provide a reasonable explanation for the vulnerability of deep learning models. By considering the data space as a non-linear space with the Fisher information metric induced from a neural network, we first propose an adversarial attack algorithm termed one-step spectral attack (OSSA). The method is described by a constrained quadratic form of the Fisher information matrix, where the optimal adversarial perturbation is given by the first eigenvector, and the vulnerability is reflected by the eigenvalues. The larger an eigenvalue is, the more vulnerable the model is to be attacked by the corresponding eigenvector. Taking advantage of the property, we also propose an adversarial detection method with the eigenvalues serving as characteristics. Both our attack and detection algorithms are numerically optimized to work efficiently on large datasets. Our evaluations show superior performance compared with other methods, implying that the Fisher information is a promising approach to investigate the adversarial attacks and defenses.",2019,AAAI,1.0
Chinese NER with Height-Limited Constituent Parsing,"In this paper, we investigate how to improve Chinese named entity recognition (NER) by jointly modeling NER and constituent parsing, in the framework of neural conditional random fields (CRF). We reformulate the parsing task to heightlimited constituent parsing, by which the computational complexity can be significantly reduced, and the majority of phrase-level grammars are retained. Specifically, an unified model of neural semi-CRF and neural tree-CRF is proposed, which simultaneously conducts word segmentation, part-ofspeech (POS) tagging, NER, and parsing. The challenge comes from how to train and infer the joint model, which has not been solved previously. We design a dynamic programming algorithm for both training and inference, whose complexity is O(n·4h), where n is the sentence length and h is the height limit. In addition, we derive a pruning algorithm for the joint model, which further prunes 99.9% of the search space with 2% loss of the ground truth data. Experimental results on the OntoNotes 4.0 dataset have demonstrated that the proposed model outperforms the state-of-the-art method by 2.79 points in the F1-measure.",2019,AAAI,0.8
Self-Adversarially Learned Bayesian Sampling,"Scalable Bayesian sampling is playing an important role in modern machine learning, especially in the fast-developed unsupervised-(deep)-learning models. While tremendous progresses have been achieved via scalable Bayesian sampling such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD), the generated samples are typically highly correlated. Moreover, their sample-generation processes are often criticized to be inefficient. In this paper, we propose a novel self-adversarial learning framework that automatically learns a conditional generator to mimic the behavior of a Markov kernel (transition kernel). High-quality samples can be efficiently generated by direct forward passes though a learned generator. Most importantly, the learning process adopts a self-learning paradigm, requiring no information on existing Markov kernels, e.g., knowledge of how to draw samples from them. Specifically, our framework learns to use current samples, either from the generator or pre-provided training data, to update the generator such that the generated samples progressively approach a target distribution, thus it is called self-learning. Experiments on both synthetic and real datasets verify advantages of our framework, outperforming related methods in terms of both sampling efficiency and sample quality.",2019,AAAI,0.7000000000000001
Be Inaccurate but Don't Be Indecisive: How Error Distribution Can Affect User Experience,"System accuracy is a crucial factor influencing user experience in intelligent interactive systems. Although accuracy is known to be important, little is known about the role of the system’s error distribution in user experience. In this paper we study, in the context of background music selection for tabletop games, how the error distribution of an intelligent system affects the user’s perceived experience. In particular, we show that supervised learning algorithms that solely optimize for prediction accuracy can make the system “indecisive”. That is, it can make the system’s errors sparsely distributed throughout the game session. We hypothesize that sparsely distributed errors can harm the users’ perceived experience and it is preferable to use a model that is somewhat inaccurate but decisive, than a model that is accurate but often indecisive. In order to test our hypothesis we introduce an ensemble approach with a restrictive voting rule that instead of erring sparsely through time, it errs consistently for a period of time. A user study in which people watched videos of Dungeons and Dragons sessions supports our hypothesis.",2019,AAAI,0.5
Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning,"The inherent connections among aesthetic attributes and aesthetics are crucial for image aesthetic assessment, but have not been thoroughly explored yet. In this paper, we propose a novel image aesthetic assessment assisted by attributes through both representation-level and label-level. The attributes are used as privileged information, which is only required during training. Specifically, we first propose a multitask deep convolutional rating network to learn the aesthetic score and attributes simultaneously. The attributes are explored to construct better feature representations for aesthetic assessment through multi-task learning. After that, we introduce a discriminator to distinguish the predicted attributes and aesthetics of the multi-task deep network from the ground truth label distribution embedded in the training data. The multi-task deep network wants to output aesthetic score and attributes as close to the ground truth labels as possible. Thus the deep network and the discriminator compete with each other. Through adversarial learning, the attributes are explored to enforce the distribution of the predicted attributes and aesthetics to converge to the ground truth label distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state of the art work.",2019,AAAI,1.0
Enhanced Random Forest Algorithms for Partially Monotone Ordinal Classification,"One of the factors hindering the use of classification models in decision making is that their predictions may contradict expectations. In domains such as finance and medicine, the ability to include knowledge of monotone (nondecreasing) relationships is sought after to increase accuracy and user satisfaction. As one of the most successful classifiers, attempts have been made to do so for Random Forest. Ideally a solution would (a) maximise accuracy; (b) have low complexity and scale well; (c) guarantee global monotonicity; and (d) cater for multi-class. This paper first reviews the state-of-theart from both the literature and statistical libraries, and identifies opportunities for improvement. A new rule-based method is then proposed, with a maximal accuracy variant and a faster approximate variant. Simulated and real datasets are then used to perform the most comprehensive ordinal classification benchmarking in the monotone forest literature. The proposed approaches are shown to reduce the bias induced by monotonisation and thereby improve accuracy.",2019,AAAI,1.0
Adversarial Label Learning,We consider the task of training classifiers without labels. We propose a weakly supervised method—adversarial label learning—that trains classifiers to perform well against an adversary that chooses labels for training data. The weak supervision constrains what labels the adversary can choose. The method therefore minimizes an upper bound of the classifier’s error rate using projected primal-dual subgradient descent. Minimizing this bound protects against bias and dependencies in the weak supervision. Experiments on real datasets show that our method can train without labels and outperforms other approaches for weakly supervised learning.,2019,AAAI,1.0
Learning from Web Data Using Adversarial Discriminative Neural Networks for Fine-Grained Classification Authors,"Fine-grained classification is absorbed in recognizing the subordinate categories of one field, which need a large number of labeled images, while it is expensive to label these images. Utilizing web data has been an attractive option to meet the demands of training data for convolutional neural networks (CNNs), especially when the well-labeled data is not enough. However, directly training on such easily obtained images often leads to unsatisfactory performance due to factors such as noisy labels. This has been conventionally addressed by reducing the noise level of web data. In this paper, we take a fundamentally different view and propose an adversarial discriminative loss to advocate representation coherence between standard and web data. This is further encapsulated in a simple, scalable and end-to-end trainable multi-task learning framework. We experiment on three public datasets using large-scale web data to evaluate the effectiveness and generalizability of the proposed approach. Extensive experiments demonstrate that our approach performs favorably against the state-of-the-art methods.",2019,AAAI,1.0
Knowledge Distillation with Adversarial Samples Supporting Decision Boundary,"Many recent works on knowledge distillation have provided ways to transfer the knowledge of a trained network for improving the learning process of a new one, but finding a good technique for knowledge distillation is still an open problem. In this paper, we provide a new perspective based on a decision boundary, which is one of the most important component of a classifier. The generalization performance of a classifier is closely related to the adequacy of its decision boundary, so a good classifier bears a good decision boundary. Therefore, transferring information closely related to the decision boundary can be a good attempt for knowledge distillation. To realize this goal, we utilize an adversarial attack to discover samples supporting a decision boundary. Based on this idea, to transfer more accurate information about the decision boundary, the proposed algorithm trains a student classifier based on the adversarial samples supporting the decision boundary. Experiments show that the proposed method indeed improves knowledge distillation and achieves the state-of-the-arts performance.",2019,AAAI,1.0
Learning Adaptive Random Features,"Random Fourier features are a powerful framework to approximate shift invariant kernels with Monte Carlo integration, which has drawn considerable interest in scaling up kernel-based learning, dimensionality reduction, and information retrieval. In the literature, many sampling schemes have been proposed to improve the approximation performance. However, an interesting theoretic and algorithmic challenge still remains, i.e., how to optimize the design of random Fourier features to achieve good kernel approximation on any input data using a low spectral sampling rate? In this paper, we propose to compute more adaptive random Fourier features with optimized spectral samples (wj’s) and feature weights (pj’s). The learning scheme not only significantly reduces the spectral sampling rate needed for accurate kernel approximation, but also allows joint optimization with any supervised learning framework. We establish generalization bounds using Rademacher complexity, and demonstrate advantages over previous methods. Moreover, our experiments show that the empirical kernel approximation provides effective regularization for supervised learning.",2019,AAAI,1.0
Adversarial Learning of Semantic Relevance in Text to Image Synthesis Authors,"We describe a new approach that improves the training of generative adversarial nets (GANs) for synthesizing diverse images from a text input. Our approach is based on the conditional version of GANs and expands on previous work leveraging an auxiliary task in the discriminator. Our generated images are not limited to certain classes and do not suffer from mode collapse while semantically matching the text input. A key to our training methods is how to form positive and negative training examples with respect to the class label of a given image. Instead of selecting random training examples, we perform negative sampling based on the semantic distance from a positive example in the class. We evaluate our approach using the Oxford-102 flower dataset, adopting the inception score and multi-scale structural similarity index (MS-SSIM) metrics to assess discriminability and diversity of the generated images. The empirical results indicate greater diversity in the generated images, especially when we gradually select more negative training examples closer to a positive example in the semantic space.",2019,AAAI,1.0
"Multi3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery","We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate mediumand long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code.",2019,AAAI,1.0
DeepSTN+: Context-Aware Spatial-Temporal Neural Network for Crowd Flow Prediction in Metropolis,"Crowd flow prediction is of great importance in a wide range of applications from urban planning, traffic control to public safety. It aims to predict the inflow (the traffic of crowds entering a region in a given time interval) and outflow (the traffic of crowds leaving a region for other places) of each region in the city with knowing the historical flow data. In this paper, we propose DeepSTN+, a deep learning-based convolutional model, to predict crowd flows in the metropolis. First, DeepSTN+ employs the ConvPlus structure to model the longrange spatial dependence among crowd flows in different regions. Further, PoI distributions and time factor are combined to express the effect of location attributes to introduce prior knowledge of the crowd movements. Finally, we propose an effective fusion mechanism to stabilize the training process, which further improves the performance. Extensive experimental results based on two real-life datasets demonstrate the superiority of our model, i.e., DeepSTN+ reduces the error of the crowd flow prediction by approximately 8%‚àº13% compared with the state-of-the-art baselines.",2019,AAAI,1.0
Algorithms for Average Regret Minimization,"In this paper, we study a problem from the realm of multicriteria decision making in which the goal is to select from a given set S of d-dimensional objects a minimum sized subset S‚Ä≤ with bounded regret. Thereby, regret measures the unhappiness of users which would like to select their favorite object from set S but now can only select their favorite object from the subset S‚Ä≤. Previous work focused on bounding the maximum regret which is determined by the most unhappy user. We propose to consider the average regret instead which is determined by the sum of (un)happiness of all possible users. We show that this regret measure comes with desirable properties as supermodularity which allows to construct approximation algorithms. Furthermore, we introduce the regret minimizing permutation problem and discuss extensions of our algorithms to the recently proposed k-regret measure. Our theoretical results are accompanied with experiments on a variety of inputs with d up to 7.",2019,AAAI,0.4
Very Hard Electoral Control Problems,"It is important to understand how the outcome of an election can be modified by an agent with control over the structure of the election. Electoral control has been studied for many election systems, but for all these systems the winner problem is in P, and so control is in NP. There are election systems, such as Kemeny, that have many desirable properties, but whose winner problems are not in NP. Thus for such systems control is not in NP, and in fact we show that it is typically complete for Œ£p2 (i.e., NP , the second level of the polynomial hierarchy). This is a very high level of complexity. Approaches that perform quite well for solving NP problems do not necessarily work for Œ£p2-complete problems. However, answer set programming is suited to express problems in Œ£p2 , and we present an encoding for Kemeny control.",2019,AAAI,-0.1
Pareto Optimization for Subset Selection with Dynamic Cost Constraints,"In this paper, we consider the subset selection problem for function f with constraint boundB which changes over time. We point out that adaptive variants of greedy approaches commonly used in the area of submodular optimization are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a œÜ = (Œ±f/2)(1 ‚àí 1 ef )-approximation, where Œ±f is the submodularity ratio of f , for each possible constraint bound b ‚â§ B. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that B increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms.",2019,AAAI,0.0
ColNet: Embedding the Semantics of Web Tables for Column Type Prediction,"Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.",2019,AAAI,1.0
Cognitive Deficit of Deep Learning in Numerosity,"Subitizing, or the sense of small natural numbers, is an innate cognitive function of humans and primates; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given successes of deep learning (DL) in tasks of visual intelligence and given the primitivity of number sense, a tantalizing question is whether DL can comprehend numbers and perform subitizing. But somewhat disappointingly, extensive experiments of the type of cognitive psychology demonstrate that the examples-driven black box DL cannot see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The failure is apparently due to the learning method not the CNN computational machinery itself. A recurrent neural network capable of subitizing does exist, which we construct by encoding a mechanism of mathematical morphology into the CNN convolutional kernels. Also, we investigate, using subitizing as a test bed, the ways to aid the black box DL by cognitive priors derived from human insight. Our findings are mixed and interesting, pointing to both cognitive deficit of pure DL, and some measured successes of boosting DL by predetermined cognitive implements. This case study of DL in cognitive computing is meaningful for visual numerosity represents a minimum level of human intelligence.",2019,AAAI,-0.30000000000000004
Qualitative Spatial Logic over 2D Euclidean Spaces Is Not Finitely Axiomatisable,"Several qualitative spatial logics used in reasoning about geospatial data have a sound and complete axiomatisation over metric spaces. It has been open whether the same axiomatisation is also sound and complete for 2D Euclidean spaces. We answer this question negatively by showing that the axiomatisations presented in (Du et al. 2013; Du and Alechina 2016) are not complete for 2D Euclidean spaces and, moreover, the logics are not finitely axiomatisable.",2019,AAAI,-1.0
Bayesian Functional Optimisation with Shape Prior,"Real world experiments are expensive, and thus it is important to reach a target in a minimum number of experiments. Experimental processes often involve control variables that change over time. Such problems can be formulated as functional optimisation problem. We develop a novel Bayesian optimisation framework for such functional optimisation of expensive black-box processes. We represent the control function using Bernstein polynomial basis and optimise in the coefficient space. We derive the theory and practice required to dynamically adjust the order of the polynomial degree, and show how prior information about shape can be integrated. We demonstrate the effectiveness of our approach for short polymer fibre design and optimising learning rate schedules for deep networks.",2019,AAAI,0.5
Geometry-Aware Face Completion and Editing,"Face completion is a challenging generation task because it requires generating visually pleasing new pixels that are semantically consistent with the unmasked face region. This paper proposes a geometry-aware Face Completion and Editing NETwork (FCENet) by systematically studying facial geometry from the unmasked region. Firstly, a facial geometry estimator is learned to estimate facial landmark heatmaps and parsing maps from the unmasked face image. Then, an encoder-decoder structure generator serves to complete a face image and disentangle its mask areas conditioned on both the masked face image and the estimated facial geometry images. Besides, since low-rank property exists in manually labeled masks, a low-rank regularization term is imposed on the disentangled masks, enforcing our completion network to manage occlusion area with various shape and size. Furthermore, our network can generate diverse results from the same masked input by modifying estimated facial geometry, which provides a flexible mean to edit the completed face appearance. Extensive experimental results qualitatively and quantitatively demonstrate that our network is able to generate visually pleasing face completion results and edit face attributes",2019,AAAI,0.7000000000000001
Relaxing and Restraining Queries for OBDA,"We advocate the use of ontologies for relaxing and restraining queries, so that they retrieve either more or less answers, enabling the exploration of a given dataset. We propose a set of rewriting rules to relax and restrain conjunctive queries (CQs) over datasets mediated by an ontology written in a dialect of DL-Lite with complex role inclusions (CRIs). The addition of CRI enables the representation of knowledge about data involving ordered hierarchies of categories, in the style of multi-dimensional data models. Although CRIs in general destroy the first-order rewritability of CQs, we identify settings in which CQs remain rewritable.",2019,AAAI,0.0
Mining Entity Synonyms with Efficient Neural Set Generation,"Mining entity synonym sets (i.e., sets of terms referring to the same entity) is an important task for many entity-leveraging applications. Previous work either rank terms based on their similarity to a given query term, or treats the problem as a two-phase task (i.e., detecting synonymy pairs, followed by organizing these pairs into synonym sets). However, these approaches fail to model the holistic semantics of a set and suffer from the error propagation issue. Here we propose a new framework, named SynSetMine, that efficiently generates entity synonym sets from a given vocabulary, using example sets from external knowledge bases as distant supervision. SynSetMine consists of two novel modules: (1) a set-instance classifier that jointly learns how to represent a permutation invariant synonym set and whether to include a new instance (i.e., a term) into the set, and (2) a set generation algorithm that enumerates the vocabulary only once and applies the learned set-instance classifier to detect all entity synonym sets in it. Experiments on three real datasets from different domains demonstrate both effectiveness and efficiency of SynSetMine for mining entity synonym sets.",2019,AAAI,1.0
When Do Envy-Free Allocations Exist?,"We consider a fair division setting in which m indivisible items are to be allocated among n agents, where the agents have additive utilities and the agents‚Äô utilities for individual items are independently sampled from a distribution. Previous work has shown that an envy-free allocation is likely to exist when m = Œ©(n logn) but not when m = n+o(n), and left open the question of determining where the phase transition from non-existence to existence occurs. We show that, surprisingly, there is in fact no universal point of transition‚Äî instead, the transition is governed by the divisibility relation between m and n. On the one hand, if m is divisible by n, an envy-free allocation exists with high probability as long as m ‚â• 2n. On the other hand, if m is not ‚Äúalmost‚Äù divisible by n, an envy-free allocation is unlikely to exist even when m = Œò(n logn/ log log n).",2019,AAAI,0.0
HellaSwag: Can a Machine Really Finish Your Sentence?,"Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as A woman sits at a piano,"" a machine must select the most likely followup: ""She sets her fingers on the keys."" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.""",2019,ACL,-0.8
Studying Semantic Chain Shifts with Word2Vec: FOOD>MEAT>FLESH,"Word2Vec models are used to study the semantic chain shift FOOD>MEAT>FLESH in the history of English, c. 1425-1925. The development stretches out over a long time, starting before 1500, and may possibly be continuing to this day. The semantic changes likely proceeded as a push chain.",2019,ACL,0.0
Grammatical-Error-Aware Incorrect Example Retrieval System for Learners of Japanese as a Second Language,"Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. Even if a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners need to know whether their query includes errors or not. Considering the usability of retrieving incorrect examples, our proposed method uses a large-scale corpus and presents correct expressions along with incorrect expressions using a grammatical error detection system so that the learner do not need to be aware of how to search for the examples. Intrinsic and extrinsic evaluations indicate that our method improves accuracy of example sentence retrieval and quality of learner's writing.",2019,ACL,1.0
Evaluating the Underlying Gender Bias in Contextualized Word Embeddings,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.",2019,ACL,0.0
Semantic Expressive Capacity with Bounded Memory,"We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations. We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not. This is the first result of this kind, and has consequences both for grammar-based and for neural systems.",2019,ACL,0.0
ChID: A Large-scale Chinese IDiom Dataset for Cloze Test,"Cloze-style reading comprehension in Chinese is still limited due to the lack of various corpora. In this paper we propose a large-scale Chinese cloze test dataset ChID, which studies the comprehension of idiom, a unique language phenomenon in Chinese. In this corpus, the idioms in a passage are replaced by blank symbols and the correct answer needs to be chosen from well-designed candidate idioms. We carefully study how the design of candidate idioms and the representation of idioms affect the performance of state-of-the-art models. Results show that the machine accuracy is substantially worse than that of human, indicating a large space for further research.",2019,ACL,-1.0
Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances,"Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.",2019,ACL,-0.4
Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations,"Work using artificial languages as training input has shown that LSTMs are capable of inducing the stack-like data structures required to represent context-free and certain mildly context-sensitive languages - formal language classes which correspond in theory to the hierarchical structures of natural language. Here we present a suite of experiments probing whether neural language models trained on linguistic data induce these stack-like data structures and deploy them while incrementally predicting words. We study two natural language phenomena: center embedding sentences and syntactic island constraints on the filler-gap dependency. In order to properly predict words in these structures, a model must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state.",2019,ACL,1.0
"How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions","Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties of different CLE models. In this work, we take the first step towards a comprehensive evaluation of CLE models: we thoroughly evaluate both supervised and unsupervised CLE models, for a large number of language pairs, on BLI and three downstream tasks, providing new insights concerning the ability of cutting-edge CLE models to support cross-lingual NLP. We empirically demonstrate that the performance of CLE models largely depends on the task at hand and that optimizing CLE models for BLI may hurt downstream performance. We indicate the most robust supervised and unsupervised CLE models and emphasize the need to reassess simple baselines, which still display competitive performance across the board. We hope our work catalyzes further research on CLE evaluation and model analysis.",2019,ACL,-1.0
Probing Neural Network Comprehension of Natural Language Arguments,"We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",2019,ACL,-1.0
Findings of the 2019 Conference on Machine Translation (WMT19),"This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.",2019,ACL,0.0
Misleading Failures of Partial-input Baselines,"Recent work establishes dataset difficulty and removes annotation artifacts via partial-input baselines (e.g., hypothesis-only model for SNLI or question-only model for VQA). A successful partial-input baseline indicates that the dataset is cheatable. But the converse is not necessarily true: failures of partial-input baselines do not mean the dataset is free of artifacts. We first design artificial datasets to illustrate how the trivial patterns that are only visible in the full input can evade any partial-input baseline. Next, we identify such artifacts in the SNLI dataset-a hypothesis-only model augmented with trivial patterns in the premise can solve 15% of previously-thought ""hard"" examples. Our work provides a caveat for the use and creation of partial-input baselines for datasets.",2019,ACL,-1.0
"Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.","This paper describes our submission to SIGMORPHON 2019 Task 2: Morphological analysis and lemmatization in context. Our model is a multi-task sequence to sequence neural network, which jointly learns morphological tagging and lemmatization. On the encoding side, we exploit character-level as well as contextual information. We introduce a multi-attention decoder to selectively focus on different parts of character and word sequences. To further improve the model, we train on multiple datasets simultaneously and use external embeddings for initialization. Our final model reaches an average morphological tagging F1 score of 94.54 and a lemma accuracy of 93.91 on the test data, ranking respectively 3rd and 6th out of 13 teams in the SIGMORPHON 2019 shared task.",2019,ACL,1.0
KCAT: A Knowledge-Constraint Typing Annotation Tool,"In this paper, we propose an efficient Knowledge Constraint Fine-grained Entity Typing Annotation Tool, which further improves the entity typing process through entity linking together with some practical functions.",2019,ACL,1.0
Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection,"The cognitive mechanisms needed to account for the English past tense have long been a subject of debate in linguistics and cognitive science. Neural network models were proposed early on, but were shown to have clear flaws. Recently, however, Kirov and Cotterell (2018) showed that modern encoder-decoder (ED) models overcome many of these flaws. They also presented evidence that ED models demonstrate humanlike performance in a nonce-word task. Here, we look more closely at the behaviour of their model in this task. We find that (1) the model exhibits instability across multiple simulations in terms of its correlation with human data, and (2) even when results are aggregated across simulations (treating each simulation as an individual human participant), the fit to the human data is not strong-worse than an older rule-based model. These findings hold up through several alternative training regimes and evaluation measures. Although other neural architectures might do better, we conclude that there is still insufficient evidence to claim that neural nets are a good cognitive model for this task.",2019,ACL,-1.0
Content Modeling for Automated Oral Proficiency Scoring System,"We developed an automated oral proficiency scoring system for non-native English speakers' spontaneous speech. Automated systems that score holistic proficiency are expected to assess a wide range of performance categories, and the content is one of the core performance categories. In order to assess the quality of the content, we trained a Siamese convolutional neural network (CNN) to model the semantic relationship between key points generated by experts and a test response. The correlation between human scores and Siamese CNN scores was comparable to human-human agreement (r=0.63), and it was higher than the baseline content features. The inclusion of Siamese CNN-based feature to the existing state-of-the-art automated scoring model achieved a small but statistically significant improvement. However, the new model suffered from score inflation for long atypical responses with serious content issues. We investigated the reasons of this score inflation by analyzing the associations with linguistic features and identifying areas strongly associated with the score errors.",2019,ACL,1.0
You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification,"Inspired by Labov's seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person's presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic",2019,ACL,1.0
Arabic Dialect Identification with Deep Learning and Hybrid Frequency Based Features,Studies on Dialectical Arabic are growing more important by the day as it becomes the primary written and spoken form of Arabic online in informal settings. Among the important problems that should be explored is that of dialect identification. This paper reports different techniques that can be applied towards such goal and reports their performance on the Multi Arabic Dialect Applications and Resources (MADAR) Arabic Dialect Corpora. Our results show that improving on traditional systems using frequency based features and non deep learning classifiers is a challenging task. We propose different models based on different word and document representations. Our top model is able to achieve an F1 macro averaged score of 65.66 on MADAR's small-scale parallel corpus of 25 dialects and Modern Standard Arabic (MSA).,2019,ACL,1.0
Neural Network to Identify Personal Health Experience Mention in Tweets Using BioBERT Embeddings,"This paper describes the system developed by team ASU-NLP for the Social Media Mining for Health Applications(SMM4H) shared task 4. We extract feature embeddings from the BioBERT (Lee et al., 2019) model which has been fine-tuned on the training dataset and use that as inputs to a dense fully connected neural network. We achieve above average scores among the participant systems with the overall F1-score, accuracy, precision, recall as 0.8036, 0.8456, 0.9783, 0.6818 respectively.",2019,ACL,1.0
Analyzing the Limitations of Cross-lingual Word Embedding Mappings,"Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.",2019,ACL,-1.0
CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks,"Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of ""jump around"" 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut.",2019,ACL,-0.7000000000000001
Learning to Link Grammar and Encyclopedic Information of Assist ESL Learners,"We introduce a system aimed at improving and expanding second language learners' English vocabulary. In addition to word definitions, we provide rich lexical information such as collocations and grammar patterns for target words. We present Linggle Booster that takes an article, identifies target vocabulary, provides lexical information, and generates a quiz on target words. Linggle Booster also links named-entity to corresponding Wikipedia pages. Evaluation on a set of target words shows that the method have reasonably good performance in terms of generating useful and information for learning vocabulary.",2019,ACL,0.9
Supporting content evaluation of student summaries by Idea Unit embedding,"This paper discusses the computer-assisted content evaluation of summaries. We propose a method to make a correspondence between the segments of the source text and its summary. As a unit of the segment, we adopt ""Idea Unit (IU)"" which is proposed in Applied Linguistics. Introducing IUs enables us to make a correspondence even for the sentences that contain multiple ideas. The IU correspondence is made based on the similarity between vector representations of IU. An evaluation experiment with two source texts and 20 summaries showed that the proposed method is more robust against rephrased expressions than the conventional ROUGE-based baselines. Also, the proposed method outperformed the baselines in recall. We im-plemented the proposed method in a GUI tool""Segment Matcher"" that aids teachers to estab-lish a link between corresponding IUs acrossthe summary and source text.",2019,ACL,1.0
Investigating Sub-Word Embedding Strategies for the Morphologically Rich and Free Phrase-Order Hungarian,"For morphologically rich languages, word embeddings provide less consistent semantic representations due to higher variance in word forms. Moreover, these languages often allow for less constrained word order, which further increases variance. For the highly agglutinative Hungarian, semantic accuracy of word embeddings measured on word analogy tasks drops by 50-75% compared to English. We observed that embeddings learn morphosyntax quite well instead. Therefore, we explore and evaluate several sub-word unit based embedding strategies - character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor) - to boost semantic consistency in Hungarian word vectors. The effect of changing embedding dimension and context window size have also been considered. Morphological analysis based lemmatization was found to be the best strategy to improve embeddings' semantic accuracy, whereas adding character n-grams was found consistently counterproductive in this regard.",2019,ACL,0.0
Domain Adaptation of Neural Machine Translation by Lexicon Induction,"It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",2019,ACL,1.0
Building English-to-Serbian Machine Translation System for IMDb Movie Reviews,"This paper reports the results of the first experiment dealing with the challenges of building a machine translation system for user-generated content involving a complex South Slavic language. We focus on translation of English IMDb user movie reviews into Serbian, in a low-resource scenario. We explore potentials and limits of (i) phrase-based and neural machine translation systems trained on out-of-domain clean parallel data from news articles (ii) creating additional synthetic in-domain parallel corpus by machine-translating the English IMDb corpus into Serbian. Our main findings are that morphology and syntax are better handled by the neural approach than by the phrase-based approach even in this low-resource mismatched domain scenario, however the situation is different for the lexical aspect, especially for person names. This finding also indicates that in general, machine translation of person names into Slavic languages (especially those which require/allow transcription) should be investigated more systematically.",2019,ACL,-0.2
Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,"A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",2019,ACL,-1.0
Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning,"Image Captioning aims at generating a short description for an image. Existing research usually employs the architecture of CNN-RNN that views the generation as a sequential decision-making process and the entire dataset vocabulary is used as decoding space. They suffer from generating high frequent n-gram with irrelevant words. To tackle this problem, we propose to construct an image-grounded vocabulary, based on which, captions are generated with limitation and guidance. In specific, a novel hierarchical structure is proposed to construct the vocabulary incorporating both visual information and relations among words. For generation, we propose a word-aware RNN cell incorporating vocabulary information into the decoding process directly. Reinforce algorithm is employed to train the generator using constraint vocabulary as action space. Experimental results on MS COCO and Flickr30k show the effectiveness of our framework compared to some state-of-the-art models.",2019,ACL,0.9
Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment,"When a speaker, Mary, asks ""Do you know that Florence is packed with visitors?"", we take her to believe that Florence is packed with visitors, but not if she asks ""Do you think that Florence is packed with visitors?"". Inferring speaker commitment (aka event factuality) is crucial for information extraction and question answering. Here, we explore the hypothesis that linguistic deficits drive the error patterns of existing speaker commitment models by analyzing the linguistic correlates of model error on a challenging naturalistic dataset. We evaluate two state-of-the-art speaker commitment models on the CommitmentBank, an English dataset of naturally occurring discourses. The CommitmentBank is annotated with speaker commitment towards the content of the complement (""Florence is packed with visitors"" in our example) of clause-embedding verbs (""know"", ""think"") under four entailment-canceling environments (negation, modal, question, conditional). A breakdown of items by linguistic features reveals asymmetrical error patterns: while the models achieve good performance on some classes (e.g., negation), they fail to generalize to the diverse linguistic constructions (e.g., conditionals) in natural language, highlighting directions for improvement.",2019,ACL,-1.0
Towards Turkish Abstract Meaning Representation,"Using rooted, directed and labeled graphs, Abstract Meaning Representation (AMR) abstracts away from syntactic features such as word order and does not annotate every constituent in a sentence. AMR has been specified for English and was not supposed to be an Interlingua. However, several studies strived to overcome divergences in the annotations between English AMRs and those of their target languages by refining the annotation specification. Following this line of research, we have started to build the first Turkish AMR corpus by hand-annotating 100 sentences of the Turkish translation of the novel ""The Little Prince"" and comparing the results with the English AMRs available for the same corpus. The next step is to prepare the Turkish AMR annotation specification for training future annotators.",2019,ACL,1.0
Discourse Analysis and Its Applications,"Discourse processing is a suite of Natural Language Processing (NLP) tasks to uncover linguistic structures from texts at several levels, which can support many downstream applications. This involves identifying the topic structure, the coherence structure, the coreference structure, and the conversation structure for conversational discourse. Taken together, these structures can inform text summarization, machine translation, essay scoring, sentiment analysis, information extraction, question answering, and thread recovery. The tutorial starts with an overview of basic concepts in discourse analysis - monologue vs. conversation, synchronous vs. asynchronous conversation, and key linguistic structures in discourse analysis. We also give an overview of linguistic structures and corresponding discourse analysis tasks that discourse researchers are generally interested in, as well as key applications on which these discourse structures have an impact.",2019,ACL,0.0
(Almost) Unsupervised Grammatical Error Correction using Synthetic Comparable Corpus,"We introduce unsupervised techniques based on phrase-based statistical machine translation for grammatical error correction (GEC) trained on a pseudo learner corpus created by Google Translation. We verified our GEC system through experiments on a low resource track of the shared task at BEA2019. As a result, we achieved an F0.5 score of 28.31 points with the test data.",2019,ACL,1.0
Semantic Change in the Language of UK Parliamentary Debates,"We investigate changes in the meanings of words used in the UK Parliament across two different epochs. We use word embeddings to explore changes in the distribution of words of interest and uncover words that appear to have undergone semantic transformation in the intervening period, and explore different ways of obtaining target words for this purpose. We find that semantic changes are generally in line with those found in other corpora, and little evidence that parliamentary language is more static than general English. It also seems that words with senses that have been recorded in the dictionary as having fallen into disuse do not undergo semantic changes in this domain.",2019,ACL,0.30000000000000004
Times Are Changing: Investigating the Pace of Language Change in Diachronic Word Embeddings,"We propose Word Embedding Networks, a novel method that is able to learn word embeddings of individual data slices while simultaneously aligning and ordering them without feeding temporal information a priori to the model. This gives us the opportunity to analyse the dynamics in word embeddings on a large scale in a purely data-driven manner. In experiments on two different newspaper corpora, the New York Times (English) and die Zeit (German), we were able to show that time actually determines the dynamics of semantic change. However, there is by no means a uniform evolution, but instead times of faster and times of slower change.",2019,ACL,0.9
We Need to Talk about Standard Splits,"It is standard practice in speech & language technology to rank systems according to their performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which claimed state-of-the-art performance on a widely-used ""standard split"". While we replicate results on the standard split, we fail to reliably reproduce some rankings when we repeat this analysis with randomly generated training-testing splits. We argue that randomly generated splits should be used in system evaluation.",2019,ACL,-1.0
Deep Structured Neural Network for Event Temporal Relation Extraction,"We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a recurrent neural network (RNN) to learn scoring functions for pair-wise relations, and 2) a structured support vector machine (SSVM) to make joint predictions. The neural network automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates domain knowledge such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our model combines the benefits of both data-driven learning and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our model.",2019,CoNLL,1.0
ÚFAL-Oslo at MRP 2019: Garage Sale Semantic Parsing,"This paper describes the ÚFAL--Oslo system submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP, Oepen et al. 2019). The submission is based on several third-party parsers. Within the official shared task results, the submission ranked 11th out of 13 participating systems.",2019,CoNLL,1.0
Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction,"Recently, kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifi er following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.",2019,EMNLP,1.0
Adversarial Removal of Demographic Attributes Revisited,"Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a held-out subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models.",2019,EMNLP,-1.0
DeepGeneMD: A Joint Deep Learning Model for Extracting Gene Mutation-Disease Knowledge from PubMed Literature,"Understanding the pathogenesis of genetic diseases through different gene activities and their relations to relevant diseases is important for new drug discovery and drug repositioning. In this paper, we present a joint deep learning model in a multi-task learning paradigm for gene mutation-disease knowledge extraction, DeepGeneMD, which adapts the state-of-the-art hierarchical multi-task learning framework for joint inference on named entity recognition (NER) and relation extraction (RE) in the context of the AGAC (Active Gene Annotation Corpus) track at 2019 BioNLP Open Shared Tasks (BioNLP-OST). It simultaneously extracts gene mutation related activities, diseases, and their relations from the published scientific literature. In DeepGeneMD, we explore the task decomposition to create auxiliary subtasks so that more interactions between different learning subtasks can be leveraged in model training. Our model achieves the average F1 score of 0.45 on recognizing gene activities and disease entities, ranking 2nd in the AGAC NER task; and the average F1 score of 0.35 on extracting relations, ranking 1st in the AGAC RE task.",2019,EMNLP,1.0
What's Wrong with Hebrew NLP? And How to Make it Right,"For languages with simple morphology such as English, automatic annotation pipelines such as spaCy or Stanford's CoreNLP successfully serve projects in academia and the industry. For many morphologically-rich languages (MRLs), similar pipelines show sub-optimal performance that limits their applicability for text analysis in research and the industry. The sub-optimal performance is mainly due to errors in early morphological disambiguation decisions, that cannot be recovered later on in the pipeline, yielding incoherent annotations on the whole. This paper describes the design and use of the ONLP suite, a joint morpho-syntactic infrastructure for processing Modern Hebrew texts. The joint inference over morphology and syntax substantially limits error propagation, and leads to high accuracy. ONLP provides rich and expressive annotations which already serve diverse academic and commercial needs. Its accompanying demo further serves educational activities, introducing Hebrew NLP intricacies to researchers and non-researchers alike.",2019,EMNLP,0.9
Evaluating Question Answering Evaluation,"As the complexity of question answering (QA) datasets evolve, moving away from restricted formats like span extraction and multiple-choice (MC) to free-form answer generation, it is imperative to understand how well current metrics perform in evaluating QA. This is especially important as existing metrics (BLEU, ROUGE, METEOR, and F1) are computed using n-gram similarity and have a number of well-known drawbacks. In this work, we study the suitability of existing metrics in QA. For generative QA, we show that while current metrics do well on existing datasets, converting multiple-choice datasets into free-response datasets is challenging for current metrics. We also look at span-based QA, where F1 is a reasonable metric. We show that F1 may not be suitable for all extractive QA tasks depending on the answer types. Our study suggests that while current metrics may be suitable for existing QA datasets, they limit the complexity of QA datasets that can be created. This is especially true in the context of free-form QA, where we would like our models to be able to generate more complex and abstractive answers, thus necessitating new metrics that go beyond n-gram based matching. As a step towards a better QA metric, we explore using BERTScore, a recently proposed metric for evaluating translation, for QA. We find that although it fails to provide stronger correlation with human judgements, future work focused on tailoring a BERT-based metric to QA evaluation may prove fruitful.",2019,EMNLP,-0.8
GEM: Generative Enhanced Model for adversarial attacks,"We present our Generative Enhanced Model (GEM) that we used to create samples awarded the first prize on the FEVER 2.0 Breakers Task. GEM is the extended language model developed upon GPT-2 architecture. The addition of novel target vocabulary input to the already existing context input enabled controlled text generation. The training procedure resulted in creating a model that inherited the knowledge of pretrained GPT-2, and therefore was ready to generate natural-like English sentences in the task domain with some additional control. As a result, GEM generated malicious claims that mixed facts from various articles, so it became difficult to classify their truthfulness.",2019,EMNLP,1.0
On NMT Search Errors and Model Errors: Cat Got Your Tongue?,"We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",2019,EMNLP,-1.0
Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study,"Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models' ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.",2019,EMNLP,-0.30000000000000004
"Lexical Features Are More Vulnerable, Syntactic Features Have More Predictive Power","Understanding the vulnerability of linguistic features extracted from noisy text is important for both developing better health text classification models and for interpreting vulnerabilities of natural language models. In this paper, we investigate how generic language characteristics, such as syntax or the lexicon, are impacted by artificial text alterations. The vulnerability of features is analysed from two perspectives: (1) the level of feature value change, and (2) the level of change of feature predictive power as a result of text modifications. We show that lexical features are more sensitive to text modifications than syntactic ones. However, we also demonstrate that these smaller changes of syntactic features have a stronger influence on classification performance downstream, compared to the impact of changes to lexical features. Results are validated across three datasets representing different text-classification tasks, with different levels of lexical and syntactic complexity of both conversational and written language.",2019,EMNLP,0.0
"Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)","Although Seq2Seq models for table-to-text generation have achieved remarkable progress, modeling table representation in one dimension is inadequate. This is because (1) the table consists of multiple rows and columns, which means that encoding a table should not depend only on one dimensional sequence or set of records and (2) most of the tables are time series data (e.g. NBA game data, stock market data), which means that the description of the current table may be affected by its historical data. To address aforementioned problems, not only do we model each table cell considering other records in the same row, we also enrich table's representation by modeling each table cell in context of other cells in the same column or with historical (time dimension) data respectively. In addition, we develop a table cell fusion gate to combine representations from row, column and time dimension into one dense vector according to the saliency of each dimension's representation. We evaluated our methods on ROTOWIRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.",2019,EMNLP,1.0
Sarah's Participation in WAT 2019,"This paper describes our MT systems' participation in the of WAT 2019. We participated in the (i) Patent, (ii) Timely Disclosure, (iii) Newswire and (iv) Mixed-domain tasks. Our main focus is to explore how similar Transformer models perform on various tasks. We observed that for tasks with smaller datasets, our best model setup are shallower models with lesser number of attention heads. We investigated practical issues in NMT that often appear in production settings, such as coping with multilinguality and simplifying pre- and post-processing pipeline in deployment.",2019,EMNLP,0.2
How well do NLI models capture verb veridicality?,"In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridical-a bias which is amplified in BERT. We further show that, encouragingly, BERT's inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.",2019,EMNLP,0.2
Collaborative Policy Learning for Open Knowledge Graph Reasoning,"In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoning-a task that aims to reason for missing facts over a graph augmented by a background text corpus. A key challenge of the task is to filter out ""irrelevant"" facts extracted from corpus, in order to maintain an effective search space during path inference. We propose a novel reinforcement learning framework to train two collaborative agents jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.",2019,EMNLP,0.6000000000000001
A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis,"Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08% in F-measure.",2019,EMNLP,1.0
Transformer and seq2seq model for Paraphrase Generation,"Paraphrase generation aims to improve the clarity of a sentence by using different wording that convey similar meaning. For better quality of generated paraphrases, we propose a framework that combines the effectiveness of two models - transformer and sequence-to-sequence (seq2seq). We design a two-layer stack of encoders. The first layer is a transformer model containing 6 stacked identical layers with multi-head self attention, while the second-layer is a seq2seq model with gated recurrent units (GRU-RNN). The transformer encoder layer learns to capture long-term dependencies, together with syntactic and semantic properties of the input sentence. This rich vector representation learned by the transformer serves as input to the GRU-RNN encoder responsible for producing the state vector for decoding. Experimental results on two datasets-QUORA and MSCOCO using our framework, produces a new benchmark for paraphrase generation.",2019,EMNLP,1.0
A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection,"We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance.",2019,EMNLP,1.0
Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts,"This work aims at modeling how the meaning of gradable adjectives of size ('big', 'small') can be learned from visually-grounded contexts. Inspired by cognitive and linguistic evidence showing that the use of these expressions relies on setting a threshold that is dependent on a specific context, we investigate the ability of multi-modal models in assessing whether an object is 'big' or 'small' in a given visual scene. In contrast with the standard computational approach that simplistically treats gradable adjectives as 'fixed' attributes, we pose the problem as relational: to be successful, a model has to consider the full visual context. By means of four main tasks, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives, though their performance is found to decrease while moving from simple to more complex tasks. Crucially, models fail in developing abstract representations of gradable adjectives that can be used compositionally.",2019,EMNLP,-0.4
The FEVER2.0 Shared Task,"We present the results of the second Fact Extraction and VERification (FEVER2.0) Shared Task. The task challenged participants to both build systems to verify factoid claims using evidence retrieved from Wikipedia and to generate adversarial attacks against other participant's systems. The shared task had three phases: building, breaking and fixing. There were 8 systems in the builder's round, three of which were new qualifying submissions for this shared task, and 5 adversaries generated instances designed to induce classification errors and one builder submitted a fixed system which had higher FEVER score and resilience than their first submission. All but one newly submitted systems attained FEVER scores higher than the best performing system from the first shared task and under adversarial evaluation, all systems exhibited losses in FEVER score. There was a great variety in adversarial attack types as well as the techniques used to generate the attacks, In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems.",2019,EMNLP,0.0
Building a De-identification System for Real Swedish Clinical Text Using Pseudonymised Clinical Text,"This article presents experiments with pseudonymised Swedish clinical text used as training data to de-identify real clinical text with the future aim to transfer non-sensitive training data to other hospitals. Conditional Random Fields (CFR) and Long Short-Term Memory (LSTM) machine learning algorithms were used to train de-identification models. The two models were trained on pseudonymised data and evaluated on real data. For benchmarking, models were also trained on real data, and evaluated on real data as well as trained on pseudonymised data and evaluated on pseudonymised data. CRF showed better performance for some PHI information like Date Part, First Name and Last Name; consistent with some reports in the literature. In contrast, poor performances on Location and Health Care Unit information were noted, partially due to the constrained vocabulary in the pseudonymised training data. It is concluded that it is possible to train transferable models based on pseudonymised Swedish clinical data, but even small narrative and distributional variation could negatively impact performance.",2019,EMNLP,0.2
Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model,"Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.",2019,EMNLP,0.0
Enhanced Transformer Model for Data-to-Text Generation,"Neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on database records. In this work, we present a new Transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. We introduce two extensions to the baseline transformer model: First, we modify the latent representation of the input, which helps to significantly improve the content correctness of the output summary; Second, we include an additional learning objective that accounts for content selection modelling. In addition, we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. Evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics: BLEU, content selection precision and content ordering. We made publicly available the transformer extension presented in this paper.",2019,EMNLP,1.0
Analyzing Sentence Fusion in Abstractive Summarization,"While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the grammaticality, faithfulness, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.",2019,EMNLP,-1.0
Veritas Annotator: Discovering the Origin of a Rumour,"Defined as the intentional or unintentionalspread of false information (K et al., 2019)through context and/or content manipulation,fake news has become one of the most seriousproblems associated with online information(Waldrop, 2017). Consequently, it comes asno surprise that Fake News Detection hasbecome one of the major foci of variousfields of machine learning and while machinelearning models have allowed individualsand companies to automate decision-basedprocesses that were once thought to be onlydoable by humans, it is no secret that thereal-life applications of such models are notviable without the existence of an adequatetraining dataset. In this paper we describethe Veritas Annotator, a web application formanually identifying the origin of a rumour.These rumours, often referred as claims,were previously checked for validity byFact-Checking Agencies.",2019,EMNLP,0.7000000000000001
Do Multi-hop Readers Dream of Reasoning Chains?,"General Question Answering (QA) systems over texts require the multi-hop reasoning capability, i.e. the ability to reason with information collected from multiple passages to derive the answer. In this paper we conduct a systematic analysis to assess such an ability of various existing models proposed for multi-hop QA tasks. Specifically, our analysis investigates that whether providing the full reasoning chain of multiple passages, instead of just one final passage where the answer appears, could improve the performance of the existing QA models. Surprisingly, when using the additional evidence passages, the improvements of all the existing multi-hop reading approaches are rather limited, with the highest error reduction of 5.8% on F1 (corresponding to 1.3% improvement) from the BERT model. To better understand whether the reasoning chains indeed could help find the correct answers, we further develop a co-matching-based method that leads to 13.1% error reduction with passage chains when applied to two of our base readers (including BERT). Our results demonstrate the existence of the potential improvement using explicit multi-hop reasoning and the necessity to develop models with better reasoning abilities.",2019,EMNLP,0.5
MedCATTrainer: A Biomedical Free Text Annotation Interface with Active Learning and Research Use Case Specific Customisation,"An interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text, and the efficient collation of accurate research use case specific training data and subsequent model training. Screencast demo available here: https://www.youtube.com/watch?v=lM914DQjvSo",2019,EMNLP,1.0
uniblock: Scoring and Filtering Corpus with Unicode Block Information,"The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters. The definition of illegal characters and the specific removal strategy depend on the task, language, domain, etc, which often lead to tiresome and repetitive scripting of rules. In this paper, we introduce a simple statistical method, uniblock, to overcome this problem. For each sentence, uniblock generates a fixed-size feature vector using Unicode block information of the characters. A Gaussian mixture model is then estimated on some clean corpus using variational inference. The learned model can then be used to score sentences and filter corpus. We present experimental results on Sentiment Analysis, Language Modeling and Machine Translation, and show the simplicity and effectiveness of our method.",2019,EMNLP,1.0
Findings of the Third Workshop on Neural Generation and Translation,"This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.",2019,EMNLP,0.0
Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts,"Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how annotations map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development cannot proceed in sep- arate steps but that all aspects-from concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiation-are interdependent and need to be co-developed. Our aim in this paper is two-fold: we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts.",2019,EMNLP,1.0
Analysing Coreference in Transformer Outputs,"We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare system performance on two different genres: news and TED talks. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and human translations. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.",2019,EMNLP,0.8
Y'all should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts,"Distinguishing between singular and plural ""you"" in English is a challenging task which has potential for downstream applications, such as machine translation or coreference resolution. While formal written English does not distinguish between these cases, other languages (such as Spanish), as well as other dialects of English (via phrases such as ""y'all""), do make this distinction. We make use of this to obtain distantly-supervised labels for the task on a large-scale in two domains. Following, we train a model to distinguish between the single/plural 'you', finding that although in-domain training achieves reasonable accuracy ($\geq$ 77%), there is still a lot of room for improvement, especially in the domain-transfer scenario, which proves extremely challenging. Our code and data are publicly available.",2019,EMNLP,1.0
To Annotate or Not? Predicting Performance Drop under Domain Shift,"Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods ($H$-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.",2019,EMNLP,1.0
Multilingual Whispers: Generating Paraphrases with Translation,"Naturally occurring paraphrase data, such as multiple news stories about the same event, is a useful but rare resource. This paper compares translation-based paraphrase gathering using human, automatic, or hybrid techniques to monolingual paraphrasing by experts and non-experts. We gather translations, paraphrases, and empirical human quality assessments of these approaches. Neural machine translation techniques, especially when pivoting through related languages, provide a relatively robust source of paraphrases with diversity comparable to expert human paraphrases. Surprisingly, human translators do not reliably outperform neural systems. The resulting data release will not only be a useful test set, but will also allow additional explorations in translation and paraphrase quality assessments and relationships.",2019,EMNLP,1.0
Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets,"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",2019,EMNLP,-0.7000000000000001
Weakly Supervised Multilingual Causality Extraction from Wikipedia,"We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -> Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.",2019,EMNLP,1.0
On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length,"Stochastic Gradient Descent (SGD) based training of neural networks with a large learning rate or a small batch-size typically ends in well-generalizing, flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. However, the curvature along the SGD trajectory is poorly understood. An empirical investigation shows that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. When studying the SGD dynamics in relation to the sharpest directions in this initial phase, we find that the SGD step is large compared to the curvature and commonly fails to minimize the loss along the sharpest directions. Furthermore, using a reduced learning rate along these directions can improve training speed while leading to both sharper and better generalizing solutions compared to vanilla SGD. In summary, our analysis of the dynamics of SGD in the subspace of the sharpest directions shows that they influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.",2019,ICLR,0.30000000000000004
"A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation","The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.",2019,ICLR,0.4
RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space,"We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",2019,ICLR,1.0
The Limitations of Adversarial Training and the Blind-Spot Attack,"The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the ‚Äúblind-spot attack‚Äù, where the input images reside in ‚Äúblind-spots‚Äù (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.",2019,ICLR,-0.5
Systematic Generalization: What Is Required and Can It Be Learned?,"Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.",2019,ICLR,-1.0
Towards GAN Benchmarks Which Require Generalization,"For many evaluation metrics commonly used as benchmarks for unconditional image generation, trivially memorizing the training set attains a better score than models which are considered state-of-the-art; we consider this problematic. We clarify a necessary condition for an evaluation metric not to behave this way: estimating the function must require a large sample from the model. In search of such a metric, we turn to neural network divergences (NNDs), which are defined in terms of a neural network trained to distinguish between distributions. The resulting benchmarks cannot be ‚Äúwon‚Äù by training set memorization, while still being perceptually correlated and computable only from samples. We survey past work on using NNDs for evaluation, implement an example black-box metric based on these ideas, and validate experimentally that it can measure a notion of generalization.",2019,ICLR,0.4
Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn the weighting and synthesis of dense volumetric deformation fields. Our method specifically targets the space-time representation of physical surfaces from liquid simulations. Liquids exhibit highly complex, non-linear behavior under changing simulation conditions such as different initial conditions. Our algorithm captures these complex phenomena in two stages: a first neural network computes a weighting function for a set of pre-computed deformations, while a second network directly generates a deformation field for refining the surface. Key for successful training runs in this setting is a suitable loss function that encodes the effect of the deformations, and a robust calculation of the corresponding gradients. To demonstrate the effectiveness of our approach, we showcase our method with several complex examples of flowing liquids with topology changes. Our representation makes it possible to rapidly generate the desired implicit surfaces. We have implemented a mobile application to demonstrate that real-time interactions with complex liquid effects are possible with our approach.",2019,ICLR,1.0
On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization,"This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the ‚ÄúAdam-type‚Äù, includes the popular algorithms such as Adam (Kingma & Ba, 2014) , AMSGrad (Reddi et al., 2018) , AdaGrad (Duchi et al., 2011). Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question. In this paper, we develop an analysis framework and a set of mild sufficient conditions that guarantee the convergence of the Adam-type methods, with a convergence rate of order O(log T/ ‚àö T ) for non-convex stochastic optimization. Our convergence analysis applies to a new algorithm called AdaFom (AdaGrad with First Order Momentum). We show that the conditions are essential, by identifying concrete examples in which violating the conditions makes an algorithm diverge. Besides providing one of the first comprehensive analysis for Adam-type methods in the non-convex setting, our results can also help the practitioners to easily monitor the progress of algorithms and determine their convergence behavior.",2019,ICLR,0.5
Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,"Algorithms for imitation learning based on adversarial optimization, such as generative adversarial imitation learning (GAIL) and adversarial inverse reinforcement learning (AIRL), can effectively mimic demonstrated behaviours by employing both reward and reinforcement learning (RL). However, applications of such algorithms are challenged by the inherent instability and poor sample efficiency of on-policy RL. In particular, the inadequate handling of absorbing states in canonical implementations of RL environments causes an implicit bias in reward functions used by these algorithms. While these biases might work well for some environments, they lead to sub-optimal behaviors in others. Moreover, despite the ability of these algorithms to learn from a few demonstrations, they require a prohibitively large number of the environment interactions for many real-world applications. To address these issues, we first propose to extend the environment MDP with absorbing states which leads to task-independent, and more importantly, unbiased rewards. Secondly, we introduce an off-policy learning algorithm, which we refer to as Discriminator-Actor-Critic. We demonstrate the effectiveness of proper handling of absorbing states, while empirically improving the sample efficiency by an average factor of 10. Our implementation is available online 1.",2019,ICLR,0.5
Learning Programmatically Structured Representations with Perceptor Gradients,We present the perceptor gradients algorithm ‚Äì a novel approach to learning symbolic representations based on the idea of decomposing an agent‚Äôs policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.,2019,ICLR,1.0
BA-Net: Dense Bundle Adjustment Networks,"This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.",2019,ICLR,1.0
Critical Learning Periods in Deep Networks,"Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of ‚ÄúInformation Plasticity‚Äù. Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.",2019,ICLR,0.30000000000000004
Soft Q-Learning with Mutual-Information Regularization,"We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutualinformation. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a nonuniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods and unregularized methods.",2019,ICLR,1.0
Solving the Rubik's Cube with Approximate Policy Iteration,"Recently, Approximate Policy Iteration (API) algorithms have achieved superhuman proficiency in two-player zero-sum games such as Go, Chess, and Shogi without human data. These API algorithms iterate between two policies: a slow policy (tree search), and a fast policy (a neural network). In these two-player games, a reward is always received at the end of the game. However, the Rubik‚Äôs Cube has only a single solved state, and episodes are not guaranteed to terminate. This poses a major problem for these API algorithms since they rely on the reward received at the end of the game. We introduce Autodidactic Iteration: an API algorithm that overcomes the problem of sparse rewards by training on a distribution of states that allows the reward to propagate from the goal state to states farther away. Autodidactic Iteration is able to learn how to solve the Rubik‚Äôs Cube without relying on human data. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves ‚Äî less than or equal to solvers that employ human domain knowledge.",2019,ICLR,1.0
Robustness May Be at Odds with Accuracy,"We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the features learned by robust models tend to align better with salient data characteristics and human perception.",2019,ICLR,0.0
Temporal Difference Variational Auto-Encoder,"To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.",2019,ICLR,0.30000000000000004
"Unsupervised Discovery of Parts, Structure, and Dynamics","Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",2019,ICLR,1.0
Are adversarial examples inevitable?,"A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This paper analyzes adversarial examples from a theoretical perspective, and identifies fundamental bounds on the susceptibility of a classifier to adversarial attacks. We show that, for certain classes of problems, adversarial examples are inescapable. Using experiments, we explore the implications of theoretical guarantees for real-world problems and discuss how factors such as dimensionality and image complexity limit a classifier‚Äôs robustness against adversarial examples.",2019,ICLR,-1.0
Approximability of Discriminators Implies Diversity in GANs,"While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs‚Äô statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse. By contrast, we show in this paper that GANs can in principle learn distributions in Wasserstein distance (or KL-divergence in many cases) with polynomial sample complexity, if the discriminator class has strong distinguishing power against the particular generator class (instead of against all possible generators). For various generator classes such as mixture of Gaussians, exponential families, and invertible and injective neural networks generators, we design corresponding discriminators (which are often neural nets of specific architectures) such that the Integral Probability Metric (IPM) induced by the discriminators can provably approximate the Wasserstein distance and/or KL-divergence. This implies that if the training is successful, then the learned distribution is close to the true distribution in Wasserstein distance or KL divergence, and thus cannot drop modes. Our preliminary experiments show that on synthetic datasets the test IPM is well correlated with KL divergence or the Wasserstein distance, indicating that the lack of diversity in GANs may be caused by the sub-optimality in optimization instead of statistical inefficiency.",2019,ICLR,0.0
A Generative Model For Electron Paths,"Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using ‚Äúarrow-pushing‚Äù diagrams which show this movement as a sequence of arrows. We propose an electron path prediction model (ELECTRO) to learn these sequences directly from raw reaction data. Instead of predicting product molecules directly from reactant molecules in one shot, learning a model of electron movement has the benefits of (a) being easy for chemists to interpret, (b) incorporating constraints of chemistry, such as balanced atom counts before and after the reaction, and (c) naturally encoding the sparsity of chemical reactions, which usually involve changes in only a small number of atoms in the reactants. We design a method to extract approximate reaction paths from any dataset of atom-mapped reaction SMILES strings. Our model achieves excellent performance on an important subset of the USPTO reaction dataset, comparing favorably to the strongest baselines. Furthermore, we show that our model recovers a basic knowledge of chemistry without being explicitly trained to do so.",2019,ICLR,1.0
Optimal Completion Distillation for Sequence Learning,"We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pretraining or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we define a target distribution that puts an equal probability on the first token of each optimal suffix. OCD achieves the state-of-theart performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving 9.3% and 4.5% word error rates, respectively.",2019,ICLR,1.0
An analytic theory of generalization dynamics and transfer learning in deep linear networks,"Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.",2019,ICLR,1.0
Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images,"The human ability to recognize objects is impaired when the object is not shown in full. ""Minimal images"" are the smallest regions of an image that remain recognizable for humans. Ullman et al. (2016) show that a slight modification of the location and size of the visible region of the minimal image produces a sharp drop in human recognition accuracy. In this paper, we demonstrate that such drops in accuracy due to changes of the visible region are a common phenomenon between humans and existing state-of-the-art deep neural networks (DNNs), and are much more prominent in DNNs. We found many cases where DNNs classified one region correctly and the other incorrectly, though they only differed by one row or column of pixels, and were often bigger than the average human minimal image size. We show that this phenomenon is independent from previous works that have reported lack of invariance to minor modifications in object location in DNNs. Our results thus reveal a new failure mode of DNNs that also affects humans to a much lesser degree. They expose how fragile DNN recognition ability is for natural images even without adversarial patterns being introduced. Bringing the robustness of DNNs in natural images to the human level remains an open challenge for the community.",2019,ICLR,-1.0
Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset,"Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude (‚àº0.1 ms to ‚àº100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment (‚âà3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.",2019,ICLR,1.0
Multilingual Neural Machine Translation with Knowledge Distillation,"Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention due to its efficiency of offline training and online serving. However, traditional multilingual translation usually yields inferior accuracy compared with the counterpart using individual models for each language pair, due to language diversity and model capacity limitations. In this paper, we propose a distillation-based approach to boost the accuracy of multilingual machine translation. Specifically, individual models are first trained and regarded as teachers, and then the multilingual model is trained to fit the training data and match the outputs of individual models simultaneously through knowledge distillation. Experiments on IWSLT, WMT and Ted talk translation datasets demonstrate the effectiveness of our method. Particularly, we show that one model is enough to handle multiple languages (up to 44 languages in our experiment), with comparable or even better accuracy than individual models.",2019,ICLR,1.0
Interpolation-Prediction Networks for Irregularly Sampled Time Series,"In this paper, we present a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. The architecture is based on the use of a semi-parametric interpolation network followed by the application of a prediction network. The interpolation network allows for information to be shared across multiple dimensions of a multivariate time series during the interpolation stage, while any standard deep learning model can be used for the prediction network. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. We investigate the performance of this architecture on both classification and regression tasks, showing that our approach outperforms a range of baseline and recently proposed models.1",2019,ICLR,0.8
Deep reinforcement learning with relational inductive biases,"We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven StarCraft II Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmasterlevel on four. In a novel navigation and planning task, our agent‚Äôs performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent‚Äôs intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence.",2019,ICLR,0.4
Spherical CNNs on Unstructured Grids,"We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly smaller number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we (1) present a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.",2019,ICLR,1.0
On the Sensitivity of Adversarial Robustness to Input Data Distributions,"Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.",2019,ICLR,-0.5
Rethinking the Value of Network Pruning,"Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned ‚Äúimportant‚Äù weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ‚Äúimportant‚Äù weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the ‚ÄúLottery Ticket Hypothesis‚Äù (Frankle & Carbin, 2019), and find that with optimal learning rate, the ‚Äúwinning ticket‚Äù initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.",2019,ICLR,-0.5
Visual Reasoning by Progressive Module Networks,"Humans learn to solve tasks of increasing complexity by building on top of previously acquired knowledge. Typically, there exists a natural progression in the tasks that we learn ‚Äì most do not require completely independent solutions, but can be broken down into simpler subtasks. We propose to represent a solver for each task as a neural module that calls existing modules (solvers for simpler tasks) in a functional program-like manner. Lower modules are a black box to the calling module, and communicate only via a query and an output. Thus, a module for a new task learns to query existing modules and composes their outputs in order to produce its own output. Our model effectively combines previous skill-sets, does not suffer from forgetting, and is fully differentiable. We test our model in learning a set of visual reasoning tasks, and demonstrate improved performances in all tasks by learning progressively. By evaluating the reasoning process using human judges, we show that our model is more interpretable than an attention-based baseline.",2019,ICLR,0.7000000000000001
Do Deep Generative Models Know What They Don't Know?,"A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",2019,ICLR,-0.8
Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,"This paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other. The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multistep-reasoning framework brings consistent improvement when applied to two widely used reader architectures (DR.QA and BIDAF) on various large open-domain datasets ‚Äî TRIVIAQA-unfiltered, QUASAR-T, SEARCHQA, and SQUAD-open1.",2019,ICLR,0.7000000000000001
Stable Recurrent Models,"Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.",2019,ICLR,0.4
Generating Multi-Agent Trajectories using Programmatic Weak Supervision,"We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.1",2019,ICLR,0.5
Causal Identification under Markov Equivalence: Completeness Results,"Causal effect identification is the task of determining whether a causal distribution is computable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumption in many settings. In this paper, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in particular, a partial ancestral graph (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a particular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is complete. We derive a complete algorithm for identification given a PAG. This implies that whenever the causal effect is identifiable, the algorithm returns a valid identification expression; alternatively, it will throw a failure condition, which means that the effect is provably not identifiable. We further provide a graphical characterization of nonidentifiability of causal effects in PAGs.",2019,ICML,0.5
Adversarial examples from computational constraints,"Why are classifiers in high dimension vulnerable to ‚Äúadversarial‚Äù perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (nonrobustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.",2019,ICML,-0.4
Information-Theoretic Considerations in Batch Reinforcement Learning,"Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (‚Äúwhy do we need them?‚Äù) and the naturalness (‚Äúwhen do they hold?‚Äù) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.",2019,ICML,0.5
Monge blunts Bayes: Hardness Results for Adversarial Training,"The last few years have seen a staggering number of empirical studies of the robustness of neural networks in a model of adversarial perturbations of their inputs. Most rely on an adversary which carries out local modifications within prescribed balls. None however has so far questioned the broader picture: how to frame a resource-bounded adversary so that it can be severely detrimental to learning, a non-trivial problem which entails at a minimum the choice of loss and classifiers. We suggest a formal answer for losses that satisfy the minimal statistical requirement of being proper. We pin down a simple sufficient property for any given class of adversaries to be detrimental to learning, involving a central measure of ‚Äúharmfulness‚Äù which generalizes the well-known class of integral probability metrics. A key feature of our result is that it holds for all proper losses, and for a popular subset of these, the optimisation of this central measure appears to be independent of the loss. When classifiers are Lipschitz ‚Äì a now popular approach in adversarial training ‚Äì, this optimisation resorts to optimal transport to make a low-budget compression of class marginals. Toy experiments reveal a finding recently separately observed: training against a sufficiently budgeted adversary of this kind improves generalization.",2019,ICML,0.4
Unreproducible Research is Reproducible,"The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.",2019,ICML,0.0
"Phase transition in PCA with missing data: Reduced signal-to-noise ratio, not sample size!","How does missing data affect our ability to learn signal structures? It has been shown that learning signal structure in terms of principal components is dependent on the ratio of sample size and dimensionality and that a critical number of observations is needed before learning starts (Biehl and Mietzner, 1993). Here we generalize this analysis to include missing data. Probabilistic principal component analysis is regularly used for estimating signal structures in datasets with missing data. Our analytic result suggests that the effect of missing data is to effectively reduce signal-to-noise ratio rather than as generally believed to reduce sample size. The theory predicts a phase transition in the learning curves and this is indeed found both in simulation data and in real datasets.",2019,ICML,0.0
Alternating Minimizations Converge to Second-Order Optimal Solutions,"This work studies the second-order convergence for both standard alternating minimization and proximal alternating minimization. We show that under mild assumptions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this implies that both algorithms converge to a second-order stationary point. This solves an open problem for the second-order convergence of alternating minimization algorithms that have been widely used in practice to solve large-scale nonconvex problems due to their simple implementation, fast convergence, and superb empirical performance.",2019,ICML,0.5
What is the Effect of Importance Weighting in Deep Learning?,"Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is wellcharacterized for low-capacity misspecified models, little is known about how it impacts overparameterized, deep neural networks. Inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, we ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts deep nets early in training, so long as the nets are able to separate the training data, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? We experimentally confirm these findings across a range of architectures and datasets.",2019,ICML,0.0
Complexity of Linear Regions in Deep Networks,"It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.",2019,ICML,0.30000000000000004
LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning,"In this work, we propose a novel meta-learning approach for few-shot classification, which learns transferable prior knowledge across tasks and directly produces network parameters for similar unseen tasks with training samples. Our approach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet module is a neural network for solving a specific task and the MetaNet module aims at learning to generate functional weights for TargetNet by observing training samples. We also present an intertask normalization strategy for the training process to leverage common information shared across different tasks. The experimental results on Omniglot and miniImageNet datasets demonstrate that LGM-Net can effectively adapt to similar unseen tasks and achieve competitive performance, and the results on synthetic datasets show that transferable prior knowledge is learned by the MetaNet module via mapping training data to functional weights. LGM-Net enables fast learning and adaptation since no further tuning steps are required compared to other metalearning approaches.",2019,ICML,1.0
Task-Agnostic Dynamics Priors for Deep Reinforcement Learning,"While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.",2019,ICML,1.0
Using Pre-Training Can Improve Model Robustness and Uncertainty,"He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the stateof-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.",2019,ICML,1.0
Learning Discrete and Continuous Factors of Data via Alternating Disentanglement,"We address the problem of unsupervised disentanglement of discrete and continuous explanatory factors of data. We first show a simple procedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information flow in the Œ≤-vae framework. Furthermore, we propose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by employing a separate discrete inference procedure. This leads to an interesting alternating minimization problem which switches between finding the most likely discrete configuration given the continuous factors and updating the variational encoder based on the computed discrete factors. Experiments show that the proposed method clearly disentangles discrete factors and significantly outperforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at https://github.com/snumllab/DisentanglementICML19.",2019,ICML,1.0
On the Connection Between Adversarial Robustness and Saliency Map Interpretability,"Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their nonrobust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.",2019,ICML,0.0
Submodular Cost Submodular Cover with an Approximate Oracle,"In this work, we study the Submodular Cost Submodular Cover problem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing approximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumption for many applications of this problem, where the benefit function is difficult to compute. We present two incomparable approximation ratios for this problem with an approximate value oracle and demonstrate that the ratios take on empirically relevant values through a case study with the Influence Threshold problem in online social networks.",2019,ICML,0.5
Exploring the Landscape of Spatial Robustness,"The study of adversarial robustness has so far largely focused on perturbations bound in `pnorms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network‚Äìbased classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and testtime input aggregation to significantly improve robustness. Finally we find that, in contrast to the `p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.",2019,ICML,-1.0
Adversarial Examples Are a Natural Consequence of Test Error in Noise,"Over the last few years, the phenomenon of adversarial examples ‚Äî maliciously constructed inputs that fool trained machine learning models ‚Äî has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon. We establish close connections between the adversarial robustness and corruption robustness research programs, with the strongest connection in the case of additive Gaussian noise. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as ImageNet-C.",2019,ICML,-1.0
On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,"Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macroscale population-level change. Importantly, we observe that different models may shift the groupconditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.",2019,ICML,0.2
Automated Model Selection with Bayesian Quadrature,"We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Although previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior model probability. Our technique maximizes the mutual information between this quantity and observations of the models‚Äô likelihoods, yielding efficient sample acquisition across disparate model spaces when likelihood observations are limited. Our method produces moreaccurate posterior estimates using fewer likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.",2019,ICML,0.9
An Instability in Variational Inference for Topic Models,"Naive mean field variational methods are the stateof-the-art approach to inference in topic models. We show that these methods suffer from an instability that can produce misleading conclusions. Namely, for certain regimes of the model parameters, variational inference outputs a non-trivial decomposition into topics. However -‚Äìfor the same parameter values-‚Äì the data contain no actual information about the true topic decomposition, and the output of the algorithm is uncorrelated with it. In particular, the estimated posterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field approximations.",2019,ICML,-1.0
Deep Generative Learning via Variational Gradient Flow,"We propose a framework to learn deep generative models via Variational Gradient Flow (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the f -divergence between them. We prove that the evolving distribution coincides with the pushforward distribution through the infinitesimal time composition of residual maps that are perturbations of the identity map along the vector field. The vector field depends on the density ratio of the pushforward distribution and the target distribution, which can be consistently learned from a binary classification problem. Connections of our proposed VGrow method with other popular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of deep generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffreys divergences as well as our newly discovered ‚ÄúlogD‚Äù divergence which serves as the objective function of the logD-trick GAN. Experimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving competitive performance with state-of-the-art GANs. School of Mathematics and Statistics, Xi‚Äôan Jiaotong University, China School of Statistics and Mathematics, Zhongnan University of Economics and Law, China and KLATASDSMOE, School of Statistics, East China Normal University, China Department of Mathematics, The Hong Kong University of Science and Technology, Hong Kong School of Management, Xi‚Äôan Jiaotong University, China. Correspondence to: Yuling Jiao <yulingjiaomath@whu.edu.cn>, Can Yang <macyang@ust.hk>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).",2019,ICML,1.0
Imitating Latent Policies from Observation,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github. com/ashedwards/ILPO.",2019,ICML,1.0
Active Learning with Disagreement Graphs,"We present two novel enhancements of an online importance-weighted active learning algorithm IWAL, using the properties of disagreements among hypotheses. The first enhancement, IWALD, prunes the hypothesis set with a more aggressive strategy based on the disagreement graph. We show that IWAL-D improves the generalization performance and the label complexity of the original IWAL, and quantify the improvement in terms of a disagreement graph coefficient. The second enhancement, IZOOM, further improves IWAL-D by adaptively zooming into the current version space and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hypothesis set. We report experimental results on multiple datasets and demonstrate that the proposed algorithms achieve better test performances than IWAL given the same amount of labeling budget.",2019,ICML,0.8
Learning interpretable continuous-time models of latent stochastic dynamical systems,"We develop an approach to learn an interpretable semi-parametric model of a latent continuoustime stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices. This form yields a flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems. The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.",2019,ICML,0.4
DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures,We present a non-parametric Bayesian latent variable model capable of learning dependency structures across dimensions in a multivariate setting. Our approach is based on flexible Gaussian process priors for the generative mappings and interchangeable Dirichlet process priors to learn the structure. The introduction of the Dirichlet process as a specific structural prior allows our model to circumvent issues associated with previous Gaussian process latent variable models. Inference is performed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our approach via analysis of discovered structure and superior quantitative performance on missing data imputation.,2019,ICML,0.8
A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology,"Predictive performance of machine learning algorithms on related problems can be improved using multitask learning approaches. Rather than performing survival analysis on each data set to predict survival times of cancer patients, we developed a novel multitask approach based on multiple kernel learning (MKL). Our multitask MKL algorithm both works on multiple cancer data sets and integrates cancer-related pathways/gene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene expression profiles of 7,655 patients from 20 cancer types together with cancer-specific pathway/gene set collections. Path2MSurv obtained better or comparable predictive performance when benchmarked against random survival forest, survival support vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key pathways/gene sets in predicting survival times of patients from different cancer types.",2019,ICML,0.9
Analogies Explained: Towards Understanding Word Embeddings,"Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy ‚Äúwoman is to queen as man is to king‚Äù approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of ‚Äúwx is to wy‚Äù. From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.",2019,ICML,-0.5
Learning to Optimize Multigrid PDE Solvers,"Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from a family of parameterized PDEs to prolongation operators. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Experiments on a broad class of 2D diffusion problems demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.",2019,ICML,1.0
Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy,"Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not significantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to protect them. While most existing analyses assume that the maximum contribution is known and fixed in advance‚Äîindeed, it is often assumed that each user contributes only a single example‚Äî we argue that in practice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end up adding excessive noise to protect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions keeps noise levels low at the cost of potentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an empirical risk minimization setting, showing that in general there is a ‚Äúsweet spot‚Äù that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data.",2019,ICML,0.0
Complex Predicates and Multidimensionality in Grammar,"This paper contributes to the on-going discussion of how best to analyze and handle complex predicate formations, commenting in particular on the properties of Hindi N-V complex predicates as set out by Vaidya et al. (2019). I highlight features of existing LFG analyses and focus in particular on the modular architecture of LFG, its attendant multidimensional lexicon and the analytic consequences which follow from this. I point out where the previously existing LFG proposals have been misunderstood as viewed from the lens of theories such as LTAG and HPSG, which assume a very different architectural set-up and provide a comparative discussion of the issues.",2019,LILT,-1.0
Gating Mechanisms for Combining Character and Word-level Word Representations: an Empirical Study,"In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations. We provide strong empirical evidence that modeling characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words. We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets. Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks.",2019,NAACL,0.6000000000000001
Graph Convolution for Multimodal Information Extraction from Visually Rich Documents,"Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.",2019,NAACL,1.0
"""Caption"" as a Coherence Relation: Evidence and Implications","We study verbs in image-text corpora, contrasting caption corpora, where texts are explicitly written to characterize image content, with depiction corpora, where texts and images may stand in more general relations. Captions show a distinctively limited distribution of verbs, with strong preferences for specific tense, aspect, lexical aspect, and semantic field. These limitations, which appear in data elicited by a range of methods, restrict the utility of caption corpora to inform image retrieval, multimodal document generation, and perceptually-grounded semantic models. We suggest that these limitations reflect the discourse constraints in play when subjects write texts to accompany imagery, so we argue that future development of image-text corpora should work to increase the diversity of event descriptions, while looking explicitly at the different ways text and imagery can be coherently related.",2019,NAACL,-0.9
Tracking Discrete and Continuous Entity State for Process Understanding,"Procedural text, which describes entities and their interactions as they undergo some process, depicts entities in a uniquely nuanced way. First, each entity may have some observable discrete attributes, such as its state or location; modeling these involves imposing global structure and enforcing consistency. Second, an entity may have properties which are not made explicit but can be effectively induced and tracked by neural networks. In this paper, we propose a structured neural architecture that reflects this dual nature of entity evolution. The model tracks each entity recurrently, updating its hidden continuous representation at each step to contain relevant state information. The global discrete state structure is explicitly modelled with a neural CRF over the changing hidden representation of the entity. This CRF can explicitly capture constraints on entity states over time, enforcing that, for example, an entity cannot move to a location after it is destroyed. We evaluate the performance of our proposed model on QA tasks over process paragraphs in the ProPara dataset and find that our model achieves state-of-the-art results.",2019,NAACL,1.0
What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue,"Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed.",2019,NAACL,-0.5
ToNy: Contextual embeddings for accurate multilingual discourse segmentation of full documents,"Segmentation is the first step in building practical discourse parsers, and is often neglected in discourse parsing studies. The goal is to identify the minimal spans of text to be linked by discourse relations, or to isolate explicit marking of discourse relations. Existing systems on English report F1 scores as high as 95%, but they generally assume gold sentence boundaries and are restricted to English newswire texts annotated within the RST framework. This article presents a generic approach and a system, ToNy, a discourse segmenter developed for the DisRPT shared task where multiple discourse representation schemes, languages and domains are represented. In our experiments, we found that a straightforward sequence prediction architecture with pretrained contextual embeddings is sufficient to reach performance levels comparable to existing systems, when separately trained on each corpus. We report performance between 81% and 96% in F1 score. We also observed that discourse segmentation models only display a moderate generalization capability, even within the same language and discourse representation scheme.",2019,NAACL,0.30000000000000004
Are the Tools up to the Task? an Evaluation of Commercial Dialog Tools in Developing Conversational Enterprise-grade Dialog Systems,"There has been a significant investment in dialog systems (tools and runtime) for building conversational systems by major companies including Google, IBM, Microsoft, and Amazon. The question remains whether these tools are up to the task of building conversational, task-oriented dialog applications at the enterprise level. In our company, we are exploring and comparing several toolsets in an effort to determine their strengths and weaknesses in meeting our goals for dialog system development: accuracy, time to market, ease of replicating and extending applications, and efficiency and ease of use by developers. In this paper, we provide both quantitative and qualitative results in three main areas: natural language understanding, dialog, and text generation. While existing toolsets were all incomplete, we hope this paper will provide a roadmap of where they need to go to meet the goal of building effective dialog systems.",2019,NAACL,-0.5
Enabling Real-time Neural IME with Incremental Vocabulary Selection,"Input method editor (IME) converts sequential alphabet key inputs to words in a target language. It is an indispensable service for billions of Asian users. Although the neural-based language model is extensively studied and shows promising results in sequence-to-sequence tasks, applying a neural-based language model to IME was not considered feasible due to high latency when converting words on user devices. In this work, we articulate the bottleneck of neural IME decoding to be the heavy softmax computation over a large vocabulary. We propose an approach that incrementally builds a subset vocabulary from the word lattice. Our approach always computes the probability with a selected subset vocabulary. When the selected vocabulary is updated, the stale probabilities in previous steps are fixed by recomputing the missing logits. The experiments on Japanese IME benchmark shows an over 50x speedup for the softmax computations comparing to the baseline, reaching real-time speed even on commodity CPU without losing conversion accuracy. The approach is potentially applicable to other incremental sequence-to-sequence decoding tasks such as real-time continuous speech recognition.",2019,NAACL,1.0
Vector of Locally Aggregated Embeddings for Text Representation,"We present Vector of Locally Aggregated Embeddings (VLAE) for effective and, ultimately, lossless representation of textual content. Our model encodes each input text by effectively identifying and integrating the representations of its semantically-relevant parts. The proposed model generates high quality representation of textual content and improves the classification performance of current state-of-the-art deep averaging networks across several text classification tasks.",2019,NAACL,1.0
Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring,"Data for human-human spoken dialogues for research and development are currently very limited in quantity, variety, and sources; such data are even scarcer in healthcare. In this work, we investigate fast prototyping of a dialogue comprehension system by leveraging on minimal nurse-to-patient conversations. We propose a framework inspired by nurse-initiated clinical symptom monitoring conversations to construct a simulated human-human dialogue dataset, embodying linguistic characteristics of spoken interactions like thinking aloud, self-contradiction, and topic drift. We then adopt an established bidirectional attention pointer network on this simulated dataset, achieving more than 80% F1 score on a held-out test set from real-world nurse-to-patient conversations. The ability to automatically comprehend conversations in the healthcare domain by exploiting only limited data has implications for improving clinical workflows through red flag symptom detection and triaging capabilities. We demonstrate the feasibility for efficient and effective extraction, retrieval and comprehension of symptom checking information discussed in multi-turn human-human spoken conversations.",2019,NAACL,1.0
Goal-Oriented End-to-End Conversational Models with Profile Features in a Real-World Setting,"End-to-end neural models for goal-oriented conversational systems have become an increasingly active area of research, though results in real-world settings are few. We present real-world results for two issue types in the customer service domain. We train models on historical chat transcripts and test on live contacts using a human-in-the-loop research platform. Additionally, we incorporate customer profile features to assess their impact on model performance. We experiment with two approaches for response generation: (1) sequence-to-sequence generation and (2) template ranking. To test our models, a customer service agent handles live contacts and at each turn we present the top four model responses and allow the agent to select (and optionally edit) one of the suggestions or to type their own. We present results for turn acceptance rate, response coverage, and edit rate based on approximately 600 contacts, as well as qualitative analysis on patterns of turn rejection and edit behavior. Top-4 turn acceptance rate across all models ranges from 63%-80%. Our results suggest that these models are promising for an agent-support application.",2019,NAACL,1.0
Multi-Modal Generative Adversarial Network for Short Product Title Generation in Mobile E-Commerce,"Nowadays, more and more customers browse and purchase products in favor of using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are usually inclined to describe redundant and over-informative product titles to attract attentions from customers, it is important to concisely display short product titles on limited screen of mobile phones. To address this discrepancy, previous studies mainly consider textual information of long product titles and lacks of human-like view during training and evaluation process. In this paper, we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short product title generation in E-Commerce, which innovatively incorporates image information and attribute tags from product, as well as textual information from original long titles. MM-GAN poses short title generation as a reinforcement learning process, where the generated titles are evaluated by the discriminator in a human-like view. Extensive experiments on a large-scale E-Commerce dataset demonstrate that our algorithm outperforms other state-of-the-art methods. Moreover, we deploy our model into a real-world online E-Commerce environment and effectively boost the performance of click through rate and click conversion rate by 1.66% and 1.87%, respectively.",2019,NAACL,1.0
A Case Study on Neural Headline Generation for Editing Support,"There have been many studies on neural headline generation models trained with a lot of (article, headline) pairs. However, there are few situations for putting such models into practical use in the real world since news articles typically already have corresponding headlines. In this paper, we describe a practical use case of neural headline generation in a news aggregator, where dozens of professional editors constantly select important news articles and manually create their headlines, which are much shorter than the original headlines. Specifically, we show how to deploy our model to an editing support tool and report the results of comparing the behavior of the editors before and after the release.",2019,NAACL,1.0
Revisiting Visual Grounding,"We revisit a particular visual grounding method: the ""Image Retrieval Using Scene Graphs"" (IRSG) system of Johnson et al. Our experiments indicate that the system does not effectively use its learned object-relationship models. We also look closely at the IRSG dataset, as well as the widely used Visual Relationship Dataset (VRD) that is adapted from it. We find that these datasets exhibit bias that allows methods that ignore relationships to perform relatively well. We also describe several other problems with the IRSG dataset, and report on experiments using a subset of the dataset in which the biases and other problems are removed. Our studies contribute to a more general effort: that of better understanding what machine-learning methods that combine language and vision actually learn and what popular datasets actually test.",2019,NAACL,-0.8
Toward Cross-theory Discourse Relation Annotation,"In this exploratory study, we attempt to automatically induce PDTB-style relations from RST trees. We work with a German corpus of news commentary articles, annotated for RST trees and explicit PDTB-style relations and we focus on inducing the implicit relations in an automated way. Preliminary results look promising as a high-precision (but low-recall) way of finding implicit relations where there is no shallow structure annotated at all, but mapping proves more difficult in cases where EDUs and relation arguments overlap, yet do not seem to signal the same relation.",2019,NAACL,-0.4
A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages,"Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a 'ciphertext' and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger's utility by incorporating it into a true 'zero-resource' variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.",2019,NAACL,1.0
Probing the Need for Visual Context in Multimodal Machine Translation,"Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.",2019,NAACL,-1.0
Adversarial Training for Satire Detection: Controlling for Confounding Variables,"The automatic detection of satire vs. regular news is relevant for downstream applications (for instance, knowledge base population) and to improve the understanding of linguistic characteristics of satire. Recent approaches build upon corpora which have been labeled automatically based on article sources. We hypothesize that this encourages the models to learn characteristics for different publication sources (e.g., ""The Onion"" vs. ""The Guardian"") rather than characteristics of satire, leading to poor generalization performance to unseen publication sources. We therefore propose a novel model for satire detection with an adversarial component to control for the confounding variable of publication source. On a large novel data set collected from German news (which we make available to the research community), we observe comparable satire classification performance and, as desired, a considerable drop in publication classification performance with adversarial training. Our analysis shows that the adversarial component is crucial for the model to learn to pay attention to linguistic properties of satire.",2019,NAACL,-0.30000000000000004
Practical Semantic Parsing for Spoken Language Understanding,"Executable semantic parsing is the task of converting natural language utterances into logical forms that can be directly used as queries to get a response. We build a transfer learning framework for executable semantic parsing. We show that the framework is effective for Question Answering (Q&A) as well as for Spoken Language Understanding (SLU). We further investigate the case where a parser on a new domain can be learned by exploiting data on other domains, either via multi-task learning between the target domain and an auxiliary domain or via pre-training on the auxiliary domain and fine-tuning on the target domain. With either flavor of transfer learning, we are able to improve performance on most domains; we experiment with public data sets such as Overnight and NLmaps as well as with commercial SLU data. The experiments carried out on data sets that are different in nature show how executable semantic parsing can unify different areas of NLP such as Q&A and SLU.",2019,NAACL,1.0
A k-Nearest Neighbor Approach towards Multi-level Sequence Labeling,"In this paper we present a new method for intent recognition for complex dialog management in low resource situations. Complex dialog management is required because our target domain is real world mixed initiative food ordering between agents and their customers, where individual customer utterances may contain multiple intents and refer to food items with complex structure. For example, a customer might say ""Can I get a deluxe burger with large fries and oh put extra mayo on the burger would you?"" We approach this task as a multi-level sequence labeling problem, with the constraint of limited real training data. Both traditional methods like HMM, MEMM, or CRF and newer methods like DNN or BiLSTM use only homogeneous feature sets. Newer methods perform better but also require considerably more data. Previous research has done pseudo-data synthesis to obtain the required amounts of training data. We propose to use a k-NN learner with heterogeneous feature set. We used windowed word n-grams, POS tag n-grams and pre-trained word embeddings as features. For the experiments we perform a comparison between using pseudo-data and real world data. We also perform semi-supervised self-training to obtain additional labeled data, in order to better model real world scenarios. Instead of using massive pseudo-data, we show that with only less than 1% of the data size, we can achieve better result than any of the methods above by annotating real world data. We achieve labeled bracketed F-scores of 75.46, 52.84 and 49.66 for the three levels of sequence labeling where each level has a longer word span than its previous level. Overall we achieve 60.71F. In comparison, two previous systems, MEMM and DNN-ELMO, achieved 52.32 and 45.25 respectively.",2019,NAACL,1.0
Permanent Magnetic Articulograph (PMA) vs Electromagnetic Articulograph (EMA) in Articulation-to-Speech Synthesis for Silent Speech Interface,"Silent speech interfaces (SSIs) are devices that enable speech communication when audible speech is unavailable. Articulation-to-speech (ATS) synthesis is a software design in SSI that directly converts articulatory movement information into audible speech signals. Permanent magnetic articulograph (PMA) is a wireless articulator motion tracking technology that is similar to commercial, wired Electromagnetic Articulograph (EMA). PMA has shown great potential for practical SSI applications, because it is wireless. The ATS performance of PMA, however, is unknown when compared with current EMA. In this study, we compared the performance of ATS using a PMA we recently developed and a commercially available EMA (NDI Wave system). Datasets with same stimuli and size that were collected from tongue tip were used in the comparison. The experimental results indicated the performance of PMA was close to, although not as equally good as that of EMA. Furthermore, in PMA, converting the raw magnetic signals to positional signals did not significantly affect the performance of ATS, which support the future direction in PMA-based ATS can be focused on the use of positional signals to maximize the benefit of spatial analysis.",2019,NAACL,0.2
Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems,"Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., ""!d10t"") or as a writing style (""1337"" in ""leet speak""), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%. We then explore three shielding methods-visual character embeddings, adversarial training, and rule-based recovery-which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.",2019,NAACL,-0.30000000000000004
Biomedical Event Extraction based on Knowledge-driven Tree-LSTM,"Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.",2019,NAACL,1.0
A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for Relation Extraction,"To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https://github.com/catcd/RbSP.",2019,NAACL,0.9
Evaluating Rewards for Question Generation Models,"Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch. We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.",2019,NAACL,-0.8
How do we feel when a robot dies? Emotions expressed on Twitter before and after hitchBOT's destruction,"In 2014, a chatty but immobile robot called hitchBOT set out to hitchhike across Canada. It similarly made its way across Germany and the Netherlands, and had begun a trip across the USA when it was destroyed by vandals. In this work, we analyze the emotions and sentiments associated with words in tweets posted before and after hitchBOT's destruction to answer two questions: Were there any differences in the emotions expressed across the different countries visited by hitchBOT? And how did the public react to the demise of hitchBOT? Our analyses indicate that while there were few cross-cultural differences in sentiment towards hitchBOT, there was a significant negative emotional reaction to its destruction, suggesting that people had formed an emotional connection with hitchBOT and perceived its destruction as morally wrong. We discuss potential implications of anthropomorphism and emotional attachment to robots from the perspective of robot ethics.",2019,NAACL,0.0
Uphill from here: Sentiment patterns in videos from left- and right-wing YouTube news channels,"News consumption exhibits an increasing shift towards online sources, which bring platforms such as YouTube more into focus. Thus, the distribution of politically loaded news is easier, receives more attention, but also raises the concern of forming isolated ideological communities. Understanding how such news is communicated and received is becoming increasingly important. To expand our understanding in this domain, we apply a linguistic temporal trajectory analysis to analyze sentiment patterns in English-language videos from news channels on YouTube. We examine transcripts from videos distributed through eight channels with pro-left and pro-right political leanings. Using unsupervised clustering, we identify seven different sentiment patterns in the transcripts. We found that the use of two sentiment patterns differed significantly depending on political leaning. Furthermore, we used predictive models to examine how different sentiment patterns relate to video popularity and if they differ depending on the channel's political leaning. No clear relations between sentiment patterns and popularity were found. However, results indicate, that videos from pro-right news channels are more popular and that a negative sentiment further increases that popularity, when sentiments are averaged for each video.",2019,NAACL,1.0
Publicly Available Clinical BERT Embeddings,"Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.",2019,NAACL,-0.30000000000000004
Cross-Corpora Evaluation and Analysis of Grammatical Error Correction Models - Is Single-Corpus Evaluation Enough?,"This study explores the necessity of performing cross-corpora evaluation for grammatical error correction (GEC) models. GEC models have been previously evaluated based on a single commonly applied corpus: the CoNLL-2014 benchmark. However, the evaluation remains incomplete because the task difficulty varies depending on the test corpus and conditions such as the proficiency levels of the writers and essay topics. To overcome this limitation, we evaluate the performance of several GEC models, including NMT-based (LSTM, CNN, and transformer) and an SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models' rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models.",2019,NAACL,-0.5
MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms,"We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/",2019,NAACL,0.9
Introduction to Discourse Relation Parsing and Treebanking (DISRPT): 7th Workshop on Rhetorical Structure Theory and Related Formalisms,"This overview summarizes the main contributions of the accepted papers at the 2019 workshop on Discourse Relation Parsing and Treebanking (DISRPT 2019). Co-located with NAACL 2019 in Minneapolis, the workshop's aim was to bring together researchers working on corpus-based and computational approaches to discourse relations. In addition to an invited talk, eighteen papers outlined below were presented, four of which were submitted as part of a shared task on elementary discourse unit segmentation and connective detection.",2019,NAACL,0.0
Evaluation of named entity coreference,"In many NLP applications like search and information extraction for named entities, it is necessary to find all the mentions of a named entity, some of which appear as pronouns (she, his, etc.) or nominals (the professor, the German chancellor, etc.). It is therefore important that coreference resolution systems are able to link these different types of mentions to the correct entity name. We evaluate state-of-the-art coreference resolution systems for the task of resolving all mentions to named entities. Our analysis reveals that standard coreference metrics do not reflect adequately the requirements in this task: they do not penalize systems for not identifying any mentions by name to an entity and they reward systems even if systems find correctly mentions to the same entity but fail to link these to a proper name (she-the student-no name). We introduce new metrics for evaluating named entity coreference that address these discrepancies and show that for the comparisons of competitive systems, standard coreference evaluations could give misleading results for this task. We are, however, able to confirm that the state-of-the art system according to traditional evaluations also performs vastly better than other systems on the named entity coreference task.",2019,NAACL,-0.8
The limits of Spanglish?,"Linguistic code-switching (C-S) is common in oral bilingual vernacular speech. When used in literature, C-S becomes an artistic choice that can mirror the patterns of bilingual interactions. But it can also potentially exceed them. What are the limits of C-S? We model features of C-S in corpora of contemporary U.S. Spanish-English literary and conversational data to analyze why some critics view the 'Spanglish' texts of Ilan Stavans as deviating from a C-S norm.",2019,NAACL,0.0
Diversifying Reply Suggestions Using a Matching-Conditional Variational Autoencoder,"We consider the problem of diversifying automated reply suggestions for a commercial instant-messaging (IM) system (Skype). Our conversation model is a standard matching based information retrieval architecture, which consists of two parallel encoders to project messages and replies into a common feature representation. During inference, we select replies from a fixed response set using nearest neighbors in the feature space. To diversify responses, we formulate the model as a generative latent variable model with Conditional Variational Auto-Encoder (M-CVAE). We propose a constrained-sampling approach to make the variational inference in M-CVAE efficient for our production system. In offline experiments, M-CVAE consistently increased diversity by ∼30−40% without significant impact on relevance. This translated to a ∼5% gain in click-rate in our online production system.",2019,NAACL,1.0
Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus,"Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks-sentiment transfer, and formality transfer-show that our model outperforms state-of-the-art approaches.Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.",2019,NAACL,0.8
Understanding the Behaviour of Neural Abstractive Summarizers using Contrastive Examples,"Neural abstractive summarizers generate summary texts using a language model conditioned on the input source text, and have recently achieved high ROUGE scores on benchmark summarization datasets. We investigate how they achieve this performance with respect to human-written gold-standard abstracts, and whether the systems are able to understand deeper syntactic and semantic structures. We generate a set of contrastive summaries which are perturbed, deficient versions of human-written summaries, and test whether existing neural summarizers score them more highly than the human-written summaries. We analyze their performance on different datasets and find that these systems fail to understand the source text, in a majority of the cases.",2019,NAACL,-1.0
Keyphrase Generation: A Text Summarization Struggle,"Authors' keyphrases assigned to scientific articles are essential for recognizing content and topic aspects. Most of the proposed supervised and unsupervised methods for keyphrase generation are unable to produce terms that are valuable but do not appear in the text. In this paper, we explore the possibility of considering the keyphrase string as an abstractive summary of the title and the abstract. First, we collect, process and release a large dataset of scientific paper metadata that contains 2.2 million records. Then we experiment with popular text summarization neural architectures. Despite using advanced deep learning models, large quantities of data and many days of computation, our systematic evaluation on four test datasets reveals that the explored text summarization methods could not produce better keyphrases than the simpler unsupervised methods, or the existing supervised ones.",2019,NAACL,0.8
Modeling Long-Distance Cue Integration in Spoken Word Recognition,"Cues to linguistic categories are distributed across the speech signal. Optimal categorization thus requires that listeners maintain gradient representations of incoming input in order to integrate that information with later cues. There is now evidence that listeners can and do integrate cues that occur far apart in time. Computational models of this integration have however been lacking. We take a first step at addressing this gap by mathematically formalizing four models of how listeners may maintain and use cue information during spoken language understanding and test them on two perception experiments. In one experiment, we find support for rational integration of cues at long distances. In a second, more memory and attention-taxing experiment, we find evidence in favor of a switching model that avoids maintaining detailed representations of cues in memory. These results are a first step in understanding what kinds of mechanisms listeners use for cue integration under different memory and attentional constraints.",2019,NAACL,0.5
Evaluating Text GANs as Language Models,"Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of ""exposure bias"". However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.",2019,NAACL,-0.2
Limitations of the empirical Fisher approximation for natural gradient descent,"Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.",2019,NIPS,-1.0
Can Unconditional Language Models Recover Arbitrary Sentences?,"Neural network-based generative language models like ELMo and BERT can work effectively as general purpose sentence encoders in text classification without further fine-tuning. Is it possible to adapt them in a similar way for use as general-purpose decoders? For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence. We set aside the difficult problem of designing an encoder that can produce such representations and, instead, ask directly whether such representations exist at all. To do this, we introduce a pair of effective, complementary methods for feeding representations into pretrained unconditional language models and a corresponding set of methods to map sentences into and out of this representation space, the reparametrized sentence space. We then investigate the conditions under which a language model can be made to generate a sentence through the identification of a point in such a space and find that it is possible to recover arbitrary sentences nearly perfectly with language models and representations of moderate size.",2019,NIPS,1.0
Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder,"In this work, we consider one challenging training time attack by modifying training data with bounded perturbation, hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this, we proposed to use an auto-encoder-like network to generate such adversarial perturbations on the training data together with one imaginary victim differentiable classifier. The perturbation generator will learn to update its weights so as to produce the most harmful noise, aiming to cause the lowest performance for the victim classifier during test time. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs, solving such problem is computationally challenging, we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. By teaching the perturbation generator to hijacking the training trajectory of the victim classifier, the generator can thus learn to move against the victim classifier step by step. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifier according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that, such bounded perturbations have good transferability across different types of victim classifiers.",2019,NIPS,1.0
ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.",2019,NIPS,0.5
Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift,"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.",2019,NIPS,-0.6000000000000001
Are Sixteen Heads Really Better than One?,"Multi-headed attention is a driving force behind recent state-of-the-art NLP models. By applying multiple attention mechanisms in parallel, it can express sophisticated functions beyond the simple weighted average. However we observe that, in practice, a large proportion of attention heads can be removed at test time without significantly impacting performance, and that some layers can even be reduced to a single head. Further analysis on machine translation models reveals that the self-attention layers can be significantly pruned, while the encoder-decoder layers are more dependent on multi-headedness.",2019,NIPS,-0.5
Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition,"Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing",2019,NIPS,1.0
Addressing Failure Prediction by Learning Model Confidence,"Assessing reliably the confidence of a deep neural net and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction.",2019,NIPS,1.0
Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers,"Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to ℓ2-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably ℓ2-robust classifiers by a significant margin on ImageNet and CIFAR-10, establishing the state-of-the-art for provable ℓ2-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at http://github.com/Hadisalman/smoothing-adversarial.",2019,NIPS,1.0
How to Initialize your Network? Robust Initialization for WeightNorm & ResNets,"Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks.",2019,NIPS,0.5
Are Anchor Points Really Indispensable in Label-Noise Learning?,"In label-noise learning, the noise transition matrix, denoting the probabilities that clean labels flip into noisy labels, plays a central role in building statistically consistent classifiers. Existing theories have shown that the transition matrix can be learned by exploiting anchor points (i.e., data points that belong to a specific class almost surely). However, when there are no anchor points, the transition matrix will be poorly learned, and those previously consistent classifiers will significantly degenerate. In this paper, without employing anchor points, we propose a transition-revision (T-Revision) method to effectively learn transition matrices, leading to better classifiers. Specifically, to learn a transition matrix, we first initialize it by exploiting data points that are similar to anchor points, having high noisy class posterior probabilities. Then, we modify the initialized matrix by adding a slack variable, which can be learned and validated together with the classifier by using noisy data. Empirical results on benchmark-simulated and real-world label-noise datasets demonstrate that without using exact anchor points, the proposed method is superior to state-of-the-art label-noise learning methods.",2019,NIPS,1.0
Generative Modeling by Estimating Gradients of the Data Distribution Authors Abstract,"We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.",2019,NIPS,1.0
On the Power and Limitations of Random Features for Understanding Neural Networks Authors Abstract,"Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient overparameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. [27, 29]). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in R and poly(d) weights), unless the network size (or magnitude of its weights) is exponentially large in d. Since a single neuron is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials.",2019,NIPS,-1.0
SHE: A Fast and Accurate Deep Neural Network for Encrypted Data Authors Abstract,"We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods.",2019,NIPS,1.0
Invariance and identifiability issues for word embeddings Authors Abstract,"Word embeddings are commonly obtained as optimizers of a criterion function f of a text corpus, but assessed on word-task performance using a different evaluation function g of the test data. We contend that a possible source of disparity in performance on tasks is the incompatibility between classes of transformations that leave f and g invariant. In particular, word embeddings defined by f are not unique; they are defined only up to a class of transformations to which f is invariant, and this class is larger than the class to which g is invariant. One implication of this is that the apparent superiority of one word embedding over another, as measured by word task performance, may largely be a consequence of the arbitrary elements selected from the respective solution sets. We provide a formal treatment of the above identifiability issue, present some numerical examples, and discuss possible resolutions.",2019,NIPS,0.0
Learning-In-The-Loop Optimization: End-To-End Control And Co-Design Of Soft Robots Through Learned Deep Latent Representations Authors Abstract,"We propose a heterogeneous meta-learning method that trains a model on tasks with various attribute spaces, such that it can solve unseen tasks whose attribute spaces are different from the training tasks given a few labeled instances. Although many meta-learning methods have been proposed, they assume that all training and target tasks share the same attribute space, and they are inapplicable when attribute sizes are different across tasks. Our model infers latent representations of each attribute and each response from a few labeled instances using an inference network. Then, responses of unlabeled instances are predicted with the inferred representations using a prediction network. The attribute and response representations enable us to make predictions based on the task-specific properties of attributes and responses even when attribute and response sizes are different across tasks. In our experiments with synthetic datasets and 59 datasets in OpenML, we demonstrate that our proposed method can predict the responses given a few labeled instances in new tasks after being trained with tasks with heterogeneous attribute spaces.",2019,NIPS,0.6000000000000001
Cold Case: The Lost MNIST Digits Authors Abstract,"Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they can be used to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our limited results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.",2019,NIPS,0.0
Mapping State Space using Landmarks for Universal Goal Reaching Authors Abstract,"Solomonoff‚Äôs general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Gr√ºnwald, 2007; Rissanen, 2007) formalize Occam‚Äôs razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks (Hinton and Van Camp, 1993; Schmidhuber, 1997). Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff‚Äôs approach.",2019,NIPS,-1.0
vGraph: A Generative Model for Joint Community Detection and Node Representation Learning Authors Abstract,"This paper focuses on two fundamental tasks of graph analysis: community detection and node representation learning, which capture the global and local structures of graphs, respectively. In the current literature, these two tasks are usually independently studied while they are actually highly correlated. We propose a probabilistic generative model called vGraph to learn community membership and node representation collaboratively. Specifically, we assume that each node can be represented as a mixture of communities, and each community is defined as a multinomial distribution over nodes. Both the mixing coefficients and the community distribution are parameterized by the low-dimensional representations of the nodes and communities. We designed an effective variational inference algorithm which regularizes the community membership of neighboring nodes to be similar in the latent space. Experimental results on multiple real-world graphs show that vGraph is very effective in both community detection and node representation learning, outperforming many competitive baselines in both tasks. We show that the framework of vGraph is quite flexible and can be easily extended to detect hierarchical communities.",2019,NIPS,0.9
Evaluating the Consistency of Word Embeddings from Small Data,"In this work, we address the evaluation of distributional semantic models trained on smaller, domain-specific texts, specifically, philosophical text. Specifically, we inspect the behaviour of models using a pre-trained background space in learning. We propose a measure of consistency which can be used as an evaluation metric when no in-domain gold-standard data is available. This measure simply computes the ability of a model to learn similar embeddings from different parts of some homogeneous data. We show that in spite of being a simple evaluation, consistency actually depends on various combinations of factors, including the nature of the data itself, the model used to train the semantic space, and the frequency of the learnt terms, both in the background space and in the in-domain data of interest.",2019,RANLP,0.30000000000000004
EmoSense at SemEval-2019 Task 3: Bidirectional LSTM Network for Contextual Emotion Detection in Textual Conversations,"In this paper, we describe a deep-learning system for emotion detection in textual conversations that participated in SemEval-2019 Task 3 ""EmoContext"". We designed a specific architecture of bidirectional LSTM which allows not only to learn semantic and sentiment feature representation, but also to capture user-specific conversation features. To fine-tune word embeddings using distant supervision we additionally collected a significant amount of emotional texts. The system achieved 72.59% micro-average F1 score for emotion classes on the test dataset, thereby significantly outperforming the officially-released baseline. Word embeddings and the source code were released for the research community.",2019,SemEval,1.0
On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference,"Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.",2019,SemEval,0.0
OleNet at SemEval-2019 Task 9: BERT based Multi-Perspective Models for Suggestion Mining,"This paper describes our system partici- pated in Task 9 of SemEval-2019: the task is focused on suggestion mining and it aims to classify given sentences into sug- gestion and non-suggestion classes in do- main specific and cross domain training setting respectively. We propose a multi- perspective architecture for learning rep- resentations by using different classical models including Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), Feed Forward Attention (FFA), etc. To leverage the semantics distributed in large amount of unsupervised data, we also have adopted the pre-trained Bidi- rectional Encoder Representations from Transformers (BERT) model as an en- coder to produce sentence and word rep- resentations. The proposed architecture is applied for both sub-tasks, and achieved f1-score of 0.7812 for subtask A, and 0.8579 for subtask B. We won the first and second place for the two tasks respec- tively in the final competition.",2019,SemEval,1.0
TDBot at SemEval-2019 Task 3: Context Aware Emotion Detection Using A Conditioned Classification Approach,"With the system description it is shown how to use the context information while detecting the emotion in a dialogue. Some guidelines about how to handle emojis was also laid out. While developing this system I realized the importance of pre-processing in conversational text data, or in general NLP related tasks; it can not be over emphasized.",2019,SemEval,0.7000000000000001
nlpUP at SemEval-2019 Task 6: A Deep Neural Language Model for Offensive Language Detection,"This paper presents our submission for the SemEval shared task 6, sub-task A on the identification of offensive language. Our proposed model, C-BiGRU, combines a Convolutional Neural Network (CNN) with a bidirectional Recurrent Neural Network (RNN). We utilize word2vec to capture the semantic similarities between words. This composition allows us to extract long term dependencies in tweets and distinguish between offensive and non-offensive tweets. In addition, we evaluate our approach on a different dataset and show that our model is capable of detecting online aggressiveness in both English and German tweets. Our model achieved a macro F1-score of 79.40% on the SemEval dataset.",2019,SemEval,1.0
SemEval-2019 Task 12: Toponym Resolution in Scientific Papers,"We present the SemEval-2019 Task 12 which focuses on toponym resolution in scientific articles. Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations. We proposed three subtasks. In Subtask 1, we asked participants to detect all toponyms in an article. In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames. In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms. A total of 29 teams registered, and 8 teams submitted a system run. We summarize the corpus and the tools created for the challenge. They are freely available at https://competitions.codalab.org/competitions/19948. We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.",2019,SemEval,0.5
Word Embeddings (Also) Encode Human Personality Stereotypes,"Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show that human stereotypes exist even for much more nuanced judgments such as personality, for a variety of person identities beyond the typically legally protected attributes and that these are similarly captured in word representations. Specifically, we collected human judgments about a person's Big Five personality traits formed solely from information about the occupation, nationality or a common noun description of a hypothetical person. Analysis of the data reveals a large number of statistically significant stereotypes in people. We then demonstrate the bias captured in lexical representations is statistically significantly correlated with the documented human bias. Our results, showing bias for a large set of person descriptors for such nuanced traits put in doubt the feasibility of broadly and fairly applying debiasing methods and call for the development of new methods for auditing language technology systems and resources.",2019,SemEval,0.0
EPITA-ADAPT at SemEval-2019 Task 3: Detecting emotions in textual conversations using deep learning models combination,"Messaging platforms like WhatsApp, Facebook Messenger and Twitter have gained recently much popularity owing to their ability in connecting users in real-time. The content of these textual messages can be a useful resource for text mining to discover and unhide various aspects, including emotions. In this paper we present our submission for SemEval 2019 task 'EmoContext'. The task consists of classifying a given textual dialogue into one of four emotion classes: Angry, Happy, Sad and Others. Our proposed system is based on the combination of different deep neural networks techniques. In particular, we use Recurrent Neural Networks (LSTM, B-LSTM, GRU, B-GRU), Convolutional Neural Network (CNN) and Transfer Learning (TL) methodes. Our final system, achieves an F1 score of 74.51% on the subtask evaluation dataset.",2019,SemEval,1.0
CoAStaL at SemEval-2019 Task 3: Affect Classification in Dialogue using Attentive BiLSTMs,"This work describes the system presented by the CoAStaL Natural Language Processing group at University of Copenhagen. The main system we present uses the same attention mechanism presented in (Yang et al., 2016). Our overall model architecture is also inspired by their hierarchical classification model and adapted to deal with classification in dialogue by encoding information at the turn level. We use different encodings for each turn to create a more expressive representation of dialogue context which is then fed into our classifier.We also define a custom preprocessing step in order to deal with language commonly used in interactions across many social media outlets. Our proposed system achieves a micro F1 score of 0.7340 on the test set and shows significant gains in performance compared to a system using dialogue level encoding.",2019,SemEval,1.0
YNU_DYX at SemEval-2019 Task 9: A Stacked BiLSTM for Suggestion Mining Classification,In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an ensemble to improve the effectiveness of our model. Our official submission results achieve an F1-score 0.5659.,2019,SemEval,1.0
Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement,"We present “AutoJudge”, an automated evaluation method for conversational dialogue systems. The method works by first generating dialogues based on self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings on these dialogues to train an automated judgement model. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing systems. This works well for re-ranking a set of candidate utterances. However, our experiments show that AutoJudge cannot be applied as reward for reinforcement learning, although the metric can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.",2019,WS,-0.4
Detecting Human-Object Interactions via Functional Generalization,"We present an approach for detecting human-object interactions (HOIs) in images, based on the idea that humans interact with functionally similar objects in a similar manner. The proposed model is simple and efficiently uses the data, visual features of the human, relative spatial orientation of the human and the object, and the knowledge that functionally similar objects take part in similar interactions with humans. We provide extensive experimental validation for our approach and demonstrate state-of-the-art results for HOI detection. On the HICO-Det dataset our method achieves a gain of over 2.5% absolute points in mean average precision (mAP) over stateof-the-art. We also show that our approach leads to significant performance gains for zero-shot HOI detection in the seen object setting. We further demonstrate that using a generic object detector, our model can generalize to interactions involving previously unseen objects.",2020,AAAI,1.0
Novel Is Not Always Better: On the Relation between Novelty and Dominance Pruning,"Novelty pruning is a planning technique that focuses on exploring states that are novel, i.e., those containing facts that have not been seen before. This seemingly simple idea has had a huge impact on the state of the art in planning though its effectiveness is not entirely understood yet. We relate novelty to dominance pruning, which compares states to previously seen states to eliminate those that are provably worse in terms of goal distance. Novelty can be interpreted as an unsafe approximation of dominance, where states containing novel facts are relevant because they enable new paths to the goal and, therefore, they are less likely to be dominated by others. This provides a framework to understand the success of novelty, resulting in new variants that combine both techniques.",2020,AAAI,0.30000000000000004
Semantics-Aligned Representation Learning for Person Re-Identification,"Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for reID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID.",2020,AAAI,1.0
A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation,"Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarseto-fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap.",2020,AAAI,0.9
"M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues","We present M3ER, a learning-based method for emotion recognition from multiple input modalities. Our approach combines cues from multiple co-occurring modalities (such as face, text, and speech) and also is more robust than other methods to sensor noise in any of the individual modalities. M3ER models a novel, data-driven multiplicative fusion method to combine the modalities, which learn to emphasize the more reliable cues and suppress others on a persample basis. By introducing a check step which uses Canonical Correlational Analysis to differentiate between ineffective and effective modalities, M3ER is robust to sensor noise. M3ER also generates proxy features in place of the ineffectual modalities. We demonstrate the efficiency of our network through experimentation on two benchmark datasets, IEMOCAP and CMU-MOSEI. We report a mean accuracy of 82.7% on IEMOCAP and 89.0% on CMU-MOSEI, which, collectively, is an improvement of about 5% over prior work.",2020,AAAI,1.0
Machine-Learning-Based Functional Microcirculation Analysis,"Analysis of microcirculation is an important clinical and research task. Functional analysis of the microcirculation allows researchers to understand how blood flowing in a tissues‚Äô smallest vessels affects disease progression, organ function, and overall health. Current methods of manual analysis of microcirculation are tedious and timeconsuming, limiting the quick turnover of results. There has been limited research on automating functional analysis of microcirculation. As such, in this paper, we propose a twostep machine-learning-based algorithm to functionally assess microcirculation videos. The first step uses a modified vessel segmentation algorithm to extract the location of vessel-like structures. While the second step uses a 3D-CNN to assess whether the vessel-like structures contained flowing blood. To our knowledge, this is the first application of machine learning for functional analysis of microcirculation. We use real-world labelled microcirculation videos to train and test our algorithm and assess its performance. More precisely, we demonstrate that our two-step algorithm can efficiently analyze real data with high accuracy (90%).",2020,AAAI,1.0
A Graph Auto-Encoder for Haplotype Assembly and Viral Quasispecies Reconstruction,"Reconstructing components of a genomic mixture from data obtained by means of DNA sequencing is a challenging problem encountered in a variety of applications including single individual haplotyping and studies of viral communities. Highthroughput DNA sequencing platforms oversample mixture components to provide massive amounts of reads whose relative positions can be determined by mapping the reads to a known reference genome; assembly of the components, however, requires discovery of the reads‚Äô origin ‚Äì an NP-hard problem that the existing methods struggle to solve with the required level of accuracy. In this paper, we present a learning framework based on a graph auto-encoder designed to exploit structural properties of sequencing data. The algorithm is a neural network which essentially trains to ignore sequencing errors and infers the posterior probabilities of the origin of sequencing reads. Mixture components are then reconstructed by finding consensus of the reads determined to originate from the same genomic component. Results on realistic synthetic as well as experimental data demonstrate that the proposed framework reliably assembles haplotypes and reconstructs viral communities, often significantly outperforming state-ofthe-art techniques. Source codes, datasets and supplementary document are available at https://github.com/WuLoli/GAEseq.",2020,AAAI,0.8
Who Likes What? ‚Äî SplitLBI in Exploring Preferential Diversity of Ratings,"In recent years, learning user preferences has received significant attention. A shortcoming of existing learning to rank work lies in that they do not take into account the multilevel hierarchies from social choice to individuals. In this paper, we propose a multi-level model which learns both the common preference or utility function over the population based on features of alternatives to-be-compared, and preferential diversity functions conditioning on user categories. Such a multi-level model, enables us to simultaneously learn a coarse-grained social preference function together with a fine-grained personalized diversity. It provides us prediction power for the choices of new users on new alternatives. The key algorithm in this paper is based on Split Linearized Bregman Iteration (SplitLBI) algorithm which generates a dynamic path from the common utility to personalized preferential diversity, at different levels of sparsity on personalization. A synchronized parallel version of SplitLBI is proposed to meet the needs of fast analysis of large-scale data. The validity of the methodology are supported by experiments with both simulated and real-world datasets such as movie and dining restaurant ratings which provides us a coarse-to-fine grained preference learning.",2020,AAAI,1.0
AI Trust in Business Processes: The Need for Process-Aware Explanations,"Business processes underpin a large number of enterprise operations including processing loan applications, managing invoices, and insurance claims. There is a large opportunity for infusing AI to reduce cost or provide better customer experience and the BPM literature is rich in machine learning solutions. More recently, deep learning models have been applied to process predictions. Unfortunately, companies have applied or adopted very few of these innovations. We assert that a reason for this lack of adoption is that business users are risk-averse and do not implicitly trust AI models. We challenge the BPM community to build on the AI interpretability literature, and the AI Trust community to understand what it means to take advantage of business process artifacts in order to provide business level explanations.",2020,AAAI,0.0
A System for Medical Information Extraction and Verification from Unstructured Text,"A wealth of medical knowledge has been encoded in terminologies like SNOMED CT, NCI, FMA, and more. However, these resources are usually lacking information like relations between diseases, symptoms, and risk factors preventing their use in diagnostic or other decision making applications. In this paper we present a pipeline for extracting such information from unstructured text and enriching medical knowledge bases. Our approach uses Semantic Role Labelling and is unsupervised. We show how we dealt with several deficiencies of SRL-based extraction, like copula verbs, relations expressed through nouns, and assigning scores to extracted triples. The system have so far extracted about 120K relations and in-house doctors verified about 5k relationships. We compared the output of the system with a manually constructed network of diseases, symptoms and risk factors build by doctors in the course of a year. Our results show that our pipeline extracts good quality and precise relations and speeds up the knowledge acquisition process considerably.",2020,AAAI,1.0
Modeling Probabilistic Commitments for Maintenance Is Inherently Harder than for Achievement,"Most research on probabilistic commitments focuses on commitments to achieve enabling preconditions for other agents. Our work reveals that probabilistic commitments to instead maintain preconditions for others are surprisingly harder to use well than their achievement counterparts, despite strong semantic similarities. We isolate the key difference as being not in how the commitment provider is constrained, but rather in how the commitment recipient can locally use the commitment specification to approximately model the provider‚Äôs effects on the preconditions of interest. Our theoretic analyses show that we can more tightly bound the potential suboptimality due to approximate modeling for achievement than for maintenance commitments. We empirically evaluate alternative approximate modeling strategies, confirming that probabilistic maintenance commitments are qualitatively more challenging for the recipient to model well, and indicating the need for more detailed specifications that can sacrifice some of the agents‚Äô autonomy.",2020,AAAI,-0.5
Learning Deep Relations to Promote Saliency Detection,"Though saliency detectors has made stunning progress recently. The performances of the state-of-the-art saliency detectors are not acceptable in some confusing areas, e.g., object boundary. We argue that the feature spatial independence should be one of the root cause. This paper explores the ubiquitous relations on the deep features to promote the existing saliency detectors efficiently. We establish the relation by maximizing the mutual information of the deep features of the same category via deep neural networks to break this independence. We introduce a threshold-constrained training pair construction strategy to ensure that we can accurately estimate the relations between different image parts in a selfsupervised way. The relation can be utilized to further excavate the salient areas and inhibit confusing backgrounds. The experiments demonstrate that our method can significantly boost the performance of the state-of-the-art saliency detectors on various benchmark datasets. Besides, our model is label-free and extremely efficient. The inference speed is 140 FPS on a single GTX1080 GPU.",2020,AAAI,1.0
Learning to Map Frequent Phrases to Sub-Structures of Meaning Representation for Neural Semantic Parsing,"Neural semantic parsers usually generate meaning representation tokens from natural language tokens via an encoderdecoder model. However, there is often a vocabularymismatch problem between natural language utterances and logical forms. That is, one word maps to several atomic logical tokens, which need to be handled as a whole, rather than individual logical tokens at multiple steps. In this paper, we propose that the vocabulary-mismatch problem can be effectively resolved by leveraging appropriate logical tokens. Specifically, we exploit macro actions, which are of the same granularity of words/phrases, and allow the model to learn mappings from frequent phrases to corresponding substructures of meaning representation. Furthermore, macro actions are compact, and therefore utilizing them can significantly reduce the search space, which brings a great benefit to weakly supervised semantic parsing. Experiments show that our method leads to substantial performance improvement on three benchmarks, in both supervised and weakly supervised settings.",2020,AAAI,1.0
COTSAE: CO-Training of Structure and Attribute Embeddings for Entity Alignment,"Entity alignment is a fundamental and vital task in Knowledge Graph (KG) construction and fusion. Previous works mainly focus on capturing the structural semantics of entities by learning the entity embeddings on the relational triples and pre-aligned ‚Äùseed entities‚Äù. Some works also seek to incorporate the attribute information to assist refining the entity embeddings. However, there are still many problems not considered, which dramatically limits the utilization of attribute information in the entity alignment. Different KGs may have lots of different attribute types, and even the same attribute may have diverse data structures and value granularities. Most importantly, attributes may have various ‚Äùcontributions‚Äù to the entity alignment. To solve these problems, we propose COTSAE that combines the structure and attribute information of entities by co-training two embedding learning components, respectively. We also propose a joint attention method in our model to learn the attentions of attribute types and values cooperatively. We verified our COTSAE on several datasets from real-world KGs, and the results showed that it is significantly better than the latest entity alignment methods. The structure and attribute information can complement each other and both contribute to performance improvement.",2020,AAAI,1.0
Asymptotically Unambitious Artificial General Intelligence,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI‚Äôs goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ‚Äúunambitiousness‚Äù includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",2020,AAAI,0.30000000000000004
Fair Procedures for Fair Stable Marriage Outcomes,"Given a two-sided market where each agent ranks those on the other side by preference, the stable marriage problem calls for finding a perfect matching such that no pair of agents prefer each other to their matches. Recent studies show that the number of stable solutions can be large in practice. Yet the classical solution to the problem, the Gale-Shapley (GS) algorithm, assigns an optimal match to each agent on one side, and a pessimal one to each on the other side; such a solution may fare well in terms of equity only in highly asymmetric markets. Finding a stable matching that minimizes the sex equality cost, an equity measure expressing the discrepancy of mean happiness among the two sides, is strongly NP-hard. Extant heuristics either (a) oblige some agents to involuntarily abandon their matches, or (b) bias the outcome in favor of some agents, or (c) need high-polynomial or unbounded time. We provide the first procedurally fair algorithms that output equitable stable marriages and are guaranteed to terminate in at most cubic time; the key to this breakthrough is the monitoring of a monotonic state function and the use of a selective criterion for accepting proposals. Our experiments with diverse simulated markets show that: (a) extant heuristics fail to yield high equity; (b) the best solution found by the GS algorithm can be very far from optimal equity; and (c) our procedures stand out in both efficiency and equity, even when compared to a non-procedurally fair approximation scheme.",2020,AAAI,0.5
Explainable Reinforcement Learning through a Causal Lens,"Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals ‚Äî things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We computationally evaluate the model in 6 domains and measure performance and task prediction accuracy. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents‚Äô behaviour. We investigate: 1) participants‚Äô understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models. Driven by lack of trust from users and proposed regulations, there are many calls for Artificial Intelligence (AI) systems to become more transparent, interpretable and explainable. This has renewed the interest in Explainable AI (XAI), which has been explored since the expert systems era (Chandrasekaran, Tanner, and Josephson 1989). A key pillar of XAI is explanation, a justification given for decisions and actions of the system. However, much research and practice in XAI pays little attention to people as intended users of these systems (Miller 2018b). If we are to build systems that are capable of providing ‚Äògood‚Äô explanations, it is plausible that explanation models should mimic models of human explanation (De Graaf and Malle 2017). Thus, to build XAI models it is essential to begin with a strong understanding of how people define, generate, select and evaluate explanations. There is a wealth of pertinent literature in cognitive psychology that explore the nature of explanations and how Copyright c ¬© 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. people understand them. As humans, we view the world through a causal lens (Sloman 2005), building mental models with causal relationships to act in the world, to understand new events and also to explain events. Importantly, causal models give people the ability to consider counterfactuals ‚Äî events that did not happen, but could have under different situations. Although this notion of causal explanation is also backed by literature in philosophy and social psychology (Hilton 2007), causality and counterfactuals are only just becoming more prevalent in XAI. Further, compared to the burst of XAI research in supervised learning, explainability in model-free reinforcement learning is hardly explored. We introduce an action influence model for model-free reinforcement learning (RL) agents and provide a formalisation of the model using structural causal models (Halpern and Pearl 2005). Action influence models approximate the causal model of the environment relative to actions taken by an agent. Our approach differs from previous work in explainable RL in that we use causal models to generate contrastive explanations for why and why not questions, which previous models lack. Given assumptions about the direction of causal relationships between variables, during the policy learning process, we also learn the quantitative influences that actions have on variables. Which enable our model to reason approximately about counterfactual states and actions. We define how to generate explanations for ‚Äòwhy?‚Äô and ‚Äòwhy not?‚Äô questions from the action influence model. We define minimally complete explanations taking inspiration from social psychology literature (McClure and Hilton 1997). We computationally evaluated our approach on 6 RL benchmarks domains using 6 different RL algorithms. Results indicate that these models are robust and accurate enough to perform task prediction (Hoffman et al. 2018, p.12) with a negligible performance impact. We conducted a human study using the implemented model for RL agents trained to play the real-time strategy game Starcraft II. Experiments were run for 120 participants, in which we evaluated the participants‚Äô performance in task prediction, explanation satisfaction, and trust. Results show that our model performs better than the tested baseline, but its impact on",2020,AAAI,0.4
Attending to Entities for Better Text Understanding,"Recent progress in NLP witnessed the development of largescale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This clearly demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.",2020,AAAI,1.0
Hard Examples for Common Variable Decision Heuristics,"The CDCL algorithm for SAT is equivalent to the resolution proof system under a few assumptions, one of them being an optimal non-deterministic procedure for choosing the next variable to branch on. In practice this task is left to a variable decision heuristic, and since the so-called VSIDS decision heuristic is considered an integral part of CDCL, whether CDCL with a VSIDS-like heuristic is also equivalent to resolution remained a significant open question. We give a negative answer by building a family of formulas that have resolution proofs of polynomial size but require exponential time to decide in CDCL with common heuristics such as VMTF, CHB, and certain implementations of VSIDS and LRB.",2020,AAAI,0.0
Deep Bayesian Nonparametric Learning of Rules and Plans from Demonstrations with a Learned Automaton Prior,"We introduce a method to learn imitative policies from expert demonstrations that are interpretable and manipulable. We achieve interpretability by modeling the interactions between high-level actions as an automaton with connections to formal logic. We achieve manipulability by integrating this automaton into planning, so that changes to the automaton have predictable effects on the learned behavior. These qualities allow a human user to first understand what the model has learned, and then either correct the learned behavior or zeroshot generalize to new, similar tasks. We build upon previous work by no longer requiring additional supervised information which is hard to collect in practice. We achieve this by using a deep Bayesian nonparametric hierarchical model. We test our model on several domains and also show results for a real-world implementation on a mobile robotic arm platform.",2020,AAAI,0.6000000000000001
A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection,"Microblogging platforms such as Twitter are increasingly being used in event detection. Existing approaches mainly use machine learning models and rely on event-related keywords to collect the data for model training. These approaches make strong assumptions on the distribution of the relevant microposts containing the keyword ‚Äì referred to as the expectation of the distribution ‚Äì and use it as a posterior regularization parameter during model training. Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword-specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent. We empirically demonstrate the merits of our approach, both in terms of accuracy and interpretability, on multiple real-world datasets and show that our approach improves the state of the art by 24.3%.",2020,AAAI,1.0
On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning,"Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layerwise and entire-model compression.",2020,AAAI,0.4
CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation,"Weakly supervised semantic segmentation with only imagelevel labels saves large human effort to annotate pixel-level labels. Cutting-edge approaches rely on various innovative constraints and heuristic rules to generate the masks for every single image. Although great progress has been achieved by these methods, they treat each image independently and do not take account of the relationships across different images. In this paper, however, we argue that the cross-image relationship is vital for weakly supervised segmentation. Because it connects related regions across images, where supplementary representations can be propagated to obtain more consistent and integral regions. To leverage this information, we propose an end-to-end cross-image affinity module, which exploits pixel-level cross-image relationships with only imagelevel labels. By means of this, our approach achieves 64.3% and 65.3% mIoU on Pascal VOC 2012 validation and test set respectively, which is a new state-of-the-art result by only using image-level labels for weakly supervised semantic segmentation, demonstrating the superiority of our approach.",2020,AAAI,1.0
Detecting Semantic Anomalies,"We critically appraise the recent interest in out-of-distribution (OOD) detection and question the practical relevance of existing benchmarks. While the currently prevalent trend is to consider different datasets as OOD, we argue that outdistributions of practical interest are ones where the distinction is semantic in nature for a specified context, and that evaluative tasks should reflect this more closely. Assuming a context of object recognition, we recommend a set of benchmarks, motivated by practical applications. We make progress on these benchmarks by exploring a multi-task learning based approach, showing that auxiliary objectives for improved semantic awareness result in improved semantic anomaly detection, with accompanying generalization benefits.",2020,AAAI,0.30000000000000004
Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation and One-Shot Channel Pruning,"We propose a novel meta-learning framework for real-time object tracking with efficient model adaptation and channel pruning. Given an object tracker, our framework learns to fine-tune its model parameters in only a few gradient-descent iterations during tracking while pruning its network channels using the target ground-truth at the first frame. Such a learning problem is formulated as a meta-learning task, where a meta-tracker is trained by updating its meta-parameters for initial weights, learning rates, and pruning masks through carefully designed tracking simulations. The integrated metatracker greatly improves tracking performance by accelerating the convergence of online learning and reducing the cost of feature computation. Experimental evaluation on the standard datasets demonstrates its outstanding accuracy and speed compared to the state-of-the-art methods.",2020,AAAI,1.0
Should You Fine-Tune BERT for Automated Essay Scoring?,"Most natural language processing research now recommends large Transformer-based models with fine-tuning for supervised classification tasks; older strategies like bag-of-words features and linear models have fallen out of favor. Here we investigate whether, in automated essay scoring (AES) research, deep neural models are an appropriate technological choice. We find that fine-tuning BERT produces similar performance to classical models at significant additional cost. We argue that while state-of-the-art strategies do match existing best results, they come with opportunity costs in computational resources. We conclude with a review of promising areas for research on student essays where the unique characteristics of Transformers may provide benefits over classical methods to justify the costs.",2020,ACL,0.8
Using PRMSE to evaluate automated scoring systems in the presence of label noise,"The effect of noisy labels on the performance of NLP systems has been studied extensively for system training. In this paper, we focus on the effect that noisy labels have on system evaluation. Using automated scoring as an example, we demonstrate that the quality of human ratings used for system evaluation have a substantial impact on traditional performance metrics, making it impossible to compare system evaluations on labels with different quality. We propose that a new metric, PRMSE, developed within the educational measurement community, can help address this issue, and provide practical guidelines on using PRMSE.",2020,ACL,1.0
Multiple Instance Learning for Content Feedback Localization without Annotation,"Automated Essay Scoring (AES) can be used to automatically generate holistic scores with reliability comparable to human scoring. In addition, AES systems can provide formative feedback to learners, typically at the essay level. In contrast, we are interested in providing feedback specialized to the content of the essay, and specifically for the content areas required by the rubric. A key objective is that the feedback should be localized alongside the relevant essay text. An important step in this process is determining where in the essay the rubric designated points and topics are discussed. A natural approach to this task is to train a classifier using manually annotated data; however, collecting such data is extremely resource intensive. Instead, we propose a method to predict these annotation spans without requiring any labeled annotation data. Our approach is to consider AES as a Multiple Instance Learning (MIL) task. We show that such models can both predict content scores and localize content by leveraging their sentence-level score predictions. This capability arises despite never having access to annotation training data. Implications are discussed for improving formative feedback and explainable AES models.",2020,ACL,1.0
A Comparative Study of Synthetic Data Generation Methods for Grammatical Error Correction,"Grammatical Error Correction (GEC) is concerned with correcting grammatical errors in written text. Current GEC systems, namely those leveraging statistical and neural machine translation, require large quantities of annotated training data, which can be expensive or impractical to obtain. This research compares techniques for generating synthetic data utilized by the two highest scoring submissions to the restricted and low-resource tracks in the BEA-2019 Shared Task on Grammatical Error Correction.",2020,ACL,0.0
Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition,"Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by ""some"" as entailments. For some presupposition triggers like ""only"", BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.",2020,ACL,1.0
End-to-End Speech Translation with Adversarial Training,"End-to-End speech translation usually leverages audio-to-text parallel data to train an available speech translation model which has shown impressive results on various speech translation tasks. Due to the artificial cost of collecting audio-to-text parallel data, the speech translation is a natural low-resource translation scenario, which greatly hinders its improvement. In this paper, we proposed a new adversarial training method to leverage target monolingual data to relieve the low-resource shortcoming of speech translation. In our method, the existing speech translation model is considered as a Generator to gain a target language output, and another neural Discriminator is used to guide the distinction between outputs of speech translation model and true target monolingual sentences. Experimental results on the CCMT 2019-BSTC dataset speech translation task demonstrate that the proposed methods can significantly improve the performance of the End-to-End speech translation system.",2020,ACL,1.0
What is Learned in Visually Grounded Neural Syntax Acquisition,"Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model's strong performance. Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better. We also find that a simple lexical signal of noun concreteness plays the main role in the model's predictions as opposed to more complex syntactic reasoning.",2020,ACL,0.0
"GECToR - Grammatical Error Correction: Tag, Not Rewrite","In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an F_0.5 of 65.3/66.5 on CONLL-2014 (test) and F_0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system.",2020,ACL,1.0
Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings,"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings - while more diverse and mature than those available for their dynamic counterparts - are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",2020,ACL,-0.9
Semi-supervised Contextual Historical Text Normalization,"Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.",2020,ACL,0.0
On the role of effective and referring questions in GuessWhat?!,"Task success is the standard metric used to evaluate referential visual dialogue systems. In this paper we propose two new metrics that evaluate how each question contributes to the goal. First, we measure how effective each question is by evaluating whether the question discards objects that are not the referent. Second, we define referring questions as those that univocally identify one object in the image. We report the new metrics for human dialogues and for state of the art publicly available models on GuessWhat?!. Regarding our first metric, we find that successful dialogues do not have a higher percentage of effective questions for most models. With respect to the second metric, humans make questions at the end of the dialogue that are referring, confirming their guess before guessing. Human dialogues that use this strategy have a higher task success but models do not seem to learn it.",2020,ACL,1.0
ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.",2020,ACL,-0.2
On Importance Sampling-Based Evaluation of Latent Language Models,"Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.",2020,ACL,-0.1
How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech,"Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we - (i) detail two non-invasive ways of detecting translationese and (ii) compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model (human vs machine) rather than to the data (written vs spoken).",2020,ACL,-0.6000000000000001
Contrastive Self-Supervised Learning for Commonsense Reasoning,"We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called ""trigger"" words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks. Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.",2020,ACL,1.0
Robust Neural Machine Translation with ASR Errors,"In many practical applications, neural machine translation systems have to deal with the input from automatic speech recognition (ASR) systems which may contain a certain number of errors. This leads to two problems which degrade translation performance. One is the discrepancy between the training and testing data and the other is the translation error caused by the input errors may ruin the whole translation. In this paper, we propose a method to handle the two problems so as to generate robust translation to ASR errors. First, we simulate ASR errors in the training data so that the data distribution in the training and test is consistent. Second, we focus on ASR errors on homophone words and words with similar pronunciation and make use of their pronunciation information to help the translation model to recover from the input errors. Experiments on two Chinese-English data sets show that our method is more robust to input errors and can outperform the strong Transformer baseline significantly.",2020,ACL,1.0
Visual Question Generation from Radiology Images,"Visual Question Generation (VQG), the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at https://github.com/sarrouti/vqgr.",2020,ACL,1.0
Detecting and understanding moral biases in news,"We describe work in progress on detecting and understanding the moral biases of news sources by combining framing theory with natural language processing. First we draw connections between issue-specific frames and moral frames that apply to all issues. Then we analyze the connection between moral frame presence and news source political leaning. We develop and test a simple classification model for detecting the presence of a moral frame, highlighting the need for more sophisticated models. We also discuss some of the annotation and frame detection challenges that can inform future research in this area.",2020,ACL,1.0
Evaluating Neural Morphological Taggers for Sanskrit,"Neural sequence labelling approaches have achieved state of the art results in morphological tagging. We evaluate the efficacy of four standard sequence labelling models on Sanskrit, a morphologically rich, fusional Indian language. As its label space can theoretically contain more than 40,000 labels, systems that explicitly model the internal structure of a label are more suited for the task, because of their ability to generalise to labels not seen during training. We find that although some neural models perform better than others, one of the common causes for error for all of these models is mispredictions due to syncretism.",2020,ACL,-1.0
A Cross-Task Analysis of Text Span Representations,"Many natural language processing (NLP) tasks involve reasoning with textual spans, including question answering, entity recognition, and coreference resolution. While extensive research has focused on functional architectures for representing words and sentences, there is less work on representing arbitrary spans of text within sentences. In this paper, we conduct a comprehensive empirical evaluation of six span representation methods using eight pretrained language representation models across six tasks, including two tasks that we introduce. We find that, although some simple span representations are fairly reliable across tasks, in general the optimal span representation varies by task, and can also vary within different facets of individual tasks. We also find that the choice of span representation has a bigger impact with a fixed pretrained encoder than with a fine-tuned encoder.",2020,ACL,0.6000000000000001
A Call for More Rigor in Unsupervised Cross-lingual Learning,"We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world's languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",2020,ACL,-1.0
Inflecting When There's No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals,"Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class - and thus fail on tasks like German number inflection, where infrequent suffixes like /-s/ can still be productively generalized. To investigate this question, we first collect a new dataset from German speakers (production and ratings of plural forms for novel nouns) that is designed to avoid sources of information unavailable to the ED model. The speaker data show high variability, and two suffixes evince 'regular' behavior, appearing more often with phonologically atypical inputs. Encoder-decoder models do generalize the most frequently produced plural class, but do not show human-like variability or 'regular' extension of these other plural markers. We conclude that modern neural models may still struggle with minority-class generalization.",2020,ACL,-1.0
KU-CST at the SIGMORPHON 2020 Task 2 on Unsupervised Morphological Paradigm Completion,"We present a model for the unsupervised dis- covery of morphological paradigms. The goal of this model is to induce morphological paradigms from the bible (raw text) and a list of lemmas. We have created a model that splits each lemma in a stem and a suffix, and then we try to create a plausible suffix list by con- sidering lemma pairs. Our model was not able to outperform the official baseline, and there is still room for improvement, but we believe that the ideas presented here are worth considering.",2020,ACL,0.9
Linguistic Features for Readability Assessment,"Readability assessment aims to automatically classify text by the level appropriate for learning readers. Traditional approaches to this task utilize a variety of linguistically motivated features paired with simple machine learning models. More recent methods have improved performance by discarding these features and utilizing deep learning models. However, it is unknown whether augmenting deep learning models with linguistically motivated features would improve performance further. This paper combines these two approaches with the goal of improving overall model performance and addressing this question. Evaluating on two large readability corpora, we find that, given sufficient training data, augmenting deep learning models with linguistically motivated features does not improve state-of-the-art performance. Our results provide preliminary evidence for the hypothesis that the state-of-the-art deep learning models represent linguistic features of the text related to readability. Future research on the nature of representations formed in these models can shed light on the learned features and their relations to linguistically motivated ones hypothesized in traditional approaches.",2020,ACL,-0.2
Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words,"Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability-but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.",2020,ACL,1.0
Investigating Word-Class Distributions in Word Vector Spaces,"This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes - the semantic class of direct objects of a verb and the semantic class in a thesaurus - and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class. Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution. We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",2020,ACL,0.0
"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly","Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (''Birds cannot [MASK]"") and non-negated (''Birds can [MASK]"") cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add ""misprimes"" to cloze questions (''Talk? Birds can [MASK]""). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",2020,ACL,-1.0
iSarcasm: A Dataset of Intended Sarcasm,"We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.",2020,ACL,-1.0
Automated Scoring of Clinical Expressive Language Evaluation Tasks,"Many clinical assessment instruments used to diagnose language impairments in children include a task in which the subject must formulate a sentence to describe an image using a specific target word. Because producing sentences in this way requires the speaker to integrate syntactic and semantic knowledge in a complex manner, responses are typically evaluated on several different dimensions of appropriateness yielding a single composite score for each response. In this paper, we present a dataset consisting of non-clinically elicited responses for three related sentence formulation tasks, and we propose an approach for automatically evaluating their appropriateness. We use neural machine translation to generate correct-incorrect sentence pairs in order to create synthetic data to increase the amount and diversity of training data for our scoring model. Our scoring model uses transfer learning to facilitate automatic sentence appropriateness evaluation. We further compare custom word embeddings with pre-trained contextualized embeddings serving as features for our scoring model. We find that transfer learning improves scoring accuracy, particularly when using pretrained contextualized embeddings.",2020,ACL,1.0
A Systematic Assessment of Syntactic Generalization in Neural Language Models,"While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.",2020,ACL,0.0
Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian-English Case Study,"Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. In this paper, we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom. Our preliminary results (64% word error rate) suggest these tools have the potential to speed data collection in this context. We provide practical examples of our data structure, details of our piloted computer-assisted processes, and fine-grained error analysis. Our study is informed and directed by genuine research questions and discussion in both the education and computational linguistics fields. We highlight some of the benefits and risks of using these emerging technologies to analyze the complex work of language teachers and in education more generally.",2020,ACL,1.0
Can Neural Networks Automatically Score Essay Traits?,"Essay traits are attributes of an essay that can help explain how well written (or badly written) the essay is. Examples of traits include Content, Organization, Language, Sentence Fluency, Word Choice, etc. A lot of research in the last decade has dealt with automatic holistic essay scoring - where a machine rates an essay and gives a score for the essay. However, writers need feedback, especially if they want to improve their writing - which is why trait-scoring is important. In this paper, we show how a deep-learning based system can outperform feature-based machine learning systems, as well as a string kernel system in scoring essay traits.",2020,ACL,1.0
Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings,"Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community.",2020,ACL,-0.6000000000000001
Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",2020,ACL,0.0
Should All Cross-Lingual Embeddings Speak English?,"Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.",2020,ACL,0.2
How Can We Accelerate Progress Towards Human-like Linguistic Generalization?,"This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",2020,ACL,-1.0
CIMA: A Large Open Access Dialogue Dataset for Tutoring,"One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective: student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The dataset enables a model to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy.",2020,ACL,1.0
"Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP","We survey 146 papers analyzing ""bias"" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ""bias"" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating ""bias"" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing ""bias"" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of ""bias""---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements-and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",2020,ACL,-0.8
Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation,"The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an $n$-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with $5+$ times speed up compared with an autoregressive model.",2020,ACL,1.0
Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing,"One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.",2020,ACL,0.9
"Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions","Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn't matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn't. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.",2020,ACL,-1.0
ERASER: A Benchmark to Evaluate Rationalized NLP Models,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the 'reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of ""rationales"" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",2020,ACL,1.0
OpusFilter: A Configurable Parallel Corpus Filtering Toolbox,"This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation.",2020,ACL,1.0
Social Bias Frames: Reasoning about Social and Power Implications of Language,"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that we shouldn't lower our standards to hire more women,"" most listeners will infer the implicature intended by the speaker - that ""women (candidates) are less qualified."" Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",2020,ACL,-0.30000000000000004
Complementary Systems for Off-Topic Spoken Response Detection,"Increased demand to learn English for business and education has led to growing interest in automatic spoken language assessment and teaching systems. With this shift to automated approaches it is important that systems reliably assess all aspects of a candidate's responses. This paper examines one form of spoken language assessment; whether the response from the candidate is relevant to the prompt provided. This will be referred to as off-topic spoken response detection. Two forms of previously proposed approaches are examined in this work: the hierarchical attention-based topic model (HATM); and the similarity grid model (SGM). The work focuses on the scenario when the prompt, and associated responses, have not been seen in the training data, enabling the system to be applied to new test scripts without the need to collect data or retrain the model. To improve the performance of the systems for unseen prompts, data augmentation based on easy data augmentation (EDA) and translation based approaches are applied. Additionally for the HATM, a form of prompt dropout is described. The systems were evaluated on both seen and unseen prompts from Linguaskill Business and General English tests. For unseen data the performance of the HATM was improved using data augmentation, in contrast to the SGM where no gains were obtained. The two approaches were found to be complementary to one another, yielding a combined F0.5 score of 0.814 for off-topic response detection where the prompts have not been seen in training.",2020,ACL,0.9
Probing for Referential Information in Language Models,"Language models keep track of complex information about the preceding context - including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.",2020,ACL,0.0
Dynamic Sentence Boundary Detection for Simultaneous Translation,"Simultaneous Translation is a great challenge in which translation starts before the source sentence finished. Most studies take transcription as input and focus on balancing translation quality and latency for each sentence. However, most ASR systems can not provide accurate sentence boundaries in realtime. Thus it is a key problem to segment sentences for the word streaming before translation. In this paper, we propose a novel method for sentence boundary detection that takes it as a multi-class classification task under the end-to-end pre-training framework. Experiments show significant improvements both in terms of translation quality and latency.",2020,ACL,1.0
Out of the Echo Chamber: Detecting Countering Debate Speeches,"An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in ""echo chambers"" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns - that of detecting articles that most effectively counter the arguments - and not just the stance - made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research.",2020,ACL,-0.5
KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation,"The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.",2020,ACL,-0.5
Adversarial NLI: A New Benchmark for Natural Language Understanding,"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",2020,ACL,-0.5
TVQA+: Spatio-Temporal Grounding for Video Question Answering,"We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.",2020,ACL,1.0
Obtaining Faithful Interpretations from Compositional Neural Networks,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",2020,ACL,0.7000000000000001
Becoming Linguistically Mature: Modeling English and German Children's Writing Development Across School Grades,"In this paper we employ a novel approach to advancing our understanding of the development of writing in English and German children across school grades using classification tasks. The data used come from two recently compiled corpora: The English data come from the the GiC corpus (983 school children in second-, sixth-, ninth- and eleventh-grade) and the German data are from the FD-LEX corpus (930 school children in fifth- and ninth-grade). The key to this paper is the combined use of what we refer to as 'complexity contours', i.e. series of measurements that capture the progression of linguistic complexity within a text, and Recurrent Neural Network (RNN) classifiers that adequately capture the sequential information in those contours. Our experiments demonstrate that RNN classifiers trained on complexity contours achieve higher classification accuracy than one trained on text-average complexity scores. In a second step, we determine the relative importance of the features from four distinct categories through a Sensitivity-Based Pruning approach.",2020,ACL,1.0
A Question Type Driven and Copy Loss Enhanced Frameworkfor Answer-Agnostic Neural Question Generation,"The answer-agnostic question generation is a significant and challenging task, which aims to automatically generate questions for a given sentence but without an answer. In this paper, we propose two new strategies to deal with this task: question type prediction and copy loss mechanism. The question type module is to predict the types of questions that should be asked, which allows our model to generate multiple types of questions for the same source sentence. The new copy loss enhances the original copy mechanism to make sure that every important word in the source sentence has been copied when generating questions. Our integrated model outperforms the state-of-the-art approach in answer-agnostic question generation, achieving a BLEU-4 score of 13.9 on SQuAD. Human evaluation further validates the high quality of our generated questions. We will make our code public available for further research.",2020,ACL,1.0
Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning,"This paper investigates whether transfer learning can improve the prediction of the difficulty and response time parameters for 18,000 multiple-choice questions from a high-stakes medical exam. The type the signal that best predicts difficulty and response time is also explored, both in terms of representation abstraction and item component used as input (e.g., whole item, answer options only, etc.). The results indicate that, for our sample, transfer learning can improve the prediction of item difficulty when response time is used as an auxiliary task but not the other way around. In addition, difficulty was best predicted using signal from the item stem (the description of the clinical case), while all parts of the item were important for predicting the response time.",2020,ACL,0.5
Do Neural Language Models Show Preferences for Syntactic Formalisms?,"Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD - with interesting variations across languages and layers - and that the strength of this preference is correlated with differences in tree shape.",2020,ACL,0.0
Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?,"While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",2020,ACL,0.0
Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment,"A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",2020,ACL,-1.0
Constructing Uyghur Name Entity Recognition System using Neural Machine Translation Tag Projection,"Although named entity recognition achieved great success by introducing the neural networks, it is challenging to apply these models to low resource languages including Uyghur while it depends on a large amount of annotated training data. Constructing a well-annotated named entity corpus manually is very time-consuming and labor-intensive. Most existing methods based on the parallel corpus combined with the word alignment tools. However, word alignment methods introduce alignment errors inevitably. In this paper, we address this problem by a named entity tag transfer method based on the common neural machine translation. The proposed method marks the entity boundaries in Chinese sentence and translates the sentences to Uyghur by neural machine translation system, hope that neural machine translation will align the source and target entity by the self-attention mechanism. The experimental results show that the Uyghur named entity recognition system trained by the constructed corpus achieve good performance on the test set, with 73.80% F1 score(3.79% improvement by baseline)",2020,CCL,0.9
Refining Data for Text Generation,"Recent work on data-to-text generation has made progress under the neural encoder-decoder architectures. However, the data input size is often enormous, while not all data records are important for text generation and inappropriate input may bring noise into the final output. To solve this problem, we propose a two-step approach which first selects and orders the important data records and then generates text from the noise-reduced data. Here we propose a learning to rank model to rank the importance of each record which is supervised by a relation extractor. With the noise-reduced data as input, we implement a text generator which sequentially models the input data records and emits a summary. Experiments on the ROTOWIRE dataset verifies the effectiveness of our proposed method in both performance and efficiency.",2020,CCL,1.0
Multilingual and Interlingual Semantic Representations for Natural Language Processing: A Brief Introduction,"We introduce the Computational Linguistics special issue on Multilingual and Interlingual Semantic Representations for Natural Language Processing. We situate the special issue's five articles in the context of our fast-changing field, explaining our motivation for this project. We offer a brief summary of the work in the issue, which includes developments on lexical and sentential semantic representations, from symbolic and neural perspectives.",2020,CL,1.0
Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics,"In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage.",2020,COLING,-0.5
Arabic Dialect Identification Using BERT-Based Domain Adaptation,"Arabic is one of the most important and growing languages in the world. With the rise of the social media giants like Twitter, Arabic spoken dialects have become more in use. In this paper we describe our effort and simple approach on the NADI Shared Task 1 that requires us to build a system to differentiate between different 21 Arabic dialects, we introduce a deep learning semisupervised fashion approach along with pre-processing that was reported on NADI shared Task 1 Corpus. Our system ranks 4th in NADI’s shared task competition achieving 23.09% F1 macro average score with a very simple yet an efficient approach on differentiating between 21 Arabic Dialects given tweets.",2020,COLING,1.0
AdelaideCyC at SemEval-2020 Task 12: Ensemble of Classifiers for Offensive Language Detection in Social Media,"This paper describes the systems our team (AdelaideCyC) has developed for SemEval Task 12 (OffensEval 2020) to detect offensive language in social media. The challenge focuses on three subtasks - offensive language identification (subtask A), offense type identification (subtask B), and offense target identification (subtask C). Our team has participated in all the three subtasks. We have developed machine learning and deep learning-based ensembles of models. We have achieved F1-scores of 0.906, 0.552, and 0.623 in subtask A, B, and C respectively. While our performance scores are promising for subtask A, the results demonstrate that subtask B and C still remain challenging to classify.",2020,COLING,1.0
NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text Using an Ensemble Model,"Sentiment Analysis refers to the process of interpreting what a sentence emotes and classifying them as positive, negative, or neutral. The widespread popularity of social media has led to the generation of a lot of text data and specifically, in the Indian social media scenario, the code-mixed Hinglish text i.e, the words of Hindi language, written in the Roman script along with other English words is a common sight. The ability to effectively understand the sentiments in these texts is much needed. This paper proposes a system titled NITS-Hinglish to effectively carry out the sentiment analysis of such code-mixed Hinglish text. The system has fared well with a final F-Score of 0.617 on the test data.",2020,COLING,1.0
"JCT at SemEval-2020 Task 12: Offensive Language Detection in Tweets Using Preprocessing Methods, Character and Word N-grams","In this paper, we describe our submissions to SemEval-2020 contest. We tackled subtask 12 - ""Multilingual Offensive Language Identification in Social Media"". We developed different models for four languages: Arabic, Danish, Greek, and Turkish. We applied three supervised machine learning methods using various combinations of character and word n-gram features. In addition, we applied various combinations of basic preprocessing methods. Our best submission was a model we built for offensive language identification in Danish using Random Forest. This model was ranked at the 6 position out of 39 submissions. Our result is lower by only 0.0025 than the result of the team that won the 4 place using entirely non-neural methods. Our experiments indicate that char ngram features are more helpful than word ngram features. This phenomenon probably occurs because tweets are more characterized by characters than by words, tweets are short, and contain various special sequences of characters, e.g., hashtags, shortcuts, slang words, and typos.",2020,COLING,1.0
Matching Theory and Data with Personal-ITY: What a Corpus of Italian YouTube Comments Reveals About Personality,"As a contribution to personality detection in languages other than English, we rely on distant supervision to create Personal-ITY, a novel corpus of YouTube comments in Italian, where authors are labelled with personality traits. The traits are derived from one of the mainstream personality theories in psychology research, named MBTI. Using personality prediction experiments, we (i) study the task of personality prediction in itself on our corpus as well as on TWISTY, a Twitter dataset also annotated with MBTI labels; (ii) carry out an extensive, in-depth analysis of the features used by the classifier, and view them specifically under the light of the original theory that we used to create the corpus in the first place. We observe that no single model is best at personality detection, and that while some traits are easier than others to detect, and also to match back to theory, for other, less frequent traits the picture is much more blurred.",2020,COLING,-0.4
LAST at SemEval-2020 Task 10: Finding Tokens to Emphasise in Short Written Texts with Precomputed Embedding Models and LightGBM,"To select tokens to be emphasised in short texts, a system mainly based on precomputed embedding models, such as BERT and ELMo, and LightGBM is proposed. Its performance is low. Additional analyzes suggest that its effectiveness is poor at predicting the highest emphasis scores while they are the most important for the challenge and that it is very sensitive to the specific instances provided during learning.",2020,COLING,-1.0
"LangResearchLab_NC at FinCausal 2020, Task 1: A Knowledge Induced Neural Net for Causality Detection",Identifying causal relationships in a text is essential for achieving comprehensive natural language understanding. The present work proposes a combination of features derived from pre-trained BERT with linguistic features for training a supervised classifier for the task of Causality Detection. The Linguistic features help to inject knowledge about the semantic and syntactic structure of the input sentences. Experiments on the FinCausal Shared Task1 datasets indicate that the combination of Linguistic features with BERT improves overall performance for causality detection. The proposed system achieves a weighted average F1 score of 0.952 on the post-evaluation dataset.,2020,COLING,1.0
What Can We Learn from Noun Substitutions in Revision Histories?,"In community-edited resources such as wikiHow, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a computational model and validate in how far the assumption that ‘revised means better’ actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including synonymy, antonymy and hypernymy. Despite the high semantic relatedness, we find that a supervised classifier can distinguish the revised version of a sentence from an original version with an accuracy close to 70%, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the ‘better version’ with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of specificity.",2020,COLING,0.1
MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing,"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu",2020,CoNLL,0.0
Processing effort is a poor predictor of cross-linguistic word order frequency,"Some have argued that word orders which are more difficult to process should be rarer cross-linguistically. Our current study fails to replicate the results of Maurits, Navarro, and Perfors (2010), who used an entropy-based Uniform Information Density (UID) measure to moderately predict the Greenbergian typology of transitive word orders. We additionally report an inability of three measures of processing difficulty - entropy-based UID, surprisal-based UID, and pointwise mutual information - to correctly predict the correct typological distribution, using transitive constructions from 20 languages in the Universal Dependencies project (version 2.5). However, our conclusions are limited by data sparsity.",2020,CoNLL,-0.7000000000000001
Filler-gaps that neural networks fail to generalize,"It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.",2020,CoNLL,-0.9
"""LazImpa"": Lazy and Impatient neural agents learn to communicate efficiently","Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, ""LazImpa"", where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.",2020,CoNLL,1.0
On the Computational Power of Transformers and Its Implications in Sequence Modeling,"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.",2020,CoNLL,0.6000000000000001
DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs,"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.",2020,CoNLL,1.0
Do Explicit Alignments Robustly Improve Multilingual Encoders?,"Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",2020,EMNLP,-1.0
Improving Low Compute Language Modeling with In-Domain Embedding Initialisation,"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.",2020,EMNLP,0.2
On the Ability and Limitations of Transformers to Recognize Formal Languages,"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.",2020,EMNLP,-0.30000000000000004
On the weak link between importance and prunability of attention heads,"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.",2020,EMNLP,-0.8
Dimsum @LaySumm 20,"Lay summarization aims to generate lay summaries of scientific papers automatically. It is an essential task that can increase the relevance of science for all of society. In this paper, we build a lay summary generation system based on BART model. We leverage sentence labels as extra supervision signals to improve the performance of lay summarization. In the CL-LaySumm 2020 shared task, our model achieves 46.00 Rouge1-F1 score.",2020,EMNLP,1.0
A Little Bit Is Worse Than None: Ranking with Limited Training Data,"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that ""some"" labeled in-domain data can be worse than none at all.",2020,EMNLP,0.4
"Overview and Insights from the Shared Tasks at Scholarly Document Processing 2020: CL-SciSumm, LaySumm and LongSumm","We present the results of three Shared Tasks held at the Scholarly Document Processing Workshop at EMNLP2020: CL-SciSumm, LaySumm and LongSumm. We report on each of the tasks, which received 18 submissions in total, with some submissions addressing two or three of the tasks. In summary, the quality and quantity of the submissions show that there is ample interest in scholarly document summarization, and the state of the art in this domain is at a midway point between being an impossible task and one that is fully resolved.",2020,EMNLP,0.0
BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter by Combining Deep Learning and Transfer Learning Models,"The outbreak COVID-19 virus caused a significant impact on the health of people all over the world. Therefore, it is essential to have a piece of constant and accurate information about the disease with everyone. This paper describes our prediction system for WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets. The dataset for this task contains size 10,000 tweets in English labeled by humans. The ensemble model from our three transformer and deep learning models is used for the final prediction. The experimental result indicates that we have achieved F1 for the INFORMATIVE label on our systems at 88.81% on the test set.",2020,EMNLP,1.0
Collecting Verified COVID-19 Question Answer Pairs,"We release a dataset of over 2,100 COVID19 related Frequently asked Question-Answer pairs scraped from over 40 trusted websites. We include an additional 24, 000 questions pulled from online sources that have been aligned by experts with existing answered questions from our dataset. This paper describes our efforts in collecting the dataset and summarizes the resulting data. Our dataset is automatically updated daily and available at https://github.com/JHU-COVID-QA/ scraping-qas. So far, this data has been used to develop a chatbot providing users information about COVID-19. We encourage others to build analytics and tools upon this dataset as well.",2020,EMNLP,1.0
BERTnesia: Investigating the capture and forgetting of knowledge in BERT,"Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT's final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.",2020,EMNLP,1.0
What Have We Achieved on Text Summarization?,"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",2020,EMNLP,0.8
LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool,"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ""strong"" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target ""weak"" alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward. We release our dataset and evaluation code at https://github.com/google-research-datasets/lareqa.",2020,EMNLP,1.0
Do sequence-to-sequence VAEs learn global features of sentences?,"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.",2020,EMNLP,-0.5
From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers,"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",2020,EMNLP,-0.30000000000000004
The Extraordinary Failure of Complement Coercion Crowdsourcing,"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied action - e.g., ""I started a new book I bought last week"", where the implied action is reading. We aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: Explicit Completion and Natural Language Inference. However, in both cases, crowdsourcing resulted in low agreement scores, even though we followed the same methodologies as in previous work. Why does the same process fail to yield high agreement scores? We specify our modeling schemes, highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. We conclude that specific phenomena require tailored solutions, not only in specialized algorithms, but also in data collection methods.",2020,EMNLP,-1.0
Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning,"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improve extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.",2020,EMNLP,1.0
Simultaneous Translation,"Simultaneous translation, which performs translation concurrently with the source speech, is widely useful in many scenarios such as international conferences, negotiations, press releases, legal proceedings, and medicine. This problem has long been considered one of the hardest problems in AI and one of its holy grails. Recently, with rapid improvements in machine translation, speech recognition, and speech synthesis, there has been exciting progress towards simultaneous translation. This tutorial will focus on the design and evaluation of policies for simultaneous translation, to leave attendees with a deep technical understanding of the history, the recent advances, and the remaining challenges in this field.",2020,EMNLP,0.0
PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",2020,EMNLP,1.0
What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models,"Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than English, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of part-of-speech tagging, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.",2020,EMNLP,-0.30000000000000004
Understanding tables with intermediate pre-training,"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.",2020,EMNLP,1.0
A Multilingual Neural Machine Translation Model for Biomedical Data,"We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages (French, German, Italian, Korean and Spanish) into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news (generic domain) and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.",2020,EMNLP,1.0
Foreigner-directed speech is simpler than native-directed: Evidence from social media,"I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addressed to natives. The second hypothesis is particularly important for theories about contact-induced simplification, since the accommodation to non-natives may explain how the simplification can spread from adult learners to the whole community. To test the hypotheses, I create a very large corpus of native and non-native written speech in four languages (English, French, Italian, Spanish), extracting data from an internet forum where native languages of the participants are known and the structure of the interactions can be inferred. The corpus data yield inconsistent evidence with respect to the first hypothesis, but largely support the second one, suggesting that foreigner-directed speech is indeed simpler than native-directed. Importantly, when testing the first hypothesis, I contrast production of different speakers, which can introduce confounds and is a likely reason for the inconsistencies. When testing the second hypothesis, the comparison is always within the production of the same speaker (but with different addressees), which makes it more reliable.",2020,EMNLP,0.0
HINT3: Raising the bar for Intent Detection in the Wild,"Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.",2020,EMNLP,-0.9
Condolence and Empathy in Online Communities,"Offering condolence is a natural reaction to hearing someone's distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal-trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",2020,EMNLP,1.0
On the Same Page? Comparing Inter-Annotator Agreement in Sentence and Document Level Human Machine Translation Evaluation,"Document-level evaluation of machine translation has raised interest in the community especially since responses to the claims of ""human parity"" (Toral et al., 2018; Läubli et al., 2018) with document-level human evaluations have been published. Yet, little is known about best practices regarding human evaluation of machine translation at the document-level. This paper presents a comparison of the differences in inter-annotator agreement between quality assessments using sentence and document-level set-ups. We report results of the agreement between professional translators for fluency and adequacy scales, error annotation, and pair-wise ranking, along with the effort needed to perform the different tasks. To best of our knowledge, this is the first study of its kind.",2020,EMNLP,0.0
Evaluation of Coreference Resolution Systems Under Adversarial Attacks,"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects-lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements.",2020,EMNLP,-1.0
Document-aligned Japanese-English Conversation Parallel Corpus,"Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.",2020,EMNLP,1.0
Evaluating Models' Local Decision Boundaries via Contrast Sets,"Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets-up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",2020,EMNLP,-0.1
"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA","Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.",2020,EMNLP,0.5
Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings,"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",2020,EMNLP,-0.2
GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems,"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",2020,EMNLP,1.0
Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The ""omni-directional"" BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.",2020,EMNLP,0.0
An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training,"Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.",2020,EMNLP,1.0
TextHide: Tackling Data Privacy in Language Understanding Tasks,"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem.",2020,EMNLP,1.0
WER we are and WER we think we are,"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.",2020,EMNLP,-0.9
High Performance Natural Language Processing,"Scale has played a central role in the rapid progress natural language processing has enjoyed in recent years. While benchmarks are dominated by ever larger models, efficient hardware use is critical for their widespread adoption and further progress in the field. In this cutting-edge tutorial, we will recapitulate the state-of-the-art in natural language processing with scale in perspective. After establishing these foundations, we will cover a wide range of techniques for improving efficiency, including knowledge distillation, quantization, pruning, more efficient architectures, along with case studies and practical implementation tricks.",2020,EMNLP,0.0
Imitation Attacks and Defenses for Black-box Machine Translation Systems,"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.",2020,EMNLP,-0.5
SIGTYP 2020 Shared Task: Prediction of Typological Features,"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world's languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.",2020,EMNLP,-0.8
Second-Order NLP Adversarial Examples,"Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve, and associated metric ACCS, as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available here.",2020,EMNLP,0.9
Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation,"To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.",2020,EMNLP,1.0
Effects of Naturalistic Variation in Goal-Oriented Dialog,"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets.",2020,EMNLP,-0.8
Double Graph Based Reasoning for Document-level Relation Extraction,"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.",2020,EMNLP,1.0
Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks,"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.",2020,EMNLP,1.0
POSTECH-ETRI's Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model,"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs: English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by TER of -3.58 and a BLEU score of +5.3 for the En-De subtask; and TER of -5.29 and a BLEU score of +7.32 for the En-Zh subtask.",2020,EMNLP,1.0
Can Knowledge Graph Embeddings Tell Us What Fact-checked Claims Are About?,"The web offers a wealth of discourse data that help researchers from various fields analyze debates about current societal issues and gauge the effects on society of important phenomena such as misinformation spread. Such analyses often revolve around claims made by people about a given topic of interest. Fact-checking portals offer partially structured information that can assist such analysis. However, exploiting the network structure of such online discourse data is as of yet under-explored. We study the effectiveness of using neural-graph embedding features for claim topic prediction and their complementarity with text embeddings. We show that graph embeddings are modestly complementary with text embeddings, but the low performance of graph embedding features alone indicate that the model fails to capture topological features pertinent of the topic prediction task.",2020,EMNLP,-0.7000000000000001
RNNs can generate bounded hierarchical languages with optimal memory,"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-$(k,m)$, the language of well-nested brackets (of $k$ types) and $m$-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use $O(k^{\fracm2})$ memory (hidden units) to generate these languages. We prove that an RNN with $O(m \log k)$ hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with $o(m \log k)$ hidden units.",2020,EMNLP,1.0
Writing Strategies for Science Communication: Data and Computational Analysis,"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128k science writing documents in English and annotate a subset of this corpus. We use the annotations to train transformer-based classifiers and measure the strategies' use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.",2020,EMNLP,1.0
GLAD: Learning Sparse Graph Recovery,"Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an `1 regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. We propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as our model inductive bias, and learns the model parameters via supervised learning. We show that GLAD learns a very compact and effective model for recovering sparse graphs from data.",2020,ICLR,1.0
What graph neural networks cannot learn: depth vs width,"This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp‚Äôs depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.",2020,ICLR,-0.7000000000000001
RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis,"Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter‚Äìconditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.",2020,ICLR,0.9
AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing,"This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation.",2020,ICLR,1.0
Learning The Difference That Makes A Difference With Counterfactually-Augmented Data,"Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available1.",2020,ICLR,0.7000000000000001
Masked Based Unsupervised Content Transfer,"We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables weakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. Our code is available at https: //github.com/rmokady/mbu-content-tansfer.",2020,ICLR,1.0
Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP,"The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a ‚Äúlucky‚Äù sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether ‚Äúwinning ticket‚Äù initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with work in supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",2020,ICLR,0.30000000000000004
A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning,"Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.",2020,ICLR,0.9
Smooth markets: A basic mechanism for organizing gradient-based learners,"With the success of modern machine learning, it is becoming increasingly important to understand and control how learning algorithms interact. Unfortunately, negative results from game theory show there is little hope of understanding or controlling general n-player games. We therefore introduce smooth markets (SM-games), a class of n-player games with pairwise zero sum interactions. SM-games codify a common design pattern in machine learning that includes (some) GANs, adversarial training, and other recent algorithms. We show that SM-games are amenable to analysis and optimization using first-order methods. ‚ÄúI began to see legibility as a central problem in modern statecraft. The premodern state was, in many respects, partially blind [. . .] It lacked anything like a detailed ‚Äòmap‚Äô of its terrain and its people. It lacked, for the most part, a measure, a metric that would allow it to ‚Äòtranslate‚Äô what it knew into a common standard necessary for a synoptic view. As a result, its interventions were often crude and self-defeating.‚Äù ‚Äì from Seeing like a State by Scott (1999)",2020,ICLR,-0.8
Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation,"Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only selfsupervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes.",2020,ICLR,1.0
Composing Task-Agnostic Policies with Deep Reinforcement Learning,"The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent‚Äôs primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers skills to new problem settings but also solves the challenging environments requiring both task planning and motion control with high data efficiency.",2020,ICLR,1.0
The Curious Case of Neural Text Degeneration,"Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration ‚Äî output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for openended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality ‚Äî as measured by human evaluation ‚Äî and as diverse as human-written text. Context: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. Beam Search, b=32: ""The study, published in the Proceedings of the National Academy of Sciences of the United States of America (PNAS), was conducted by researchers from the Universidad Nacional Aut√≥noma de M√©xico (UNAM) and the Universidad Nacional Aut√≥noma de M√©xico (UNAM/Universidad Nacional Aut√≥noma de M√©xico/Universidad Nacional Aut√≥noma de M√©xico/Universidad Nacional Aut√≥noma de M√©xico/Universidad Nacional Aut√≥noma de ...‚Äù Pure Sampling: They were cattle called Bolivian Cavalleros; they live in a remote desert uninterrupted by town, and they speak huge, beautiful, paradisiacal Bolivian linguistic thing. They say, 'Lunch, marge.' They don't tell what the lunch is,"" director Professor Chuperas Omwell told Sky News. ""They've only been talking to scientists, like we're being interviewed by TV reporters. We don't even stick around to be interviewed by TV reporters. Maybe that's how they figured out that they're cosplaying as the Bolivian Cavalleros."" Figure 1: Even with substantial human context and the powerful GPT-2 Large language model, Beam Search (size 32) leads to degenerate repetition (highlighted in blue) while pure sampling leads to incoherent gibberish (highlighted in red). When b ‚â• 64, both GPT-2 Large and XL (774M and 1542M parameters, respectively) prefer to stop generating immediately after the given context.",2020,ICLR,1.0
Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks,"We propose Additive Powers-of-Two (APoT) quantization, an efficient nonuniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model is capable to decrease 22% computational cost compared with the uniformly quantized counterpart. 1",2020,ICLR,1.0
Decentralized Deep Learning with Arbitrary Communication Compression,"Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that CHOCO-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter.",2020,ICLR,0.5
Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs,"We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.",2020,ICLR,1.0
Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?,"Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (valuebased, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.",2020,ICLR,-0.5
Understanding and Improving Information Transfer in Multi-Task Learning,"We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks‚Äô data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks‚Äô embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35% GLUE score average improvement on 5 GLUE tasks over BERTLARGE using our alignment method. We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.",2020,ICLR,1.0
Inductive Matrix Completion Based on Graph Neural Networks,"We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user‚Äôs age or movie‚Äôs genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive ‚Äì it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.",2020,ICLR,1.0
Data-Independent Neural Pruning via Coresets,"Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample. We propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input x ‚àà R, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving classification accuracy.",2020,ICLR,1.0
On the Need for Topology-Aware Generative Models for Manifold-Based Defenses,"Machine-learning (ML) algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, researchers have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause misclassification). The existence of adversarial examples has hindered the deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifoldbased defenses, where a sample is ‚Äúpulled back‚Äù into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution. In this paper, we investigate the following question: do the generative models used in manifold-based defenses need to be topology-aware? We suggest the answer is yes, and we provide theoretical and empirical evidence to support our claim.",2020,ICLR,0.0
The Early Phase of Neural Network Training,"Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.",2020,ICLR,-0.5
Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint,"This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples n, features d, and neurons h tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the double descent phenomenon: a cusp in the population risk appears at h ‚âà n and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how different scales of initialization lead to different inductive bias, and show that the resulting risk is independent of overparameterization. Our theoretical and experimental results suggest that previously studied model setups that provably give rise to double descent might not translate to optimizing two-layer neural networks.",2020,ICLR,-0.5
Dynamics-Aware Unsupervised Discovery of Skills,"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make modelbased planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparsereward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery. We have open-sourced our implementation at: https://github.com/google-research/dads Figure 1: A humanoid agent discovers diverse locomotion primitives without any reward using DADS. We show zero-shot generalization to downstream tasks by composing the learned primitives using model predictive control, enabling the agent to follow an online sequence of goals (green markers) without any additional training.",2020,ICLR,1.0
Lagrangian Fluid Simulation with Continuous Convolutions,"We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed.",2020,ICLR,1.0
On the Weaknesses of Reinforcement Learning for Neural Machine Translation,"Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve.",2020,ICLR,-0.5
Truth or backpropaganda? An empirical investigation of deep learning theory,"We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.",2020,ICLR,-0.5
Dynamics-Aware Embeddings,"In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment‚Äôs dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",2020,ICLR,1.0
Implementation Matters in Deep RL: A Case Study on PPO and TRPO,"We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of ‚Äúcode-level optimizations:‚Äù algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO‚Äôs gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.",2020,ICLR,-0.2
Neural Machine Translation with Universal Visual Representation,"Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of largescale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pretrained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodal NMT. Experiments on four widely used translation datasets, including the WMT‚Äô16 English-to-Romanian, WMT‚Äô14 English-to-German, WMT‚Äô14 Englishto-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.",2020,ICLR,1.0
Effect of Activation Functions on the Training of Overparametrized Neural Nets,"It is well-known that overparametrized neural networks trained using gradientbased methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: ‚Ä¢ For non-smooth activations such as ReLU,SELU,ELU, which are not smooth because there is a point where either the first order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. ‚Ä¢ For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satisfies another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is sufficient. We discuss a number of extensions and applications of these results.",2020,ICLR,0.0
Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML,"An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML‚Äôs popularity, a fundamental open question remains ‚Äì is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML‚Äôs performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.",2020,ICLR,0.0
Understanding the Limitations of Conditional Generative Models,"Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs. Our theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.",2020,ICLR,-0.5
Fast Neural Network Adaptation via Parameter Remapping and Architecture Search,"Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737√ó less than DPC, 6.8√ó less than Auto-DeepLab and 7.4√ó less than DetNAS. The code is available at https://github.com/JaminFong/FNA.",2020,ICLR,1.0
Rethinking the Hyperparameters for Fine-tuning,"Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous theoretical findings. (2) Optimal hyperparameters for fine-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for ‚Äúdissimilar‚Äù datasets. Our findings challenge common practices of finetuning and encourages deep learning practitioners to rethink the hyperparameters for fine-tuning.",2020,ICLR,-0.5
Abductive Commonsense Reasoning,"Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks ‚Äì (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform‚Äîdespite their strong performance on the related but more narrowly defined task of entailment NLI‚Äîpointing to interesting avenues for future research.",2020,ICLR,-1.0
Black-Box Adversarial Attack with Transferable Model-based Embedding,"We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate.",2020,ICLR,0.7000000000000001
A Closer Look at Deep Policy Gradients,"We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a finegrained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: the surrogate objective does not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the ‚Äútrue‚Äù gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.",2020,ICLR,-1.0
Estimating counterfactual treatment outcomes over time through adversarially balanced representations,"Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.",2020,ICLR,1.0
Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,"In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.",2020,ICLR,0.7000000000000001
Self-Supervised Learning of Appliance Usage,"Learning home appliance usage is important for understanding people‚Äôs activities and optimizing energy consumption. The problem is modeled as an event detection task, where the objective is to learn when a user turns an appliance on, and which appliance it is (microwave, hair dryer, etc.). Ideally, we would like to solve the problem in an unsupervised way so that the method can be applied to new homes and new appliances without any labels. To this end, we introduce a new deep learning model that takes input from two home sensors: 1) a smart electricity meter that outputs the total energy consumed by the home as a function of time, and 2) a motion sensor that outputs the locations of the residents over time. The model learns the distribution of the residents‚Äô locations conditioned on the home energy signal. We show that this cross-modal prediction task allows us to detect when a particular appliance is used, and the location of the appliance in the home, all in a self-supervised manner, without any labeled data.",2020,ICLR,0.7000000000000001
Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,"An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",2020,ICLR,0.6000000000000001
Graph inference learning for semi-supervised classification,"In this work, we address semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner, but the performance could degrade significantly when labeled data is scarce. To this end, we propose a Graph Inference Learning (GIL) framework to boost the performance of semisupervised node classification by learning the inference of node labels on graph topology. To bridge the connection between two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths, and local topological structures together, which can make the inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted to testing nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed, and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.",2020,ICLR,1.0
Inductive representation learning on temporal graphs,"Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner‚Äôs theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",2020,ICLR,0.8
Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,"Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",2020,ICLR,0.1
U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation,"We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATITpytorch.",2020,ICLR,1.0
Pure and Spurious Critical Points: a Geometric Study of Linear Networks,"The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network‚Äôs weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of ‚Äúbad‚Äù local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (‚Äúfilling architectures‚Äù) but it holds only for the quadratic loss when the functional space is a determinantal variety (‚Äúnon-filling architectures‚Äù). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.",2020,ICLR,-1.0
Scalable Model Compression by Entropy Penalized Reparameterization,"We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a ‚Äúlatent‚Äù space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate‚Äìaccuracy trade-off specified by a hyperparameter. We evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. Our results show that state-of-the-art model compression can be achieved in a scalable and general way without requiring complex procedures such as multi-stage training.",2020,ICLR,1.0
Lipschitz constant estimation of Neural Networks via sparse polynomial optimization,"We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the `‚àû-Lipschitz constant, our approach yields superior estimates, compared to baselines available in the literature.",2020,ICLR,1.0
Fantastic Generalization Measures and Where to Find Them,"Generalization of deep networks has lately been of great interest, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.",2020,ICLR,0.0
FasterSeg: Searching for Faster Real-time Semantic Segmentation,"We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to ‚Äúcollapsing‚Äù to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model‚Äôs accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy.",2020,ICLR,1.0
"On the ""steerability"" of generative adversarial networks","An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise ‚Äì these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by ‚Äústeering‚Äù in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: https://ali-design.github.io/gan_steerability/.",2020,ICLR,0.1
Batch-shaping for learning conditional channel gated networks,"We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.",2020,ICLR,0.8
Pruned Graph Scattering Transforms,"Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded energy. Experiments showcase that i) pGST performs comparably to the baseline GST that uses all scattering features, while achieving significant computational savings; ii) pGST achieves comparable performance to state-of-the-art GCNs; and iii) Graph data from various domains lead to different scattering patterns, suggesting domain-adaptive pGST network architectures.",2020,ICLR,1.0
Image-guided Neural Object Rendering,"We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours & sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on ‚Äúremembering‚Äù object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.",2020,ICLR,0.7000000000000001
Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories,"We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learningbased agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the largescale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four shape segmentation baselines shows that our approach achieve the state-of-the-art performance.",2020,ICLR,1.0
NAS evaluation is frustratingly hard,"Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method‚Äôs relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods. The code used is available at https://github.com/antoyang/NAS-Benchmark.",2020,ICLR,-1.0
Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video,"We propose a model that is able to perform unsupervised physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a physics-as-inverse-graphics approach that brings together vision-as-inverse-graphics and differentiable physics engines, enabling objects and explicit state and velocity representations to be discovered. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller‚Äôs interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.",2020,ICLR,1.0
Stochastic Subspace Cubic Newton Method,"In this paper, we propose a new randomized second-order optimization algorithm‚ÄîStochastic Subspace Cubic Newton (SSCN)‚Äîfor minimizing a high dimensional convex function f . Our method can be seen both as a stochastic extension of the cubically-regularized Newton method of Nesterov and Polyak (2006), and a second-order enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function 12 (x‚àíx ‚àó)>‚àá2f(x‚àó)(x‚àíx‚àó), where x‚àó is the minimizer of f , and hence depends on the properties of f at the optimum only. Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.",2020,ICML,1.0
Model Fusion with Kullback-Leibler Divergence,"We propose a method to fuse posterior distributions learned from heterogeneous datasets. Our algorithm relies on a mean field assumption for both the fused model and the individual dataset posteriors and proceeds using a simple assign-andaverage approach. The components of the dataset posteriors are assigned to the proposed global model components by solving a regularized variant of the assignment problem. The global components are then updated based on these assignments by their mean under a KL divergence. For exponential family variational distributions, our formulation leads to an efficient non-parametric algorithm for computing the fused model. Our algorithm is easy to describe and implement, efficient, and competitive with state-of-the-art on motion capture analysis, topic modeling, and federated learning of Bayesian neural networks.1",2020,ICML,1.0
Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems,"We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with stochastic gradient descent. We show that the proposed method can successfully segment time series data, including videos and 3D human pose, into meaningful ‚Äúregimes‚Äù by using the piece-wise nonlinear dynamics.",2020,ICML,1.0
Why bigger is not always better: on finite and infinite neural networks,"Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning. One approach to understanding and improving neural networks is to perform Bayesian inference in an infinitely wide network (Lee et al., 2018; Matthews et al., 2018; GarrigaAlonso et al., 2019; Novak et al., 2019). In this limit the outputs become Gaussian process distributed, enabling efficient and exact reasoning about uncertainty, and giving a means of interpretation using the parameter-free kernel function (which depends only on network hyperparameters such as depth). However, the performance of Bayesian infinite networks lags considerably behind state-of-the-art finite University of Bristol, Bristol, UK. Correspondence to: Laurence Aitchison <laurence.aitchison@gmail.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). networks trained using SGD (e.g. compare performance in Garriga-Alonso et al. (2019), Novak et al. (2019) and Arora et al. (2019) against He et al. (2016) and Chen et al. (2018)). This seems surprising, because, to our knowledge, there are no reports of wider networks degrading classification performance (indeed, the opposite is sometimes argued; see Zagoruyko & Komodakis, 2016), and because exact Bayesian inference is provably optimal, if the prior accurately describes our beliefs (Ramsey, 1926). Indeed, recent work on the Neural Tangent Kernel (NTK) (Li et al., 2019) has suggested that deterministic gradient descent in an infinite network gives slighly lower performance than Bayesian inference in the same network. Our hypothesis is that the poor performance of Bayesian infinite networks arises because the top-layer representation (equivalent to the kernel), is fixed by the network hyperparameters, and thus cannot be learned from data. This breaks many of our key intuitions about why deep networks are effective. For instance in transfer learning (Huh et al., 2016) we use a large-scale dataset such as ImageNet to a learn a good high-level representation, then apply this representation to other tasks where less data is available. However, transfer learning is impossible in infinite Bayesian neural networks, because the top-layer representation is fixed by the network hyperparameters and so cannot be learned using e.g. ImageNet. To understand these issues, we analysed finite networks using tools from the infinite network literature (Lee et al., 2018; Matthews et al., 2018; Garriga-Alonso et al., 2019; Novak et al., 2019). We begin by giving a toy, two-layer example, contrasting the flexibility of finite networks with the inflexibility of infinite networks, showing that flexible finite networks offer benefits under conditions of model-mismatch. We then introduce infinite networks with bottlenecks, which combine the theoretical tractability of infinite networks with the flexibility of finite networks. To obtain an analytic understanding of kernel/representation flexibility and learning in such networks, we consider linear infinite networks with bottlenecks, which are equivalent to finite deep linear networks. We took two approaches to characterising these networks. First, we considered the prior viewpoint, i.e. the covariance in the top-layer kernel induced by randomness in the lower-layer weights. In particular, we showed that narrower, deeper networks offer more flexibility, and that Why bigger is not always better: on finite and infinite neural networks CNNs offer more flexibility than locally connected networks (LCNs) when the input is spatially structured. Second, we considered the posterior viewpoint, showing that under both MAP inference and posterior sampling, the representations in learned neural networks slowly transition from being similar to the input kernel (i.e. the inner product of the inputs) to being similar to the output kernel (i.e. the inner product of one-hot vectors representing the labels). We found an important difference between MAP inference and sampling: for MAP inference, the learned representations transition from the input to the output kernel, irrespective of the network width. Bayesian networks behave similarly when the network width and the number of output channels are equal, but as the network width increases, the learned representations become increasingly dominated by the prior, and insensitive to the outputs. Remarkably, we find that in a ResNet trained using SGD on CIFAR-10, the representation differs dramatically from the corresponding infinite network and is instead very close to the output kernel, as suggested by our deep linear results. This confirms the importance of working with a theoretical model, such as infinite networks with bottlenecks, that is capable of capturing representation learning.",2020,ICML,-0.5
On the Convergence of Nesterov‚Äôs Accelerated Gradient Method in Stochastic Settings,"We study Nesterov‚Äôs accelerated gradient method with constant step-size and momentum parameters in the stochastic approximation setting (unbiased gradients with bounded variance) and the finite-sum setting (where randomness is due to sampling mini-batches). To build better insight into the behavior of Nesterov‚Äôs method in stochastic settings, we focus throughout on objectives that are smooth, strongly-convex, and twice continuously differentiable. In the stochastic approximation setting, Nesterov‚Äôs method converges to a neighborhood of the optimal point at the same accelerated rate as in the deterministic setting. Perhaps surprisingly, in the finite-sum setting, we prove that Nesterov‚Äôs method may diverge with the usual choice of step-size and momentum, unless additional conditions on the problem related to conditioning and data coherence are satisfied. Our results shed light as to why Nesterov‚Äôs method may fail to converge or achieve acceleration in the finite-sum setting.",2020,ICML,-0.5
PackIt: A Virtual Environment for Geometric Planning,"The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. We refer to this ability as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. We present PackIt, a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space. We also construct a set of challenging packing tasks using an evolutionary algorithm. Further, we study various baselines for the task that include model-free learning-based and heuristic-based methods, as well as search-based optimization methods that assume access to the model of the environment.2",2020,ICML,0.4
The Usual Suspects? Reassessing Blame for VAE Posterior Collapse,"In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.",2020,ICML,-1.0
Equivariant Neural Rendering,"We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.",2020,ICML,1.0
Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?,"Deep neural networks are typically initialized with random weights, with variances chosen to facilitate signal propagation and stable gradients. It is also believed that diversity of features is an important property of these initializations. We construct a deep convolutional network with identical features by initializing almost all the weights to 0. The architecture also enables perfect signal propagation and stable gradients, and achieves high accuracy on standard benchmarks. This indicates that random, diverse initializations are not necessary for training neural networks. An essential element in training this network is a mechanism of symmetry breaking; we study this phenomenon and find that standard GPU operations, which are non-deterministic, can serve as a sufficient source of symmetry breaking to enable training.",2020,ICML,1.0
Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing,"A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.",2020,ICML,0.4
PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination,"We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate transformer block outputs) and eliminating the redundant vectors. b) determining which wordvectors to eliminate by developing a strategy for measuring their significance, based on the selfattention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with < 1% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with < 1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT. The code for PoWER-BERT is publicly available at https: //github.com/IBM/PoWER-BERT.",2020,ICML,1.0
Circuit-Based Intrinsic Methods to Detect Overfitting,"The focus of this paper is on intrinsic methods to detect overfitting. By intrinsic methods, we mean methods that rely only on the model and the training data, as opposed to traditional methods (we call them extrinsic methods) that rely on performance on a test set or on bounds from model complexity. We propose a family of intrinsic methods called Counterfactual Simulation (CFS) which analyze the flow of training examples through the model by identifying and perturbing rare patterns. By applying CFS to logic circuits we get a method that has no hyper-parameters and works uniformly across different types of models such as neural networks, random forests and lookup tables. Experimentally, CFS can separate models with different levels of overfit using only their logic circuit representations without any access to the high level structure. By comparing lookup tables, neural networks, and random forests using CFS, we get insight into why neural networks generalize. In particular, we find that stochastic gradient descent in neural nets does not lead to ‚Äúbrute force‚Äù memorization, but finds common patterns (whether we train with actual or randomized labels), and neural networks are not unlike forests in this regard. Finally, we identify a limitation with our proposal that makes it unsuitable in an adversarial setting, but points the way to future work on robust intrinsic methods.",2020,ICML,-0.2
"Calibration, Entropy Rates, and Memory in Language Models","Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that stateof-the-art language models, including LSTMs and Transformers, are miscalibrated: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.",2020,ICML,-0.5
More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models,"Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under `‚àû attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.",2020,ICML,0.0
Low-loss connection of weight vectors: distribution-based approaches,"Recent research shows that sublevel sets of the loss surfaces of overparameterized networks are connected, exactly or approximately. We describe and compare experimentally a panel of methods used to connect two low-loss points by a low-loss curve on this surface. Our methods vary in accuracy and complexity. Most of our methods are based on ‚Äúmacroscopic‚Äù distributional assumptions, and some are insensitive to the detailed properties of the points to be connected. Some methods require a prior training of a ‚Äúglobal connection model‚Äù which can then be applied to any pair of points. The accuracy of the method generally correlates with its complexity and sensitivity to the endpoint detail.",2020,ICML,0.0
Interference and Generalization in Temporal Difference Learning,"We study the link between generalization and interference in temporal-difference (TD) learning. Interference is defined as the inner product of two different gradients, representing their alignment; this quantity emerges as being of interest from a variety of observations about neural networks, parameter sharing and the dynamics of learning. We find that TD easily leads to low-interference, under-generalizing parameters, while the effect seems reversed in supervised learning. We hypothesize that the cause can be traced back to the interplay between the dynamics of interference and bootstrapping. This is supported empirically by several observations: the negative relationship between the generalization gap and interference in TD, the negative effect of bootstrapping on interference and the local coherence of targets, and the contrast between the propagation rate of information in TD(0) versus TD(Œª) and regression tasks such as Monte-Carlo policy evaluation. We hope that these new findings can guide the future discovery of better bootstrapping methods.",2020,ICML,0.0
Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions,"Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.",2020,ICML,0.6000000000000001
Model-Based Reinforcement Learning with Value-Targeted Regression,"This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model P belongs to a known family of models P , a special case of which is when models in P take the form of linear mixtures: P‚úì = P d i=1 ‚úìiPi. We propose a model based RL algorithm that is based on the optimism principle: In each episode, the set of models that are ‚Äòconsistent‚Äô with the data collected is constructed. The criterion of consistency is based on the total squared error that the model incurs on the task of predicting state values as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, takes the form √ï(d p H3T ), where H , T and d are the horizon, the total number of steps and the dimension of ‚úì, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound ‚å¶( p HdT ). For a general model family P , the regret bound is derived based on the Eluder dimension.",2020,ICML,0.0
Online Multi-Kernel Learning with Graph-Structured Feedback,"is more powerful, as it learns the optimal kernel from a dicMulti-kernel learning (MKL) exhibits reliable performance in nonlinear function approximation tasks. Instead of using one kernel, it learns the optimal kernel from a pre-selected dictionary of kernels. The selection of the dictionary has crucial impact on both the performance and complexity of MKL. Specifcally, inclusion of a large number of irrelevant kernels may impair the accuracy, and increase the complexity of MKL algorithms. To enhance the accuracy, and alleviate the computational burden, the present paper develops a novel scheme which actively chooses relevant kernels. The proposed framework models the pruned kernel combination as feedback collected from a graph, that is refned ‚Äòon the fy.‚Äô Leveraging the random feature approximation, we propose an online scalable multi-kernel learning approach with graph feedback, and prove that the proposed algorithm enjoys sublinear regret. Numerical tests on real datasets demonstrate the effectiveness of the novel approach.",2020,ICML,0.7000000000000001
Training Linear Neural Networks: Non-Local Convergence and Complexity Results,"Linear networks provide valuable insights into the workings of neural networks in general. This paper identifies conditions under which the gradient flow provably trains a linear network, in spite of the non-strict saddle points present in the optimization landscape. This paper also provides the computational complexity of training linear networks with gradient flow. To achieve these results, this work develops a machinery to provably identify the stable set of gradient flow, which then enables us to improve over the state of the art in the literature of linear networks (Bah et al., 2019; Arora et al., 2018a). Crucially, our results appear to be the first to break away from the lazy training regime which has dominated the literature of neural networks. This work requires the network to have a layer with one neuron, which subsumes the networks with a scalar output, but extending the results of this theoretical work to all linear networks remains a challenging open problem.",2020,ICML,1.0
Bayesian Graph Neural Networks with Adaptive Connection Sampling,"We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates oversmoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters as in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boosting the performance of GNNs in semi-supervised node classification, making them less prone to over-smoothing and over-fitting with more robust prediction.",2020,ICML,0.8
Identifying Statistical Bias in Dataset Replication,"Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models‚Äô ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6%¬±1.5% of the original 11.7%¬±1.0% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available.",2020,ICML,0.2
Revisiting Fundamentals of Experience Replay,"Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay ‚Äî greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.",2020,ICML,0.4
Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models,"Discrete choice models with unobserved heterogeneity are commonly used Econometric models for dynamic Economic behavior which have been adopted in practice to predict behavior of individuals and firms from schooling and job choices to strategic decisions in market competition. These models feature optimizing agents who choose among a finite set of options in a sequence of periods and receive choice-specific payoffs that depend on both variables that are observed by the agent and recorded in the data and variables that are only observed by the agent but not recorded in the data. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. We show that in an important class of discrete choice models the value function is globally concave in the policy. That means that simple algorithms that do not require fixed point computation, such as the policy gradient algorithm, globally converge to the optimal policy. This finding can both be used to relax behavioral assumption regarding the optimizing agents and to facilitate Econometric analysis of dynamic behavior. In particular, we demonstrate significant computational advantages in using a simple implementation policy gradient algorithm over existing ‚Äúnested fixed point‚Äù algorithms used in Econometrics. Equal contribution Department of Computer Science, Northwestern University, Evanston, IL, USA Department of Economics, University of Virginia, Charlottesville, VA, USA. Correspondence to: Y.F. <yidingfeng2021@u.northwestern.edu>, E.K. <eak5rf@virginia.edu>, D.N. <denis@virginia.edu>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).",2020,ICML,0.2
Let‚Äôs Agree to Agree: Neural Networks Share Classification Order on Real Datasets,"We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries ‚Äì models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Specifically, when fixing the architecture, we describe synthetic datasets for which this pattern is no longer observed. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.",2020,ICML,0.5
Machine Translation Aided Bilingual Data-to-Text Generation and Semantic Parsing,"We present a system for bilingual Data-ToText Generation and Semantic Parsing. We use a text-to-text generator to learn a single model that works for both languages on each of the tasks. The model is aided by machine translation during both pre-training and fine-tuning. We evaluate the system on WebNLG 2020 data 1 , which consists of RDF triples in English and natural language sentences in English and Russian for both the tasks. We achieve considerable gains over monolingual models, especially on unseen relations and Russian.",2020,INLG,1.0
Towards a Multi-Dataset for Complex Emotions Learning Based on Deep Neural Networks,"In sentiment analysis, several researchers have used emoji and hashtags as specific forms of training and supervision. Some emotions, such as fear and disgust, are underrepresented in the text of social media. Others, such as anticipation, are absent. This research paper proposes a new dataset for complex emotion detection using a combination of several existing corpora in order to represent and interpret complex emotions based on the Plutchik’s theory. Our experiments and evaluations confirm that using Transfer Learning (TL) with a rich emotional corpus, facilitates the detection of complex emotions in a four-dimensional space. In addition, the incorporation of the rule on the reverse emotions in the model’s architecture brings a significant improvement in terms of precision, recall, and F-score.",2020,LREC,1.0
Out-of-the-Box and into the Ditch? Multilingual Evaluation of Generic Text Extraction Tools,"This article examines extraction methods designed to retain the main text content of web pages and discusses how the extraction could be oriented and evaluated: can and should it be as generic as possible to ensure opportunistic corpus construction? The evaluation grounds on a comparative benchmark of open-source tools used on pages in five different languages (Chinese, English, Greek, Polish and Russian), it features several metrics to obtain more fine-grained differentiations. Our experiments highlight the diversity of web page layouts across languages or publishing countries. These discrepancies are reflected by diverging performances so that the right tool has to be chosen accordingly.",2020,LREC,0.2
Stress Test Evaluation of Transformer-based Models in Natural Language Understanding Tasks,"There has been significant progress in recent years in the field of Natural Language Processing thanks to the introduction of the Transformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have shown impressive results on several downstream tasks. Many researchers have studied previous (non-Transformer) models to understand their actual behavior under different scenarios, showing that these models are taking advantage of clues or failures of datasets and that slight perturbations on the input data can severely reduce their performance. In contrast, recent models have not been systematically tested with adversarial-examples in order to show their robustness under severe stress conditions. For that reason, this work evaluates three Transformer-based models (RoBERTa, XLNet, and BERT) in Natural Language Inference (NLI) and Question Answering (QA) tasks to know if they are more robust or if they have the same flaws as their predecessors. As a result, our experiments reveal that RoBERTa, XLNet and BERT are more robust than recurrent neural network models to stress tests for both NLI and QA tasks. Nevertheless, they are still very fragile and demonstrate various unexpected behaviors, thus revealing that there is still room for future improvement in this field.",2020,LREC,-0.4
Beyond lexical semantics: notes on pragmatic frames,"Framenets as an incarnation of frame semantics have been set up to deal with lexicographic issues (cf. Fillmore and Baker 2010, among others). They are thus concerned with lexical units (LUs) and the conceptual structure which categorizes these together. These lexically-evoked frames, however, do not reflect pragmatic properties of constructions (LUs and other types of constructions), such as expressing illocutions or being considered polite or very informal. From the viewpoint of a multilingual annotation effort, the Global FrameNet Shared Annotation Task, we discuss two phenomena, greetings and tag questions, which highlight the necessity both to investigate the role between construction and frame annotation on the one hand and to develop pragmatic frames describing social interactions which are not explicitly lexicalized.",2020,LREC,-1.0
Dataset Reproducibility and IR Methods in Timeline Summarization,"Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems.",2020,LREC,-0.4
Preparation of Bangla Speech Corpus from Publicly Available Audio & Text,"Automatic speech recognition systems require large annotated speech corpus. The manual annotation of a large corpus is very difficult. In this paper, we focus on the automatic preparation of a speech corpus for Bangladeshi Bangla. We have used publicly available Bangla audiobooks and TV news recordings as audio sources. We designed and implemented an iterative algorithm that takes as input a speech corpus and a huge amount of raw audio (without transcription) and outputs a much larger speech corpus with reasonable confidence. We have leveraged speaker diarization, gender detection, etc. to prepare the annotated corpus. We also have prepared a synthetic speech corpus for handling out-of-vocabulary word problems in Bangla language. Our corpus is suitable for training with Kaldi. Experimental results show that the use of our corpus in addition to the Google Speech corpus (229 hours) significantly improves the performance of the ASR system.",2020,LREC,1.0
Irony Detection in Persian Language: A Transfer Learning Approach Using Emoji Prediction,"Irony is a linguistic device used to intend an idea while articulating an opposing expression. Many text analytic algorithms used for emotion extraction or sentiment analysis, produce invalid results due to the use of irony. Persian speakers use this device more often due to the language’s nature and some cultural reasons. This phenomenon also appears in social media platforms such as Twitter where users express their opinions using ironic or sarcastic posts. In the current research, which is the first attempt at irony detection in Persian language, emoji prediction is used to build a pretrained model. The model is finetuned utilizing a set of hand labeled tweets with irony tags. A bidirectional LSTM (BiLSTM) network is employed as the basis of our model which is improved by attention mechanism. Additionally, a Persian corpus for irony detection containing 4339 manually-labeled tweets is introduced. Experiments show the proposed approach outperforms the adapted state-of-the-art method tested on Persian dataset with an accuracy of 83.1%, and offers a strong baseline for further research in Persian language.",2020,LREC,1.0
Distributional Semantics for Neo-Latin,"We address the problem of creating and evaluating quality Neo-Latin word embeddings for the purpose of philosophical research, adapting the Nonce2Vec tool to learn embeddings from Neo-Latin sentences. This distributional semantic modeling tool can learn from tiny data incrementally, using a larger background corpus for initialization. We conduct two evaluation tasks: definitional learning of Latin Wikipedia terms, and learning consistent embeddings from 18th century Neo-Latin sentences pertaining to the concept of mathematical method. Our results show that consistent Neo-Latin word embeddings can be learned from this type of data. While our evaluation results are promising, they do not reveal to what extent the learned models match domain expert knowledge of our Neo-Latin texts. Therefore, we propose an additional evaluation method, grounded in expert-annotated data, that would assess whether learned representations are conceptually sound in relation to the domain of study.",2020,LREC,0.9
A Limitation of the PAC-Bayes Framework,"PAC-Bayes is a useful framework for deriving generalization bounds which was introduced by McAllester ('98). This framework has the flexibility of deriving distribution- and algorithm-dependent bounds, which are often tighter than VC-related uniform convergence bounds. In this manuscript we present a limitation for the PAC-Bayes framework. We demonstrate an easy learning task which is not amenable to a PAC-Bayes analysis. Specifically, we consider the task of linear classification in 1D; it is well-known that this task is learnable using just O(log(1/δ)/ϵ)",2020,NIPS,-0.5
Spin-Weighted Spherical CNNs Authors Abstract,"Learning equivariant representations is a promising way to reduce sample and model complexity and improve the generalization performance of deep neural networks. The spherical CNNs are successful examples, producing SO(3)-equivariant representations of spherical inputs. There are two main types of spherical CNNs. The first type lifts the inputs to functions on the rotation group SO(3) and applies convolutions on the group, which are computationally expensive since SO(3) has one extra dimension. The second type applies convolutions directly on the sphere, which are limited to zonal (isotropic) filters, and thus have limited expressivity. In this paper, we present a new type of spherical CNN that allows anisotropic filters in an efficient way, without ever leaving the spherical domain. The key idea is to consider spin-weighted spherical functions, which were introduced in physics in the study of gravitational waves. These are complex-valued functions on the sphere whose phases change upon rotation. We define a convolution between spin-weighted functions and build a CNN based on it. The spin-weighted functions can also be interpreted as spherical vector fields, allowing applications to tasks where the inputs or outputs are vector fields. Experiments show that our method outperforms previous methods on tasks like classification of spherical images, classification of 3D shapes and semantic segmentation of spherical panoramas.",2020,NIPS,1.0
Meta-learning from Tasks with Heterogeneous Attribute Spaces Authors Abstract,"We propose a heterogeneous meta-learning method that trains a model on tasks with various attribute spaces, such that it can solve unseen tasks whose attribute spaces are different from the training tasks given a few labeled instances. Although many meta-learning methods have been proposed, they assume that all training and target tasks share the same attribute space, and they are inapplicable when attribute sizes are different across tasks. Our model infers latent representations of each attribute and each response from a few labeled instances using an inference network. Then, responses of unlabeled instances are predicted with the inferred representations using a prediction network. The attribute and response representations enable us to make predictions based on the task-specific properties of attributes and responses even when attribute and response sizes are different across tasks. In our experiments with synthetic datasets and 59 datasets in OpenML, we demonstrate that our proposed method can predict the responses given a few labeled instances in new tasks after being trained with tasks with heterogeneous attribute spaces.",2020,NIPS,0.9
Learning Dynamic Belief Graphs to Generalize on Text-Based Games Authors Abstract,"Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%.",2020,NIPS,1.0
Understanding the Role of Training Regimes in Continual Learning Authors Abstract,"Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes ‚Äì learning rate, batch size, regularization method‚Äì can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks‚Äô local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.",2020,NIPS,1.0
Learning Certified Individually Fair Representations Authors Abstract,"Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at `‚àû-distance at most , thus allowing data consumers to certify individual fairness by proving -robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.",2020,NIPS,1.0
Continual Learning with Node-Importance based Adaptive Group Sparse Regularization Authors Abstract,"We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each neural network node based on its the importance, which is adaptively updated after learning each task. By utilizing the proximal gradient descent method, the exact sparsity and freezing of the model is guaranteed during the learning process, and thus, the learner explicitly controls the model capacity. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to facilitate efficient learning and prevent the negative transfer. Throughout the extensive experimental results, we show that our AGS-CL uses orders of magnitude less memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative benchmarks for both supervised and reinforcement learning.",2020,NIPS,1.0
On ranking via sorting by estimated expected utility Authors Abstract,"Ranking tasks are defined through losses that measure trade-offs between different desiderata such as the relevance and the diversity of the items at the top of the list. This paper addresses the question of which of these tasks are asymptotically solved by sorting by decreasing order of expected utility, for some suitable notion of utility, or, equivalently, when is square loss regression consistent for ranking via score-andsort? We answer to this question by finding a characterization of ranking losses for which a suitable regression is consistent. This characterization has two strong corollaries. First, whenever there exists a consistent approach based on convex risk minimization, there also is a consistent approach based on regression. Second, when regression is not consistent, there are data distributions for which consistent surrogate approaches necessarily have non-trivial local minima, and for which optimal scoring function are necessarily discontinuous, even when the underlying data distribution is regular. In addition to providing a better understanding of surrogate approaches for ranking, these results illustrate the intrinsic difficulty of solving general ranking problems with the score-and-sort approach.",2020,NIPS,-0.30000000000000004
Greedy inference with structure-exploiting lazy maps Authors Abstract,"We propose a framework for solving high-dimensional Bayesian inference problems using structure-exploiting low-dimensional transport maps or flows. These maps are confined to a low-dimensional subspace (hence, lazy), and the subspace is identified by minimizing an upper bound on the Kullback‚ÄìLeibler divergence (hence, structured). Our framework provides a principled way of identifying and exploiting low-dimensional structure in an inference problem. It focuses the expressiveness of a transport map along the directions of most significant discrepancy from the posterior, and can be used to build deep compositions of lazy maps, where low-dimensional projections of the parameters are iteratively transformed to match the posterior. We prove weak convergence of the generated sequence of distributions to the posterior, and we demonstrate the benefits of the framework on challenging inference problems in machine learning and differential equations, using inverse autoregressive flows and polynomial maps as examples of the underlying density estimators.",2020,NIPS,1.0
BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images Authors Abstract,"We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the whole scene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects‚Äô appearance, such as shadow and lighting, and provides control over each object‚Äôs 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity). Our code is available at https://github.com/thunguyenphuoc/BlockGAN.",2020,NIPS,1.0
Critic Regularized Regression Authors Abstract,"Offline reinforcement learning (RL), also known as batch RL, offers the prospect of policy optimization from large pre-recorded datasets without online environment interaction. It addresses challenges with regard to the cost of data collection and safety, both of which are particularly pertinent to real-world applications of RL. Unfortunately, most off-policy algorithms perform poorly when learning from a fixed dataset. In this paper, we propose a novel offline RL algorithm to learn policies from data using a form of critic-regularized regression (CRR). We find that CRR performs surprisingly well and scales to tasks with high-dimensional state and action spaces ‚Äì outperforming several state-of-the-art offline RL algorithms by a significant margin on a wide range of benchmark tasks.",2020,NIPS,0.8
Semantic Visual Navigation by Watching YouTube Videos Authors Abstract,"The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.",2020,NIPS,1.0
Bayesian Bits: Unifying Quantization and Pruning Authors Abstract,"We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents.",2020,NIPS,1.0
SMYRF - Efficient Attention using Asymmetric Clustering Authors Abstract,"We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from O(N) to O(N logN), where N is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using 50% less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.",2020,NIPS,1.0
Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction Authors Abstract,"We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is 10√ó larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by 42.7% reduction in Chamfer and Point-to-Surface Distances, and 19.4% reduction in normal estimation errors.",2020,NIPS,1.0
Bad Global Minima Exist and SGD Can Reach Them Authors Abstract,"Several works have aimed to explain why overparameterized neural networks generalize well when trained by Stochastic Gradient Descent (SGD). The consensus explanation that has emerged credits the randomized nature of SGD for the bias of the training process towards low-complexity models and, thus, for implicit regularization. We take a careful look at this explanation in the context of image classification with common deep neural network architectures. We find that if we do not regularize explicitly, then SGD can be easily made to converge to poorlygeneralizing, high-complexity models: all it takes is to first train on a random labeling on the data, before switching to properly training with the correct labels. In contrast, we find that in the presence of explicit regularization, pretraining with random labels has no detrimental effect on SGD. We believe that our results give evidence that explicit regularization plays a far more important role in the success of overparameterized neural networks than what has been understood until now. Specifically, by penalizing complicated models independently of their fit to the data, regularization affects training dynamics also far away from optima, making simple models that fit the data well discoverable by local methods, such as SGD.",2020,NIPS,0.0
Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study Authors Abstract,"The notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. This notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. Recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic ConvexOptimization. As a first step, we provide a simple construction that rules out the existence of a distribution-independent implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of distribution-dependent implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to significant difficulties in providing a comprehensive explanation of an algorithm‚Äôs generalization performance by solely arguing about its implicit regularization properties.",2020,NIPS,0.0
A Scalable Approach for Privacy-Preserving Collaborative Machine Learning Authors Abstract,"We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to 16√ó speedup in the training time against the benchmark protocols.",2020,NIPS,0.4
Reinforcement Learning for Control with Multiple Frequencies Authors Abstract,"Many real-world sequential decision problems involve multiple action variables whose control frequencies are different, such that actions take their effects at different periods. While these problems can be formulated with the notion of multiple action persistences in factored-action MDP (FA-MDP), it is non-trivial to solve them efficiently since an action-persistent policy constructed from a stationary policy can be arbitrarily suboptimal, rendering solution methods for the standard FA-MDPs hardly applicable. In this paper, we formalize the problem of multiple control frequencies in RL and provide its efficient solution method. Our proposed method, Action-Persistent Policy Iteration (AP-PI), provides a theoretical guarantee on the convergence to an optimal solution while incurring only a factor of |A| increase in time complexity during policy improvement step, compared to the standard policy iteration for FA-MDPs. Extending this result, we present ActionPersistent Actor-Critic (AP-AC), a scalable RL algorithm for high-dimensional control tasks. In the experiments, we demonstrate that AP-AC significantly outperforms the baselines on several continuous control tasks and a traffic control simulation, which highlights the effectiveness of our method that directly optimizes the periodic non-stationary policy for tasks with multiple control frequencies.",2020,NIPS,0.5
Diverse Image Captioning with Context-Object Split Latent Spaces Authors Abstract,"Diverse image captioning models aim to learn one-to-many mappings that are innate to cross-domain datasets, such as of images and texts. Current methods for this task are based on generative latent variable models, e.g. VAEs with structured latent spaces. Yet, the amount of multimodality captured by prior work is limited to that of the paired training data ‚Äì the true diversity of the underlying generative process is not fully captured. To address this limitation, we leverage the contextual descriptions in the dataset that explain similar contexts in different visual scenes. To this end, we introduce a novel factorization of the latent space, termed contextobject split, to model diversity in contextual descriptions across images and texts within the dataset. Our framework1 not only enables diverse captioning through context-based pseudo supervision, but extends this to images with novel objects and without paired captions in the training data. We evaluate our COS-CVAE approach on the standard COCO dataset and on the held-out COCO dataset consisting of images with novel objects, showing significant gains in accuracy and diversity.",2020,NIPS,0.6000000000000001
Restoring Negative Information in Few-Shot Object Detection Authors Abstract,"Few-shot learning has recently emerged as a new challenge in the deep learning field: unlike conventional methods that train the deep neural networks (DNNs) with a large number of labeled data, it asks for the generalization of DNNs on new classes with few annotated samples. Recent advances in few-shot learning mainly focus on image classification while in this paper we focus on object detection. The initial explorations in few-shot object detection tend to simulate a classification scenario by using the positive proposals in images with respect to certain object class while discarding the negative proposals of that class. Negatives, especially hard negatives, however, are essential to the embedding space learning in few-shot object detection. In this paper, we restore the negative information in few-shot object detection by introducing a new negativeand positive-representative based metric learning framework and a new inference scheme with negative and positive representatives. We build our work on a recent few-shot pipeline RepMet [1] with several new modules to encode negative information for both training and testing. Extensive experiments on ImageNet-LOC and PASCAL VOC show our method substantially improves the state-of-the-art few-shot object detection solutions. Our code is available at https://github.com/yang-yk/NP-RepMet.",2020,NIPS,1.0
Learning Physical Constraints with Neural Projections Authors Abstract,"We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator lies at the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.",2020,NIPS,0.7000000000000001
Dissecting Neural ODEs Authors Abstract,"Continuous deep learning architectures have recently re‚Äìemerged as Neural Ordinary Differential Equations (Neural ODEs). This infinite‚Äìdepth approach theoretically bridges the gap between deep learning and dynamical systems, offering a novel perspective. However, deciphering the inner working of these models is still an open challenge, as most applications apply them as generic black‚Äìbox modules. In this work we ‚Äúopen the box‚Äù, further developing the continuous‚Äìdepth formulation with the aim of clarifying the influence of several design choices on the underlying dynamics.",2020,NIPS,0.4
Seeing the Forest and the Trees: Detection and Cross-Document Coreference Resolution of Militarized Interstate Disputes,"Previous efforts to automate the detection of social and political events in text have primarily focused on identifying events described within single sentences or documents. Within a corpus of documents, these automated systems are unable to link event references—recognize singular events across multiple sentences or documents. A separate literature in computational linguistics on event coreference resolution attempts to link known events to one another within (and across) documents. I provide a data set for evaluating methods to identify certain political events in text and to link related texts to one another based on shared events. The data set, Headlines of War, is built on the Militarized Interstate Disputes data set and offers headlines classified by dispute status and headline pairs labeled with coreference indicators. Additionally, I introduce a model capable of accomplishing both tasks. The multi-task convolutional neural network is shown to be capable of recognizing events and event coreferences given the headlines’ texts and publication dates.",2020,WS,1.0
Event Clustering within News Articles,"This paper summarizes our group’s efforts in the event sentence coreference identification shared task, which is organized as part of the Automated Extraction of Socio-Political Events from News (AESPEN) Workshop. Our main approach consists of three steps. We initially use a transformer based model to predict whether a pair of sentences refer to the same event or not. Later, we use these predictions as the initial scores and recalculate the pair scores by considering the relation of sentences in a pair with respect to other sentences. As the last step, final scores between these sentences are used to construct the clusters, starting with the pairs with the highest scores. Our proposed approach outperforms the baseline approach across all evaluation metrics.",2020,WS,1.0
Supervised Event Coding from Text Written in Arabic: Introducing Hadath,"This article introduces Hadath, a supervised protocol for coding event data from text written in Arabic. Hadath contributes to recent efforts in advancing multi-language event coding using computer-based solutions. In this application, we focus on extracting event data about the conflict in Afghanistan from 2008 to 2018 using Arabic information sources. The implementation relies first on a Machine Learning algorithm to classify news stories relevant to the Afghan conflict. Then, using Hadath, we implement the Natural Language Processing component for event coding from Arabic script. The output database contains daily geo-referenced information at the district level on who did what to whom, when and where in the Afghan conflict. The data helps to identify trends in the dynamics of violence, the provision of governance, and traditional conflict resolution in Afghanistan for different actors over time and across space.",2020,WS,0.7000000000000001
Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation,"The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a Heterogeneous Graph-Based Encoder to represent the conversation content (i.e., the dialogue history, its emotion flow, facial expressions, and speakers‚Äô personalities) with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an Emotion-Personality-Aware Decoder to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",2021,AAAI,1.0
A SAT-based Resolution of Lam's Problem,"In 1989, computer searches by Lam, Thiel, and Swiercz experimentally resolved Lam‚Äôs problem from projective geometry‚Äîthe long-standing problem of determining if a projective plane of order ten exists. Both the original search and an independent verification in 2011 discovered no such projective plane. However, these searches were each performed using highly specialized custom-written code and did not produce nonexistence certificates. In this paper, we resolve Lam‚Äôs problem by translating the problem into Boolean logic and use satisfiability (SAT) solvers to produce nonexistence certificates that can be verified by a third party. Our work uncovered consistency issues in both previous searches‚Äîhighlighting the difficulty of relying on specialpurpose search code for nonexistence results.",2021,AAAI,-0.6000000000000001
Joint Semantic-geometric Learning for Polygonal Building Segmentation,"Building extraction from aerial or satellite images has been an important research problem in remote sensing and computer vision domains for decades. Compared with pixel-wise semantic segmentation models that output raster building segmentation map, polygonal building segmentation approaches produce more realistic building polygons that are in the desirable vector format for practical applications. Despite the substantial efforts over recent years, state-of-the-art polygonal building segmentation methods still suffer from several limitations, e.g., (1) relying on a perfect segmentation map to guarantee the vectorization quality; (2) requiring a complex post-processing procedure; (3) generating inaccurate vertices with a fixed quantity, a wrong sequential order, self-intersections, etc. To tackle the above issues, in this paper, we propose a polygonal building segmentation approach and make the following contributions: (1) We design a multitask segmentation network for joint semantic and geometric learning via three tasks, i.e., pixel-wise building segmentation, multi-class corner prediction, and edge orientation prediction. (2) We propose a simple but effective vertex generation module for transforming the segmentation contour into high-quality polygon vertices. (3) We further propose a polygon refinement network that automatically moves the polygon vertices into more accurate locations. Results on two popular building segmentation datasets demonstrate that our approach achieves significant improvements for both building instance segmentation (with 2% F1-score gain) and polygon vertex prediction (with 6% F1-score gain) compared with current state-of-the-art methods.",2021,AAAI,0.9
A Scalable Reasoning and Learning Approach for Neural-Symbolic Stream Fusion,"Driven by deep neural networks (DNN), the recent development of computer vision makes vision sensors such as stereo cameras and Lidars ubiquitous in autonomous cars, robotics and traffic monitoring. However, a traditional DNN-based data fusion pipeline like object tracking has to hard-wire an engineered set of DNN models to a fixed processing logic, which makes it difficult to infuse new models to that pipeline. To overcome this, we propose a novel neural-symbolic stream reasoning approach realised by semantic stream reasoning programs which specify DNN-based data fusion pipelines via logic rules with learnable probabilistic degrees as weights. The reasoning task over this program is governed by a novel incremental reasoning algorithm, which lends itself also as a core building block for a scalable and parallel algorithm to learn the weights for such program. Extensive experiments with our first prototype on multi-object tracking benchmarks for autonomous driving and traffic monitoring show that our flexible approach can considerably improve both accuracy and processing throughput compared to the DNN-based counterparts.",2021,AAAI,0.9
Nutri-bullets: Summarizing Health Studies by Composing Segments,"We introduce Nutri-bullets, a multi-document summarization task for health and nutrition. First, we present two datasets of food and health summaries from multiple scientific studies. Furthermore, we propose a novel extract-compose model to solve the problem in the regime of limited parallel data. We explicitly select key spans from several abstracts using a policy network, followed by composing the selected spans to present a summary via a task specific language model. Compared to state-of-the-art methods, our approach leads to more faithful, relevant and diverse summarization ‚Äì properties imperative to this application. For instance, on the BreastCancer dataset our approach gets a more than 50% improvement on relevance and faithfulness.",2021,AAAI,1.0
Simpson's Bias in NLP Training,"In most machine learning tasks, we evaluate a modelM on a given data population S by measuring a population-level metric F(S;M). Examples of such evaluation metric F include precision/recall for (binary) recognition, the F1 score for multi-class classification, and the BLEU metric for language generation. On the other hand, the modelM is trained by optimizing a sample-level loss G(St;M) at each learning step t, where St is a subset of S (a.k.a. the mini-batch). Popular choices of G include cross-entropy loss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption behind this paradigm is that the mean value of the samplelevel loss G, if averaged over all possible samples, should effectively represent the population-level metric F of the task, such as, that E[G(St;M)] ‚âà F(S;M). In this paper, we systematically investigate the above assumption in several NLP tasks. We show, both theoretically and experimentally, that some popular designs of the sample-level loss G may be inconsistent with the true population-level metric F of the task, so that models trained to optimize the former can be substantially sub-optimal to the latter, a phenomenon we call it, Simpson‚Äôs bias, due to its deep connections with the classic paradox known as Simpson‚Äôs reversal paradox in statistics and social sciences.",2021,AAAI,-1.0
Learning to Resolve Conflicts for Multi-Agent Path Finding with Conflict-Based Search,"Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent path finding. On the high level, CBS repeatedly detects conflicts and resolves one of them by splitting the current problem into two subproblems. Previous work chooses the conflict to resolve by categorizing conflicts into three classes and always picking one from the highest-priority class. In this work, we propose an oracle for conflict selection that results in smaller search tree sizes than the one used in previous work. However, the computation of the oracle is slow. Thus, we propose a machine-learning (ML) framework for conflict selection that observes the decisions made by the oracle and learns a conflict-selection strategy represented by a linear ranking function that imitates the oracle‚Äôs decisions accurately and quickly. Experiments on benchmark maps indicate that our approach, ML-guided CBS, significantly improves the success rates, search tree sizes and runtimes of the current state-of-the-art CBS solver.",2021,AAAI,1.0
RESA: Recurrent Feature-Shift Aggregator for Lane Detection,"Lane detection is one of the most important tasks in selfdriving. Due to various complex scenarios (e.g., severe occlusion, ambiguous lanes, etc.) and the sparse supervisory signals inherent in lane annotations, lane detection task is still challenging. Thus, it is difficult for the ordinary convolutional neural network (CNN) to train in general scenes to catch subtle lane feature from the raw image. In this paper, we present a novel module named REcurrent Feature-Shift Aggregator (RESA) to enrich lane feature after preliminary feature extraction with an ordinary CNN. RESA takes advantage of strong shape priors of lanes and captures spatial relationships of pixels across rows and columns. It shifts sliced feature map recurrently in vertical and horizontal directions and enables each pixel to gather global information. RESA can conjecture lanes accurately in challenging scenarios with weak appearance clues by aggregating sliced feature map. Moreover, we propose a Bilateral Up-Sampling Decoder that combines coarse-grained and fine-detailed features in the upsampling stage. It can recover the low-resolution feature map into pixel-wise prediction meticulously. Our method achieves state-of-the-art results on two popular lane detection benchmarks (CULane and Tusimple). Code has been made available at: https://github.com/ZJULearning/resa.",2021,AAAI,1.0
On the Softmax Bottleneck of Recurrent Language Models,"Recent research has pointed to a limitation of word-level neural language models with softmax outputs. This limitation, known as the ‚Äúsoftmax bottleneck‚Äù refers to the inability of these models to produce high-rank log probability (logP ) matrices. Various solutions have been proposed to break this bottleneck, including Mixture of Softmaxes, SigSoftmax, and Linear Monotonic Softmax with Piecewise Linear Increasing Functions. They were reported to offer better performance in terms of perplexity on test data. A natural perception from these results is a strong positive correlation between the rank of the logP matrix and the model‚Äôs performance. In this work, we show via an extensive empirical study that such a correlation is fairly weak and that the high-rank of the logP matrix is neither necessary nor sufficient for better test perplexity. Although our results are empirical, they are established in part via the construction of a rich family of models, which we call Generalized SigSoftmax. They are able to create diverse ranks for the logP matrices. We also present an investigation as to why the proposed solutions achieve better performance.",2021,AAAI,0.0
Understanding Decoupled and Early Weight Decay,"Weight decay (WD) is a traditional regularization technique in deep learning, but despite its ubiquity, its behavior is still an area of active research. Golatkar et al. have recently shown that WD only matters at the start of the training in computer vision, upending traditional wisdom. Loshchilov et al. show that for adaptive optimizers, manually decaying weights can outperform adding an l2 penalty to the loss. This technique has become increasingly popular and is referred to as decoupled WD. The goal of this paper is to investigate these two recent empirical observations. We demonstrate that by applying WD only at the start, the network norm stays small throughout training. This has a regularizing effect as the effective gradient updates become larger. However, traditional generalizations metrics fail to capture this effect of WD, and we show how a simple scale-invariant metric can. We also show how the growth of network weights is heavily influenced by the dataset and its generalization properties. For decoupled WD, we perform experiments in NLP and RL where adaptive optimizers are the norm. We demonstrate that the primary issue that decoupled WD alleviates is the mixing of gradients from the objective function and the l2 penalty in the buffers of Adam (which stores the estimates of the first-order moment). Adaptivity itself is not problematic and decoupled WD ensures that the gradients from the l2 term cannot ‚Äùdrown out‚Äù the true objective, facilitating easier hyperparameter tuning.",2021,AAAI,-0.5
"What's the Best Place for an AI Conference, Vancouver or _______: Why Completing Comparative Questions is Difficult","Although large neural language models (LMs) like BERT can be finetuned to yield state-of-the-art results on many NLP tasks, it is often unclear what these models actually learn. Here we study using such LMs to fill in entities in humanauthored comparative questions, like ‚ÄúWhich country is older, India or ?‚Äù‚Äîi.e., we study the ability of neural LMs to ask (not answer) reasonable questions. We show that accuracy in this fill-in-the-blank task is well-correlated with human judgements of whether a question is reasonable, and that these models can be trained to achieve nearly human-level performance in completing comparative questions in three different subdomains. However, analysis shows that what they learn fails to model any sort of broad notion of which entities are semantically comparable or similar‚Äîinstead the trained models are very domain-specific, and performance is highly correlated with co-occurrences between specific entities observed in the training set. This is true both for models that are pretrained on general text corpora, as well as models trained on a large corpus of comparison questions. Our study thus reinforces recent results on the difficulty of making claims about a deep model‚Äôs world knowledge or linguistic competence based on performance on specific benchmark problems. We make our evaluation datasets publicly available to foster future research on complex understanding and reasoning in such models at standards of human interaction.",2021,AAAI,-0.5
Semantics-Aware Inferential Network for Natural Language Understanding,"For natural language understanding tasks, either machine reading comprehension or natural language inference, both semantics-aware and inference are favorable features of the concerned modeling for better understanding performance. Thus we propose a Semantics-Aware Inferential Network (SAIN) to meet such a motivation. Taking explicit contextualized semantics as a complementary input, the inferential module of SAIN enables a series of reasoning steps over semantic clues through an attention mechanism. By stringing these steps, the inferential network effectively learns to perform iterative reasoning which incorporates both explicit semantics and contextualized representations. In terms of well pre-trained language models as front-end encoder, our model achieves significant improvement on 11 tasks including machine reading comprehension and natural language inference.",2021,AAAI,1.0
A Lightweight Neural Model for Biomedical Entity Linking,"Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base. The specific challenge in this context is that the same biomedical entity can have a wide range of names, including synonyms, morphological variations, and names with different word orderings. Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources. Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks.",2021,AAAI,0.5
Social-DPF: Socially Acceptable Distribution Prediction of Futures,"We consider long-term path forecasting problems in crowds, where future sequence trajectories are generated given a short observation. Recent methods for this problem have focused on modeling social interactions and predicting multi-modal futures. However, it is not easy for machines to successfully consider social interactions, such as avoiding collisions while considering the uncertainty of futures under a highly interactive and dynamic scenario. In this paper, we propose a model that incorporates multiple interacting motion sequences jointly and predicts multi-modal socially acceptable distributions of futures. Specifically, we introduce a new aggregation mechanism for social interactions, which selectively models long-term inter-related dynamics between movements in a shared environment through a message passing mechanism. Moreover, we propose a loss function that not only accesses how accurate the estimated distributions of the futures are but also considers collision avoidance. We further utilize mixture density functions to describe the trajectories and learn multi-modality of future paths. Extensive experiments over several trajectory prediction benchmarks demonstrate that our method is able to forecast socially acceptable distributions in complex scenarios.",2021,AAAI,1.0
ACT: an Attentive Convolutional Transformer for Efficient Text Classification,"Recently, Transformer has been demonstrating promising performance in many NLP tasks and showing a trend of replacing Recurrent Neural Network (RNN). Meanwhile, less attention is drawn to Convolutional Neural Network (CNN) due to its weak ability in capturing sequential and longdistance dependencies, although it has excellent local feature extraction capability. In this paper, we introduce an Attentive Convolutional Transformer (ACT) that takes the advantages of both Transformer and CNN for efficient text classification. Specifically, we propose a novel attentive convolution mechanism that utilizes the semantic meaning of convolutional filters attentively to transform text from complex word space to a more informative convolutional filter space where important n-grams are captured. ACT is able to capture both local and global dependencies effectively while preserving sequential information. Experiments on various text classification tasks and detailed analyses show that ACT is a lightweight, fast, and effective universal text classifier, outperforming CNNs, RNNs, and attentive models including Transformer.",2021,AAAI,1.0
Exploratory Machine Learning with Unknown Unknowns,"In conventional supervised learning, a training dataset is given with ground-truth labels from a known label set, and the learned model will classify unseen instances to known labels. In real situations, when the learned models do not work well, learners generally attribute the model failure to the inadequate selection of learning algorithms or the lack of enough labeled training samples. In this paper, we point out that there is an important category of failure, which owes to the fact that there are unknown classes in the training data misperceived as other labels, and thus their existence is unknown from the given supervision. Such problems of unknown unknown classes can hardly be addressed by common re-selection of algorithms or accumulation of training samples. For this purpose, we propose the exploratory machine learning, where in this paradigm once learner encounters unsatisfactory learning performance, she can examine the possibility and, if unknown unknowns really exist, deploy the optimal strategy of feature space augmentation to make unknown classes observable and be enabled for learning. Theoretical analysis and empirical study on both synthetic and real datasets validate the efficacy of our proposal.",2021,AAAI,1.0
A Flexible Framework for Communication-Efficient Machine Learning,"With the increasing scale of machine learning tasks, it has become essential to reduce the communication between computing nodes. Early work on gradient compression focused on the bottleneck between CPUs and GPUs, but communicationefficiency is now needed in a variety of different system architectures, from high-performance clusters to energyconstrained IoT devices. In the current practice, compression levels are typically chosen before training and settings that work well for one task may be vastly sub-optimal for another dataset on another architecture. In this paper, we propose a flexible framework which adapts the compression level to the true gradient at each iteration, maximizing the improvement in the objective function that is achieved per communicated bit. Our framework is easy to adapt from one technology to the next by modeling how the communication cost depends on the compression level for the specific technology. Theoretical results and practical experiments indicate that the automatic tuning strategies significantly increase communication efficiency on several state-of-the-art compression schemes.",2021,AAAI,1.0
Learning Accurate and Interpretable Decision Rule Sets from Neural Networks,"This paper proposes a new paradigm for learning a set of independent logical rules in disjunctive normal form as an interpretable model for classification. We consider the problem of learning an interpretable decision rule set as training a neural network in a specific, yet very simple two-layer architecture. Each neuron in the first layer directly maps to an interpretable if-then rule after training, and the output neuron in the second layer directly maps to a disjunction of the firstlayer rules to form the decision rule set. Our representation of neurons in this first rules layer enables us to encode both the positive and the negative association of features in a decision rule. State-of-the-art neural net training approaches can be leveraged for learning highly accurate classification models. Moreover, we propose a sparsity-based regularization approach to balance between classification accuracy and the simplicity of the derived rules. Our experimental results show that our method can generate more accurate decision rule sets than other state-of-the-art rule-learning algorithms with better accuracy-simplicity trade-offs. Further, when compared with uninterpretable black-box machine learning approaches such as random forests and full-precision deep neural networks, our approach can easily find interpretable decision rule sets that have comparable predictive performance.",2021,AAAI,1.0
A Hybrid Probabilistic Approach for Table Understanding,"Tables of data are used to record vast amounts of socioeconomic, scientific, and governmental information. Although humans create tables using underlying organizational principles, unfortunately AI systems struggle to understand the contents of these tables. This paper introduces an end-to-end system for table understanding, the process of capturing the relational structure of data in tables. We introduce models that identify cell types, group these cells into blocks of data that serve a similar functional role, and predict the relationships between these blocks. We introduce a hybrid, neuro-symbolic approach, combining embedded representations learned from thousands of tables with probabilistic constraints that capture regularities in how humans organize tables. Our neurosymbolic model is better able to capture positional invariants of headers and enforce homogeneity of data types. One limitation in this research area is the lack of rich datasets for evaluating end-to-end table understanding, so we introduce a new benchmark dataset comprised of 431 diverse tables from data.gov. The evaluation results show that our system achieves the state-of-the-art performance on cell type classification, block identification, and relationship prediction, improving over prior efforts by up to 7% of macro F1 score.",2021,AAAI,1.0
Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval,"Cross-modal retrieval has become an active study field with the expanding scale of multimodal data. To date, most existing methods transform multimodal data into a common representation space where semantic similarities between items can be directly measured across different modalities. However, these methods typically suffer from following limitations: 1) They usually attempt to bridge the modality gap by designing losses in the common representation space which may not be sufficient to eliminate potential heterogeneity of different modalities in the common space. 2) They typically treat labels as independent individuals and ignore label relationships which are important for constructing semantic links between multimodal data. In this work, we propose a novel Dual Adversarial Graph Neural Networks (DAGNN) composed of the dual generative adversarial networks and the multi-hop graph neural networks, which learn modality-invariant and discriminative common representations for cross-modal retrieval. Firstly, we construct the dual generative adversarial networks to project multimodal data into a common representation space. Secondly, we leverage the multi-hop graph neural networks, in which a layer aggregation mechanism is proposed to exploit multi-hop propagation information, to capture the label correlation dependency and learn inter-dependent classifiers. Comprehensive experiments conducted on two cross-modal retrieval benchmark datasets, NUS-WIDE and MIRFlickr, indicate the superiority of DAGNN.",2021,AAAI,0.8
GTA: Graph Truncated Attention for Retrosynthesis,"Retrosynthesis is the task of predicting reactant molecules from a given product molecule and is, important in organic chemistry because the identification of a synthetic path is as demanding as the discovery of new chemical compounds. Recently, the retrosynthesis task has been solved automatically without human expertise using powerful deep learning models. Recent deep models are primarily based on seq2seq or graph neural networks depending on the function of molecular representation, sequence, or graph. Current state-of-theart models represent a molecule as a graph, but they require joint training with auxiliary prediction tasks, such as the most probable reaction template or reaction center prediction. Furthermore, they require additional labels by experienced chemists, thereby incurring additional cost. Herein, we propose a novel template-free model, i.e., Graph Truncated Attention (GTA), which leverages both sequence and graph representations by inserting graphical information into a seq2seq model. The proposed GTA model masks the self-attention layer using the adjacency matrix of product molecule in the encoder and applies a new loss using atom mapping acquired from an automated algorithm to the cross-attention layer in the decoder. Our model achieves new state-of-the-art records, i.e., exact match top-1 and top-10 accuracies of 51.1% and 81.6% on the USPTO-50k benchmark dataset, respectively, and 46.0% and 70.0% on the USPTO-full dataset, respectively, both without any reaction class information. The GTA model surpasses prior graph-based template-free models by 2% and 7% in terms of the top-1 and top-10 accuracies on the USPTO-50k dataset, respectively, and by over 6% for both the top-1 and top-10 accuracies on the USPTO-full dataset.",2021,AAAI,1.0
Learning from the Best: Rationalizing Predictions by Adversarial Information Calibration,"Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance. Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features (i.e., the rationale) followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.",2021,AAAI,1.0
Learning Local Neighboring Structure for Robust 3D Shape Representation,"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node‚Äôs neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer ‚Äì a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",2021,AAAI,1.0
Contrastive Clustering,"In this paper, we propose an online clustering method called Contrastive Clustering (CC) which explicitly performs the instanceand cluster-level contrastive learning. To be specific, for a given dataset, the positive and negative instance pairs are constructed through data augmentations and then projected into a feature space. Therein, the instanceand cluster-level contrastive learning are respectively conducted in the row and column space by maximizing the similarities of positive pairs while minimizing those of negative ones. Our key observation is that the rows of the feature matrix could be regarded as soft labels of instances, and accordingly the columns could be further regarded as cluster representations. By simultaneously optimizing the instanceand cluster-level contrastive loss, the model jointly learns representations and cluster assignments in an end-to-end manner. Besides, the proposed method could timely compute the cluster assignment for each individual, even when the data is presented in streams. Extensive experimental results show that CC remarkably outperforms 17 competitive clustering methods on six challenging image benchmarks. In particular, CC achieves an NMI of 0.705 (0.431) on the CIFAR-10 (CIFAR-100) dataset, which is an up to 19% (39%) performance improvement compared with the best baseline. The code is available at https://github.com/XLearning-SCU/2021-AAAI-CC.",2021,AAAI,1.0
Frivolous Units: Wider Networks Are Not Really That Wide,"A remarkable characteristic of overparameterized deep neural networks (DNNs) is that their accuracy does not degrade when the network width is increased. Recent evidence suggests that developing compressible representations allows the complexity of large networks to be adjusted for the learning task at hand. However, these representations are poorly understood. A promising strand of research inspired from biology involves studying representations at the unit level as it offers a more granular interpretation of the neural mechanisms. In order to better understand what facilitates increases in width without decreases in accuracy, we ask: Are there mechanisms at the unit level by which networks control their effective complexity? If so, how do these depend on the architecture, dataset, and hyperparameters? We identify two distinct types of ‚Äúfrivolous‚Äù units that proliferate when the network‚Äôs width increases: prunable units which can be dropped out of the network without significant change to the output and redundant units whose activities can be expressed as a linear combination of others. These units imply complexity constraints as the function the network computes could be expressed without them. We also identify how the development of these units can be influenced by architecture and a number of training factors. Together, these results help to explain why the accuracy of DNNs does not degrade when width is increased and highlight the importance of frivolous units toward understanding implicit regularization in DNNs.",2021,AAAI,0.0
MARTA: Leveraging Human Rationales for Explainable Text Classification,"Explainability is a key requirement for text classification in many application domains ranging from sentiment analysis to medical diagnosis or legal reviews. Existing methods often rely on ‚Äúattention‚Äù mechanisms for explaining classification results by estimating the relative importance of input units. However, recent studies have shown that such mechanisms tend to mis-identify irrelevant input units in their explanation. In this work, we propose a hybrid human-AI approach that incorporates human rationales into attention-based text classification models to improve the explainability of classification results. Specifically, we ask workers to provide rationales for their annotation by selecting relevant pieces of text. We introduce MARTA, a Bayesian framework that jointly learns an attention-based model and the reliability of workers while injecting human rationales into model training. We derive a principled optimization algorithm based on variational inference with efficient updating rules for learning MARTA parameters. Extensive validation on real-world datasets shows that our framework significantly improves the state of the art both in terms of classification explainability and accuracy.",2021,AAAI,1.0
Inference Fusion with Associative Semantics for Unseen Object Detection,"We study the problem of object detection when training and test objects are disjoint, i.e. no training examples of the target classes are available. Existing unseen object detection approaches usually combine generic detection frameworks with a single-path unseen classifier, by aligning object regions with semantic class embeddings. In this paper, inspired from human cognitive experience, we propose a simple but effective dual-path detection model that further explores associative semantics to supplement the basic visual-semantic knowledge transfer. We use a novel target-centric multipleassociation strategy to establish concept associations, to ensure that the predictor generalized to unseen domain can be learned during training. In this way, through a reasonable inference fusion mechanism, those two parallel reasoning paths can strengthen the correlation between seen and unseen objects, thus improving detection performance. Experiments show that our inductive method can significantly boost the performance by 7.42% over inductive models, and even 5.25% over transductive models on MSCOCO dataset.",2021,AAAI,1.0
Minimax Regret Optimisation for Robust Planning in Uncertain Markov Decision Processes,"The parameters for a Markov Decision Process (MDP) often cannot be specified exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets which the parameters belong to. Minimax regret has been proposed as an objective for planning in UMDPs to find robust policies which are not overly conservative. In this work, we focus on planning for Stochastic Shortest Path (SSP) UMDPs with uncertain cost and transition functions. We introduce a Bellman equation to compute the regret for a policy. We propose a dynamic programming algorithm that utilises the regret Bellman equation, and show that it optimises minimax regret exactly for UMDPs with independent uncertainties. For coupled uncertainties, we extend our approach to use options to enable a trade off between computation and solution quality. We evaluate our approach on both synthetic and real-world domains, showing that it significantly outperforms existing baselines.",2021,AAAI,1.0
Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency,"We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are publicly available.",2021,AAAI,1.0
Learning to Reweight with Deep Interactions,"Recently, the concept of teaching has been introduced into machine learning, in which a teacher model is used to guide the training of a student model (which will be used in real tasks) through data selection, loss function design, etc. Learning to reweight, which is a specific kind of teaching that reweights training data using a teacher model, receives much attention due to its simplicity and effectiveness. In existing learning to reweight works, the teacher model only utilizes shallow/surface information such as training iteration number and loss/accuracy of the student model from training/validation sets, but ignores the internal states of the student model, which limits the potential of learning to reweight. In this work, we propose an improved data reweighting algorithm, in which the student model provides its internal states to the teacher model, and the teacher model returns adaptive weights of training samples to enhance the training of the student model. The teacher model is jointly trained with the student model using meta gradients propagated from a validation set. Experiments on image classification with clean/noisy labels and neural machine translation empirically demonstrate that our algorithm makes significant improvement over previous methods.",2021,AAAI,1.0
Deductive Learning for Weakly-Supervised 3D Human Pose Estimation via Uncalibrated Cameras,"Without prohibitive and laborious 3D annotations, weaklysupervised 3D human pose methods mainly employ the model regularization with geometric projection consistency or geometry estimation from multi-view images. Nevertheless, those approaches explicitly need known parameters of calibrated cameras, exhibiting a limited model generalization in various realistic scenarios. To mitigate this issue, in this paper, we propose a Deductive Weakly-Supervised Learning (DWSL) for 3D human pose machine. Our DWSL firstly learns latent representations on depth and camera pose for 3D pose reconstruction. Since weak supervision usually causes ill-conditioned learning or inferior estimation, our DWSL introduces deductive reasoning to make an inference for human pose from a view to another and develops a reconstruction loss to demonstrate what the model learns and infers is reliable. This learning by deduction strategy employs the view-transform demonstration and structural rules derived from depth, geometry and angle constraints, which improves the reliability of the model training with weak supervision. On three 3D human pose benchmarks, we conduct extensive experiments to evaluate our proposed method, which achieves superior performance in comparison with state-of-the-art weak-supervised methods. Particularly, our model shows an appealing potential for learning from 2D data captured in dynamic outdoor scenes, which demonstrates promising robustness and generalization in realistic scenarios. Our code is publicly available at https://github.com/XipengChen/DWSL-3D-pose. Introduction 3D human pose estimation is a fundamental problem in computer vision for many applications, such as human-robot interaction, virtual reality, and action recognition, etc. However, it is greatly bottlenecked by the availability of abundant 3D annotated data, since 3D images are usually subject to specific conditions with constrained laboratory environments and thus have limited pose variations and simple backgrounds, and particularly, accurate 3D annotation demands prohibitively expensive cost. Accordingly, they cause the poor generalization of 3D pose models to the cases in the wild. Without any 3D pose annotation, many researchers resort to Weakly-Supervised Learning (WSL) methods (Kocabas, *Pengxu Wei is the corresponding author. Copyright ¬© 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Karagoz, and Akbas 2019; Rhodin et al. 2018; Rhodin, Salzmann, and Fua 2018; Chen et al. 2019a), which inherit the benefits of rich annotation and diversity of 2D pose datasets. They usually utilize annotated 2D pose images by lifting 2D poses to the 3D space together with geometric consistency constraints and train models without 3D pose labels for 3D human pose estimation. (Chen et al. 2019a) proposes a method to learn from single-view self-supervision, but requires a very large amount of diverse 2D human poses. (Kocabas, Karagoz, and Akbas 2019; Rhodin et al. 2018; Rhodin, Salzmann, and Fua 2018) propose a multi-view consistency from images which are taken for the same person from different viewpoints. Nevertheless, these methods have to obtain well-defined rigid transformation from annotations (Rhodin, Salzmann, and Fua 2018) or predictions from off-the-shelf methods (Kocabas, Karagoz, and Akbas 2019; Rhodin et al. 2018). Meanwhile, they employ the view synthesis strategy to produce 3D poses which supervise the training of 3D pose detectors. This casts the weakly-supervised learning problem of 3D pose estimation with only 2D annotation into a conventional fully-supervised learning task with synchronized information from multi-view images (Chen et al. 2019a). Essentially, fully supervised models are trained inductively in a data-driven manner, which greatly depends on abundant observations or samples with labels. Nevertheless, following the same spirit, with weak supervision or without annotation, the training of models suffers from a large knowledge of uncertainty or controversial ambiguity, which would cause ill-conditioned learning or inferior estimation. To mitigate this problem, we propose Deductive WeaklySupervised Learning (DWSL) for 3D human pose estimation. Rather than following the spirit of data-driven inductive learning in most existing methods, the proposed paradigm of learning by deduction utilizes deduction with view-transform demonstration and structural rules to infer the plausible 2D pose from another view and develop a reconstruction loss for training. This is regarded as a self-demonstration with deductive reasoning from one view to another view, namely, deduction with view-transform demonstration, and the derived reconstruction loss provides a checkpoint for the current weakly-supervised learning. At the same time, we also introduce structural rules to further promote the learning by deduction, which would ease the model training and reduce the searching space of parameters. We conduct experiments The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)",2021,AAAI,1.0
SD-Pose: Semantic Decomposition for Cross-Domain 6D Object Pose Estimation,"The current leading 6D object pose estimation methods rely heavily on annotated real data, which is highly costly to acquire. To overcome this, many works have proposed to introduce computer-generated synthetic data. However, bridging the gap between the synthetic and real data remains a severe problem. Images depicting different levels of realism/semantics usually have different transferability between the synthetic and real domains. Inspired by this observation, we introduce an approach, SD-Pose, that explicitly decomposes the input image into multi-level semantic representations and then combines the merits of each representation to bridge the domain gap. Our comprehensive analyses and experiments show that our semantic decomposition strategy can fully utilize the different domain similarities of different representations, thus allowing us to outperform the state of the art on modern 6D object pose datasets without accessing any real data during training.",2021,AAAI,1.0
Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders,"Representation learning for knowledge graphs (KGs) has focused on the problem of answering simple link prediction queries. In this work we address the more ambitious challenge of predicting the answers of conjunctive queries with multiple missing entities. We propose Bidirectional Query Embedding (BIQE), a method that embeds conjunctive queries with models based on bi-directional attention mechanisms. Contrary to prior work, bidirectional selfattention can capture interactions among all the elements of a query graph. We introduce two new challenging datasets for studying conjunctive query inference and conduct experiments on several benchmark datasets that demonstrate BIQE significantly outperforms state of the art baselines.",2021,AAAI,1.0
Consistent Right-Invariant Fixed-Lag Smoother with Application to Visual Inertial SLAM,"State estimation problems without absolute position measurements routinely arise in navigation of unmanned aerial vehicles, autonomous ground vehicles, etc. whose proper operation relies on accurate state estimates and reliable covariances. Unaware of absolute positions, these problems have immanent unobservable directions. Traditional causal estimators, however, usually gain spurious information on the unobservable directions, leading to over-confident covariance inconsistent with actual estimator errors. The consistency problem of fixed-lag smoothers (FLSs) has only been attacked by the first estimate Jacobian (FEJ) technique because of the complexity to analyze their observability property. But the FEJ has several drawbacks hampering its wide adoption. To ensure the consistency of a FLS, this paper introduces the right invariant error formulation into the FLS framework. To our knowledge, we are the first to analyze the observability of a FLS with the right invariant error. Our main contributions are twofold. As the first novelty, to bypass the complexity of analysis with the classic observability matrix, we show that observability analysis of FLSs can be done equivalently on the linearized system. Second, we prove that the inconsistency issue in the traditional FLS can be elegantly solved by the right invariant error formulation without artificially correcting Jacobians. By applying the proposed FLS to the monocular visual inertial simultaneous localization and mapping (SLAM) problem, we confirm that the method consistently estimates covariance similarly to a batch smoother in simulation and that our method achieved comparable accuracy as traditional FLSs on real data. Introduction Positioning and navigation of a variety of vehicles, e.g., unmanned aerial vehicles (UAVs), autonomous ground vehicles (AGVs), depends on real-time state estimation. Accurate system state and reasonable covariance output by state estimators in real time are necessary for the proper operation of these systems. For state estimation, these systems usually fuse measurements captured by sensors that do not provide absolute positions, like cameras, lidars, inertial measurement units (IMUs), etc. It is well known that estimators which fuse such measurements have unobservable directions (Jones and Soatto 2011). ‚àóCorresponding author, yuan.zhuang@whu.edu.cn Copyright c ¬© 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. As reported in the literature, traditional real-time estimators, e.g., filters, fixed-lag smoothers (FLSs), tend to gain fictitious information on unobservable directions (Huang, Mourikis, and Roumeliotis 2010; Dong-Si and Mourikis 2011), and to output falsely optimistic covariance inconsistent to the actual state error. This inconsistency is caused by the marginalization step of real-time estimators which removes old state variables and measurements (i.e., factors) from an estimator and approximates those measurements by a linear prior factor. A deeper cause is that for a variable in the prior factor, its linearization point used by the prior factor differs from that used by the remaining factors. Obviously, the batch estimator and its incremental variants, e.g., iSAM2 (Kaess et al. 2012), do not have this issue as they do not marginalize variables. To fix the estimator inconsistency, techniques that modify the measurement Jacobians to fit certain criteria have been proposed. For instance, the ‚Äúfirst estimate Jacobian (FEJ)‚Äù technique (Huang, Mourikis, and Roumeliotis 2010) evaluates Jacobians relative to variables in the linear prior factor at their estimates upon marginalization. Because the Jacobian computation depends on specifics, such as an earlier estimate of a variable, it is usually difficult to apply such techniques to an existing estimator framework. A new trend is to use right invariant error formulation (Barrau and Bonnabel 2016a) where a navigation state variable (consisting of orientation, position, and velocity) is associated to a Lie group SE2(3) and the error vector is invariant to transforming the trajectory by a right multiplication. Besides mathematically elegant, it is easy to implement as it fits the conventional filtering framework. However, this formulation has not been used in FLSs, mainly because of the challenge to analyze their consistency property. Previous work has shown that the estimator inconsistency comes along with the observability issue where the unobservable directions become spuriously observable (Hesch et al. 2014a). Thus, consistency has been predominantly studied by examining rank deficiency of the linearized observability matrix, e.g., (Huang, Mourikis, and Roumeliotis 2010; Dong-Si and Mourikis 2012; Brossard, Barrau, and Bonnabel 2018). The local observability matrix is acceptable in complexity for analyzing filters, but becomes very involved for dealing with FLSs, e.g., (Dong-Si and Mourikis 2012). Because the observability matrix is a derivative of the The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)",2021,AAAI,1.0
ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs,"We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%.",2021,AAAI,1.0
How Linguistically Fair Are Multilingual Pre-Trained Language Models?,"Massively multilingual pre-trained language models, such as mBERT and XLM-RoBERTa, have received significant attention in the recent NLP literature for their excellent capability towards crosslingual zero-shot transfer of NLP tasks. This is especially promising because a large number of languages have no or very little labeled data for supervised learning. Moreover, a substantially improved performance on low resource languages without any significant degradation of accuracy for high resource languages lead us to believe that these models will help attain a fairer distribution of language technologies despite the prevalent unfair and extremely skewed distribution of resources across the world‚Äôs languages. Nevertheless, these models, and the experimental approaches adopted by the researchers to arrive at those, have been criticised by some for lacking a nuanced and thorough comparison of benefits across languages and tasks. A related and important question that has received little attention is how to choose from a set of models, when no single model significantly outperforms the others on all tasks and languages. As we discuss in this paper, this is often the case, and the choices are usually made without a clear articulation of reasons or underlying fairness assumptions. In this work, we scrutinize the choices made in previous work, and propose a few different strategies for fair and efficient model selection based on the principles of fairness in economics and social choice theory. In particular, we emphasize Rawlsian fairness, which provides an appropriate framework for making fair (with respect to languages, or tasks, or both) choices while selecting multilingual pre-trained language models for a practical or",2021,AAAI,0.5
TaLNet: Voice Reconstruction from Tongue and Lip Articulation with Transfer Learning from Text-to-Speech Synthesis,"This paper presents TaLNet, a model for voice reconstruction with ultrasound tongue and optical lip videos as inputs. TaLNet is based on an encoder-decoder architecture. Separate encoders are dedicated to processing the tongue and lip data streams respectively. The decoder predicts acoustic features conditioned on encoder outputs and speaker codes. To mitigate for having only relatively small amounts of dual articulatory-acoustic data available for training, and since our task here shares with text-to-speech (TTS) the common goal of speech generation, we propose a novel transfer learning strategy to exploit the much larger amounts of acoustic-only data available to train TTS models. For this, a Tacotron 2 TTS model is first trained, and then the parameters of its decoder are transferred to the TaLNet decoder. We have evaluated our approach on an unconstrained multi-speaker voice recovery task. Our results show the effectiveness of both the proposed model and the transfer learning strategy. Speech reconstructed using our proposed method significantly outperformed all baselines (DNN, BLSTM and without transfer learning) in terms of both naturalness and intelligibility. When using an ASR model decoding the recovery speech, the WER of our proposed method shows a relative reduction of over 30% compared to baselines.",2021,AAAI,1.0
Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation,"This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusion and missing. Without online learning and finetuning, our STG-Net achieves state-of-the-art performance on four large benchmarks (DAVIS, YouTube-VOS, SegTrackv2, and YouTube-Objects), demonstrating the effectiveness of the proposed approach.",2021,AAAI,1.0
Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork,"AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue ‚Äúnot necessarily‚Äù ‚Äî predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team‚Äôs expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.",2021,AAAI,-0.5
PoA of Simple Auctions with Interdependent Values,"We expand the literature on the price of anarchy (PoA) of simultaneous item auctions by considering settings with correlated values; we do this via the fundamental economic model of interdependent values (IDV). It is well-known that in multi-item settings with private values, correlated values can lead to bad PoA, which can be polynomially large in the number of agents n. In the more general model of IDV, we show that the PoA can be polynomially large even in singleitem settings. On the positive side, we identify a natural condition on information dispersion in the market, which enables good PoA guarantees. Under this condition, we show that for single-item settings, the PoA of standard mechanisms degrades gracefully. For settings with multiple items we show a separation between two domains: If there are more buyers, we devise a new simultaneous item auction with good PoA, under limited information asymmetry. To the best of our knowledge, this is the first positive PoA result for correlated values in multi-item settings. The main technical difficulty in establishing this result is that the standard tool for establishing PoA results ‚Äî the smoothness framework ‚Äî is unsuitable for IDV settings, and so we must introduce new techniques to address the unique challenges imposed by such settings. In the domain of more items, we establish impossibility results even for surprisingly simple scenarios.",2021,AAAI,0.5
Multi-Dimensional Explanation of Target Variables from Documents,"Automated predictions require explanations to be interpretable by humans. Past work used attention and rationale mechanisms to find words that predict the target variable of a document. Often though, they result in a tradeoff between noisy explanations or a drop in accuracy. Furthermore, rationale methods cannot capture the multi-faceted nature of justifications for multiple targets, because of the non-probabilistic nature of the mask. In this paper, we propose the Multi-Target Masker (MTM) to address these shortcomings. The novelty lies in the soft multi-dimensional mask that models a relevance probability distribution over the set of target variables to handle ambiguities. Additionally, two regularizers guide MTM to induce long, meaningful explanations. We evaluate MTM on two datasets and show, using standard metrics and human annotations, that the resulting masks are more accurate and coherent than those generated by the state-of-the-art methods. Moreover, MTM is the first to also achieve the highest F1 scores for all the target variables simultaneously.",2021,AAAI,0.8
Peer Collaborative Learning for Online Knowledge Distillation,"Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a highcapacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-theart methods.",2021,AAAI,1.0
Focused Inference and System P,"We bring in the concept of focused inference into the field of qualitative nonmonotonic reasoning by applying focused inference to System P. The idea behind drawing focused inferences is to concentrate on knowledge which seems to be relevant for answering a query while completely disregarding the remaining knowledge even at the risk of missing some meaningful information. Focused inference is motivated by mimicking snap decisions of human reasoners and aims on rapidly drawing still reasonable inferences from large sets of knowledge. In this paper, we define a series of query-dependent, syntactically-driven focused inference relations, elaborate on their formal properties, and show that the series converges against System P. We take advantage of this result in form of an anytime algorithm for drawing inferences which is accompanied by a thorough complexity analysis.",2021,AAAI,0.5
Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning,"Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly, data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods",2021,AAAI,0.0
A Few Queries Go a Long Way: Information-Distortion Tradeoffs in Matching,"Georgios Amanatidis,1,2 Georgios Birmpas,3 Aris Filos-Ratsikas,4 Alexandros A. Voudouris5 1Department of Mathematical Sciences, University of Essex 2ILLC, University of Amsterdam 3Department of Computer, Control and Management Engineering, Sapienza University of Rome 4Department of Computer Science, University of Liverpool 5School of Computer Science and Electronic Engineering, University of Essex georgios.amanatidis@essex.ac.uk, george.birbas@diag.uniroma1.it, aris.filos-ratsikas@liverpool.ac.uk, alexandros.voudouris@essex.ac.uk Abstract",2021,AAAI,0.0
Deep Metric Learning with Self-Supervised Ranking,"Deep metric learning aims to learn a deep embedding space, where similar objects are pushed towards together and different objects are repelled against. Existing approaches typically use inter-class characteristics, e.g., class-level information or instance-level similarity, to obtain semantic relevance of data points and get a large margin between different classes in the embedding space. However, the intra-class characteristics, e.g., local manifold structure or relative relationship within the same class, are usually overlooked in the learning process. Hence the data structure cannot be fully exploited and the output embeddings have limitation in retrieval. More importantly, retrieval results lack in a good ranking. This paper presents a novel self-supervised ranking auxiliary framework, which captures intra-class characteristics as well as inter-class characteristics for better metric learning. Our method defines specific transform functions to simulates the local structure change of intra-class in the initial image domain, and formulates a self-supervised learning procedure to fully exploit this property and preserve it in the embedding space. Extensive experiments on three standard benchmarks show that our method significantly improves and outperforms the state-of-the-art methods on the performances of both retrieval and ranking by 2%-4%.",2021,AAAI,1.0
PAC Learning of Causal Trees with Latent Variables,"Learning causal probabilistic models with latent variables from observational and experimental data is an important problem. In this paper we present a polynomial-time algorithm that PAC-learns the structure and parameters of a rooted, tree-structured causal network of bounded degree where the internal nodes of the tree cannot be observed or manipulated. Our algorithm is the first of its kind to provably learn the structure and parameters of tree-structured causal models with latent internal variables from random examples and active experiments.",2021,AAAI,1.0
CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting,"This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step ‚Äì an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequenceto-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",2021,AAAI,0.6000000000000001
Learning Deep Generative Models for Queuing Systems,"Modern society is heavily dependent on large scale client-server systems with applications ranging from Internet and Communication Services to sophisticated logistics and deployment of goods. To maintain and improve such a system, a careful study of client and server dynamics is needed ‚Äì e.g. response/service times, average number of clients at given times, etc. To this end, one traditionally relies, within the queuing theory formalism, on parametric analysis and explicit distribution forms. However, parametric forms limit the model‚Äôs expressiveness and could struggle on extensively large",2021,AAAI,-0.7000000000000001
Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,"Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it. In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models. Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.",2021,AAAI,1.0
Generalized Adversarially Learned Inference,"Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator‚Äôs objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks. The appendix of the paper can be found at the following link: https://drive.google.com/file/ d/1i99e682CqYWMEDXlnqkqrctGLVA9viiz/view?usp= sharing",2021,AAAI,0.5
Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation,"This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object‚Äôs features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to ‚Äúguide‚Äù the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.",2021,AAAI,1.0
An Analysis of Approval-Based Committee Rules for 2D-Euclidean Elections,"We study approval-based committee elections for the case where the voters‚Äô preferences come from a 2D-Euclidean model. We consider two main issues: First, we ask for the complexity of computing election results. Second, we evaluate election outcomes experimentally, following the visualization technique of Elkind et al. (2017). Regarding the first issue, we find that many NP-hard rules remain intractable for 2D-Euclidean elections. For the second one, we observe that the behavior and nature of many rules strongly depend on the exact protocol for choosing the approved candidates.",2021,AAAI,0.0
DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning,"This paper proposes DeepSynth, a method for effective training of deep Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian, but at the same time progress towards the reward requires achieving an unknown sequence of high-level objectives. Our method employs a novel algorithm for synthesis of compact automata to uncover this sequential structure automatically. We synthesise a humaninterpretable automaton from trace data collected by exploring the environment. The state space of the environment is then enriched with the synthesised automaton so that the generation of a control policy by deep RL is guided by the discovered structure encoded in the automaton. The proposed approach is able to cope with both high-dimensional, low-level features and unknown sparse non-Markovian rewards. We have evaluated DeepSynth‚Äôs performance in a set of experiments that includes the Atari game Montezuma‚Äôs Revenge. Compared to existing approaches, we obtain a reduction of two orders of magnitude in the number of iterations required for policy synthesis, and also a significant improvement in scalability.",2021,AAAI,0.8
Initiative Defense against Facial Manipulation,"Benefiting from the development of generative adversarial networks (GAN), facial manipulation has achieved significant progress in both academia and industry recently. It inspires an increasing number of entertainment applications but also incurs severe threats to individual privacy and even political security meanwhile. To mitigate such risks, many countermeasures have been proposed. However, the great majority methods are designed in a passive manner, which is to detect whether the facial images or videos are tampered after their wide propagation. These detection-based methods have a fatal limitation, that is, they only work for ex-post forensics but can not prevent the engendering of malicious behavior. To address the limitation, in this paper, we propose a novel framework of initiative defense to degrade the performance of facial manipulation models controlled by malicious users. The basic idea is to actively inject imperceptible venom into target facial data before manipulation. To this end, we first imitate the target manipulation model with a surrogate model, and then devise a poison perturbation generator to obtain the desired venom. An alternating training strategy are further leveraged to train both the surrogate model and the perturbation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in our initiative defense framework. Extensive experiments demonstrate the effectiveness and robustness of our framework in different settings. Finally, we hope this work can shed some light on initiative countermeasures against more adversarial",2021,AAAI,0.6000000000000001
Humor Knowledge Enriched Transformer for Understanding Multimodal Humor,"Recognizing humor from a video utterance requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. In this paper, we propose Humor Knowledge enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the language, acoustic, vision, and humor centric features separately using Transformer based encoders, followed by a cross attention layer to exchange information among them. Our model achieves 77.36% and 79.41% accuracy in humorous punchline detection on UR-FUNNY and MUStaRD datasets ‚Äì achieving a new state-of-the-art on both datasets with the margin of 4.93% and 2.94% respectively. Furthermore, we demonstrate that our model can capture interpretable, humorinducing patterns from all modalities.",2021,AAAI,1.0
A General Setting for Gradual Semantics Dealing with Similarity,"The paper discusses theoretical foundations that describe principles and processes involved in defining semantics that deal with similarity between arguments. Such semantics compute the strength of an argument on the basis of the strengths of its attackers, similarities between those attackers, and an initial weight ascribed to the argument. We define a semantics by three functions: an adjustment function that updates the strengths of attackers on the basis of their similarities, an aggregation function that computes the strength of the group of attackers, and an influence function that evaluates the impact of the group on the argument‚Äôs initial weight. We propose intuitive constraints for the three functions and key rationality principles for semantics, and show how the former lead to the satisfaction of the latter. Then, we propose a broad family of semantics whose instances satisfy the principles. Finally, we analyse the existing adjustment functions and show that they violate some properties, then we propose novel ones and use them for generalizing h-Categorizer.",2021,AAAI,0.2
DenserNet: Weakly Supervised Visual Localization Using Multi-Scale Feature Aggregation,"In this work, we introduce a Denser Feature Network (DenserNet) for visual localization. Our work provides three principal contributions. First, we develop a convolutional neural network (CNN) architecture which aggregates feature maps at different semantic levels for image representations. Using denser feature maps, our method can produce more keypoint features and increase image retrieval accuracy. Second, our model is trained end-to-end without pixel-level annotation other than positive and negative GPS-tagged image pairs. We use a weakly supervised triplet ranking loss to learn discriminative features and encourage keypoint feature repeatability for image representation. Finally, our method is computationally efficient as our architecture has shared features and parameters during forwarding propagation. Our method is flexible and can be crafted on a light-weighted backbone architecture to achieve appealing efficiency with a small penalty on accuracy. Extensive experiment results indicate that our method sets a new state-of-the-art on four challenging large-scale localization benchmarks and three image retrieval benchmarks with the same level of supervision. The code is available at https://github.com/goodproj13/",2021,AAAI,1.0
Turbocharging Treewidth-Bounded Bayesian Network Structure Learning,"We present a new approach for learning the structure of a treewidth-bounded Bayesian Network (BN). The key to our approach is applying an exact method (based on MaxSAT) locally, to improve the score of a heuristically computed BN. This approach allows us to scale the power of exact methods‚Äî so far only applicable to BNs with several dozens of random variables‚Äîto large BNs with several thousands of random variables. Our experiments show that our method improves the score of BNs provided by state-of-the-art heuristic methods, often significantly.",2021,AAAI,1.0
Generalization in Portfolio-Based Algorithm Selection,"Portfolio-based algorithm selection has seen tremendous practical success over the past two decades. This algorithm configuration procedure works by first selecting a portfolio of diverse algorithm parameter settings, and then, on a given problem instance, using an algorithm selector to choose a parameter setting from the portfolio with strong predicted performance. Oftentimes, both the portfolio and the algorithm selector are chosen using a training set of typical problem instances from the application domain at hand. In this paper, we provide the first provable guarantees for portfolio-based algorithm selection. We analyze how large the training set should be to ensure that the resulting algorithm selector‚Äôs average performance over the training set is close to its future (expected) performance. This involves analyzing three key reasons why these two quantities may diverge: 1) the learningtheoretic complexity of the algorithm selector, 2) the size of the portfolio, and 3) the learning-theoretic complexity of the algorithm‚Äôs performance as a function of its parameters. We introduce an end-to-end learning-theoretic analysis of the portfolio construction and algorithm selection together. We prove that if the portfolio is large, overfitting is inevitable, even with an extremely simple algorithm selector. With experiments, we illustrate a tradeoff exposed by our theoretical analysis: as we increase the portfolio size, we can hope to include a well-suited parameter setting for every possible problem instance, but it becomes impossible to avoid overfitting.",2021,AAAI,-0.5
Modular Graph Transformer Networks for Multi-Label Image Classification,"With the recent advances in graph neural networks, there is a rising number of studies on graph-based multi-label classification with the consideration of object dependencies within visual data. Nevertheless, graph representations can become indistinguishable due to the complex nature of label relationships. We propose a multi-label image classification framework based on graph transformer networks to fully exploit inter-label interactions. The paper presents a modular learning scheme to enhance the classification performance by segregating the computational graph into multiple sub-graphs based on modularity. Our approach, named Modular Graph Transformer Networks (MGTN), is capable of employing multiple backbones for better information propagation over different sub-graphs guided by graph transformers and convolutions. We validate our framework on MS-COCO and Fashion550K datasets to demonstrate improvements for multilabel image classification. The source code is available at https://github.com/ReML-AI/MGTN.",2021,AAAI,0.7000000000000001
Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale,"We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework‚Äôs guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation. Our code and supplementary material are now available at: https://github.com/TZYSJTU/Sketch-Generation-withDrawing-Process-Guided-by-Vector-Flow-and-Grayscale",2021,AAAI,1.0
BERT & Family Eat Word Salad: Experiments with Text Understanding,"In this paper, we study the response of large models from the BERT family to incoherent inputs that should confuse any model that claims to understand natural language. We define simple heuristics to construct such examples. Our experiments show that state-of-the-art models consistently fail to recognize them as ill-formed, and instead produce high confidence predictions on them. As a consequence of this phenomenon, models trained on sentences with randomly permuted word order perform close to state-of-the-art models. To alleviate these issues, we show that if models are explicitly trained to recognize invalid inputs, they can be robust to such attacks without a drop in performance.",2021,AAAI,-0.5
Differentially Private and Communication Efficient Collaborative Learning,"Collaborative learning has received huge interests due to its capability of exploiting the collective computing power of the wireless edge devices. However, during the learning process, model updates using local private samples and large-scale parameter exchanges among agents impose severe privacy concerns and communication bottleneck. In this paper, to address these problems, we propose two differentially private (DP) and communication efficient algorithms, called Q-DPSGD-1 and Q-DPSGD-2. In Q-DPSGD-1, each agent first performs local model updates by a DP gradient descent method to provide the DP guarantee and then quantizes the local model before transmitting it to neighbors to improve communication efficiency. In Q-DPSGD-2, each agent injects discrete Gaussian noise to enforce DP guarantee after first quantizing the local model. Moreover, we track the privacy loss of both approaches under the R√©nyi DP and provide convergence analysis for both convex and non-convex loss functions. The proposed methods are evaluated in extensive experiments on real-world datasets and the empirical results validate our theoretical findings.",2021,AAAI,0.7000000000000001
Discriminative Region Suppression for Weakly-Supervised Semantic Segmentation,"Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate highquality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach.",2021,AAAI,0.8
In-game Residential Home Planning via Visual Context-aware Global Relation Learning,"In this paper, we propose an effective global relation learning algorithm to recommend an appropriate location of a building unit for in-game customization of residential home complex. Given a construction layout, we propose a visual contextaware graph generation network that learns the implicit global relations among the scene components and infers the location of a new building unit. The proposed network takes as input the scene graph and the corresponding top-view depth image. It provides the location recommendations for a newlyadded building units by learning an auto-regressive edge distribution conditioned on existing scenes. We also introduce a global graph-image matching loss to enhance the awareness of essential geometry semantics of the site. Qualitative and quantitative experiments demonstrate that the recommended location well reflects the implicit spatial rules of components in the residential estates, and it is instructive and practical to locate the building units in the 3D scene of the complex con-",2021,AAAI,0.6000000000000001
Towards Topic-Aware Slide Generation For Academic Papers With Unsupervised Mutual Learning,"Slides are commonly used to present information and tell stories. In academic and research communities, slides are typically used to summarize findings in accepted papers for presentation in meetings and conferences. These slides for academic papers usually contain common and essential topics such as major contributions, model design, experiment details and future work. In this paper, we aim to automatically generate slides for academic papers. We first conducted an in-depth analysis of how humans create slides. We then mined frequently used slide topics. Given a topic, our approach extracts relevant sentences in the paper to provide the draft slides. Due to the lack of labeling data, we integrate prior knowledge of ground truth sentences into a log-linear model to create an initial pseudo-target distribution. Two sentence extractors are learned collaboratively and bootstrap the performance of each other. Evaluation results on a labeled test set show that our model can extract more relevant sentences than baseline methods. Human evaluation also shows slides generated by our model can serve as a good basis for preparing the final",2021,AAAI,0.6000000000000001
Does Explainable Artificial Intelligence Improve Human Decision-Making?,"Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has typically focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decisionmaking and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decisionmaking, and further research may be needed to understand how to integrate explainable AI into real systems.",2021,AAAI,-0.8
Escaping Local Optima with Non-Elitist Evolutionary Algorithms,"Most discrete evolutionary algorithms (EAs) implement elitism, meaning that they make the biologically implausible assumption that the fittest individuals never die. While elitism favours exploitation and ensures that the best seen solutions are not lost, it has been widely conjectured that non-elitism is necessary to explore promising fitness valleys without getting stuck in local optima. Determining when non-elitist EAs outperform elitist EAs has been one of the most fundamental open problems in evolutionary computation. A recent analysis of a non-elitist EA shows that this algorithm does not outperform its elitist counterparts on the benchmark problem JUMP. We solve this open problem through rigorous runtime analysis of elitist and non-elitist population-based EAs on a class of multi-modal problems. We show that with 3-tournament selection and appropriate mutation rates, the non-elitist EA optimises the multi-modal problem in expected polynomial time, while an elitist EA requires exponential time with overwhelmingly high probability. A key insight in our analysis is the non-linear selection profile of the tournament selection mechanism which, with appropriate mutation rates, allows a small sub-population to reside on the local optimum while the rest of the population explores the fitness valley. In contrast, we show that the comma-selection mechanism which does not have this non-linear profile, fails to optimise this problem in polynomial time. The theoretical analysis is complemented with an empirical investigation on instances of the set cover problem, showing that non-elitist EAs can perform better than the elitist ones. We also provide examples where usage of mutation rates close to the error thresholds is beneficial when employing non-elitist population-based EAs.",2021,AAAI,-1.0
FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding,"Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and XLM, have achieved great success in cross-lingual representation learning. However, when applied to zero-shot cross-lingual transfer tasks, most existing methods use only single-language input for LM finetuning, without leveraging the intrinsic cross-lingual alignment between different languages that proves essential for multilingual tasks. In this paper, we propose FILTER, an enhanced fusion method that takes cross-lingual data as input for XLM finetuning. Specifically, FILTER first encodes text input in the source language and its translation in the target language independently in the shallow layers, then performs crosslanguage fusion to extract multilingual knowledge in the intermediate layers, and finally performs further languagespecific encoding. During inference, the model makes predictions based on the text input in the target language and its translation in the source language. For simple tasks such as classification, translated text in the target language shares the same label as the source language. However, this shared label becomes less accurate or even unavailable for more complex tasks such as question answering, NER and POS tagging. To tackle this issue, we further propose an additional KL-divergence self-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in the target language. Extensive experiments demonstrate that FILTER achieves new state of the art on two challenging multilingual multi-task benchmarks, XTREME and XGLUE.",2021,AAAI,1.0
Neighborhood Consensus Networks for Unsupervised Multi-view Outlier Detection,"Multi-view outlier detection recently attracted rapidly growing attention with the development of multi-view learning. Although promising performance demonstrated, we observe that identifying outliers in multi-view data is still a challenging task due to the complicated characteristics of multi-view data. Specifically, an effective multi-view outlier detection method should be able to handle (1) different types of outliers; (2) two or more views; (3) samples without clusters; (4) high dimensional data. Unfortunately, little is known about how these four issues can be handled simultaneously. In this paper, we propose an unsupervised multi-view outlier detection method to address these issues. Our method is based on the proposed novel neighborhood consensus networks termed NC-Nets, which automatically encodes intrinsic information into a comprehensive latent space for each view (for issue (4)) and uniforms the neighborhood structures among different views (for issue (2)). Accordingly, we propose an outlier score measurement which consists of two parts: the withinview reconstruction score and the cross-view neighborhood consensus score. The measurement is designed based on the characteristics of the different outlier types (for issue (1)) and no cluster assumption is needed (for issue (3)). Experimental results show that our method significantly outperforms state-of-the-art methods. On average, our method achieves 11.2% ‚àº 96.2% improvement in term of AUC and 33.5% ‚àº 352.7% improvement in term of F1-Score.",2021,AAAI,1.0
Majority Opinion Diffusion in Social Networks: An Adversarial Approach,"We introduce and study a novel majority based opinion diffusion model. Consider a graph G, which represents a social network. Assume that initially a subset of nodes, called seed nodes or early adopters, are colored either black or white, which correspond to positive or negative opinion regarding a consumer product or a technological innovation. Then, in each round an uncolored node, which is adjacent to at least one colored node, chooses the most frequent color among its",2021,AAAI,0.7000000000000001
Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data,"We propose a new probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, our method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. In this setting, direct application of classical variational methods often gives rise to collapsed densities that do not adequately explore the solution space. Instead, we derive our novel reduced entropy condition approximate inference method that results in rich posteriors. We test our model in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets. We further show higher classification accuracy after imputation, proving the advantage of propagating uncertainty to downstream tasks with our model.",2021,ICLR,1.0
Understanding the role of importance weighting for deep learning,"The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",2021,ICLR,0.5
DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues,"To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.1",2021,ICLR,1.0
AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition,"Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on Something V1&V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/",2021,ICLR,1.0
Predicting Classification Accuracy When Adding New Unobserved Classes,"Multiclass classifiers are often designed and evaluated only on a sample from the classes on which they will eventually be applied. Hence, their final accuracy remains unknown. In this work we study how a classifier‚Äôs performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes. For this, we define a measure of separation between correct and incorrect classes that is independent of the number of classes: the reversed ROC (rROC), which is obtained by replacing the roles of classes and data-points in the common ROC. We show that the classification accuracy is a function of the rROC in multiclass classifiers, for which the learned representation of data from the initial class sample remains unchanged when new classes are added. Using these results we formulate a robust neural-network-based algorithm, CleaneX, which learns to estimate the accuracy of such classifiers on arbitrarily large sets of classes. Unlike previous methods, our method uses both the observed accuracies of the classifier and densities of classification scores, and therefore achieves remarkably better predictions than current state-of-the-art methods on both simulations and real datasets of object detection, face recognition, and brain decoding.",2021,ICLR,1.0
Explaining the Efficacy of Counterfactually Augmented Data,"In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a human-in-the-loop process in which given some documents and their (initial) labels, humans must revise the text to make a counterfactual label applicable. Importantly, edits that are not necessary to flip the applicable label are prohibited. Models trained on the augmented (original and revised) data appear, empirically, to rely less on semantically irrelevant words and to generalize better out of domain. While this work draws loosely on causal thinking, the underlying causal model (even at an abstract level) and the principles underlying the observed out-of-domain improvements remain unclear. In this paper, we introduce a toy analog based on linear Gaussian models, observing interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Our analysis provides some insights that help to explain the efficacy of CAD. Moreover, we develop the hypothesis that while adding noise to causal features should degrade both in-domain and out-ofdomain performance, adding noise to non-causal features should lead to relative improvements in out-of-domain performance. This idea inspires a speculative test for determining whether a feature attribution technique has identified the causal spans. If adding noise (e.g., by random word flips) to the highlighted spans degrades both in-domain and out-of-domain performance on a battery of challenge datasets, but adding noise to the complement gives improvements out-of-domain, this suggests we have identified causal spans. Thus, we present a large-scale empirical study comparing spans edited to create CAD to those selected by attention and saliency maps. Across numerous challenge domains and models, we find that the hypothesized phenomenon is pronounced for CAD.",2021,ICLR,0.5
Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors,"Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively. Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github‚Ä†.",2021,ICLR,1.0
EEC: Learning to Encode and Regenerate Images for Continual Learning,"The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. During training on a new task, reconstructed images from encoded episodes are replayed in order to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable while using less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.1",2021,ICLR,1.0
Self-training For Few-shot Transfer Across Extreme Task Differences,"Most few-shot learning techniques are pre-trained on a large, labeled ‚Äúbase dataset‚Äù. In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different ‚Äúsource‚Äù problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains. Our code is available at https://github.com/cpphoo/STARTUP.",2021,ICLR,0.9
Robust Overfitting may be mitigated by properly learned smoothening,"A recent study (Rice et al., 2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by 3.72% ‚àº 6.68% and robust accuracy by 0.22% ‚àº 2.03%, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types (`‚àû and `2), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models‚Äô robustness against transfer attacks. Codes are available at https: //github.com/VITA-Group/Alleviate-Robust-Overfitting.",2021,ICLR,1.0
Lipschitz Recurrent Neural Networks,"Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state‚Äôs evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.",2021,ICLR,1.0
Decoupling Global and Local Representations via Invertible Generative Flows,"In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder. Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.",2021,ICLR,1.0
Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues,"Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multiturn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.",2021,ICLR,1.0
A Critique of Self-Expressive Deep Subspace Clustering,"Subspace clustering is an unsupervised clustering technique designed to cluster data that is supported on a union of linear subspaces, with each subspace defining a cluster with dimension lower than the ambient space. Many existing formulations for this problem are based on exploiting the self-expressive property of linear subspaces, where any point within a subspace can be represented as linear combination of other points within the subspace. To extend this approach to data supported on a union of non-linear manifolds, numerous studies have proposed learning an embedding of the original data using a neural network which is regularized by a self-expressive loss function on the data in the embedded space to encourage a union of linear subspaces prior on the data in the embedded space. Here we show that there are a number of potential flaws with this approach which have not been adequately addressed in prior work. In particular, we show the model formulation is often ill-posed in that it can lead to a degenerate embedding of the data, which need not correspond to a union of subspaces at all and is poorly suited for clustering. We validate our theoretical results experimentally and also repeat prior experiments reported in the literature, where we conclude that a significant portion of the previously claimed performance benefits can be attributed to an ad-hoc post processing step rather than the deep subspace clustering model.",2021,ICLR,1.0
What are the Statistical Limits of Offline RL with Linear Function Approximation?,"Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions. This work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of every policy is linear in a given set of features and 2) our off-policy data has good coverage over all features (under a strong spectral condition), any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon to nontrivially estimate the value of any given policy. Our results highlight that sampleefficient offline policy evaluation is not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability).",2021,ICLR,-1.0
What Makes Instance Discrimination Good for Transfer Learning?,"Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation. It comes as a surprise that image annotations would be better left unused for transfer learning. In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models? From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations. Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.",2021,ICLR,0.0
Generalization bounds via distillation,"This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds. The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation. This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fullyconnected layers, and skip connections, to name a few. To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar10 and mnist demonstrating similar generalization performance between the original network and its distillation.",2021,ICLR,0.0
Proximal Gradient Descent-Ascent: Variable Convergence under K≈Å Geometry,"The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function values or gradient norms. However, the variable convergence of GDA has been proved only under convexity geometries, and there lacks understanding for general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequence to a critical point. By leveraging this Lyapunov function and the K≈Å geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a critical point x‚àó, i.e., xt ‚Üí x‚àó, yt ‚Üí y‚àó(x‚àó). Furthermore, over the full spectrum of the K≈Å-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the K≈Å parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization.",2021,ICLR,0.5
Solving Compositional Reinforcement Learning Problems via Task Reduction,"We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/ view/sir-compositional.",2021,ICLR,1.0
Saliency is a Possible Red Herring When Diagnosing Poor Generalization,"Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert has labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as ‚Äúattribution priors‚Äù as well as saliency maps for explainable predictions.",2021,ICLR,0.0
Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval,"We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.1",2021,ICLR,1.0
Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration,"We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. DropBottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension‚Äôs relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.",2021,ICLR,1.0
DrNAS: Dirichlet Neural Architecture Search,"This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for ImageNet under the mobile setting. On NASBench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.",2021,ICLR,1.0
Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability,"We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.",2021,ICLR,-0.5
EVALUATION OF NEURAL ARCHITECTURES TRAINED WITH SQUARE LOSS VS CROSS-ENTROPY IN CLASSIFICATION TASKS,"Modern neural architectures for classification tasks are trained using the crossentropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be wellfounded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.",2021,ICLR,0.5
The role of Disentanglement in Generalisation,"Combinatorial generalisation ‚Äî the ability to understand and produce novel combinations of familiar elements ‚Äî is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation.",2021,ICLR,-0.5
Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients,"A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonlyused techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias.",2021,ICLR,0.2
Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization,"We propose a simple, general and effective technique, Reward Randomization for discovering diverse strategic policies in complex multi-agent games. Combining reward randomization and policy gradient, we derive a new algorithm, RewardRandomized Policy Gradient (RPG). RPG is able to discover multiple distinctive human-interpretable strategies in challenging temporal trust dilemmas, including grid-world games and a real-world game Agar.io, where multiple equilibria exist but standard multi-agent policy gradient algorithms always converge to a fixed one with a sub-optimal payoff for every player even using state-of-the-art exploration techniques. Furthermore, with the set of diverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning the best policy from the set; and (2) obtain an adaptive agent by using this set of strategies as its training opponents. The source code and example videos can be found in our website: https://sites.google. com/view/staghuntrpg.",2021,ICLR,1.0
Topology-Aware Segmentation Using Discrete Morse Theory,"In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.",2021,ICLR,1.0
On the Bottleneck of Graph Neural Networks and its Practical Implications,"Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffer from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .",2021,ICLR,0.8
Combining Physics and Machine Learning for Network Flow Estimation,"The flow estimation problem consists of predicting missing edge flows in a network (e.g., traffic, power, and water) based on partial observations. These missing flows depend both on the underlying physics (edge features and a flow conservation law) as well as the observed edge flows. This paper introduces an optimization framework for computing missing edge flows and solves the problem using bilevel optimization and deep learning. More specifically, we learn regularizers that depend on edge features (e.g., number of lanes in a road, resistance of a power line) using neural networks. Empirical results show that our method accurately predicts missing flows, outperforming the best baseline, and is able to capture relevant physical properties in traffic and power networks.",2021,ICLR,0.8
Mind the Pad -- CNNs Can Develop Blind Spots,"We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We propose solutions to mitigate spatial bias and demonstrate how they can improve model accuracy. 1 MOTIVATION Convolutional neural networks (CNNs) serve as feature extractors for a wide variety of machinelearning tasks. Little attention has been paid to the spatial distribution of activation in the feature maps a CNN computes. Our interest in analyzing this distribution is triggered by mysterious failure cases of a traffic light detector: The detector successfully detects a small but visible traffic light in a road scene. However, it fails completely in detecting the same traffic light in the next frame captured by the ego-vehicle. The major difference between both frames is a limited shift along the vertical dimension as the vehicle moves forward. Therefore, the drastic difference in object detection is surprising given that CNNs are often assumed to have a high degree of translation invariance [8; 17]. The spatial distribution of activation in feature maps varies with the input. Nevertheless, by closely examining this distribution for a large number of samples, we found consistent patterns among them, often in the form of artifacts that do not resemble any input features. This work aims to analyze the root cause of such artifacts and their impact on CNNs. We show that these artifacts are responsible for the mysterious failure cases mentioned earlier, as they can induce ‚Äòblind spots‚Äô for the object detection head. Our contributions are: ‚Ä¢ Demonstrating how the padding mechanism can induce spatial bias in CNNs (Section 2). ‚Ä¢ Demonstrating how spatial bias can impair downstream tasks (Section 3). ‚Ä¢ Identifying uneven application of 0-padding as a resolvable source of bias (Section 5). ‚Ä¢ Relating the padding mechanism with the foveation behavior of CNNs (Section 6). ‚Ä¢ Providing recommendations to mitigate spatial bias and demonstrating how this can prevent blind spots and boost model accuracy. 2 THE EMERGENCE OF SPATIAL BIAS IN CNNS Our aim is to determine to which extent activation magnitude in CNN feature maps is influenced by location. We demonstrate our analysis on a publicly-available traffic-light detection model [36]. This model implements the SSD architecture [26] in TensorFlow [1], using MobileNet-v1 [13] as a feature extractor. The model is trained on the BSTLD dataset [4] which annotates traffic lights in road scenes. Figure 1 shows two example scenes from the dataset. For each scene, we show two feature maps computed by two filters in the 11th convolutional layer. This layer contains 512 filters whose feature maps are used directly by the first box predictor in the SSD to detect small objects. 1 Published as a conference paper at ICLR 2021 Figure 1: Averaging feature maps per input (column marginal) and per filter (row marginal) in the last convolutional layer of a traffic light detector. Color indicates activation strength (the brighter, the higher), revealing line artifacts in the maps. These artifacts are the manifestation of spatial bias. The bottom row in Figure 1 shows the average response of each of the two aforementioned filters, computed over the test set in BSTLD. The first filter seems to respond mainly to features in the top half of the input, while the second filter responds mainly to street areas. There are visible lines in the two average maps that do not seem to resemble any scene features and are consistently present in the individual feature maps. We analyzed the prevalence of these line artifacts in the feature maps of all 512 filters. The right column in Figure 1 shows the average of these maps per scene, as well as over the entire test set (see supplemental for all 512 maps). The artifacts are largely visible in the average maps, with variations per scene depending on which individual maps are dominant. A useful way to make the artifacts stand out is to neutralize scene features by computing the feature maps for a zero-valued input. Figure 2 depicts the resulting average map for each convolutional layer after applying ReLU units. The first average map is constant as we expect with a 0-valued input. The second map is also constant except for a 1-pixel boundary where the value is lower at the left border and higher at the other three borders. We magnify the corners to make these deviations visible. The border deviations increase in thickness and in variance at subsequent layers, creating multiple line artifacts at each border. These artifacts become quite pronounced at ReLU 8 where they start to propagate inwards, resembling the ones in Figure 1. Figure 2: Activation maps for a 0 input, averaged over each layer‚Äôs filters (title format: H‚á•W‚á•C). 2 Published as a conference paper at ICLR 2021 It is evident that the 1-pixel border variations in the second map are caused by the padding mechanism in use. This mechanism pads the output of the previous layer with a 1-pixel 0-valued border in order to maintain the size of the feature map after applying 3x3 convolutional. The maps in the first layer are not impacted because the input we feed is zero valued. Subsequent layers, however, are increasingly impacted by the padding, as preceding bias terms do not warrant 0-valued input. It is noticeable in Figure 2 that the artifacts caused by the padding differ across the four borders. To investigate this asymmetry, we analyze the convolutional kernels (often called filters) that produce the feature maps. Figure 3 depicts a per-layer mean of these 3x3 kernels. These mean kernels exhibit different degrees of asymmetry in the spatial distribution of their weights. For example, the kernels in L1 assign (on average) a negative weight at the left border, and a positive weight at the bottom. This directly impacts the padding-induced variation at each border. Such asymmetries are related to uneven application of padding as we explain in Section 5. Figure 3: Mean kernel per convolutional layer. All kernels are 3‚á• 3, the titles show their counts. 3 IMPLICATIONS OF SPATIAL BIAS We demonstrate how feature-map artifacts can cause blind spots for the SSD model. Similar issues arise in several small-object detectors, e.g., for faces and masks, as well as in pixel-oriented tasks such as semantic segmentation and image inpainting (see supplemental for examples). Figure 4 illustrates how the SSD predicts small objects based on the feature maps of the 11-th convolutional layer. The SSD uses the pixel positions in these maps as anchors of object proposals. Each proposal is scored by the SSD to represent a target category, with ‚Äùbackground‚Äú being an implicit category that is crucial to exclude irrelevant parts of the input. In addition to these scores, the SSD computes a bounding box to localize the predicted object at each anchor. We examine Figure 4: The formation of blind spots in SSD, illustrated via its box predictor internals with a zero-valued input. The predictor uses spatial anchors to detect and localize the target object at 45 ‚á• 80 possible locations based on 512 feature maps. Certain anchors are predisposed to predict background due to feature-map artifacts, as evident in the logit maps. Traffic lights at the corresponding location cannot be detected as demonstrated with a real scene (middle one in the bottom). 3 Published as a conference paper at ICLR 2021 Figure 5: (a) A map showing via color the detection score the SSD computes for a traffic light when present at various locations. The detection is muted when the stimulus lies in the area impacted by the artifacts. (b) The same map after changing the padding method to SYMMETRIC. The detection scores are rather constant except for periodic variations due to the SSD‚Äôs reliance on anchors. object proposals computed at 1:2 aspect ratio, as they resemble the shape of most traffic lights in the dataset. We visualize the resulting score maps both for the background category and for traffic lights, when feeding a 0-valued input to the SSD. We also visualize the bounding boxes of these proposals in the image space. The SSD predicts the image content to be of background category at all anchor locations, as evident from the value range in both score maps. Such predictions are expected with an input that contains no traffic lights. However, the line artifacts in the feature maps have a strong impact on the score maps. These artifacts elevate the likelihood of anchors closer to the top to be classified as background (see the yellow band in the background score map). Conversely, these anchors have significantly lower scores for the traffic light category, compared with other anchors in the feature map. Such difference in the impact on the target categories is due to the different weights the SSD assigns to the feature maps for each target. As a result, the artifacts lead to potential blind spots in which the scores for certain categories are artificially muted. To validate whether or not the blind spots hinder object detection, we examine road scenes that contain highly-visible traffic light instances in the impacted area. Figure 4-bottom shows an example of such a scene. The SSD computes a low detection score of 7% when the traffic light lies in the blind spot (see middle image), far below the detection false-positive cutoff. Shifting the scene image upwards or downwards makes the instance detectable with a high score as long as it lies outside the blind spot. This explains the failure cases mentioned in Section 1. To further validate this effect, we run the SSD on baseline images that each contains one traffic light instance at a specific location in the input. We store the detection score for each instance. Figure 5a depicts the computed scores in a 2D map. It is evident that the model fails to detect the traffic light instance exactly when it is located within the ‚Äúblind spot‚Äù band. The artifacts further disrupt the localization of the objects as evident in the top-right plot in Figure 4 which shows per-anchor object proposals computed for a 0 input. 4 REMINDER: WHY IS PADDING NEEDED IN CNNS? Padding is applied at most convolutional layers in CNNs to serve two fundamental purposes: Maintaining feature map size A padding that satisfies this property is often described as SAME or HALF padding. FULL padding expands the maps by kernel size 1 along each dimension. VALID padding performs no padding, eroding the maps by the same amount. SAME padding is important to (1) design deep networks that can handle arbitrary input size (a challenge in the presence of gradual erosion), (2) maintain the aspect ratio of non-square input, and (3) concatenate feature maps from different layers as in Inception [39] and ResNet [12] models. Reducing information bias against the boundary Consider a 3‚á•3 kernel applied to a 2D input. An input location at least 2 pixels away from the boundary contributes to nine local convolution operations when computing the feature map. On the other hand, the corner is involved only one time under VALID padding, four times under a 1-pixel SAME 0-padding, and nine times under a 2-pixel FULL 0-padding. With SAME 0-padding, the cumulative contribution differences among the input pixels grow exponentially over the CNN layers. We refer to such uneven treatment of input pixels as the foveation behavior of the padding mechanism and elaborate on this in Section 6. We next explore solutions to the issues that cause padding to induce spatial bias. 4 Published as a conference paper at ICLR 2021 Figure 6: (a) Illustrating the problem of uneven padding when down-sampling at a stride of 2. The padding along x-axis is consumed only at the left side. (b) Mean 3‚á•3 filters in three ResNet models, trained on ImageNet with two input sizes. Color encodes average weight (green is positive). A size that induces uneven padding (top row) can lead to asymmetries, esp. around down-sampling layers. These asymmetries are mitigated when the input size induces no uneven padding (bottom row). 5 ELIMINATING UNEVEN APPLICATION OF PADDING While useful to reduce bias against the boundary, applying padding at down-sampling layers can lead to asymmetry in CNN internals. Figure 6a illustrates the source of this asymmetry when strided convolution is used for downsampling: At one side of the feature map, the padding is consumed by the kernel while at the other side it is not. To warrant even application of padding throughout the CNN, the following must hold at all d down-sampling layers, where (hi, wi) is the output shape at the i-th layer with kh i ‚á• kw i as kernel size, (si , si ) as strides, and = (pi , pi ) as padding amount (refer to appendix A for a proof): 8i 2 {1, . . , d} : hi 1 = si ¬∑ (hi 1) + k i 2 ¬∑ pi ^ wi 1 = si ¬∑ (wi 1) + k i 2 ¬∑ pi (1) The values h0 and w0 represent the CNN input dimensions. The above constraints are not always satisfied during training or inference with arbitrary input dimensions. For example, ImageNet classifiers based on ResNet [12] and MobileNet [13] contain five down-sampling layers (d = 5) that apply 1-pixel 0-padding before performing 2-strided convolution. To avoid uneven application of padding, the input to these CNNs must satisfy the following, as explained in appendix A: h0 = a1‚á•2+1 = 32 ¬∑a1+1 and w0 = a2‚á•2+1 = 32 ¬∑a2+1 where a1, a2 2 N (2) The traditional 1 and prevalent input size for training ImageNet models is 224‚á•224. This size violates Eq. 2, leading to uneven padding at every down-sampling layer in ResNet and MobileNet models where 0-padding is effectively applied only at the left and top sides of layer input. This over-represents zeros at the top and left sides of 3‚á• 3 feature-map patches the filters are convolved with during training. The top row of Figure 6b shows per-layer mean filters in three ResNet models in PyTorch [33], pre-trained on ImageNet with 224‚á•224 images. In all of these models, a few of the mean filters, adjacent to down-sampling layers, exhibit stark asymmetry about their centers. We increase the image size to 225‚á•225 without introducing additional image information2. This size satisfies Eq. 2, warranting even application of padding at every downsampling layer in the above models. Retraining the models with this size strongly reduces this asymmetry as evident in the bottom row of Figure 6b. This, in turn, visibly boosts the accuracy in all models we experimented with as we report in Table 1. The accuracy did not improve further when we retrained two of the models, ResNet-18 and ResNet-34, on 226 ‚á• 226 images. This provides evidence that the boost is due to eliminating uneven padding and not merely due to increasing the input size. 1 This size has been used to facilitate model comparison on ImageNet, since the inception of AlexNet. 2 This is done via constant padding. The side to pad with one pixel is chosen at random to balance out the application of padding at both sides over the training set. No additional padding is applied at further layers. 5 Published as a conference paper at ICLR 2021 Replacing 0-padding with a padding method that reuses feature map values can alleviate the asymmetry in the learned filters in the presence of unevenly applied padding. Another possibility is to use a rigid downsampling kernel, such as max-pooling, instead of a learned one. Appendix C demonstrates both possibilities. Finally, antialiasing before downsampling [43] can strongly reduce the asymmetry as we elaborate in Section 8 and in Appendix E. Table 1: Top-1 (and top-5) accuracy of five ImageNet classifiers trained with different input sizes. Input Size 2 MobileNet ResNet-18 ResNet-34 ResNet-50 ResNet-101 224‚á•224 68.19 (88.44) 69.93 (89.22) 73.30 (91.42) 75.65 (92.47) 77.37 (93.56) 225‚á•225 68.80 (88.78) 70.27 (89.52) 73.72 (91.58) 76.01 (92.90) 77.67 (93.81) Even when no padding is applied (pi = 0 or pi = 0), an input size that does no satisfy Eq. 1 can lead to uneven erosion of feature maps, in turn, reducing the contribution of pixels from the impacted sides (Fig 7e. Satisfying Eq 1 imposes a restriction on input size, e.g., to values in increments of 2d = 32 with the above models (193‚á•193, 225‚á•225, 257‚á•257, ...). Depending on the application domain, this can be guaranteed either by resizing an input to the closest increment, or by padding it accordingly with suited values. 6 PADDING MECHANISM AND FOVEATION By foveation we mean the unequal involvement of input pixels in convolutional operations throughout the CNN. Padding plays a fundamental role in the foveation behavior of CNNs. We visualize this behavior by means of a foveation map that counts for each input pixel the number of convolutional paths through which it can propagate information to the CNN output. We obtain these counts by computing the effective receptive field [28] for the sum of the final convolutional layer after assigning all weights in the network to 1 (code in supplemental). Neutralizing the weights is essential to obtain per-pixel counts of input-output paths that reflect the foveation behavior. f Figure 7: Foveation behavior of different padding methods applied to VGG-19 [37], and illustrated in a 512 ‚á• 512 input space (unless otherwise stated). Color represents the number of paths to the output for each input pixel. (a) The difference between VALID, FULL, and SAME 0-padding. (b) SAME alternatives to 0-padding. (c) Dilation amplifies foveation of SAME 0-padding. (d) Strides can lead to checkerboard patterns. (e) Foveation effects are more extensive in smaller inputs (relative to input size) and are sensitive to uneven padding. Figure 7a shows the extensive foveation effect when no padding is applied. The diminishing contribution of vast areas of the input explains the drastic drop in accuracy recently observed under VALID padding [16]. In contrast, FULL 0-padding does not incur foveation, however, at the cost of increasing the output size after each layer, making it impractical as explained in Section 4. SAME 0-padding incurs moderate foveation at the periphery, whose absolute extent depends on the number of convolutional layers and their filter sizes. Its relative extent depends on the input size: the larger the input, the larger the ratio of the constant area in yellow (refer to appendix B for a detailed example). 6 Published as a conference paper at ICLR 2021 Figure 7b shows the foveation behavior of alternatives to SAME 0-padding that have roots in wavelet analysis [19] and image processing [27]. Mirror padding mirrors pixels at the boundary to fill the padding area. When the border is included (SYMMETRIC mode in TensorFlow) all input pixels have an equal number of input-output paths 3, resulting in a uniform foveation map. When the border is not included (REFLECT mode both in PyTorch and in TensorFlow), the map exhibits bias against the border and towards a contour in its proximity. This bias is amplified over multiple layers. Replication padding exhibits the opposite bias when the padding area is wider than 1 pixel. This is because it replicates the outer 1-pixel border multiple times to fill this area 3. The method is equivalent to SYMMETRIC if the padding area is 1-pixel wide. Circular padding wraps opposing borders, enabling the kernels to seamlessly operate on the boundary and resulting in a uniform map. Partial Convolution [22] has been proposed as a padding method that treats pixels outside the original image as missing values and rescales the computed convolutions accordingly [23]. Its foveation behavior resembles reflective padding 3. Distribution padding [30] resizes the input to fill the padding area around the original feature map, aiming at preserving the distribution of the map. Its foveation map is largely uniform, except for the corners and edges. Impact of input size Besides influencing the relative extent of foveation effects, the input size also determines the presence of uneven padding (or uneven feature-map erosion), as we discussed in Section 5. Figure 7e shows the foveation map for VGG-19 with a 127‚á•127 input. This input violates Eq. 1 at every downsampling layer (appendix A), leading to successive feature map erosion at the bottom and right sides which is reflected in the foveation map (see appendix B for a detailed example). The bottom-right part of the input is hence less involved in the CNN computations. Impact of dilation We assign a dilation factor of 2 to all VGG-19 convolutional layers. While this exponentially increases the receptive field of the neurons at deeper layers [42], dilation doubles the extent of the non-uniform peripheral areas that emerge with SAME 0-padding as evident in Figure 7c. SYMMETRIC and circular padding maintain uniform foveation maps regardless of dilation 3. In contrast, dilation increases the complexity of these maps for REFLECT and replication padding. Impact of strides Whether learned on based on pooling, downsampling layers can amplify the impact of succeeding convolutional layers on foveation behaviour. Furthermore, these layers can cause input pixels to vary in the count of their input-output paths. This can happen when the kernel size is not divisible by the stride, leading to a checkerboard pattern in the foveation maps. This manifests in ResNet models as we illustrate in appendix B. In VGG-19, all max-pooling layers use a stride of 2 and kernel size of 2. Changing the kernel size to 3 leads to a checkerboard pattern as evident in Figure 7d. Such effects were shown to impact pixel-oriented tasks [32]. The padding technique and its foveation behaviour have direct impact on feature-map artifacts (Section 7), and on the ability of CNNs to encode spatial information (Section 8). Understanding the foveation behavior is key to determine how suited a padding method is for a given task. For example, small object detection is known to be challenging close to the boundary [26], in part due to the foveation behavior of SAME 0-padding. In Figure 5b, we change the padding method in the SSD to SYMMETRIC. The stimulus is noticeably more detectable at the boundary, compared with 0-padding 4. In contrast, ImageNet classification is less sensitive to foveation effects because the target objects are mostly located away from the periphery. Nevertheless, the padding method was shown to impact classification accuracy [23] because it still affects feature map artifacts. 7 PADDING METHODS AND FEATURE MAP ARTIFACTS It is also noticeable that the score map in Figure 5b is more uniform than in Figure 5a. In particular, under SYMMETRIC padding the model is able to detect traffic lights placed in the blind spots of the original 0-padded model. To verify whether the line artifacts in Figure 2 are mitigated, we inspect the mean feature maps of the adapted model. With a constant input, SYMMETRIC padding warrants constant maps throughout the CNN because it reuses the border to fill the padding area. Instead, we average these maps over 30 samples generated uniformly at random. Figure 8 depicts the mean maps which are largely uniform, unlike the case with 0-padding. 3 Refer to appendix F or to http://mind-the-pad.github.io for visual illustration and further theoretical analysis of the foveation behavior. Since the input size causes uneven application of padding, the right and bottom borders are still challenging. 7 Published as a conference paper at ICLR 2021 Figure 8: The same feature maps in Figure 2, generated under mirror padding and averaged over 30 randomly-generated input samples. The line artifacts induced by 0-padding are largely mitigated. To further analyze the impact of SYMMETRIC padding, we retrain the adapted model following the original training protocol. This significantly improves the average precision (AP) as reported in Table 2 under different overlap thresholds (matching IoU), confirming that small object detection is particularly sensitive to feature-map artifacts. Table 2: Performance of the SSD traffic light detector, trained under two different padding schemes. Average Precision (AP) AP@.20IOU AP@.50IOU AP@.75IOU AP@.90IOU Zero Padding 80.24% 49.58% 3.7% 0.007% Mirror Padding 83.20% 57% 8.44% 0.02% Of the padding methods listed in Section 6, mirror padding in both SYMMETRIC and REFLECT modes, PartialConv, and circular padding are generally effective at reducing feature map artifacts that emerge under zero padding, in particular salient line patterns. In contrast, distribution padding can induce significant artifacts. Refer to appendix D for comparative examples of artifacts under the aforementioned padding schemes. Artifact magnitude and propagation While feature-map artifacts are induced by the padding mechanism at the boundary, their magnitude and inward propagation in the maps are impacted by several architectural aspects of CNNs. In particular, certain normalization schemes such as batchnorm [15] tend to limit the range of variation within a feature map and to relatively harmonize this range across different maps. This, in turn, impacts how possible artifacts in these maps accumulate when they are processed by the next convolutional layer. Similarly, artifacts that manifest after applying ReLU units are of a positive sign. These factors were instrumental in the formation of potential blind spots described in Section 3. We hence recommend to involve non-convolutional layers when inspecting the feature maps. Besides having possible impact on artifact magnitude, several aspects of convolution arithmetic, such as filter size and dilation factors, can also impact the spatial propagation of these artifacts. 8 RELATED FINDINGS AND TAKEAWAYS Handling the boundary is an inherent challenge when dealing with spatial data [9]. Mean padding is known to cause visual artifacts in traditional image processing, with alternative methods proposed to mitigate them [24]. CNNs have been often assumed to deal with such effects implicitly. Innamorati et al [14] propose learning separate sets of filters dedicated to the boundaries to avoid impacting the weights learned by regular filters. A grouped padding strategy, proposed to support 2‚á•2 filters [41], offers avenues to mitigate uneven padding and corresponding skewness in foveation maps without restrictions on input size (see our note in appendix B for explanation). Finally, insights from signal and image processing [10; 11] could inspire further CNN padding schemes. Zero padding has been recently linked to CNNs‚Äô ability to encode position information [7; 16; 18; 29]. In contrast, circular padding was shown to limit this ability [7] and to boost shift invariance [35]. The input sizes in those studies do induce uneven padding. This can be, in part, the underlying mechanism behind the aforementioned ability. Whether or not this ability is desirable depends on the task, with several methods proposed to explicitly encode spatial information [5; 6; 20; 25; 29; 31]. 8 Published as a conference paper at ICLR 2021 Downsampling using max-pooling or strided convolution has been shown to impact shift invariance in CNNs by incurring aliasing effects [3; 38; 43]. These effects can manifest in the same symptoms we reported in Section 1, albeit for a different reason. Zhang [43] demonstrated how blurring the feature maps before subsampling mitigates aliasing effects and improves ImageNet classification accuracy of various popular CNNs. We analyzed the mean filters in antialiased MobileNet and ResNet models pre-trained on ImageNet under 0-padding, with 224‚á•224 as input size (refer to Appendix E). We found that antialiasing can also mitigate the asymmetry of mean filters that exhibited high asymmetry in the baseline models, especially at deeper layers. This is remarkable given that these models are trained on 224‚á•224 images, which incurs one-sided zero padding at every downsampling layer. This could, in part, be attributed to the ability of the BlurPool operator used in antialiased CNN to smoothen the acuity of zero-padded borders, in turn, reducing the value imbalance incurred by one-sided padding. Further analysis is needed to examine the interaction between padding and aliasing effects in CNNs and to establish possible synergy between antialiasing and eliminating uneven application of padding. Luo et al [28] drew connections between effective receptive fields and foveated vision. Our analysis links foveation behavior with the padding scheme and suggests that it might occur implicitly in CNNs when using VALID or SAME 0-padding, without the need for explicit mechanisms [2; 21]. Furthermore, it explains the drastic accuracy drop noted by [16] under VALID padding, which is amplified by feature map erosion. Choosing a padding method SAME 0-padding is by far the most widely-used method. Compared with other methods, it can enable as much as 50% faster training and inference. Problem-specific constraints can dictate different choices [34; 35; 40]. In the lack of a universally superior padding method, we recommend considering multiple ones while paying attention to the nature of the data and the task, as well as to the following aspects: ‚Ä¢ Feature-map statistics: 0-padding can alter the value distribution within the feature maps and can shift their mean value in the presence of ReLU units. The alternatives presented in Section 6 tend to preserve this distribution, thanks to reusing existing values in the maps. ‚Ä¢ Foveation behavior: 0-padding might not be suited for tasks that require high precision at the periphery, unlike circular and SYMMETRIC mirror padding. ‚Ä¢ Interference with image semantics (esp. with a padding amount > 1 pixel): For example, circular padding could introduce border discontinuities unless the input is panoramic [35]. ‚Ä¢ Potential to induce feature map artifacts: All alternatives to 0-padding induce relatively fewer artifacts, except for Distribution padding [30] (see appendix D). We also recommend eliminating uneven padding at downsampling layers both at training and at inference time, as we illustrated in Section 5. This is especially important when zero padding is applied and the downsampling is learned. The scripts used to generate the visualizations in this paper are available in the supplemental as well as at http://mind-the-pad.github.io. Summary We demonstrated how the padding mechanism can induce spatial bias in CNNs, in the form of skewed kernels and feature-map artifacts. These artifacts can be highly pronounced with the widely-used 0-padding when applied unevenly at the four sides of the feature maps. We demonstrated how such uneven padding can inherently take place in state-of-the-art CNNs, and how the artifacts it causes can be detrimental to certain tasks such as small object detection. We provided visualization methods to expose these artifacts and to analyze the implication of various padding schemes on boundary pixels. We further proposed solutions to eliminate uneven padding and to mitigate spatial bias in CNNs. Further work is needed to closely examine the implications of spatial bias and foveation in various applications (see supplementary for examples), as well as padding impact on recurrent models and 1-D CNNs. ACKNOWLEDGEMENT We are thankful to Ross Girshick for providing useful recommendations and experiment ideas, and to Shubham Muttepawar for implementing an interactive tool out of our analysis scripts, guided by our front-end specialist Edward Wang and our AI user-experience designer Sara Zhang. 9 Published as a conference paper at ICLR 2021 REFERENCES [1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, et al. TensorFlow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. [2] E. Akbas and M. P. Eckstein. Object detection through search with a foveated visual system. PLoS computational biology, 13(10):e1005743, 2017. [3] A. Azulay and Y. Weiss. Why do deep convolutional networks generalize so poorly to small image transformations? Journal of Machine Learning Research (JMLR), 20(184):1‚Äì25, 2019. [4] K. Behrendt, L. Novak, and R. Botros. A deep learning appro",2021,ICLR,0.5
The geometry of integration in text classification RNNs,"Despite the widespread application of recurrent neural networks (RNNs), a unified understanding of how RNNs solve particular tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of text classification, building on earlier work studying the dynamics of binary sentiment-classification networks (Maheswaranathan et al., 2019). We study text-classification tasks beyond the binary case, exploring the dynamics of RNNs trained on both natural and synthetic datasets. These dynamics, which we find to be both interpretable and low-dimensional, share a common mechanism across architectures and datasets: specifically, these text-classification networks use low-dimensional attractor manifolds to accumulate evidence for each class as they process the text. The dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset, with the dimensionality reflecting the number of scalar quantities the network remembers in order to classify. In categorical classification, for example, we show that this dimensionality is one less than the number of classes. Correlations in the dataset, such as those induced by ordering, can further reduce the dimensionality of the attractor manifold; we show how to predict this reduction using simple word-count statistics computed on the training dataset. To the degree that integration of evidence towards a decision is a common computational primitive, this work continues to lay the foundation for using dynamical systems techniques to study the inner workings of RNNs.",2021,ICLR,0.0
The Recurrent Neural Tangent Kernel,"The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN). In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets.",2021,ICLR,1.0
Individually Fair Rankings,We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.,2021,ICLR,0.8
On the Impossibility of Global Convergence in Multi-Loss Optimization,"Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any ‚Äòreasonable‚Äô algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum. Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.",2021,ICLR,-1.0
Learning to Recombine and Resample Data For Compositional Generalization,"Flexible neural sequence models outperform grammarand automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data‚Äîparticularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems‚Äîinstruction following (SCAN) and morphological analysis (SIGMORPHON 2018)‚Äîwhere R&R enables learning of new constructions and tenses from as few as eight initial examples.",2021,ICLR,0.6000000000000001
Tent: Fully Test-Time Adaptation by Entropy Minimization,"A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent1): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.",2021,ICLR,1.0
On the role of planning in model-based deep reinforcement learning,"Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero [58], a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research. Model-based reinforcement learning (MBRL) has seen much interest in recent years, with advances yielding impressive gains over model-free methods in data efficiency [12, 15, 25, 76], zeroand few-shot learning [16, 37, 60], and strategic thinking [3, 62, 63, 64, 58]. These methods combine planning and learning in a variety of ways, with planning specifically referring to the process of using a learned or given model of the world to construct imagined future trajectories or plans. Some have suggested that models will play a key role in generally intelligent artificial agents [14, 50, 55, 56, 57, 67], with such arguments often appealing to model-based aspects of human cognition as proof of their importance [24, 26, 28, 41]. While the recent successes of MBRL methods lend evidence to this hypothesis, there is huge variance in the algorithmic choices made to support such advances. For example, planning can be used to select actions at evaluation time [e.g., 12] and/or for policy learning [e.g., 34]; models can be used within discrete search [e.g., 58] or gradient-based planning [e.g., 25, 29]; and models can be given [e.g., 45] or learned [e.g., 12]. Worryingly, some works even come to contradictory conclusions, such as that long rollouts can hurt performance due to compounding model errors in some settings [e.g., 34], while performance continues to increase with search depth in others [58]. Given the inconsistencies and non-overlapping choices across the literature, it can be hard to get a clear picture of the full MBRL space. This in turn makes it difficult for practitioners to decide which form of MBRL is best for a given problem (if any). The aim of this paper is to assess the strengths and weaknesses of recent advances in MBRL to help clarify the state of the field. We systematically study the role of planning and its algorithmic design choices in a recent state-of-the-art MBRL algorithm, MuZero [58]. Beyond its strong performance, MuZero‚Äôs use of multiple canonical MBRL components (e.g., search-based planning, a learned model, value estimation, and policy optimization) make it a good candidate for building intuition about the roles of these components and other methods that use them. Moreover, as discussed in the ‚àóCorrespondence addressed to: {jhamrick,theophane}@google.com",2021,ICLR,0.0
Dataset Inference: Ownership Resolution in Machine Learning,"With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model‚Äôs decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model‚Äôs training set is what is common to all stolen copies. The adversary‚Äôs goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model‚Äôs owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce dataset inference, the process of identifying whether a suspected model copy has private knowledge from the original model‚Äôs dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model‚Äôs training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.1",2021,ICLR,0.7000000000000001
"SAFENet: A Secure, Accurate and Fast Neural Network Inference","The advances in neural networks have driven many companies to provide prediction services to users in a wide range of applications. However, current prediction systems raise privacy concerns regarding the user‚Äôs private data. A cryptographic neural network inference service is an efficient way to allow two parties to execute neural network inference without revealing either party‚Äôs data or model. Nevertheless, existing cryptographic neural network inference services suffer from enormous running latency; in particular, the latency of communication-expensive cryptographic activation function is 3 orders of magnitude higher than plaintextdomain activation function. And activations are the necessary components of the modern neural networks. Therefore, slow cryptographic activation has become the primary obstacle of efficient cryptographic inference. In this paper, we propose a new technique, called SAFENet, to enable a Secure, Accurate and Fast nEural Network inference service. To speedup secure inference and guarantee inference accuracy, SAFENet includes channel-wise activation approximation with multiple-degree options. This is implemented by keeping the most useful activation channels and replacing the remaining, less useful, channels with variousdegree polynomials. SAFENet also supports mixed-precision activation approximation by automatically assigning different replacement ratios to various layer; further increasing the approximation ratio and reducing inference latency. Our experimental results show SAFENet obtains the state-of-the-art inference latency and performance, reducing latency by 38% ‚àº 61% or improving accuracy by 1.8% ‚àº 4% over prior techniques on various encrypted datasets.",2021,ICLR,1.0
R-GAP: Recursive Gradient Attack on Privacy,"Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network‚Äôs security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.",2021,ICLR,0.7000000000000001
Shapley explainability on the data manifold,"Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model‚Äôs predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model‚Äôs features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While ‚Äúoff-manifold‚Äù Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",2021,ICLR,0.5
Online Adversarial Purification based on Self-supervised Learning,"Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with selfsupervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the labelindependent nature of self-supervised signals, and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.",2021,ICLR,0.9
Structured Prediction as Translation between Augmented Natural Languages,"We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",2021,ICLR,1.0
Bag of Tricks for Adversarial Training,"Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7%, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results1. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.",2021,ICLR,1.0
Learning to Generate 3D Shapes with Generative Cellular Automata,"We present a probabilistic 3D generative model, named Generative Cellular Automata, which is able to produce diverse and high quality shapes. We formulate the shape generation process as sampling from the transition kernel of a Markov chain, where the sampling chain eventually evolves to the full shape of the learned distribution. The transition kernel employs the local update rules of cellular automata, effectively reducing the search space in a high-resolution 3D grid space by exploiting the connectivity and sparsity of 3D shapes. Our progressive generation only focuses on the sparse set of occupied voxels and their neighborhood, thus enabling the utilization of an expressive sparse convolutional network. We propose an effective training scheme to obtain the local homogeneous rule of generative cellular automata with sequences that are slightly different from the sampling chain but converge to the full shapes in the training data. Extensive experiments on probabilistic shape completion and shape generation demonstrate that our method achieves competitive performance against recent methods.",2021,ICLR,0.9
Deep Equals Shallow for ReLU Networks in Kernel Regimes,"Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their ‚Äúshallow‚Äù two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere.",2021,ICLR,-1.0
GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing,"We present GRAPPA, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table semantic parsing into the pre-training language model. To maintain the model‚Äôs ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre-training process. Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GRAPPA achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks. The pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt.",2021,ICLR,1.0
Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs,"The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they‚Äôre embedded. Class selectivity‚Äîtypically defined as how different a neuron‚Äôs responses are across different classes of stimuli or data samples‚Äîis commonly used for this purpose. However, it remains an open question whether it is necessary and/or sufficient for deep neural networks (DNNs) to learn class selectivity in individual units. We investigated the causal impact of class selectivity on network function by directly regularizing for or against class selectivity. Using this regularizer to reduce class selectivity across units in convolutional neural networks increased test accuracy by over 2% in ResNet18 and 1% in ResNet50 trained on Tiny ImageNet. For ResNet20 trained on CIFAR10 we could reduce class selectivity by a factor of 2.5 with no impact on test accuracy, and reduce it nearly to zero with only a small (‚àº2%) drop in test accuracy. In contrast, regularizing to increase class selectivity significantly decreased test accuracy across all models and datasets. These results indicate that class selectivity in individual units is neither sufficient nor strictly necessary, and can even impair DNN performance. They also encourage caution when focusing on the properties of single units as representative of the mechanisms by which DNNs function.",2021,ICLR,-1.0
Pruning Neural Networks at Initialization: Why Are We Missing the Mark?,"Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, we find that they remain below the accuracy of magnitude pruning after training. We show that, unlike magnitude pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.",2021,ICLR,0.6000000000000001
Representation Learning for Sequence Data with Deep Autoencoding Predictive Components,"We propose Deep Autoencoding Predictive Components (DAPC) ‚Äì a selfsupervised representation learning method for sequence data, based on the intuition that useful representations of sequence data should exhibit a simple structure in the latent space. We encourage this latent structure by maximizing an estimate of predictive information of latent feature sequences, which is the mutual information between the past and future windows at each time step. In contrast to the mutual information lower bound commonly used by contrastive learning, the estimate of predictive information we adopt is exact under a Gaussian assumption. Additionally, it can be computed without negative sampling. To reduce the degeneracy of the latent space extracted by powerful encoders and keep useful information from the inputs, we regularize predictive information learning with a challenging masked reconstruction loss. We demonstrate that our method recovers the latent space of noisy dynamical systems, extracts predictive features for forecasting tasks, and improves automatic speech recognition when used to pretrain the encoder on large amounts of unlabeled data. 1",2021,ICLR,0.8
Growing Efficient Deep Networks by Structured Continuous Sparsification,"We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve 49.7% inference FLOPs and 47.4% training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining 75.2% top-1 accuracy ‚Äî all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.",2021,ICLR,1.0
The Risks of Invariant Risk Minimization,"Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective‚Äîas well as these recently proposed alternatives‚Äîunder a fairly natural and general model. In the linear case, we give simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution‚Äîthis is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.",2021,ICLR,-1.0
Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation,"Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also ‚Äúexplain‚Äù which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG‚Äôs semantics and structure. Our findings raise doubts about KG-augmented models‚Äô ability to reason about KG information and give sensible explanations.",2021,ICLR,-1.0
Noise or Signal: The Role of Image Backgrounds in Object Recognition,"We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds‚Äîup to 88% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models‚Äô out of distribution performance.",2021,ICLR,-1.0
Network Pruning That Matters: A Case Study on Retraining Variants,"Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith & Topin, 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining ‚Äì a detail often overlooked by practioners during the implementation of network pruning.",2021,ICLR,0.5
How Benign is Benign Overfitting ?,"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji & Long, 2020). However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don‚Äôt fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness.",2021,ICLR,-0.5
