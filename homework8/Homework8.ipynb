{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2548a7c-8677-49d2-9a80-7f95aa5f5085",
   "metadata": {},
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e6395-c414-4180-8fb2-6a581b0e768e",
   "metadata": {},
   "source": [
    "### 8.1 (2p)\n",
    "\n",
    "In this task, you will build a function *unigram_tagger (genre, tag)* that creates and evaluates a Unigram part-of-speech tagger. Your function should take a name of one of the categories from the\n",
    "Brown corpus and a pos-tag tag (both as strings) and return an evaluation of the accuracy of your tagger for tag as the backoff strategy. The sentences of the given categories should be loaded and divided into the training, development and test sets as follows:\n",
    "\n",
    "-  the training corpus – first 60% of the corpus\n",
    "- the development corpus – 60-80% of the corpus\n",
    "- the test corpus – last 20% of the corpus (not needed in this task)\n",
    "\n",
    "As a backoff tagger we will use nltk’s DefaultTagger with 3 tag options —- “NNS”, “NN” and “VB” —- in order to\n",
    "find the best value of this hyperparameter. Train your tagger on the training set and assess it on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "faa9e064-2b01-4fe7-aae7-21369e2cc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies.\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "from nltk.probability import ConditionalFreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ca006a2-9924-4294-ba08-c4041f1d9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tagger(genre:str, tag:str) -> float:\n",
    "    '''\n",
    "    @param genre: name of one of the categories from the Brown coprus\n",
    "    @param tag: pos tag\n",
    "    @return float: evaluation of the tagger\n",
    "\n",
    "    devide all sentences from the respected category into 3 parts\n",
    "    initialize the unigram tagger\n",
    "    train and evaluate\n",
    "    '''\n",
    "    # Initialization\n",
    "    brown_tagged_Sents=brown.tagged_sents(categories=genre)\n",
    "    brown_tokens=list(word[0] for sent in brown_tagged_Sents for word in sent)\n",
    "    train_size=int(len(brown_tagged_Sents)*0.6)\n",
    "    dev_size=int(len(brown_tagged_Sents)*0.2)\n",
    "    train_sets=brown_tagged_Sents[:train_size]\n",
    "    dev_sets=brown_tagged_Sents[train_size:train_size+dev_size]\n",
    "    dev_tokens=brown_tokens[train_size:train_size+dev_size]\n",
    "    test_sets=brown_tagged_Sents[train_size+dev_size:]\n",
    "    \n",
    "    # Initialize tagger\n",
    "    default_tagger=nltk.DefaultTagger(tag)\n",
    "    unigram_tagger=nltk.UnigramTagger(train_sets,backoff=default_tagger)\n",
    "    return unigram_tagger.evaluate(test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001038c-b932-4ea5-871b-eeee7d1e8e62",
   "metadata": {},
   "source": [
    "Report your results for the following:\n",
    "\n",
    "```\n",
    "print(unigram_tagger(\"adventure\", \"NNS\"))\n",
    "print(unigram_tagger(\"adventure\", \"NN\"))\n",
    "print(unigram_tagger(\"adventure\", \"VB\"))\n",
    "\n",
    "```\n",
    "\n",
    "Which Default Tagger should be preferred?\n",
    "- NNS\n",
    "- NN\n",
    "- VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8120f620-7973-4005-a942-e23b7d84146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7991757474154791\n",
      "0.8246018440905281\n",
      "0.7882089969265158\n"
     ]
    }
   ],
   "source": [
    "print(unigram_tagger(\"adventure\", \"NNS\"))\n",
    "print(unigram_tagger(\"adventure\", \"NN\"))\n",
    "print(unigram_tagger(\"adventure\", \"VB\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96d85-760e-4dac-9e79-56077270131d",
   "metadata": {},
   "source": [
    "### Homework 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c093d-02cb-41f6-bf79-9e79b6637d27",
   "metadata": {},
   "source": [
    "Let’s continue working on our Unigram tagger. We will slightly modify the function from the task 8.1 and turn it into the function unigram_tagger(genre, train_size). Your function should now takea name of one of the categories as a string and the size of the training corpus and return an evaluation of the accuracy of your tagger. Set the value of the hyperparameter tag to “NN” (now and in the remainder of this homework). The sentences of the given categories should be loaded and divided into the training, development and test sets as follows:\n",
    "\n",
    "- the training corpus — first X%\n",
    "- the test corpus — last 20% of the corpus\n",
    "- the development corpus — remaining part of the corpus\n",
    "\n",
    "Create a Unigram Tagger and use the DefaultTagger with “NN” as a backoff. Train your tagger on the training set and test it on the test set. Evaluate the accuracy of the tagger and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b4cf20b-1550-4588-b21d-d16ac3a34e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tagger(genre:str, train_size:float) -> float:\n",
    "    '''\n",
    "    @param genre: name of one of the categories from the Brown coprus\n",
    "    @param train_size: the size of the train corpus\n",
    "    @return float: evaluation of the tagger\n",
    "    devide all sentences from the respected category into 3 parts\n",
    "    initialize the unigram tagger\n",
    "    train and evaluate\n",
    "    '''\n",
    "    # Initialization\n",
    "    # *sets is the list of tokens with its pos. *tokens is the list of tokens\n",
    "    tag='NN'\n",
    "    brown_tagged_Sents=brown.tagged_sents(categories=genre)\n",
    "    brown_tokens=list(word[0] for sent in brown_tagged_Sents for word in sent)\n",
    "    train_size=int(len(brown_tagged_Sents)*train_size)\n",
    "    test_size=int(len(brown_tagged_Sents)*0.2)\n",
    "    dev_size=int(len(brown_tagged_Sents)*(train_size-0.2))\n",
    "    train_sets=brown_tagged_Sents[:train_size]\n",
    "    dev_sets=brown_tagged_Sents[train_size:train_size+dev_size]\n",
    "    dev_tokens=brown_tokens[train_size:train_size+dev_size]\n",
    "    test_sets=brown_tagged_Sents[-test_size:]\n",
    "    \n",
    "    default_tagger=nltk.DefaultTagger(tag)\n",
    "    unigram_tagger=nltk.UnigramTagger(train_sets,backoff=default_tagger)\n",
    "    \n",
    "    return unigram_tagger.evaluate(test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc8b57-0526-40fe-ae42-95c208e3f1f4",
   "metadata": {},
   "source": [
    "Report the value of the function for the following conditions:\n",
    "```\n",
    "print(unigram_tagger(\"adventure\", 0.4))\n",
    "print(unigram_tagger(\"adventure\", 0.5))\n",
    "print(unigram_tagger(\"adventure\", 0.6))\n",
    "print(unigram_tagger(\"adventure\", 0.8))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c330fb10-b635-491b-9b65-acfbf6598a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8064042508564637\n",
      "0.8172411382227505\n",
      "0.8245822554708803\n",
      "0.8412920366356709\n"
     ]
    }
   ],
   "source": [
    "print(unigram_tagger(\"adventure\", 0.4))\n",
    "print(unigram_tagger(\"adventure\", 0.5))\n",
    "print(unigram_tagger(\"adventure\", 0.6))\n",
    "print(unigram_tagger(\"adventure\", 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdfe7f-48ce-4de8-a010-fd1bde82ed77",
   "metadata": {},
   "source": [
    "### Homework 8.3\n",
    "\n",
    "This task is similar to the previous one. Build a function bigram_tagger(genre,train_size) that creates a Bigram Tagger. Divide all sentences according to the scheme above. As a backoff you should use the Unigram Tagger like the one from the previous task. Train your tagger on the train set and test it on the test set. The output should be the evaluation of the performance of your tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5cec3e8-6410-472c-a9d0-8d97347b038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_tagger(genre:str, train_size:float) -> float:\n",
    "    '''\n",
    "    @param genre: name of one of the categories from the Brown coprus\n",
    "    @param train_size: the size of the train corpus\n",
    "    @return float: evaluation of the tagger\n",
    "\n",
    "    devide all sentences from the respected category into 3 parts\n",
    "    initialize the bigram tagger\n",
    "    train and evaluate\n",
    "    '''\n",
    "    # Initialization\n",
    "    # *sets is the list of tokens with its pos. *tokens is the list of tokens\n",
    "    tag='NN'\n",
    "    brown_tagged_Sents=brown.tagged_sents(categories=genre)\n",
    "    brown_tokens=list(word[0] for sent in brown_tagged_Sents for word in sent)\n",
    "    train_size=int(len(brown_tagged_Sents)*train_size)\n",
    "    test_size=int(len(brown_tagged_Sents)*0.2)\n",
    "    dev_size=int(len(brown_tagged_Sents)*(train_size-0.2))\n",
    "    train_sets=brown_tagged_Sents[:train_size]\n",
    "    dev_sets=brown_tagged_Sents[train_size:train_size+dev_size]\n",
    "    dev_tokens=brown_tokens[train_size:train_size+dev_size]\n",
    "    test_sets=brown_tagged_Sents[-test_size:]\n",
    "    \n",
    "    # initialize tagger\n",
    "    default_tagger=nltk.DefaultTagger(tag)\n",
    "    unigram_tagger=nltk.UnigramTagger(train_sets,backoff=default_tagger)\n",
    "    bigram_tagger=nltk.BigramTagger(train_sets, backoff=unigram_tagger)\n",
    "    \n",
    "    return bigram_tagger.evaluate(test_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee383fc1-d4e4-471d-93d7-cce5e061d273",
   "metadata": {},
   "source": [
    "Report the value of the function for the following conditions again:\n",
    "\n",
    "```\n",
    "print(bigram_tagger(\"adventure\", 0.4))\n",
    "print(bigram_tagger(\"adventure\", 0.5))\n",
    "print(bigram_tagger(\"adventure\", 0.6))\n",
    "print(bigram_tagger(\"adventure\", 0.8))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a71ef30-35aa-433b-91ab-96f400b36d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815982660980214\n",
      "0.8278682793819478\n",
      "0.8352793120324408\n",
      "0.8514996853806893\n"
     ]
    }
   ],
   "source": [
    "print(bigram_tagger(\"adventure\", 0.4))\n",
    "print(bigram_tagger(\"adventure\", 0.5))\n",
    "print(bigram_tagger(\"adventure\", 0.6))\n",
    "print(bigram_tagger(\"adventure\", 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad105e6-a3ab-4dbc-afbf-589be5de4de4",
   "metadata": {},
   "source": [
    "### Homework 8.4\n",
    "\n",
    "Let’s return to the function unigram_tagger(genre, train_size) from the task 8.2 to investigate the most common types of errors that our tagger makes. Modify your function into a function unigram_tag_errors(genre, train_size), which should take a category name and the size of the training corpus and proceed with the creating a pos tagger like in the task 8.2. In the next step, your function should compare two lists of tagged sentences — test and gold. \n",
    "\n",
    "**The test** sentences should be the ones that have been automatically tagged (you might need the method `tag_sents` for it), and **the gold** should be the ones that have been manually labeled — the Brown corpus contains the correct human labels already. \n",
    "\n",
    "The result should be stored in a list of tuples with incorrect and correct tags. Create a **frequency distribution** on the error list and report the **two most frequent errors of your tagger together with their frequencies.** The output of your function should look like the following:\n",
    "\n",
    "$$\n",
    "[(('VB', 'NN'), 207), (('NN','NNS'),198)]\n",
    "$$\n",
    "\n",
    "meaning that the tagger wrongly labeled (confused) an ‘NN’ with a ‘VB’ in 207 cases, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ee9597d-3a28-4a07-9981-165513e0447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tag_errors(genre:str, train_size:float) -> list:\n",
    "    '''\n",
    "    @param genre: name of one of the categories from the Brown coprus\n",
    "    @param train_size: the size of the train corpus\n",
    "    @return list: the two most frequent errors of your tagger with their frequencies\n",
    "    devide all sentences from the respected category into 3 parts\n",
    "    initialize the unigram tagger and train it\n",
    "\n",
    "    create a list of tuples (wrong,correct) given automatically tagged data and the\n",
    "    gold standard for that data\n",
    "\n",
    "    errors=[]\n",
    "\n",
    "    create a frequency distribution on that list and return the two most common\n",
    "    errors with their frequencies\n",
    "\n",
    "    return nltk.FreqDist(errors).most_common(2)\n",
    "    '''\n",
    "    # Initialization\n",
    "    # *sets is the list of tokens with its pos. *tokens is the list of tokens\n",
    "    tag='NN'\n",
    "    brown_tagged_Sents=brown.tagged_sents(categories=genre)\n",
    "\n",
    "    train_size=int(len(brown_tagged_Sents)*train_size)\n",
    "    test_size=int(len(brown_tagged_Sents)*0.2)\n",
    "    dev_size=int(len(brown_tagged_Sents)*(train_size-0.2))\n",
    "    train_sets=brown_tagged_Sents[:train_size]\n",
    "    dev_sets=brown_tagged_Sents[train_size:train_size+dev_size]\n",
    "    test_sets=brown_tagged_Sents[-test_size:]\n",
    "    \n",
    "    test_sents=[]\n",
    "    for sent in test_sets:\n",
    "        s=[]\n",
    "        for element in sent:\n",
    "            s.append(element[0])\n",
    "        test_sents.append(s)\n",
    "    \n",
    "    # Test sents\n",
    "    unigram_tagger=nltk.UnigramTagger(train_sets)\n",
    "    testTaggerRes=unigram_tagger.tag_sents(test_sents)\n",
    "    \n",
    "    \n",
    "    # Gold sents\n",
    "    goldsents=test_sets\n",
    "    \n",
    "    currentList=[]\n",
    "    wrongList=[]\n",
    "    for index_sent,sent in enumerate(goldsents):\n",
    "        for index_word, word in enumerate(sent):\n",
    "            wordinTest=testTaggerRes[index_sent][index_word] \n",
    "            if ((word[0]==wordinTest[0]) & (word[1]==wordinTest[1])):\n",
    "                currentList.append(wordinTest)\n",
    "            else:\n",
    "                wrongList.append(wordinTest)\n",
    "    \n",
    "    freq=nltk.FreqDist(wrongList)\n",
    "    return freq.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5785a400-a325-4902-9d15-cb1746694cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('to', 'TO'), 90), (('that', 'CS'), 47)]\n",
      "[(('to', 'TO'), 90), (('that', 'CS'), 47)]\n",
      "[(('to', 'TO'), 90), (('that', 'CS'), 47)]\n",
      "[(('to', 'TO'), 90), (('that', 'CS'), 47)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(unigram_tag_errors(\"adventure\", 0.4))\n",
    "print(unigram_tag_errors(\"adventure\", 0.5))\n",
    "print(unigram_tag_errors(\"adventure\", 0.6))\n",
    "print(unigram_tag_errors(\"adventure\", 0.8))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
